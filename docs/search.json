[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Modeling of Hydrological Systems in Semi-Arid Central Asia",
    "section": "Welcome",
    "text": "Welcome\nThis handbook on hydrological modeling of Central Asian river basins is geared towards young water professionals in Central Asia. They inherit fascinatingly complex natural and man-made hydrological systems. They face work where opportunities for modernization abound after decades of limited investments in the water sectors of the countries and where continuous population growth and a changing climate pose emerging challenges. At the same time, they face work in a field that has enabled the region to prosper and flourish over hundreds if not thousands of years.\nThe authors hope that this easily online translatable textbook provides a source of inspiration for these students and that the text and the methods presented will also be used by teachers and integrated in university curricula locally.\nThe book is dedicated to colleagues at the Central Asian Hydrometeorological Agencies whose tireless work in collecting and analyzing hydro-meteorological data in Central Asia has helped to significantly improve our understanding of the complex runoff generation processes at work in the region.\nThe authors are grateful for the support by the Global Water Programme of the Swiss Agency for Development and Cooperation who greatly helped to push the envelop further with regard to modern water education in the Central Asia region. Finally, Mr. Andrey Yakovlev and his tremendous knowledge of the region is acknowledged."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Modeling of Hydrological Systems in Semi-Arid Central Asia",
    "section": "License",
    "text": "License\n\n\nDOI\n\n\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\nThe development of this book was supported by the Swiss Agency for Development and Cooperation."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "This is a book and study guide about the hydrology of semi-arid Central Asia and applied hydrological modeling in the region. It is geared towards students and young professionals in Central Asia who are interested in learning modern hydrological modeling approaches. The book teaches by example and focuses on two example catchments in the Syr Darya and Amu Darya river basins as case studies. The methods demonstrated here can be applied elsewhere.\n\n\n\n\n\n\nThe What and Why of Hydrological Modeling\n\n\n\nHydrological models come in different incarnations and flavors. Hydrological water balance models, in this text also sometimes referred to as rainfall-runoff models, were developed to help us gain an understanding of the partitioning of available water into different fluxes and storage compartments over time in the natural system under consideration. This natural system normally consists of of different interlinked compartments, including surface water and the unsaturated (soil moisture) and unsaturated (groundwater) zones. These models simulate the flow of water through these compartments. They are most often used in the context of water management and planning applications, i.e., for basin planning under climate change and population growth scenarios to allocate water between different uses and users on the one hand. One the other, these models are also used operationally to close supply-demand gaps in real-time management tasks and for short-term forecasting.\nWhere large amounts of data are available, empirical models can be implemented. These models strictly speaking do rely on the explicit simulation of the water balance of individual compartments. Rather, they learn patterns from measured time series of discharge and other relevant variables, such as temperature, precipitation, snow cover and then use these pattern between the time-ordered data for forecasting variable of interest (i.e., discharge, water levels, etc.).\n\n\nIn Part I of the book, key hydro-climatological characteristics of the region are presented. This Section draws inspiration from Victor Shults’ “Rivers of Middle Asia” (Shults 1965). Through the collection of a large number of in-situ hydrological data from all over the region and in combination with a plethora of newly available data, a grand modern regional perspective on Central Asian hydrology becomes possible in the tradition of Shults.\nTwo important basins are highlighted as in-depth case studies, i.e. the Gunt River Basins in the Amu Darya catchment and the Chirchik River basin in the Syr Darya. The analyses of these catchments draws on available data from the Central Asian Hydrometeorological Services and on global and entirely public hydro-climatological as well as land cover datasets. The goal of these introductory chapters is to familiarize the student with the first steps prior to any hydrological modeling, i.e. to obtain a robust understand of the system of interest through a thorough hydro-climatological characterization of the study area.\nPart II is a rather large section of the book which is then devoted to data where different open data sources are presented. Retrieval and data preparation in the context of hydrological modeling are discussed. These data include data on topography, land cover, climate reanalysis data and parameters on biophysical climate and climate projections data. The preparation of these data often requires significant work. Hence, a focus lies on demonstrating workflows to facilitate the handling and preparation of these type of data for hydrological modeling.\nIn Part III, three different modeling approach are presented and discussed. First, long-term water balance modeling using the Budyko framework is presented and discussed in depth with an application to a large dataset from the region. This approach can yield powerful insights for example on regional hydrological changes due to climatic changes and where the detailed hydrological-hydraulic modeling of a large number of rivers is impracticable. Second, detailed hydrological-hydraulic modeling of individual river basins is presented. These types of models are normally developed for tradeoff analysis between different water uses and users in a basin, i.e. in the planning context and also under different climate scenarios. They can also be run in operational mode to respond to real-time management challenges. Finally, the last modeling chapter introduces modeling through predictive inference where empirical data-drive models are setup for forecasting discharge at particular locations in a basin. These models rely on large amounts of measured discharge data and hence, their application is limited to places where such data are available.\nPart III also includes two Chapters on hydrological model applications where real-world model deployment is presented and discussed. A special emphasis here is aspects of operation and maintenance of deployed models in local agencies and options for this, also in relation to staff education and learning.\nThe book / study guide is accompanied by a Study Exercise Pack that encompasses data from 7 Central Asian catchments which can be used by students for learning and applying skills acquired to real-world examples in the region. The Exercise Pack can accessed and downloaded here. Furthermore, a dedicated R Package has been developed which implements many of the data analyses and processing steps shown in this book (see also Section for more information).\nWith everything that is presented, the focus is on the use of open source and free software. For data preparation and analysis as well as for water balance and empirical modeling, R and RStudio are utilized (R Core Team 2022). For the processing of geographic data, workflows in QGIS are demonstrated (QGIS Development Team 2021). For hydrological-hydraulic modeling, the free RS MINERVE is utilized which is a environment for the modeling of free surface runoff flow formation and propagation (Foehn et al. 2020; Garcia Hernandez et al. 2020). The reader is expected to have a basic understanding of R and QGIS and how to use these software for data analysis and processing.\nThe outlook having to learn hydrology together with quantitative geospatial analysis and programming may sound overwhelming at the beginning. Really the best way is just to dive into the book and learn through the many examples provided. All code with which the analysis and modeling is carried out is provided and can be thus adapted to any other local context or relevant task. So, this handbook on applied hydrological modeling is hopefully inviting students to learn through experimentation and not to get scared.\nBefore we get going, a small note on how to translate this text in any other language of interest. Should the reader struggle with the English language, there is a very easy way to translate this book into any of the local languages spoken in Central Asia, including Russian. The picture below shows a screenshot from the online book translated into Russian language via Google’s translation service. The screenshot shows how to activiate the translation panel (1). The translated book text then appears (2).\n\n\n\n\n\nFoehn, A., J. Garcia Hernandez, B. Roquier, J. Fluixa-Sanmartin, T. Brauchli, J. Paredes Arquiola, and G. De Cesare. 2020. “RS MINERVE - User Manual, V2.15.” ISSN 2673-2653. Switzerland: Ed. CREALP.\n\n\nGarcia Hernandez, J., A. Foehn, J. Fluixa-Sanmartin, B. Roquier, T. Brauchli, J. Paredes Arquiola, and De Cesare G. 2020. “RS MINERVE - Technical Manual, V2.25.” ISSN 2673-2661. Switzerland: Ed. CREALP.\n\n\nQGIS Development Team. 2021. QGIS Geographic Information System. QGIS Association.\n\n\nR Core Team. 2022. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nShults, Victor. 1965. Rivers of Middle Asia. 2nd Edition. Gidrometeoizdat, Leningrad."
  },
  {
    "objectID": "study_guide_materials.html#sec-study-guide",
    "href": "study_guide_materials.html#sec-study-guide",
    "title": "Study Guide and Materials",
    "section": "Study Guide",
    "text": "Study Guide\nOver the duration of the course and as part of the Applied Modeling track, students are guided through implementing their own conceptual hydrological rainfall-runoff model of one of the Central Asian sample catchments that they can choose from the Case Studies Pack.\nStudents are required to work through the Chapters, including the occasional tasks that serve to deepen reflection on the course material and to do their daily homework assignments. As the final exam, the homework results are presented in a final student conference for which the students have to submit a conference abstract prior to the conference.\nThis Chapter explains how to use this course book.\nDifferent callout blocks appear throughout the text. These include Exercise, Tasks and Take Home Messages. Caution and Warning callouts highlight possibly problematic issues.\n\n\n\n\n\n\nEXERCISE\n\n\n\nExercise boxes are highlighted in blue color. With the description of the exercise, hints and a link to the solution are provided. Wherever they appear in the text, exercises should be completed before starting the next course chapter.\n\n\n\n\n\n\n\n\nTASK\n\n\n\n\n\n\n\n\n\n\n\n\nTAKE HOME MESSAGE\n\n\n\n\n\n\n\n\n\n\n\n\nCAUTION\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING\n\n\n\n\n\n\nCode blocks of R code with corresponding output are regularly shown throughout the text and look like this. Note that in grayed-out code cell, the code can be copied and the pasted into RStudio locally. Note that code blocks in Chapters are executed sequentially.\n\na <- 1 + 1\nprint(paste(\"a is set to\", a))\n\n[1] \"a is set to 2\""
  },
  {
    "objectID": "study_guide_materials.html#sec-materials",
    "href": "study_guide_materials.html#sec-materials",
    "title": "Study Guide and Materials",
    "section": "Materials",
    "text": "Materials\nDay 1: Introduction & Installation of Software\nRead Chapter 1: A short history of Water in Central Asia and Chapter 2: Hydrological Systems in Semi-Arid Central Asia in the course book. Then make sure the required software for this course is installed on your computer. Section Open-source resources of the Appendix includes installation instructions and the on-line learning material that can get you started with the software. Below is a quick summary:\n\n\nQGIS\n\nR\n\nRStudio\n\nRS Minerve\n\nIf you have not used the software above before we recommend the following resources to get your started (remember, more detailed instructions are available in the Appendix):\n\n\nQGIS training manual\n\n\nModern Dive for getting started with R and RStudio\n\nRS Minerve User Manual\n\nInevitably, you will also perform a lot of geocomputations with R in the future. After all, a GIS system like QGIS is nothing more than a nicely packed bunch of geocomputation algorithms and a window for visualizing geospatial assets. Well, rest assured, all of this can be done inside R. It is recommended therefore that you also consult the following excellent online resource Geocomputation with R.\n\n\n\n\n\n\nTASK\n\n\n\nFind a peer or two with whom you will work on a basin of your selection. Download the data from the corresponding river basin an\n\n\nDay 2: Catchment Characterization\nRead the River Basin sample studies in Part I: Chapter 3 and familiarize yourself with the Data available in Part II. Do the catchment characterization of the basin that you selected to work on by filling in the Table Table 1 below. If you have downloaded the entire folder on your local drive, you already have all the data available for the analysis.\n\n\nTable 1: As an example, key relevant basin statistics for Gunt river basin are shown with individual data sources indicated. Using the data available in the data pack, you should characterize your case study basin in a similar way.\n\n\n\n\n\nATTRIBUTE\nVALUE\n\n\n\n\nGeography (srtmgl12020?)\n\n\n\n\nBasin Area \\(A\\)\n\n13’693 km2\n\n\n\nMinimum Elevation \\(h_{min}\\)\n\n2’068 masl\n\n\nMaximum Elevation \\(h_{max}\\)\n\n6’652 masl\n\n\nMean Elevation \\(h_{mean}\\)\n\n4’267 masl\n\n\nHydrology [Source: Tajik Hydromet Service]\n\n\n\nNorm hydrological year discharge \\(Q_{norm}\\)\n\n103.8 m3/s\n\n\nNorm cold season discharge (Oct. - Mar., Q4/Q1)\n19.8 m3/s\n\n\nNorm warm season discharge (Apr. - Sept., Q2/Q3)\n84.2 m3/s\n\n\nAnnual norm discharge volume\n3.28 km3\n\n\n\nAnnual norm specific discharge\n239 mm\n\n\nClimate\n\n\n\nMean basin temperature \\(T\\) (Karger et al. 2017)\n\n-5.96 deg. Celsius\n\n\nMean basin precipitation \\(P\\) (Beck et al. 2020)\n\n351 mm\n\n\nPotential Evaporation \\(E_{pot}\\) (Trabucco and Zomer 2019)\n\n929 mm\n\n\nAridity Index \\(\\phi = E_{pot} / P\\)\n\n2.7\n\n\nAridity Index (Trabucco and Zomer 2019)\n\n3.6\n\n\nLand Cover (CopernicusLandCover?)\n\n\n\nShrubland\n8 km2\n\n\n\nHerbaceous Vegetation\n4’241 km2\n\n\n\nCrop Land\n0.5 km2\n\n\n\nBuilt up\n4 km2\n\n\n\nBare / Sparse Vegetation\n8’410 km2\n\n\n\nSnow and Ice\n969 km2\n\n\n\nPermanent Water Bodies\n80 km2\n\n\n\nLand Ice\n\n\n\nTotal glacier area (GLIMS and NSIDC 2005, updated 2018)\n\n875 km2\n\n\n\nTotal glacier volume (calculated with (Erasov 1968))\n699 km3\n\n\n\n\n\nDay 3: Geospatial Data & Introduction to Linear Reservoir Models\nRead Chapter 5 on Geospatial Data and prepare the GIS layers for import into RS Minerve for your sample catchment. Step-by-step instructions of how to do this are given there.\nAs homework, read the Chapter 9.2 and do the exercise on the linear reservoir model. [//]: # (TODO: Proper Chapter reference here!)\nDay 4: Discussion of Types of Hydrological Models\nHydrological models in general are discussed. Consult the introductory Section of Part III: Hydrological Modeling and Applications. All three types of modeling approaches will be presented but with a focus on hydraulic-hydrological rainfall-runoff modeling.\nAs homework, choose an R tutorial to work on from the Appendix A: Software which suits your skills. If you are already fluent in R, you can for example work on your visualization skills in R by producing nice plots of your sample catchment which you may include in the final presentation on the last day of the workshop. You may let yourselves be inspired by the R code supplied in this book.\nDay 5: The HBV Model\nA commonly used hydrological model is introduced. Read Chapters 9.4 on the data preparation and 6.5 on the HBV model and the pre-requisite reading listed therein. It is recommended to do the tasks suggested in the course book to get more familiar with RSMinerve.\nAs an exercise, you will implement a hydrological model of your catchment based on the catchment characterization that you performed on day 2 and the geospatial layers that you prepared on day 3.\nDay 6: Model Calibration and Validation\nRead Chapter 6.6 and go through the example of the Nauvalisoy catchment which illustrates the iterative model refinement process.\nAs homework students will calibrate and validate the hydrological models of their own sample catchment.\nDay 7: Case Studies\nDiscussion of the calibration and validation exercise followed by a presentation of the Gunt basin case study. As homework students will write an abstract about your modeling work for the final conference. Instructions for abstract writing are available here. The abstract submission deadline will be communicated at the beginning of the course.\nDay 8: Real-World Applications\nPresentation of examples of real-world applications of hydrological models.\nAs homework, students will finalize the presentation of their course work. The presentation will have to submitted to the conference board prior to the start of the conference. Details will be communicated during the course.\nDay 9: Student Conference & Course Wrap Up\nThe last day of the course is organized as a student conference where students present their modeling work on their respective case study catchment. The groups need to prepare a presentation of 12 minutes duration. Each presentation will be followed by a 3 minutes Q&A session. After all the groups have presented, impressions and feedback will be shared by the teachers followed by a larger group discussion.\nAt the end, students are invited to provide feedback with regard to their impression of the course. A key question will be hoe the course can be further improved to reach future students even more effectively.\n\n\n\n\nBeck, Hylke E., Eric F. Wood, Tim R. McVicar, Mauricio Zambrano-Bigiarini, Camila Alvarez-Garreton, Oscar M. Baez-Villanueva, Justin Sheffield, and Dirk N. Karger. 2020. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1.\n\n\nErasov, N. V. 1968. “Method for Determining of Volume of Mountain Glaciers.” MGI, no. 14: 307–8.\n\n\nGLIMS, and NSIDC. 2005, updated 2018. Global Land Ice Measurements from Space Glacier Database. Compiled and made available by the international GLIMS community and the National Snow and Ice Data Center, Boulder CO, U.S.A. DOI:10.7265/N5V98602.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nTrabucco, Antonio, and Robert Zomer. 2019. “Global Aridity Index and Potential Evapotranspiration (ET0) Climate Database v2,” January. https://doi.org/10.6084/m9.figshare.7504448.v3."
  },
  {
    "objectID": "hydrology_of_central_asia.html",
    "href": "hydrology_of_central_asia.html",
    "title": "Part I: Hydrology of Semi-Arid Central Asia",
    "section": "",
    "text": "The arid plains of Central Asia have been teeming with life thanks to the rivers cutting through them and bringing water to the parched plains when its most needed, in the hot and dry summer months. Why is it that the natural, undisturbed rivers swelled exactly during that time? How do we explain this phenomenon that has enabled for the Silk Road and unique cultural centers to emerge over the course of history and have brought riches to the communities there?\nThis Chapter describes the key elements of the regional hydrology of semi-arid Central Asia that will help to understand this coincidence of nature. Leaving the large scale perspective, 2 example river basins are discussed in greater detail with a special focus on the runoff generation mechanisms there.\nApart from looking at past data for explanation, a forward view will also be taken to discuss the recent and anticipated future hydrological changes. It will be highlighted how our current understanding of these changes will impact societies and the environment alike."
  },
  {
    "objectID": "hydrological_systems.html#sec-regional-hydroclimatological-features",
    "href": "hydrological_systems.html#sec-regional-hydroclimatological-features",
    "title": "1  Hydrological Systems in Semi-Arid Central Asia",
    "section": "1.1 Regional Hydroclimatological Features",
    "text": "1.1 Regional Hydroclimatological Features"
  },
  {
    "objectID": "hydrological_systems.html#sec-zone-of-runoff-formation",
    "href": "hydrological_systems.html#sec-zone-of-runoff-formation",
    "title": "1  Hydrological Systems in Semi-Arid Central Asia",
    "section": "1.2 Zone of Runoff Formation",
    "text": "1.2 Zone of Runoff Formation"
  },
  {
    "objectID": "hydrological_systems.html#sec-zone-of-water-distribution-and-use",
    "href": "hydrological_systems.html#sec-zone-of-water-distribution-and-use",
    "title": "1  Hydrological Systems in Semi-Arid Central Asia",
    "section": "1.3 Zone of Water Distribution and Use",
    "text": "1.3 Zone of Water Distribution and Use"
  },
  {
    "objectID": "example_river_basins.html#sec-example-gunt-river-basin",
    "href": "example_river_basins.html#sec-example-gunt-river-basin",
    "title": "2  Case Study River Basins",
    "section": "\n2.1 Gunt River Basin",
    "text": "2.1 Gunt River Basin\n\n2.1.1 Gunt Basin Characterization\nThe Gunt river basin is located in the Pamir mountains in the Gorno Badakhshan Autonomous Region in south-east Tajikistan. The basin covers approximately 14’000 km2. The Gunt river is a large right tributary of the upstream Pyandzh and joins the latter downstream of the town of Khorog. Mean elevation is approx 4’270 meters above mean sea level (masl) with an altitude range from 2’000 – 6’700 masl. The highest elevations in the catchment are Peak Karl Marx (6726 m) and Peak Engels (6510 m) at the southern border of the catchment.\nInformation on the available discharge and meteorological stations in the basin is provided in Table 2.1.\n\n\n\n\nTable 2.1: Details on meteorological and discharge measurement stations from which data is available.\n\n\n\n\n\n\n\n\n\n\n\nStationName\nStationCode\nlat\nlon\neasting\nnorthing\nmasl\ntype\n\n\n\nBulunkul\n38953\n37.70417\n72.94583\n847890.5\n4180325\n3746\nMeteo\n\n\nKhorog\n38954\n37.50361\n71.51500\n722309.0\n4153714\n2075\nMeteo\n\n\nKhorog\n17050\n37.50361\n71.51500\n722309.0\n4153714\n2075\nDischarge Gauge\n\n\nJavshangoz\n38956\n37.39083\n72.29583\n791785.1\n4143330\n3438\nMeteo\n\n\nNavobod\n38950\n37.59417\n71.86556\n752995.5\n4164650\n2566\nMeteo\n\n\n\n\n\n\nA map of the basin is provided in Figure 2.2.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to experiment with the R code yourself, the data for this Chapter can be downloaded here. You can store these data locally and then perform the analysis on your local computer by correspondingly adjusting the data path in the code block below before loading the data.\n\n\n\ndata_dir <- \"../caham_data/AmuDarya/gunt_data/geospatial\"\n\n# Load vector data (shapefiles)\ngunt_basin_shp <- st_read(file.path(data_dir, \"gunt_basin_shp.shp\"), quiet = TRUE)\ngunt_rivers_shp <- st_read(file.path(data_dir,\"gunt_rivers_shp.shp\"), quiet = TRUE)\ngunt_subbasins_shp <- st_read(file.path(data_dir, \"gunt_subbasins_shp.shp\"), quiet = TRUE)\n\n# Load raster data (.tif-files)\ngunt_dem <- raster::raster(file.path(data_dir,\"gunt_dem.tif\"))\ngunt_dem_hillshade <- raster::raster(file.path(data_dir,\"gunt_dem_hillshade.tif\"))\n\nAfter successfully loading the data, we can easily visualize it.\n\n# Visualize data\ntm_shape(gunt_dem_hillshade) +\n tm_raster(palette = gray(0:100 / 100), n = 100, legend.show = FALSE)  +\n  tm_shape(gunt_dem) +\n  tm_raster(alpha = 0.5, palette = terrain.colors(25), legend.show = TRUE, title = \"Elevation (masl)\") +\n  tm_shape(gunt_basin_shp) +\n  tm_polygons(\"area\", alpha = 0, legend.show = FALSE) +\n  tm_shape(gunt_subbasins_shp) +\n  tm_polygons(\"ID\", alpha = 0, legend.show = FALSE) +\n  tm_layout(legend.position = c(\"right\",\"bottom\")) +\n  tm_shape(gunt_rivers_shp) +\n  tm_lines(col = \"name\", scale = 3) + \n  tm_shape(meteo_stations_sf) +\n  tm_dots(col = \"black\", scale = 2) +\n  tm_shape(meteo_stations_sf) +\n  tm_text(\"StationName\", size = .75, auto.placement = TRUE, just = \"left\")\n\n\n\nFigure 2.2: The Gunt River basin, its topography and the main tributaries are shown (see corresponding legend). The discharge measurement station 17050 is located in Khorog on the Gunt Downstream, shortly after the confluence of Gunt Upstream and Shakara rivers.\n\n\n\n\nThe Table below summarizes key basin statistics that are relevant from the hydro-climatological perspective. Data from various sources are summarized here. These data are presented in Section II of the book in ?sec-data and discussed in great detail there.\nThe basin area has been derived from the basin shapefile. Raster statistics of the SRTM digital elevation model (srtmgl12020?), the climate raster files as well as the land cover raster are calculated using the QGIS Raster Layer Statistics processing toolbox algorithm. The land ice total polygon area is computed with the Statistical Summary Option in QGIS.\nThe norm hydrological year discharge and the corresponding norm cold and warm season discharge values have been computed with data from the Tajik Hydrometeorological Service. The mean basin precipitation is computed using a state-of-the-art bias corrected high-resolution reanalysis product Beck et al. (2020). Such data will also be used for hydro-climatological modeling in later Chapters. Potential evaporation is from (Trabucco and Zomer 2019) using the Penman-Montieth equation.\n\n\nTable 2.2: Key relevant basin statistics for Gunt river basin. Individual data sources are indicated. Note that the hydrological year in Central Asia is defined as starting from October in year 1 and lasts until end of September in the subsequent year 2. More information on the relevance of hydrological year-based water accounting is given below.\n\n\n\n\n\nATTRIBUTE\nVALUE\n\n\n\n\nGeography (srtmgl12020?)\n\n\n\n\nBasin Area \\(A\\)\n\n13’693 km2\n\n\n\nMinimum Elevation \\(h_{min}\\)\n\n2’068 masl\n\n\nMaximum Elevation \\(h_{max}\\)\n\n6’652 masl\n\n\nMean Elevation \\(h_{mean}\\)\n\n4’267 masl\n\n\nHydrology [Source: Tajik Hydromet Service]\n\n\n\nNorm hydrological year discharge \\(Q_{norm}\\)\n\n103.8 m3/s\n\n\nNorm cold season discharge (Oct. - Mar., Q4/Q1)\n19.8 m3/s\n\n\nNorm warm season discharge (Apr. - Sept., Q2/Q3)\n84.2 m3/s\n\n\nAnnual norm discharge volume\n3.28 km3\n\n\n\nAnnual norm specific discharge\n239 mm\n\n\nClimate\n\n\n\nMean basin temperature \\(T\\) (Karger et al. 2017)\n\n-5.96 deg. Celsius\n\n\nMean basin precipitation \\(P\\) (Beck et al. 2020)\n\n351 mm\n\n\nPotential Evaporation \\(E_{pot}\\) (Trabucco and Zomer 2019)\n\n929 mm\n\n\nAridity Index \\(\\phi = E_{pot} / P\\)\n\n2.7\n\n\nAridity Index (Trabucco and Zomer 2019)\n\n3.6\n\n\nLand Cover (CopernicusLandCover?)\n\n\n\nShrubland\n8 km2\n\n\n\nHerbaceous Vegetation\n4’241 km2\n\n\n\nCrop Land\n0.5 km2\n\n\n\nBuilt up\n4 km2\n\n\n\nBare / Sparse Vegetation\n8’410 km2\n\n\n\nSnow and Ice\n969 km2\n\n\n\nPermanent Water Bodies\n80 km2\n\n\n\nLand Ice\n\n\n\nTotal glacier area (glims2005?)\n\n875 km2\n\n\n\nTotal glacier volume (calculated with (Erasov 1968))\n699 km3\n\n\n\n\n\nWith the values provided in the table above, the discharge index \\(Q/P\\) is 68.5 % and the evaporative index \\(E/P\\) is 31.5 %. In other words, the long-term water balance shows that 3 precipitation units gets partitioned into 2 discharge units and 1 evaporation unit, approximately. The aridity index \\(\\phi\\) , when calculated using \\(P\\) from (Karger et al. 2020) and \\(E_{pot}\\) from (Trabucco and Zomer 2019) is 2.7. The aridity index from (Trabucco and Zomer 2019) is 3.6. These values indicate some uncertainty in relation to the global climate products used. Despite this, they confirm the highly arid characteristics of the basin.\n\n2.1.2 Gunt Basin Hydrology\nFor the analysis of the key hydro-climatological characteristics, we first load the available decadal and monthly station data1. The data used in this Chapter can be accessed downloaded from the following online repository.\nFirst, we load the available station data of the Gunt River basin into R. Note that the monthly data is available for one discharge station and 4 meteorological stations. See also Table 2.1 for more information on the stations.\n\ndata_dir <- \"../caham_data/AmuDarya/gunt_data/station_data/\"\nfile_name = 'gunt_data_cleaned.Rds'\ngunt_station_data <- read_rds(file.path(data_dir, file_name))\ngunt_station_data\n\n# A tibble: 23,352 × 10\n   date        data  norm units type  code  station     river basin  resolution\n   <date>     <dbl> <dbl> <chr> <chr> <chr> <chr>       <chr> <chr>  <fct>     \n 1 1940-01-31  30.5  32.9 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 2 1940-02-29  27.3  30.1 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 3 1940-03-31  24.9  28.4 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 4 1940-04-30  26.4  30.7 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 5 1940-05-31  59    68.5 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 6 1940-06-30 309   232.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 7 1940-07-31 224   319.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 8 1940-08-31 201   237.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 9 1940-09-30 121   117.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n10 1940-10-31  60.8  63.1 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n# … with 23,342 more rows\n\n\nThis dataframe now contains all available data hydro-meteorological data from the basin. Most data are available at monthly time scales. As an example, the monthly discharge data from Gauge 17050 can be accessed and extracted from the Gunt dataset in the following way.\n\nq_17050_mon <- gunt_station_data %>% filter(type == \"Q\" & code == '17050' & resolution == 'mon')\nq_17050_mon\n\n# A tibble: 972 × 10\n   date        data  norm units type  code  station     river basin  resolution\n   <date>     <dbl> <dbl> <chr> <chr> <chr> <chr>       <chr> <chr>  <fct>     \n 1 1940-01-31  30.5  32.9 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 2 1940-02-29  27.3  30.1 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 3 1940-03-31  24.9  28.4 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 4 1940-04-30  26.4  30.7 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 5 1940-05-31  59    68.5 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 6 1940-06-30 309   232.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 7 1940-07-31 224   319.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 8 1940-08-31 201   237.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 9 1940-09-30 121   117.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n10 1940-10-31  60.8  63.1 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n# … with 962 more rows\n\n\nWhen we plot the data, we see that we have a near complete monthly record from 1940 onward (see Figure 2.3). The data gap in the 1990ies was during the Tajik civil war.\n\nq_17050_mon %>% \n  plot_time_series(date,\n    data,\n    .smooth        = FALSE,\n    .interactive   = TRUE,\n    .title         = \"\",    \n    .x_lab         = 'Year',\n    .y_lab         = 'Mean monthly Q [m3/s]',\n    .plotly_slider = TRUE)\n\n\nFigure 2.3: Visualized monthly discharge data at Gunt gauging station (17050)\n\n\n\nThere are visible changes in the winter low flow regime from 2007 onward. This is because of hydropower production that started upstream at that time. Pamir Energy, the local generation company supplies hydropower electricity especially during the cold winter months to the communities in the valley. When hydropower is required, the water table of the Yashikul Lake in the Pamir plateau (Alishur catchment, see Figure 2.2) gets lowered to increase the discharge for energy production in the downstream.\n\n\nFigure 2.4: Since 2006, a run off the river hydropower plant operated by Pamir Energy produces hydropower to cover local electric energy demand. Lake Yashikul is used as a regulator. The increase in winter discharge from 2007 onwards is due to HPP operations.\n\n\nThe seasonal diagnostics of the monthly discharge time series is shown in @ref(fig-gunt-seasonal-diagnostics). As is easily visible, the peak discharge of Gunt river measured at Khorog station is in July.\n\nq_17050_mon %>% \n  plot_seasonal_diagnostics(.date_var      = date,\n                            .value         = data,\n                            .title         = \"\",\n                            .feature_set   = c(\"month.lbl\"),\n                            .interactive   = FALSE,\n                            .x_lab         = \"Year\",\n                            .y_lab         = \"Mean monthly Q [m3/s]\") +\n  scale_x_discrete(breaks = c(\"January\", \"February\", \"March\", \"April\", \"May\", \n                            \"June\", \"July\", \"August\", \"September\", \"October\", \n                            \"November\", \"December\", \"1\", \"2\", \"3\", \"4\"),\n                   labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \"J\", \"A\", \"S\", \"O\", \"N\", \"D\",\"1\", \"2\", \"3\", \"4\"))\n\n\n\nFigure 2.5: Seasonal diagnostics of the monthly discharge time series at the Gunt-Khorog gauging station (17050). The analysis can be easily carried out using the function plot_seasonal_diagnostics from the R package timetk.\n\n\n\n\nunlike in the lower lying Chirchik tributaries as shown in Figure 2.17 further below in Section 2.2.\n\n\n\n\n\n\nTip\n\n\n\nCompare the discharge seasonality of Gunt River with the seasonality of the large and small Chirchik River tributaries? Obtain the information of all the other rivers in the Case Study packs and their seasonality. What is the single most important determinant of peak discharge timing in Central Asia rivers?\n\n\nBelow in Figure 2.6, we are plotting changes to monthly flows over time by binning all available data in the corresponding monthly slots. The red lines are linear regression lines that indicate trends for the individual months. Over the observational record of approx. 80 years, changes in monthly discharge regimes are clearly visible. On the one hand, summer discharge of Gunt river during the third quarter (Q3), i.e., July, August and September, is decreasing whereas the cold season discharge in Q1 and Q4 is increasing. This is a clear indication that the basin hydrology is undergoing changes over the long run. These could either be climate-related or, as discussed above, also the result of man-made interventions such as the regulation of river discharge for winter hydropower energy production. However, the shift of discharge from the warm season (Q2 and Q3) towards the cold season (Q4 and Q1) has already happened before river regulation started and hence, it is likely that we see a compound effect here.\n\nq_17050_mon %>% \n  summarise_by_time(.date_var = date, \n                    .by       = \"month\",\n                    value     = mean(data)) %>% \n  tk_ts(frequency = 12) %>% \n  forecast::ggsubseriesplot(year.labels = FALSE) + \n              geom_smooth(method = \"lm\", color = \"red\") +\n              xlab('Month') +\n              ylab('Mean monthly Q [m3/s]')\n\n\n\nFigure 2.6: The Figure shows data from the entire record. All monthly data are binned in their corresponding month (black lines). A large interannual variability (year-to-year variations in discharge in the same months) is visible. The red lines show simple regression lines for each month separately.\n\n\n\n\nWhenever we analyze annual data and changes therein, we should work with data as observed during the hydrological year. The hydrological year in Central Asia is defined as:\n\nmonHY(Oct) = 1\nmonHY(Nov) = 2\n…\nmonHY(Sep) = 12\n\nThis also holds for meteorological data. Using this definition, we can further define cold and warm seasons easily where the cold season lasts from October through end of March (Q4 to Q1 the following year) and the warm season from April through end of September (Q2 and Q3). With this in mind, we can define the hydrological year discharge.\nGiven a time series of observations, the function convert2HYY() as part of the riversCentralAsia package provides a convenient way to compute hydrological year mean discharge, including for cold and warm seasons. For monthly mean temperatures mean(T), it computes hydrological year mean temperatures, including for cold and warm seasons. Finally, for precipitation, the function computes the hydrological year sum, including also for cold and warm season months. Figure 2.7 shows the discharge time series analysis for the Khorog gauging station.\n\nqHYY <- q_17050_mon %>% convert2HYY(.,'17050','Q')\nqHYY %>%   pivot_longer(-hyYear) %>% \n  plot_time_series(hyYear,value,name,\n                   .title = '',\n                   .x_lab = 'Year',\n                   .y_lab = 'Mean monthly Q [m3/s]',\n                   .interactive = TRUE,\n                   .smooth = FALSE)\n\n\nFigure 2.7: Hydrological year discharge time series, incl. cold and warm season values. If data are not complete for all 12 months, the hydrological year statistics are not computed. Q_mean_ann: entire hydrological year discharge, Q_mean_cs: cold season Q1/Q4 discharge and Q_mean_ws: warm season Q2/Q3 discharge.\n\n\n\nFigure 2.7 confirms the findings from the seasonal analysis. However, it also shows that the first two decades of the 21st century show a marked decline in total discharge as compared to the period between 1960 to 2000.\nA common way to plot changes over time in hydro-meteorological time series is to plot annual deviations from corresponding long-term norms (long-term mean values). For this, we can use the plotNormDevHYY() function from the riversCentralAsia package. Given the three hydrological year annual time series, it computes long-term norms over the entire data set and subtracts actual annual values from the norm value. Like this, temporal changes and trends become even better visible. Figure 2.8 shows the results for the hydrological year data.\n\nplotNormDevHYY(qHYY,'Q','Khorog-Gunt 17050')\n\n\n\nFigure 2.8: Deviations from the corresponding long-term norms for the discharge time series at gauging station 17050. It should be noted that the values shown are deviations from the corresponding norms which are shown in the subtitles above the Figure plates.\n\n\n\n\nFigure 2.8 shows that, in absolute terms, the discharge in the high-flow season is undergoing a much greater reduction than an increase in the low-flow season. Hence, we cannot simply explain the decline of discharge in one season with the increase in the other. In other words, the early melting of the winter snow pack cannot alone explain the summer decline in water availability. Some other mechanism much be at work which we still need to better understand. One hypothesis could be that an increase in summer temperatures leads to higher evaporation over the basin thus leading to reduced discharge (see also Section 2.1.3 below).\nAlso and as mentioned above, winter discharge is influence by human regulation after 2006. This needs to be carefully taken into account when carrying out climate change impact analysis over the period of the observational record. For example, the cold season discharge deviation from the norm in 2006 and 2007 is 10 m3/s (see Figure 2.8) indicating that this amount of additional water was used for hydropower energy production during the winter.\nIn order to gauge whether there is a robust trend in discharge over the observed time period, we compute decadal (10 year means) and plot the results.\n\nmean10yearQ <- qHYY %>% \n  filter(hyYear < '2020-01-01') %>% \n  pivot_longer(-hyYear) %>% \n  group_by(name) %>% \n  summarise_by_time(hyYear,value, .by = \"10 year\", mean10yearQ = mean(value, na.rm = TRUE)) %>%\n  dplyr::select(-value) %>% \n  distinct() %>% \n  ungroup()\nmean10yearQ %>% pivot_wider(names_from = name,values_from = mean10yearQ)\n\n# A tibble: 8 × 4\n  hyYear     Q_mean_ann Q_mean_cs Q_mean_ws\n  <date>          <dbl>     <dbl>     <dbl>\n1 1940-01-01      111.       40.3      182.\n2 1950-01-01      108.       39.0      177.\n3 1960-01-01       96.2      35.7      156.\n4 1970-01-01      105.       35.9      174.\n5 1980-01-01      105.       37.5      171.\n6 1990-01-01      114.       41.0      188.\n7 2000-01-01      104.       43.8      165.\n8 2010-01-01       94.2      44.7      143.\n\nmean10yearQ %>%  plot_time_series(hyYear,mean10yearQ,name,\n                                  .smooth = FALSE,\n                                  .x_lab  = \"Year\",\n                                  .y_lab  = \"Q [m^3/s]\",\n                                  .title  = \"\")\n\n\nFigure 2.9: 10-year mean hydrological year discharge of Gunt River, including the cold and warm season components. The decadal mean values are related in time to the beginning of the corresponding decade in the Figure. The strongly declining trend in warm season discharge causes the overall observed decline in hydrological year discharge.\n\n\n\nThis is informative. From the 1990ies onwards, a strong reduction in mean hydrological year warm season discharge is observed of about -16 % relative to the mean 1940 - 1989 values. At the same time, 10-year mean hydrological year cold season discharge remained almost stable. As already mentioned, such types of findings are the a key motivation to study possible climate impacts in such basins in greater detail with hydrological modeling.\n\n2.1.3 Gunt Basin Climate\nA significant amount of meteorological station data are available. Some of these data are analyzed in this Section. While we mostly concentrate on mean monthly data for temperature, we should note that the available data record also contains data on absolute and mean minimum and maximum temperatures.\n\n# Extracting mean station data from the four stations.\nTmean_38954 <- gunt_station_data %>% \n  filter(code == \"38954\" & type == 'mean(T)') %>% \n  filter(date >= '1939-01-01') %>% \n  dplyr::select(date, data) %>% \n  rename(Tmean_38954 = data)\nTmean_38950 <- gunt_station_data %>% \n  filter(code == \"38950\" & type == 'mean(T)') %>% \n  filter(date >= '1939-01-01') %>% \n  dplyr::select(date, data) %>% \n  rename(Tmean_38950 = data)\nTmean_38953 <- gunt_station_data %>% \n  filter(code == \"38953\" & type == 'mean(T)') %>% \n  filter(date >= '1939-01-01') %>% \n  dplyr::select(date, data) %>% \n  rename(Tmean_38953 = data)\nTmean_38956 <- gunt_station_data %>% \n  filter(code == \"38956\" & type == 'mean(T)') %>% \n  filter(date >= '1939-01-01') %>% \n  dplyr::select(date, data) %>% \n  rename(Tmean_38956 = data)\n# Assembling the data. \nT <- full_join(Tmean_38950, Tmean_38953, by = \"date\")\nT <- full_join(T, Tmean_38954, by = \"date\")\nT <- full_join(T, Tmean_38956, by = \"date\")\n# Plotting the dataframe \nT %>% pivot_longer(-date) %>% \n  filter(date >= '1940-01-01') %>% \n  plot_time_series(date,\n                   value,\n                   name,\n                   .smooth = FALSE,\n                   .x_lab = 'Year',\n                   .y_lab = 'Mean monthly T [deg. C]',\n                   .title = \"\",\n                   .interactive = TRUE)\n\n\nFigure 2.10: Mean Monthly Temperature Climatology in the Gunt River Basin from 1940 - 2020. While first observations are available from the very beginning of the 20th century, data are only shown from 1940 onward which marks the start of a coherent record.\n\n\n# add a month identifier\nT <- T %>% \n  mutate(mon = month(date))\n\nBecause of the high quality and the consistency of the long-term record of the data at Khorog station 39854, we focus the further climatological analysis there. Figure 2.11 shows deviations from norm mean temperatures over the last 120 years. The recent two decades stand out because of the pronounced warming observed at the station, especially during the cold season where norm deviations on average range between 1 - 2 degrees Celsius (deg. C.).\n\n# Station Khorog 38954\nmeanTHYY_38954 <- gunt_station_data %>% convert2HYY(38954,'mean(T)') %>% filter(hyYear >= \"1900-10-01\")\nmeanTHYY_38954 %>% plotNormDevHYY(.,'mean(T)','Khorog 38954')\n\n\n\nFigure 2.11: Annual devations from the norm of the mean temperature for the Khorog station 38954 record are shown for the entire hydrological year and for the corresponding cold and warm seasons. Note that the entire data record is taken into account here from the start of the 20th century."
  },
  {
    "objectID": "example_river_basins.html#sec-example-chirchik-river-basin",
    "href": "example_river_basins.html#sec-example-chirchik-river-basin",
    "title": "2  Case Study River Basins",
    "section": "\n2.2 Chirchik River Basin",
    "text": "2.2 Chirchik River Basin\nThe Chirchik is a river in the Tashkent region of Uzbekistan. Its natural basin covers 13’112 km2, not accounting for the modern-time interbasin water transfers to the neighboring Akhangaran basin in the south (the outline of the basin is shown in Figure 2.12) and to the north. In terms of total runoff contribution, it is the biggest right tributary of the Syr Darya (see also further below in Section Section 2.2.1).\nThe river is formed by the confluence of the Chatkal and the Pskem rivers. They emerge at the south-western end of the Tien Shan mountains, i.e. the Talas Alatau, in the border region of Kyrgyzstan, Kazakhstan and Uzbekistan. The main tributaries are in clock-wise direction starting from north: Ugam, Pskem, Kosku and Chatkal. The Charvak reservoir receives water from these rivers. Ugam is the largest right tributary downstream of the reservoir and Aksak Ata the largest left-side tributary.\nBelow the Charvak hydroelectric power station, the river water gets diverted in numerous canals for irrigation in and around the Tashkent oasis and for interbasin water transfer to the Akhangaran basin in the south. As part of the Chirchik-Bozsuu cascade, several smaller dams along the river serve hydropower production and irrigation purposes.\n\n\nFigure 2.12: Overview over the Chirchik river basin with tributaries and the location of the main gauging stations in the zone of runoff formation and near the confluence with the Syr Darya.\n\n\nFigure 2.12 shows a comprehensive overview of the Chirchik river basin and its tributaries as well as relevant modern gauging stations. Gauges are indicated with the semi-round shapes and the corresponding five digit official code as utilized by the Uzbek Hydrometeorological Service (HMS) indicated. The gauge 16924 is not a real gauge in the sense that reservoir inflow is not measured at one point but rather is calculated from all contributing tributary flow components, i.e. the Chatkal river, the Pskem river, Nauvalisoy and the Koksu River.\nKoksu however, with a basin area of 392 km\\(^2\\), is ungauged. Its discharge contribution is calculated using an established empirical relationship between discharge in Chatkal River and discharge in Koksu. The empirical relationship is derived in Section Section 2.2.3. First, we now turn our attention to the description of key hydrological basin features.\n\n2.2.1 Chirchik Basin Characterization\nThis Section available data to characterize the Chirchik River Basin from the hydro-climatological perspective. Data access and modeling is further described in Chapter ?sec-data in Part II and Part III ?sec-hydrological-modeling of this Book.\nThe available discharge data is shown in Figure 2.13. These are near complete historic records. See above Figure 2.12 for the station locations.\n\nchirchik_river_data <- ChirchikRiverBasin\nchirchik_river_data %>% \n  filter(type == 'Q') %>% \n  group_by(type,code,station,resolution) %>% \n  plot_time_series(date,\n                   data,\n                   .facet_ncol      = 2,\n                   .interactive     = FALSE, \n                   .smooth          = FALSE,\n                   .title           = '')\n\n\n\nFigure 2.13: Available discharge data of Chirchik River basin.\n\n\n\n\nThe discharge measurements at Gazalkent gauge (number 16262) started already in 1900. It is one of the longest complete records available in Central Asia. The monthly record of the station is shown in Figure 2.14. You can zoom into the time series and investigate it in detail.\n\nchirchik_river_data %>% \n  filter(code == '16262') %>% \n  plot_time_series(date,data,\n    .interactive = TRUE,\n    .smooth = FALSE,\n    .title = \"\",\n    .x_lab = 'date',\n    .y_lab = 'Discharge in cubic meters per second',\n    .plotly_slider = TRUE)\n\n\nFigure 2.14: Monthly discharge at Gauge 16262, Gazalkent.\n\n\n\nAs is easily visible, the June 1969 discharge was the historic monthly mean maximum with 1’220 m3/s. The time series features the typical snow-melt-driven runoff pattern with pronounced seasonality and interannual variability.\nAt Chinaz near the confluence of the Chirchik River with the Syr Darya (Gauge 16275), however, a changing discharge regime can be identified over time (see Figure 2.15). The drastic decrease in discharge there is due to two effects. First, water diversions and interbasin water transfers for irrigation purposes have greatly increased over the course of the 20th century. Second, the closure of the Charvak dam in 1974 and the subsequent filling of the dam decreased discharge during the filling period. Furthermore, the interannual variability of flows decreased from there onward due to the now regulated flow regime. This latter effect is also visible at the Gazalkent gauge (Figure 2.14). The non-stationarity in the discharge time series at these stations is thus explained by anthropogenic effects.\n\nchirchik_river_data %>% \n  filter(code == '16275') %>% \n  plot_time_series(date,data,\n    .interactive = TRUE,\n    .smooth = FALSE,\n    .title = \"\",\n    .x_lab = 'date',\n    .y_lab = 'Discharge in cubic meters per second',\n    .plotly_slider = TRUE)\n\n\nFigure 2.15: Monthly discharge at Gauge 16275, Chinaz.\n\n\n\nThe effect of water diversion becomes even more apparent when the annual discharge at Gazalkent gauging station upstream of any major water diversion and at Chinaz gauge, which is in the very downstream of Chirchik River right before its confluence with the Syr Darya, are compared. The corresponding annual time series are shown in Figure 2.16 together with the difference of the two time series.\n\n\n\n\nFigure 2.16: Annual discharge at Gauge 16262, Gazalkent and Gauge 16275, Chinaz and the difference of the two timeseries. The difference of the two time series is from the allocation of water for human purposes, mostly for irrigation.\n\n\n\n\nFigure 2.16 shows the growing water allocation in the catchment from the 1930ies up to the end of the 20th century. Allocation grew almost 3-fold over this period. Interestingly, in the first decade of the 21st century, trends in allocation completely reversed and in 2009, roughly one third of the total flow at Gazalkent was allocated consumptively. The trend reversal might be due to a change in irrigation policy, problems with intake infrastructure between the two gauges, or both.\n\n\n# A tibble: 7 × 5\n# Groups:   code [7]\n  code   mean   min    max    sd\n  <chr> <dbl> <dbl>  <dbl> <dbl>\n1 16262 229.   48.7 1220   186. \n2 16275 105.    1.2  912   121  \n3 16279 116.   21.1  729   110. \n4 16290  79.4  12.7  438    69.3\n5 16298   3.8   0.9   21.1   2.8\n6 16300  22.4   3.9  114    19.3\n7 16924 205.   40.7 1231   183. \n\n\n\n\nTable 2.3: Key statistics of Chirchik basin rivers.\n\ncode\nmean\nmin\nmax\nsd\n\n\n\n16262\n228.6\n48.7\n1220.0\n186.4\n\n\n16275\n104.9\n1.2\n912.0\n121.0\n\n\n16279\n115.7\n21.1\n729.0\n109.9\n\n\n16290\n79.4\n12.7\n438.0\n69.3\n\n\n16298\n3.8\n0.9\n21.1\n2.8\n\n\n16300\n22.4\n3.9\n114.0\n19.3\n\n\n16924\n205.3\n40.7\n1231.0\n183.2\n\n\n\n\n\n\nThe largest left tributary to Chirchik below the Charvak reservoir Aksak Ata. The gauging station on the river got dismantled a long time ago. An average long-term mean discharge of 2.35 m\\(^{3}\\)/s is a solid estimated of its contribution to the overall discharge of Chirchik. Thus, if we add up long-term average discharge at Gazalkent and the one from Aksak Ata, we obtain an annual norm discharge (total average water availability) of 231 m\\(^{3}\\)/s.\nChirchik river is thus the biggest right-tributary of the Syr Darya. Chatkal river contributes exactly half to it (115.7 m\\(^{3}\\)/s) and Pskem river approximately one third (34.4 % or 79.4 m\\(^{3}\\)/s). Nauvalisoy is only a very small river with 1.6 % runoff contribution (3.8 m\\(^{3}\\)/s). From the available data, the long-term average runoff contribution by the ungauged Koksu river can be estimated to be 6.4 m\\(^{3}\\)/s or 2.8 %. Downstream of the reservoir, Ugam river contributes an additional 9.7 % (22.4 m\\(^{3}\\)/s) to the total flow.\nLet us now turn our attention to the seasonality of the tributaries. We exclude both, the Chinaz Gauge and Gazalkent Gauge data in our analysis for the above-mentioned reason that flows there are no longer representing a natural runoff regimes but are influenced by human interference. For the analysis, we plot seasonalities of the key gauged and unregulated tributaries, i.e. Chatkal, Pskem, Nauvalisoy and Ugam rivers in Figure 2.17 and Figure 2.18 below.\n\nchirchik_river_data %>% \n  filter(type == 'Q', \n         code != \"16275\",\n         code != \"16262\",\n         code != \"16924\",\n         code != \"16298\",\n         code != \"16300\") %>% \n  dplyr::select(date,data,code,river) %>% \n  group_by(code, river) %>% \n  plot_seasonal_diagnostics(.date_var = date,\n                            .value = data,\n                            .interactive = FALSE,\n                            .feature_set = c(\"week\",\"month.lbl\"),\n                            .title = \"\")\n\nWarning: Removed 108 rows containing non-finite values (stat_boxplot).\n\n\n\n\nFigure 2.17: Seasonality diagnostics of the two large tributaries, i.e. the Chatkal and Pskem rivers. Analyses of weekly (top row) and monthly data (bottom row) are shown. Year to year variability is pronounced during the peak runoff season in Q2 and Q3 where the amount of water in the rivers is critically determined by the amount of snow deposited in the mountains during the previous winter season.\n\n\n\n\nDischarge seasonality of the gauging stations downstream of Charvak reservoir is shown below. Note that we only have monthly values for Ugam station which explains the appearance of the weekly plot in the upper right panel of @ref(fig:seasonalitySmallTribs).\n\nchirchik_river_data %>% \n  filter(type == 'Q', \n         code != \"16275\",\n         code != \"16262\",\n         code != \"16924\",\n         code != \"16279\",\n         code != \"16290\") %>% \n  dplyr::select(date,data,code,river) %>% \n  group_by(code, river) %>% \n  plot_seasonal_diagnostics(.date_var = date,\n                            .value = data,\n                            .interactive = FALSE,\n                            .feature_set = c(\"week\",\"month.lbl\"),\n                            .title = \"\")\n\n\n\nFigure 2.18: Seasonality diagnostics of the two minor tributaries to the Chirchik River that are gauged.\n\n\n\n\nThe seasonality with the spring (small rivers) and summer (large tributaries) runoff peaks is striking in all the rivers. Nauvalisoy discharge peaks, on average, during or around week 20. Chatkal river discharge peaks around week 23 and Pskem river around week 26. These differences can be explained with the difference in mean catchment elevations which are as follows (ordered according to descending mean catchment elevation:\n\nPskem Catchment: 2’795 masl,\nChatkal catchment: 2’692 masl,\nNauvalisoy catchment: 2’160 masl,\nUgam catchment: 803 masl,\n\nwhere Ugam is the lowest lying and Pskem catchment the highest catchment when measured according to mean catchment elevation. Figure 2.19 shows the hypsometric curves of the main tributaries to the Chirchik River.\n\n\nFigure 2.19: Hypsometric Curves of the tributaries to the Chirchik River Basin.\n\n\nUsing a LOESS smoother, we can remove discharge time series seasonality and catch a glimpse of the underlying long-term trends. This is shown for gauging station 16294, i.e. the inflow to the Charvak Reservoir, in Figure 2.20. If anything, a slightly increasing trend in mean discharge can be observed over the last 40 years. We will further discuss this finding also in the context of the analysis of the meteorological data record in the next Section.\n\nchirchik_river_data %>% \n  filter(code == \"16924\") %>% \n  dplyr::select(date, data, code, river)  %>% \n  summarise_by_time(.date_var = date, .by = \"month\", value = mean(data)) %>% \n  plot_time_series(date,value)\n\n\nFigure 2.20: Changes in mean monthly discharges are plotted with black lines over the entire observational record for Charvak Reservoir gauge (16924).The blue line shows a smoothed trend using a LOESS smoother.\n\n\n\nBut what about changes for particular seasons and months? To understand these changes, we plot monthly average data grouped together individually for all months. Figure 2.21 shows the resulting graphs together with their best fit regression lines for each month. Several interesting observations can be done.\n\nchirchik_river_data %>% \n  filter(code == \"16924\") %>% \n  dplyr::select(date, data, code, river)  %>% \n  summarise_by_time(.date_var = date, .by = \"month\", value = mean(data)) %>% \n  tk_ts(frequency = 12) %>% \n  forecast::ggsubseriesplot(year.labels = FALSE) + \n              geom_smooth(method = \"lm\", color = \"red\") +\n              xlab('month') +\n              ylab('m^3/month')\n\n\n\nFigure 2.21: Changes in mean monthly discharges are plotted with black lines over the entire observational record for Charvak Reservoir gauge (16924).The red lines are the per month best fit regression lines.\n\n\n\n\nFirst, cold season discharge in quarter 1 (Q1) and Q4 have a slightly increasing trend. Converse to this, the warm season quarterly trends are not uniform where Q2 trends are strongly increasing and Q3 trends are markedly decreasing. This is in line with what one would expect from a warming climate, i.e. that the snow-melt driven hydrograph peak flows shift in their timing towards earlier towards spring. At the same time, Q3 warm season discharge diminishes because of the earlier snow melt, assuming no changes in the precipitated water (compare also with the findings in ?sec-gunt-river-basin where a high elevation basin from the Pamir mountains is discussed). We will investigate the available climate and precipitation record of Chirchik River basin in the following section.\n\n\n\n\nFigure 2.22: The plates show mean, minimum and maximum quarterly discharges for Q1 (upper left plate), Q2 (upper right plate), Q3 (lower left plate) and Q4 (lower right plate). All values are in mean quarterly discharge per second.\n\n\n\n\nThe development of the quarterly minimum, maximum and mean discharge Q over the years for Gauge 16924 (Charvak reservoir inflow) is shown in Figure 2.22. The increasing trends in cold season discharge (Q1 and Q4) is confirmed. In these quarters, minimum, mean and maximum discharges appear to increase with a probably link to temperature increases during these quarters (see Section 2.2.2 for a discussion). In Q2, minimum and mean discharges have an increasing trend. In Q3, maximum discharge appears to decrease over time.\n\n2.2.2 Chirchik Basin Climate\nLong-term climate data from three different stations located in the vicinity and upstream of Charvak Reservoir is available. The stations are meteorological stations 38642 and 38339, both in Pskem River Basin, station 38471, Chatkal River Basin and station 38464 in the vicinity of the Charvak Reservoir (see also Figure 2.12 above for the locations of these stations.\nThe raw temperature data is shown in Figure 2.23 whereas the per month temperature trends are shown in Figure 2.24 and Figure 2.25. At both stations, a significant cold season warning trend is visible.\n\nchirchik_river_data %>% \n  filter(type == 'T') %>% \n  group_by(type,code,station,resolution) %>% \n  plot_time_series(date,\n                   data,\n                   .facet_ncol      = 1,\n                   .interactive     = FALSE, \n                   .smooth          = TRUE,\n                   .title           = '')\n\n\n\nFigure 2.23: Available decadal temperature records at Pskem and Chatkal meteorological stations. The record at the Kyrgyz Chatkal Meteo Station shows a large data gap in the post-transition years. The blue trend lines (LOESS smoother) indicate an increasing temperature trend at both mountain stations.\n\n\n\n\n\nchirchik_river_data %>% \n  filter(type == \"T\" & code == \"38462\") %>% \n  dplyr::select(date,data,code,river)  %>% \n  summarise_by_time(.date_var = date, .by = \"month\", value = mean(data)) %>% \n  tk_ts(frequency = 12) %>% \n  forecast::ggsubseriesplot(year.labels = FALSE) + \n              geom_smooth(method = \"lm\", color = \"red\") +\n              xlab('month') +\n              ylab('deg. C.')\n\n\n\nFigure 2.24: Changes in mean monthly temperatures are plotted with black lines over the entire observational record for Pskem meteorological station.The red lines are the per month best fit regression lines.\n\n\n\n\n\nchirchik_river_data %>% \n  filter(type == \"T\" & code == \"38471\") %>% \n  dplyr::select(date, data, code, river)  %>% \n  summarise_by_time(.date_var = date, .by = \"month\", value = mean(data)) %>% \n  tk_ts(frequency = 12) %>% \n  forecast::ggsubseriesplot(year.labels = FALSE) + \n              geom_smooth(method = \"lm\", color = \"red\") +\n              xlab('month') +\n              ylab('deg. C.')\n\n\n\nFigure 2.25: Changes in mean monthly temperatures are plotted with black lines over the entire observational record for Chatkal meteorological station.The red lines are the per month best fit regression lines. The one spike in the September bin is an erroneous time series entry.\n\n\n\n\nThe increasing cold season temperatures have an impact on the snow fractions in the basins, i.e., on the fraction falling as solid precipitation versus total precipitation. Generally speaking, warming winter temperatures will lead to an increase in elevation of the snowline and thus reduce the area and volume of the winter snow deposits.\nSimilarly to the analysis carried out above for the development of quarterly flows, we can analyze the development of quarterly temperature statistics. Figure 2.26 shows the results.\n\nCodequarterT_mean <- chirchik_river_data %>% \n  filter(type == \"T\" & code == \"38471\") %>% \n  dplyr::select(date, data, code, river)  %>% \n  summarise_by_time(.date_var = date, .by = \"quarter\",value = mean(data, na.rm = TRUE)) %>% \n  rename(mean = value) %>% \n  na.omit()\n\nquarterT_max <- chirchik_river_data %>% \n  filter(type == \"T\" & code == \"38471\") %>% \n  dplyr::select(date,data,code,river)  %>% \n  summarise_by_time(.date_var = date, .by = \"quarter\",value = max(data, na.rm = TRUE)) %>% \n  rename(max = value) %>% \n  na.omit()\n\nquarterT_min <- chirchik_river_data %>% \n  filter(type == \"T\" & code == \"38471\") %>% \n  dplyr::select(date, data, code, river)  %>% \n  summarise_by_time(.date_var = date,.by = \"quarter\",value = min(data, na.rm = TRUE)) %>% \n  rename(min = value) %>% na.omit()\n\nquarterT <- left_join(quarterT_mean, quarterT_max, by = 'date')\nquarterT <- left_join(quarterT, quarterT_min, by = 'date')\n\nquarterT <- \n  bind_cols(quarterT, quarterT$date %>% \n              tsibble::yearquarter()) %>% \n  rename(quarter = '...5')\nquarterT$quarter <- quarterT$quarter %>% \n  format(., format = \"Q%q\")\n\nq1T <- quarterT %>% filter(quarter == 'Q1') %>% dplyr::select(-quarter) %>% pivot_longer(-date)\nq2T <- quarterT %>% filter(quarter == 'Q2') %>% dplyr::select(-quarter) %>% pivot_longer(-date)\nq3T <- quarterT %>% filter(quarter == 'Q3') %>% dplyr::select(-quarter) %>% pivot_longer(-date)\nq4T <- quarterT %>% filter(quarter == 'Q4') %>% dplyr::select(-quarter) %>% pivot_longer(-date)\n\npQ1 <- q1T  %>% \n  plot_time_series(.date_var = date,.value = value,.color_var = name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',.title = \"Q1\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"year\", .y_lab = \"deg. C.\",.legend_show = TRUE)\npQ2 <- q2T  %>% \n  plot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',.title = \"Q2\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"year\",.y_lab = \"deg. C.\",.legend_show = FALSE)\npQ3 <- q3T  %>%\nplot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',.title = \"Q3\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"year\",.y_lab = \"deg. C.\", .legend_show = FALSE)\npQ4 <- q4T  %>% na.omit() %>%\nplot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',.title = \"Q4\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"year\",.y_lab = \"deg. C.\",.legend_show = FALSE)\n\npQ1 + pQ2 + pQ3 + pQ4 + plot_layout(guides = 'collect') & theme(legend.position = 'bottom')\n\n\n\nFigure 2.26: Development of mean, minimum and maximum quarterly temperatures for Q1 at Station 38462.\n\n\n\n\nApart from a generally increasing trend in temperature, especially in the cold seasons, there are also significant positive anomalies since the year 2000 at Station 38462 in the maximum winter (Q1) temperatures. Changes in the fraction of precipitation falling as snow are expected under such developments.\nWe can in fact compute these changes with high resolution daily climate fields that are nowadays available (see Karger et al. (2017), Karger et al. (2020) and Karger et al. (2021) and also more on these in ?sec-data). Figure 2.27 shows the corresponding results.\n\n\nFigure 2.27: Changes in fractional snow cover in the in the Western Tien Shan mountains, including the Chirchik River basin. Shown are percentage changes over the period 1980 - 2011. Changes in snow cover fraction was computed using CHELSA V21 daily precipitation and temperature with a liquid-solid temperature threshold value of 1 deg. C.. (@karger_2017, @karger_2020, @karger_2021).\n\n\nThe atmospheric warming between 1980 and 2011 (this is period for which daily precipitation and temperature fields are available at 1 km2 resolution) has lead to a reduction of the fraction of the total precipitation falling as snow between 5 - 24 % within the elevation range from 800 masl to 1’500 masl, approx. Changes for pixels where the trend is not significant at the 95 % confidence level are not shown, i.e., they are transparent. In the region shown, no significant positive trends could be identified over the period of consideration. Finally, at higher elevations in the Chirchik River basin, no significant changes can be seen in most places.\nNow we look into the development of precipitation over the available record of station data.\n\nchirchik_river_data %>% \n  filter(type == 'P') %>% \n  group_by(type,code,station,resolution) %>% \n  plot_time_series(date,\n                   data,\n                   .facet_ncol      = 1,\n                   .interactive     = FALSE, \n                   .smooth          = FALSE,\n                   .title           = '')\n\n\n\nFigure 2.28: Available decadal and monthly data records from different meteorological stations that are located in the zone of runoff formation. As in the case of temperature, the precipitation record at the Kyrgyz Chatkal Meteo Station shows a large data gap in the post-transition years.\n\n\n\n\n\nCodequarterP_mean <- chirchik_river_data %>% \n  filter(type == \"P\" & code == \"38464\") %>% \n  dplyr::select(date,data,code,river)  %>% \n  summarise_by_time(.date_var = date, .by = \"quarter\",value = mean(data,na.rm = TRUE)) %>% \n  rename(mean = value) %>% na.omit()\n\nquarterP_max <- chirchik_river_data %>% \n  filter(type == \"P\" & code == \"38464\") %>% \n  dplyr::select(date,data,code,river)  %>% \n  summarise_by_time(.date_var = date, .by = \"quarter\",value = max(data,na.rm = TRUE)) %>% \n  rename(max = value) %>% na.omit()\n\nquarterP_min <- chirchik_river_data %>% \n  filter(type == \"P\" & code == \"38464\") %>% \n  dplyr::select(date,data,code,river)  %>% \n  summarise_by_time(.date_var = date,.by = \"quarter\",value = min(data,na.rm = TRUE)) %>% \n  rename(min = value) %>% na.omit()\n\nquarterP <- left_join(quarterP_mean, quarterP_max, by = 'date')\nquarterP <- left_join(quarterP, quarterP_min, by = 'date')\n\nquarterP <- \n  bind_cols(quarterP,quarterP$date %>% \n              tsibble::yearquarter()) %>% \n  rename(quarter = '...5')\nquarterP$quarter <- quarterP$quarter %>% \n  format(., format = \"Q%q\")\n\nq1P <- quarterP %>% filter(quarter == 'Q1') %>% dplyr::select(-quarter) %>% pivot_longer(-date)\nq2P <- quarterP %>% filter(quarter == 'Q2') %>% dplyr::select(-quarter) %>% pivot_longer(-date)\nq3P <- quarterP %>% filter(quarter == 'Q3') %>% dplyr::select(-quarter) %>% pivot_longer(-date)\nq4P <- quarterP %>% filter(quarter == 'Q4') %>% dplyr::select(-quarter) %>% pivot_longer(-date)\n\npQ1 <- q1P  %>% #group_by(name) %>% \n  plot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',\n      .title = \"Q1\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"year\", .y_lab = \"mm / 10 days\",.legend_show = TRUE\n      )\npQ2 <- q2P  %>% #group_by(name) %>% \n  plot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',\n      .title = \"Q2\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"year\",.y_lab = \"mm / 10 days\",.legend_show = FALSE\n      )\npQ3 <- q3P  %>% #group_by(name) %>% \nplot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',\n      .title = \"Q3\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"year\",.y_lab = \"mm / 10 days\", .legend_show = FALSE\n      )\npQ4 <- q4P  %>% na.omit() %>% #group_by(name) %>% \nplot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',\n      .title = \"Q4\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"year\",.y_lab = \"mm / 10 days\",.legend_show = FALSE\n      )\n\npQ1 + pQ2 + pQ3 + pQ4 + plot_layout(guides = 'collect') & theme(legend.position = 'bottom')\n\n\n\nFigure 2.29: Development of mean, minimum and maximum quarterly precipitation at Station 38462 in the Chirchik River basin.\n\n\n\n\nFigure 2.29 shows that at the meteorological station 38462, precipitation levels have been increasing until the 1980ies for Q3 and Q4. Contrary to that, there is a slightly increasing trend in Q1 since the year 2000.\nIn summary, the general global warming trend is clearly visible in in-situ station records as well as in high-resolution climatologies. As our quick analyis of the changes in snow cover fractions show, the warming translates into less snow cover in intermediate elevation ranges but not in the high-mountain zones where the critical snow storage for warm season runoff is found. This might be one of the possible explanations that we do not see any reduction in observed discharge in the basin.\n\n2.2.3 Discharge Estimation from the Ungauged Kosku Tributary\nThe construction of the Charvak reservoir and the subsequent water impoundment of the rivers water in the reservoir led to a destruction of the previous gauge that measured Chatkal discharge, including the contribution from the Koksu right-tributary. To be able to estimate the Koksu discharge contribution to the Charvak reservoir, even after the impoundment, an empirical relationship between Chatkal river discharge and Koksu was established. The procedure allows for the more or less precise estimation of Koksu discharge, even in the absence of direct measurements, as described below.\n\n\n\n\n\n\nNote\n\n\n\nThe model shown here to estimate Koksu discharge using measured quantities at Khudaydod is a simple type of an empirical model. You will learn more about such types of models in Section 10.\n\n\nBefore the closure of the Charvak dam and the subsequent filling of the reservoir in and after 1974, the Uzbek experts from the Hydrometeorological Agency started a detailed 3-years measurement comparison campaign at Charvak gauge and at gauge 16279 in Khudaydod (see Figure 2.12). Both are located on Chatkal river\nThe confluence of Koksu river with Charvak river is just upstream of the former Charvak gauge and hence between gauge 16279 and the former Charvak gauge. Using daily data from the measurement comparisons campaign, Charvak gauge discharge was related to Khudaydod discharge using a linear relationship. At the same time, they were now able to relate Koksu discharge to the discharge at Chatkal River in Khudaydod as shown in Equation 2.1.\n\\[\nQ_{Koksu} \\propto Q_{Charvak} - Q_{16279}\n\\qquad(2.1)\\]\nWe show the procedure here. After loading the riversCentralAsia Package as shown above, the relevant daily data from 01/01/1965 - 31/12/1967 can be loaded as follows (the data was kindly provided by Mr. Andrey Yakovlev).\n\ndata_koksu_discharge_derivation <- riversCentralAsia:::KoksuDischargeDerivation # load Data\ndata_koksu_discharge_derivation\n\n# A tibble: 1,976 × 4\n   date        data code    units\n   <date>     <dbl> <chr>   <chr>\n 1 1965-01-01  35.6 Charvak m3/s \n 2 1965-01-02  35.6 Charvak m3/s \n 3 1965-01-03  33   Charvak m3/s \n 4 1965-01-04  33   Charvak m3/s \n 5 1965-01-05  33   Charvak m3/s \n 6 1965-01-06  33   Charvak m3/s \n 7 1965-01-07  34.3 Charvak m3/s \n 8 1965-01-08  35.6 Charvak m3/s \n 9 1965-01-09  38.4 Charvak m3/s \n10 1965-01-10  35.6 Charvak m3/s \n# … with 1,966 more rows\n\n\nThe data is stored in long format, meaning that measurements in time and for the two gauging stations are just stacked on top of each other in one long table with a total of 1’976 measurement points. For the purpose here, we prefer the wide format where we have one date column with unique dates and then the data listed for each station in corresponding columns.\n\ndata_koksu_discharge_derivation_wide <- data_koksu_discharge_derivation %>% pivot_wider(id_cols = 'date',values_from = 'data',names_from = \"code\")\ndata_koksu_discharge_derivation_wide\n\n# A tibble: 988 × 3\n   date       Charvak `16279`\n   <date>       <dbl>   <dbl>\n 1 1965-01-01    35.6    33.4\n 2 1965-01-02    35.6    32.4\n 3 1965-01-03    33      30.4\n 4 1965-01-04    33      29.4\n 5 1965-01-05    33      30.4\n 6 1965-01-06    33      30.4\n 7 1965-01-07    34.3    31.4\n 8 1965-01-08    35.6    32.4\n 9 1965-01-09    38.4    34.4\n10 1965-01-10    35.6    32.4\n# … with 978 more rows\n\n\nThe runoff contribution of Koksu can be calculated in a simple manner.\n\n# Adding Koksu discharge to the dataframe\ndata_koksu_discharge_derivation_wide <- data_koksu_discharge_derivation_wide %>% mutate(Koksu = Charvak - `16279`)\ndata_koksu_discharge_derivation_wide\n\n# A tibble: 988 × 4\n   date       Charvak `16279` Koksu\n   <date>       <dbl>   <dbl> <dbl>\n 1 1965-01-01    35.6    33.4  2.20\n 2 1965-01-02    35.6    32.4  3.20\n 3 1965-01-03    33      30.4  2.6 \n 4 1965-01-04    33      29.4  3.6 \n 5 1965-01-05    33      30.4  2.6 \n 6 1965-01-06    33      30.4  2.6 \n 7 1965-01-07    34.3    31.4  2.9 \n 8 1965-01-08    35.6    32.4  3.20\n 9 1965-01-09    38.4    34.4  4   \n10 1965-01-10    35.6    32.4  3.20\n# … with 978 more rows\n\n\nThe relationship can now be visualized as shown in Figure 2.30.\n\nggplot(data_koksu_discharge_derivation_wide, aes(`16279`, Koksu)) +\n  geom_point() + \n  xlab(bquote('Discharge at Gauge 16279 Khudaydod in '~m^3/s)) +\n  ylab (bquote('Koksu river discharge in '~m^3/s))\n\n\n\nFigure 2.30: Scatterplot showing the relation between daily mean discharge as measured at gauge 16279 and Koksu river. The data was acquired during a compaign between 1965 and 1967.\n\n\n\n\n::: {callout icon=false} The relation between Chatkal River discharge and Koksu River discharge is remarkable, despite the complex mountain terrain from which these neighboring rivers emerge. Can you think of reasons why this is so? How do you explain outlier data? Hint: To answer these questions, you might want to consult @#sec-hydrological-systems. :::\nWe can perform a linear regression to related discharge at Khudaydod to the one at Koksu. The coefficients of the linear regression can be obtained in the following way:\n\nlm_koksu <- lm(Koksu ~ 0 + `16279`,data_koksu_discharge_derivation_wide)\nsummary(lm_koksu)\n\n\nCall:\nlm(formula = Koksu ~ 0 + `16279`, data = data_koksu_discharge_derivation_wide)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.295  -5.867  -1.918   1.416 120.419 \n\nCoefficients:\n        Estimate Std. Error t value Pr(>|t|)    \n`16279`  0.14469    0.00298   48.55   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.4 on 987 degrees of freedom\nMultiple R-squared:  0.7049,    Adjusted R-squared:  0.7046 \nF-statistic:  2357 on 1 and 987 DF,  p-value: < 2.2e-16\n\n\nPlease note, in the specification of the linear model we add the 0 term to force the regression through the origin. Hence, the discharge contribution of Koksu River is estimated to be\n\\[\nQ_{Koksu} = 0.145 * Q_{Khudaydod}\n\\qquad(2.2)\\]\nThe resulting model can easily be visualized as is shown in Figure 2.31.\n\nggplot(data_koksu_discharge_derivation_wide, aes(`16279`, Koksu)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE, formula = y~0+x) +\n  xlab(bquote('Discharge at Gauge 16279 Khudaydod in '~m^3/s)) +\n  ylab(bquote('Koksu river discharge in '~m^3/s))\n\n\n\nFigure 2.31: The linear model in $eq-koksu-estimated-model is shown.\n\n\n\n\n\n\n\n\nBeck, Hylke E., Eric F. Wood, Tim R. McVicar, Mauricio Zambrano-Bigiarini, Camila Alvarez-Garreton, Oscar M. Baez-Villanueva, Justin Sheffield, and Dirk N. Karger. 2020. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1.\n\n\nErasov, N. V. 1968. “Method for Determining of Volume of Mountain Glaciers.” MGI, no. 14: 307–8.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nKarger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-resolution monthly precipitation and temperature time series from 2006 to 2100.” Scientific Data 7 (1): 248. https://doi.org/10.1038/s41597-020-00587-y.\n\n\nKarger, Dirk Nikolaus, Adam M. Wilson, Colin Mahony, Niklaus E. Zimmermann, and Walter Jetz. 2021. “Global daily 1 km land surface precipitation based on cloud cover-informed downscaling.” Scientific Data 8 (1): 307. https://doi.org/10.1038/s41597-021-01084-6.\n\n\nTrabucco, Antonio, and Robert Zomer. 2019. “Global Aridity Index and Potential Evapotranspiration (ET0) Climate Database v2,” January. https://doi.org/10.6084/m9.figshare.7504448.v3."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Part II: Data Sources, Retrieval and Preparation",
    "section": "",
    "text": "In this Chapter and its Sections, we will discuss how to retrieve, prepare and process the data that is required for hydrological modeling.\nData include - in-situ station data, - geospatial data, - snow and glacier data, and - climate reanalysis and projections data.\nAs will become clear, the preparation of these data requires a substantial amount of work, local storage space and, in some instances, computational power.\nData needs vary according to the modeling approach and the model chain. First, the preparation of the discharge data as described in Section 4 is a necessary step for data quality control independent of the type of hydrological modeling approach it is aimed for. The Section 5 shows the necessary geospatial analysis steps for basin delineation and the generation of the required input files for physically-based modeling using RSMinerve as described in Chapter ?sec-hydrological-modeling and the ?sec-hydrological-hydraulic-modeling.\nThe Section 6 and ?climate-data demonstrate the generation of time series data for individual hydrological response units with regard to glacier contributions and climate forcing.\nThe following diagram shows the entire modeling chain for hydrological modeling using RSMinerve. It shows that the data preparation step involves many interrelated components that partially depend on each other in a sequential way. This Chapter aims at carefully working through this modeling chain to carefully demonstrate the preparation of all the relevant data.\n\n\n\nFigure 1: Example hydrological modeling chain that include data preparation, the implementation of the hydrological model and the analysis of results. As is visbile, the data preparation steps are quite substantial."
  },
  {
    "objectID": "data_sources.html#sec-geospatial-data-sources",
    "href": "data_sources.html#sec-geospatial-data-sources",
    "title": "3  Sources of Relevant Data",
    "section": "\n3.1 Vector data",
    "text": "3.1 Vector data\nFor our purpose, we define the area of interest (AOI) as 55 deg. E - 85. deg. E and 30 deg. N - 50 deg. N..\nGlobal political boundaries can be obtained from the Global Administrative Divisions database at gadm.org. Except for Turkmenistan, data on first (Oblast) and second-level (Rayon) administrative divisions is available for all Central Asian states.\nShapefiles of large river basins can be retrieved from the Global Runoff Data Center. Note that for the Central Asian region, the flat downstream areas of these basins are delineating natural hydrological borders. They do not account for man-made inter-basin transfers and thus would need to be corrected where necessary.\nThe river network can be obtained from the Global Runoff Data Center WMOBB data that was released in 2021. It can be downloaded via this link. With 161 MB, approx., data covering the whole globe can be downloaded in a straight forward manner. The global data can be clipped easily with the bounding box as defined above.\nData for the large rivers can be extracted from the layer wmobb_rivnets_Q09_10 (containing line sections representing an upland area above 4’504 km2). The layer called wmobb_rivnets_Q08_09 contains line sections representing an upland area between 1’150 km2 and 4’504 km2 and, finally, the wmobb_rivnets_Q07_08 (containing line sections representing an upland area above between 487 and 1’150 km2) (GRDC, Koblenz, Germany: Federal Institute of Hydrology (BfG). 2020). Smaller rivers can be further added to a QGIS project on a case per case base using the additional datasets in the files obtained from GRDC.\nIn total, data from 277 gauging stations from Afghanistan, Kyrgyzstan, Kazakhstan, Uzbekistan and Tajikistan are available with hydrosolutions GmbH where requests for access to these data can be sent to. The locations were obtained from the Central Asian Hydrometeorological Organizations, public reports and the Soviet compendia Surface Water Resources, Vol 14 Issues 1 and 3. It should be emphasized that the location featured on the World Bank Group website Hydrometeorological Services in Central Asia are not correctly georeferenced in most cases.\nExcept for the Afghan stations, all stations were manually located in a Geographic Information System (GIS) using the relevant Soviet Military Topographic maps (1:200’000) from the corresponding region. The maps can be downloaded from https://maps.vlasenko.net and subsequently be georeferenced in QGIS with the Raster/Georeferencr tool there (QGIS Development Team 2021). Data from northern Afghan rivers’ stream flow characteristics and the location of gauging stations there can be obtained from (Olson and Williams-Sether 2010).\nPermanent water bodies and courses can be obtained from the global HydroLakes Database (Messager et al. 2016). It can be downloaded via this link.\nGlacier data can be taken from the Randolph Glacier Inventory (RGI) 6.0. The inventory contains a global archive of glacier outlines and can be obtained via this website. Information from 16’617 glaciers is available in the AOI. The corresponding data on glacier thickness and glacier thinning rates is available as complementary data Hugonnet et al. (2021). More information on these data can be found in Section 6 whereas galciers models are dicussed in a separate Section in the Chapter on Hydrological Modeling.\nData on dams is available from the GOODD data set. Information from 88 dams in the region of interest is mapped (Mulligan, Soesbergen, and Sáenz 2020). The data is available from Global Dams Watch."
  },
  {
    "objectID": "data_sources.html#sec-raster-data-sources",
    "href": "data_sources.html#sec-raster-data-sources",
    "title": "3  Sources of Relevant Data",
    "section": "\n3.2 Raster Data",
    "text": "3.2 Raster Data\nThe NASA SRTM digital elevation model 1 Arc-second (30 m) global product is used as a DEM (“NASA Shuttle Radar Topography Mission (SRTM)(2013)” 2013). There are many ways to access these data, some more, some less convenient. An easy way to access these data is in QGIS by using the SRTM-Downloader Plugin. For a web-based access it is recommended to use the NASA EarthExplorer. Sample instructions on how to download DEM data from the EarthExplorer can for example be found by watching the following youtube tutorial.\n\n\n\n\n\n\n\n\n\nLand cover information can be obtained from the Copernicus Global Land Service: Land Cover 100m: collection 3: epoch 2019: Globe data (Buchhorn et al. 2019). The Global Land Cover Viewer allows to access, view and download annual land cover data from 2015 - 2019.\nHigh resolution climate data can be obtained from Climatologies at high resolution for the earth’s land surface areas (CHELSA) dataset via www.chelsa-climate.org. For the Version 2.1 product, climatologies for the periods 1981 - 2010, 2011 - 2040, 2041 - 2070 and 2071 - 2100 for a large number of variables are available for download as GeoTiff-files. With regard to the daily data, it is recommended to use the Global daily 1km land surface precipitation based on cloud cover-informed downscaling. This precipitation product reflects actual conditions in high mountain Asia in a much better way than the precipitation from the CHELSA-W5E5 V1.1 product. For daily temperature, the data from the CHELSA-W5E5 V1.1 product can be downloaded for a given domain of interest via this link.\n\n\n\n\n\n\nWarning\n\n\n\nNote that the daily high resolution climate fields for the entire Central Asia domain require a lot of storage space. Their processing for later analysis is computationally intensive.\n\n\nThe FLO1K, global maps of mean, maximum and minimum annual stream flow at 1 km resolution from 1960 through 2015 can be retrieved from this website. FLO1K delivers relevant data for water resources analyses at a global scale and yet high spatial resolution (Barbarossa et al. 2018). These data can be useful for long-term water balance assessments and for the study of the hydropower potential in the high mountain regions where flow measurements are sparse.\nThe CHELSA V21 global daily high-resolution climatology, available from 01-01-1979 until 31-12-2011 was processed over the Central Asia domain to map climate trends, including on temperature, precipitation, snow fraction. The data is available upon request from this site: https://chelsa-climate.org Karger et al. (2017), Karger et al. (2020), Karger et al. (2021). The CHELSA V21 product is corrected for snow undercatch in the high elevation ranges and thus is able to better represent actual high mountain precipitation than other available global climatologies (Beck et al. 2020). The aridity index (AI) fields were taken from the bio-climate CHELSA V21 data set and compared with the CGIAR AI product (Trabucco and Zomer 2019). Data on an additional 70 bio-climatic indicators were downloaded from the CHELSA V21 1980 - 2010 climatology and statistics extracted for each of the 277 gauged catchments, together with the AI.\nHigh-resolution crop disaggregated irrigated areas were mapped over the entire Central Asia domain by hydrosolutions GmbH (see also (Ragettli, Herberz, and Siegfried 2018) for more information). Like this 30 m crop maps were produced with Google Earth Engine using unsupervised classification for the years 2016 - 2020. These maps, in conjunction with estimates of irrigation water intake volumes and estimates of actual evapotranspiration help in irrigation scheme performance assessments. In hydrological modeling, these data can be used to introduce sectoral consumption estimates and help to come up with sound and effective basin planning.\n\n\n\n\nBarbarossa, Valerio, Mark A. J. Huijbregts, Arthur H. W. Beusen, Hylke E. Beck, Henry King, and Aafke M. Schipper. 2018. “Flo1k, Global Maps of Mean, Maximum and Minimum Annual Streamflow at 1 Km Resolution from 1960 Through 2015.” Scientific Data 5 (1): 180052. https://doi.org/10.1038/sdata.2018.52.\n\n\nBeck, Hylke E., Eric F. Wood, Tim R. McVicar, Mauricio Zambrano-Bigiarini, Camila Alvarez-Garreton, Oscar M. Baez-Villanueva, Justin Sheffield, and Dirk N. Karger. 2020. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1.\n\n\nBuchhorn, M., B. Smets, L. Bertels, B. De Roo, M. Lesiv, N. E. Tsendbazar, M. Herold, and S. Fritz. 2019. “Copernicus Global Land Service: Land Cover 100m: Collection 3: Epoch 2019: Globe.”\n\n\nFarinotti, Daniel, Matthias Huss, Johannes J. Fürst, Johannes Landmann, Horst Machguth, Fabien Maussion, and Ankur Pandit. 2019. “A Consensus Estimate for the Ice Thickness Distribution of All Glaciers on Earth.” Nature Geoscience 12 (3): 168–73. https://doi.org/10.1038/s41561-019-0300-3.\n\n\nGRDC, Koblenz, Germany: Federal Institute of Hydrology (BfG). 2020. “Major River Basins of the World / Global Runoff Data Centre, GRDC. 2nd, Rev. Ext. Ed.” Shape.\n\n\nHugonnet, Romain, Robert McNabb, Etienne Berthier, Brian Menounos, Christopher Nuth, Luc Girod, Daniel Farinotti, et al. 2021. “Accelerated Global Glacier Mass Loss in the Early Twenty-First Century.” Nature 592 (7856): 726–31. https://doi.org/10.1038/s41586-021-03436-z.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nKarger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-resolution monthly precipitation and temperature time series from 2006 to 2100.” Scientific Data 7 (1): 248. https://doi.org/10.1038/s41597-020-00587-y.\n\n\nKarger, Dirk Nikolaus, Adam M. Wilson, Colin Mahony, Niklaus E. Zimmermann, and Walter Jetz. 2021. “Global daily 1 km land surface precipitation based on cloud cover-informed downscaling.” Scientific Data 8 (1): 307. https://doi.org/10.1038/s41597-021-01084-6.\n\n\nMessager, M. L., B. Lehner, Grill G., I. Nedeva, and O. Schmitt. 2016. “Estimating the Volume and Age of Water Stored in Global Lakes Using a Geo-Statistical Approach.” Nature Communications 13603.\n\n\nMulligan, Mark, Arnout van Soesbergen, and Leonardo Sáenz. 2020. “GOODD, a Global Dataset of More Than 38,000 Georeferenced Dams.” Scientific Data 7 (1): 31. https://doi.org/10.1038/s41597-020-0362-5.\n\n\n“NASA Shuttle Radar Topography Mission (SRTM)(2013).” 2013. NASA. https://earthdata.nasa.gov/learn/articles/nasa-shuttle-radar-topography-mission-srtm-version-3-0-global-1-arc-second-data-released-over-asia-and-australia.\n\n\nOlson, S. A., and T. Williams-Sether. 2010. “Streamflow Characteristics at Streamgages in Northern Afghanistan and Selected Locations.” U.S. Geological Survey Data Series 529. USGS.\n\n\nQGIS Development Team. 2021. QGIS Geographic Information System. QGIS Association.\n\n\nRagettli, Silvan, Timo Herberz, and Tobias Siegfried. 2018. “An Unsupervised Classification Algorithm for Multi- Temporal Irrigated Area Mapping in Central Asia.” Remote Sensing 10 (11): 1823. https://doi.org/10.3390/rs10111823.\n\n\nTrabucco, Antonio, and Robert Zomer. 2019. “Global Aridity Index and Potential Evapotranspiration (ET0) Climate Database v2,” January. https://doi.org/10.6084/m9.figshare.7504448.v3."
  },
  {
    "objectID": "discharge_station_data.html#sec-available-data",
    "href": "discharge_station_data.html#sec-available-data",
    "title": "4  Discharge Station Data",
    "section": "\n4.1 Available Data",
    "text": "4.1 Available Data\nThe riversCentralAsia Package provides available data of the gauging and meteorological stations in the Chirchik River Basin (where other data are used, their source and access options are indicated). This is the time then to install and load the package with\n\ndevtools::install_github(\"hydrosolutions/riversCentralAsia\") # download package from github\nlibrary('riversCentralAsia') # load package\n\nBefore starting any type of modeling, it is important to get a good understanding of the data that we are dealing with and whether there exist problems with the raw data that need to be addressed prior to modeling. This is actually also one of the more hidden agendas when doing a basin characterization.\nProblems in real-world data usually include data gaps and outliers as data records that one obtains are usually neither complete nor cleaned (of errors).\nThe steps performed here are thus required steps for any type of successful modeling and should be performed with great care prior to starting hydrological modeling.\n\n\n\n\n\n\nGarbage in - Garbage out\n\n\n\nThe importance of good quality data for modeling cannot be overstated. It can very easily be summarized in the following way\n\nData \\(\\rightarrow\\) Model \\(\\rightarrow\\) Results\n\nIf the underlying data is erroneous, then this translated into\n\nGarbage in \\(\\rightarrow\\) Model \\(\\rightarrow\\) Garbage out\n\n\n\nWe concentrate our efforts here on discharge records and data from meteorological stations in the Chirchik River Basin for demonstration purposes. The techniques shown here for decadal (10-days) data naturally extend to monthly data and also, to data from other basins and other sources."
  },
  {
    "objectID": "discharge_station_data.html#sec-gap-filling-discharge-data",
    "href": "discharge_station_data.html#sec-gap-filling-discharge-data",
    "title": "4  Discharge Station Data",
    "section": "\n4.2 Gap Filling Discharge Data",
    "text": "4.2 Gap Filling Discharge Data\nIn the following, we will work with decadal discharge data from the two main tributaries of the Chirchik River, i.e. the Chatkal River (Gauge 16279) and the Pskem River (Gauge 16290) as well as on the data of the inflow to the Charvak reservoir (Gauge 16924). The goal is to analyze the data and prepare for modeling. First, let us load the relevant discharge data.\n\ndata <- ChirchikRiverBasin # load data\nq_dec_tbl <- data %>% filter(code == '16279' | code == '16290' | code == '16924') # Note for the new name of the data object, we use snake notation. We choose to add periodicity (_dec_) and data type (_tbl for tibble/dataframe) to the data name. This just helps to stay organized and is good practice in R programming.\nq_dec_tbl\n\n# A tibble: 9,072 × 14\n   date        data  norm units type  code  station   river   basin   resolution\n   <date>     <dbl> <dbl> <chr> <fct> <chr> <chr>     <chr>   <chr>   <fct>     \n 1 1932-01-10  48.8  38.8 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 2 1932-01-20  48.4  37.5 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 3 1932-01-31  42.4  36.6 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 4 1932-02-10  43.7  36.4 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 5 1932-02-20  44.2  36.3 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 6 1932-02-29  47.7  36.9 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 7 1932-03-10  54.1  39.4 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 8 1932-03-20  63.2  47.6 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 9 1932-03-31 103    60.5 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n10 1932-04-10 103    86.4 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n# … with 9,062 more rows, and 4 more variables: lon_UTM42 <dbl>,\n#   lat_UTM42 <dbl>, altitude_masl <dbl>, basinSize_sqkm <dbl>\n\n\nYou can get more information about the available data by typing ?ChirchikRiverBasin. Note that the original time series data has been packaged in this format by the riversCentralAsia::loadTabularData() function which takes a simple .csv file as input.\nIt is advisable to check at this stage for missing data in time series and to fill gaps where present. Are there missing data? How can these be filled so as to arrive at complete time series that are required for hydrological modeling?\nAs can be seen in Figure ?fig-discharge-data-chirchik-river-basin, close inspection of the time series indeed reveals some missing data in the 1940ies.\n\n\n\n\n\n\nTip\n\n\n\nNote, ?fig-discharge-data-chirchik-river-basin is an interactive figure where you can zoom in. Try it and zoom into the 1940ies to visualize the missing data fore clearly. You can zoom out again by clicking the Autoscale hover over. For the viualization of time series, we normally use the excellent timetk R Package. Check it out and try yourself!\n\n\n\nq_dec_tbl %>% plot_time_series(date,data,\n                               .facet_vars  = code,\n                               .smooth      = FALSE,\n                               .interactive = TRUE,\n                               .x_lab       = \"year\",\n                               .y_lab       = \"m^3/s\",\n                               .title       = \"\"\n                               )\n\n\nFigure 4.1: Discharge data of selected gauges in the upstream zone of runoff formation in the Chirchik River Basin. Data Source: Uzbek Hydrometeorological Service.\n\n\n\nMissing data are also confirmed by the warning that the function timetk::plot_time_series() throws (suppressed here). Statistics of the missing data can be easily obtained. As the Table below shows, we can do this analysis for each discharge station separately.\n\nq_dec_tbl %>% group_by(code) %>% \n  summarize(n.na = sum(is.na(data)), na.perc = n.na/n()*100)\n\n# A tibble: 3 × 3\n  code   n.na na.perc\n  <chr> <int>   <dbl>\n1 16279    15   0.496\n2 16290    39   1.29 \n3 16924    42   1.39 \n\n\nSummarizing the number of observation with missing data reveals that 15 data points for station 16279 (0.5 % of total record length) and 39 for station 16290 (1.3 % of total record length) are missing. As there are only very few gaps in the existing time series, we use a simple method to fill these. Wherever there is a gap, we fill in the corresponding decadal norm as stored in the norm column in the object q_dec_tbl at the timestamp of the missing data. The visualization of the results confirms that our simple gap filling approach is indeed satisfactory (see Figure 4.2).\n\n# Make a copy of the original data\nq_dec_filled_tbl <- q_dec_tbl\n\n# Actual gap filling step\nq_dec_filled_tbl$data[is.na(q_dec_filled_tbl$data)] = \n  q_dec_filled_tbl$norm[is.na(q_dec_filled_tbl$data)] \n\n# Inspect results\nq_dec_filled_tbl %>% plot_time_series(date, data, \n                                      .facet_vars  = code, \n                                      .smooth      = FALSE,\n                                      .interactive = TRUE,\n                                      .x_lab       = \"year\",\n                                      .y_lab       = \"m^3/s\",\n                                      .title       = \"\"\n                                      )\n\n\nFigure 4.2: Gap filled Pskem and Chatkal river discharges.\n\n\n\nAll missing data are gone now as can easily be validated.\n\nq_dec_filled_tbl %>% group_by(code) %>% \n  summarize(n.na = sum(is.na(data)), na.perc = n.na/n()*100)\n\n# A tibble: 3 × 3\n  code   n.na na.perc\n  <chr> <int>   <dbl>\n1 16279     0       0\n2 16290     0       0\n3 16924     0       0\n\n\nA note of caution here. This simple gap filling technique reduces variance in the time series. It should only be used when the percentage of missing data is low. As will be discussed in the next Section Section 4.3 below, more sophisticated techniques should be utilized when there exist substantial gaps and in the case of less regular data.\nFinally, we discard the data that we no longer need, including the norm data, which we used for gap filling of the missing discharge data and convert the data to wide format (see ?tbl-gap-filled-discharge-result-tibble below) to add to it meteorological data in the next Section.\n\nq_dec_filled_wide_tbl <- q_dec_filled_tbl %>% # again we use the name convention of objects as introduced above\n  mutate(code = paste0('Q',code %>% as.character())) %>% # Since we convert everything to long form, we want to keep information as compact as possible. Hence, we paste the type identifier (Q for discharge here) in from of the 5 digit station code.\n  dplyr::select(date,data,code) %>% # ... and then ditch all the remainder information\n  pivot_wider(names_from = \"code\",values_from = \"data\") # in order to pivot to the long format, we need to make a small detour via the wide format.\n\nq_dec_filled_long_tbl <- q_dec_filled_wide_tbl %>% pivot_longer(-date) # and then pivot back\nq_dec_filled_wide_tbl\n\n\n?(caption)\n\n\n\n# A tibble: 3,024 × 4\n   date       Q16279 Q16290 Q16924\n   <date>      <dbl>  <dbl>  <dbl>\n 1 1932-01-10   48.8   38.3   87.1\n 2 1932-01-20   48.4   37.7   86.1\n 3 1932-01-31   42.4   36.2   78.6\n 4 1932-02-10   43.7   35.6   79.3\n 5 1932-02-20   44.2   35     79.2\n 6 1932-02-29   47.7   37.1   84.8\n 7 1932-03-10   54.1   43.1   97.2\n 8 1932-03-20   63.2   47    110  \n 9 1932-03-31  103     72.1  175  \n10 1932-04-10  103     73.2  176  \n# … with 3,014 more rows\n\n\n\n\nAs a result, we now have a complete record of decadal discharge data for the two main tributaries of the Chirchik river and the inflow time series to Charvak Reservoir from the beginning of 1932 until and including 2015, i.e. 84 years. The same type of preparatory analysis will now be carried out for the meteorological data but in a slightly more sophisticated way."
  },
  {
    "objectID": "discharge_station_data.html#sec-gap-filling-meteorological-data",
    "href": "discharge_station_data.html#sec-gap-filling-meteorological-data",
    "title": "4  Discharge Station Data",
    "section": "\n4.3 Gap Filling Meteorological Data",
    "text": "4.3 Gap Filling Meteorological Data\nHere, we use precipitation and temperature data from Pskem (38462), Chatkal (38471) and Charvak Reservoir (38464) Meteorological Stations (see Section 2.2 for more information on these stations). We also have data from Oygaing station (Station Code 38339) but the record only starts in 1962 and the time resolution is monthly. Therefore, we do not take this station into account here for the time being.\nWe start with precipitation and plot the available data.\n\np_dec_tbl <- data %>% filter(type == \"P\" & code != \"38339\") \np_dec_tbl %>% plot_time_series(date,data,\n                               .facet_vars  = code,\n                               .interactive = TRUE,\n                               .smooth      = FALSE,\n                               .title       = \"\",\n                               .y_lab       = \"mm/decade\",\n                               .x_lab       = \"year\"\n                               )\n\n\nFigure 4.3: Raw decadal precipitation data from Pskem (38462), Charvak Reservoir (38471) and Chatkal Meteo Station (38471).\n\n\n\nThe precipitation data from these 3 stations shows some significant data gaps. The Chatkal Meteorological Station that is located in Kyrgyzstan apparently did not work in the post-transition years as continuous measurements were only resumed there in 1998.\nLet us see what happens if we were to use the same simple gap filling technique that we introduced above for discharge.\n\np_dec_filled_tbl <- p_dec_tbl\np_dec_filled_tbl$data[is.na(p_dec_filled_tbl$data)] = p_dec_filled_tbl$norm[is.na(p_dec_filled_tbl$data)]\np_dec_filled_tbl %>% plot_time_series(date,data,\n                                      .facet_vars  = code,\n                                      .interactive = TRUE,\n                                      .smooth      = FALSE,\n                                      .title       = \"\",\n                                      .y_lab       = \"mm/decade\",\n                                      .x_lab       = \"year\"\n                                      )\n\n\nFigure 4.4: Precipitation Data gap-filled with norms. The filled values from 1990 - 2000 in the case of the Station 38471 indicate that the norm-filling technique is not adequate for this type of data.\n\n\n\nClosely inspect the significant data gap in the 1990ies at Station 38741. Play around and zoom into the time series in the 1990ies in Figure 4.3 and compare it with the resulting gap-filled time series in Figure 4.4. We see that our technique of gap filling with long-term norms is not suitable for this type of data and the significant gap size. The effect of variance reduction is clearly visible.\nHence, we resort to a more powerful gap filling technique that uses a (regression) model to impute the missing values from existing ones at the neighboring stations, i.e. Stations 38462 and 38464. To do so, we utilize the simputation R package. Please note that if you do not have the required package installed locally, you should install it prior to its use with the following command install.packages('simputation')\n\nlibrary(simputation)\n# First, we bring the data into the suitable format. \np_dec_wide_tbl <- p_dec_tbl %>% \n  mutate(code = paste0('P',code %>% as.character())) %>% \n  dplyr::select(date,data,code) %>% \n  pivot_wider(names_from = \"code\",values_from = \"data\")\n\n# Second, we impute missing values.\np_dec_filled_wide_tbl <- p_dec_wide_tbl  %>% \n  impute_rlm(P38471 ~ P38462 + P38464) %>% # Imputing precipitation at station 38471 using a robust linear regression model\n  impute_rlm(P38462 ~ P38471 + P38464) %>% # Imputing precipitation at station 38462 using a robust linear regression model\n  impute_rlm(P38464 ~ P38462 + P38471) # Imputing precipitation at station 38464 using a robust linear regression model\n\np_dec_filled_long_tbl <- p_dec_filled_wide_tbl %>% pivot_longer(c('P38462','P38464','P38471')) \n\np_dec_filled_long_tbl %>% plot_time_series(date,value,\n                                          .facet_vars  = name,\n                                          .interactive = TRUE,\n                                          .smooth      = FALSE,\n                                          .title       = '',\n                                          .y_lab       = \"mm/decade\",\n                                          .x_lab       = \"year\"\n                                          )\n\n\nFigure 4.5: Precipitation Data gap filled with a robust linear regression modeling approach\n\n\n\nAs you can see, we use simple linear regression models to impute missing value in the target time series using observations from the neighboring stations. This is of course only possible where data is not missing across the time series, as we will discuss below.\nThrough simple visual inspection, it becomes clear that this type of regression model for gap filling is better suited than the previous approach chosen. Let us check whether we could successfully fill all gaps with this robust linear regression approach.\n\np_dec_filled_long_tbl %>% \n  group_by(name) %>% \n  summarize(n.na = sum(is.na(value)), n.na.perc = n.na / n() * 100)\n\n# A tibble: 3 × 3\n  name    n.na n.na.perc\n  <chr>  <int>     <dbl>\n1 P38462    12     0.402\n2 P38464    12     0.402\n3 P38471     3     0.100\n\n\nIt turns out that we still have very few gaps to deal with. We can see them by simply visualizing the wide tibble. The problem persisted at times when two or more values were missing across the available stations at the same time and where thus the linear regression could not be carried out. Let us look at the start of the record…\n\np_dec_filled_wide_tbl %>% \n  head(10)\n\n# A tibble: 10 × 4\n   date       P38462 P38464 P38471\n   <date>      <dbl>  <dbl>  <dbl>\n 1 1933-01-10     NA   NA        2\n 2 1933-01-20     NA   NA       10\n 3 1933-01-31     NA   NA        5\n 4 1933-02-10     NA   NA       33\n 5 1933-02-20     NA   NA        8\n 6 1933-02-28     NA   NA       10\n 7 1933-03-10     NA   NA       31\n 8 1933-03-20     NA   NA       50\n 9 1933-03-31     NA   NA        6\n10 1933-04-10     23   21.3     13\n\n\n… and the end of the record. The missing values are easily spotted.\n\np_dec_filled_wide_tbl %>% \n  tail()\n\n# A tibble: 6 × 4\n  date       P38462 P38464 P38471\n  <date>      <dbl>  <dbl>  <dbl>\n1 2015-11-10     72     81     19\n2 2015-11-20    122     76     43\n3 2015-11-30      7      2      3\n4 2015-12-10     NA     NA     NA\n5 2015-12-20     NA     NA     NA\n6 2015-12-31     NA     NA     NA\n\n\nWe can solve the issues related to the missing values at the start of the observation record by using the same technique as above and by only regressing P38462 and P38464 on P38471.\n\np_dec_filled_wide_tbl <- \n  p_dec_filled_wide_tbl  %>% \n  impute_rlm(P38462 ~ P38471) %>% # Imputing precipitation at station 38462 using a robust linear regression model\n  impute_rlm(P38464 ~ P38471) # Imputing precipitation at station 38464 using a robust linear regression model\np_dec_filled_wide_tbl %>% head(10)\n\n# A tibble: 10 × 4\n   date       P38462 P38464 P38471\n   <date>      <dbl>  <dbl>  <dbl>\n 1 1933-01-10   5.60   5.08      2\n 2 1933-01-20  18.3   16.7      10\n 3 1933-01-31  10.4    9.46      5\n 4 1933-02-10  54.9   50.3      33\n 5 1933-02-20  15.2   13.8       8\n 6 1933-02-28  18.3   16.7      10\n 7 1933-03-10  51.8   47.3      31\n 8 1933-03-20  82.0   75.0      50\n 9 1933-03-31  12.0   10.9       6\n10 1933-04-10  23     21.3      13\n\n\nConverse to this, the complete set of observations is missing for December 2015. We will thus remove these non-observations from our tibble. This can be done once and for all with na.omit() as shown in the code block below.\n\np_dec_filled_wide_tbl <- p_dec_filled_wide_tbl %>% na.omit()\np_dec_filled_wide_tbl %>% tail()\n\n# A tibble: 6 × 4\n  date       P38462 P38464 P38471\n  <date>      <dbl>  <dbl>  <dbl>\n1 2015-10-10      5      1      0\n2 2015-10-20     89    108     58\n3 2015-10-31     34     40     12\n4 2015-11-10     72     81     19\n5 2015-11-20    122     76     43\n6 2015-11-30      7      2      3\n\np_dec_filled_long_tbl <-  p_dec_filled_wide_tbl %>% pivot_longer(-date)\n\nInspecting the temperature data, we see similar data issues as in the precipitation data set and can proceed accordingly for gap filling.\n\nt_dec_tbl <- data %>% filter(type == \"T\") \nt_dec_tbl %>% plot_time_series(date,data,\n                               .facet_vars  = code,\n                               .interactive = TRUE,\n                               .smooth      = FALSE,\n                               .title       = '',\n                               .y_lab       = \"deg. Celsius\",\n                               .x_lab       = \"year\"\n                               )\n\n\nFigure 4.6: Raw temperature data from the meteorological stations Pskem (38462) and Chatkal (38471)\n\n\n\n\n# First, we bring the data into the suitable format. \nt_dec_wide_tbl <- t_dec_tbl %>% \n  mutate(code = paste0('T',code %>% as.character())) %>% \n  dplyr::select(date,data,code) %>% \n  pivot_wider(names_from = \"code\",values_from = \"data\")\n\n# Second, we impute missing values.\nt_dec_filled_wide_tbl <- t_dec_wide_tbl  %>% \n  impute_rlm(T38471 ~ T38462) %>% # Imputing precipitation at station 38471 using a robust linear regression model\n  impute_rlm(T38462 ~ T38471) # Imputing precipitation at station 38462 using a robust linear regression model\n\nt_dec_filled_long_tbl <- t_dec_filled_wide_tbl %>% \n  pivot_longer(c('T38462','T38471')) \n\nt_dec_filled_long_tbl %>% \n  plot_time_series(date,value,\n                   .facet_vars  = name,\n                   .interactive = TRUE,\n                   .smooth      = FALSE,\n                   .title       = '',\n                   .y_lab       = \"deg. Celsius\",\n                   .x_lab       = \"year\"\n                   )\n\n\nFigure 4.7: Temperature data gap filled with robust linear regression modeling.\n\n\n\nThere are some irregularities in the temperature time series of Chatkal Meteorological Station in the first decade of the 20th century (tip: zoom in to see these more clearly). Note that these were not introduced by the gap filling technique that we used but are most likely wrong temperature readings or recordings. We will return to these in the outlier analysis below in Section 4.4.\nAny missing values left in the temperature time series? Let’s check!\n\nt_dec_filled_long_tbl %>% \n  group_by(name) %>% \n  summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100)\n\n# A tibble: 2 × 3\n  name    n.na n.na.perc\n  <chr>  <int>     <dbl>\n1 T38462     3     0.100\n2 T38471     3     0.100\n\n\nTo see where the missing value are, we find them easily again by looking at the head and tail of the tibble.\n\nt_dec_filled_wide_tbl %>% head()\n\n# A tibble: 6 × 3\n  date       T38462 T38471\n  <date>      <dbl>  <dbl>\n1 1933-01-10   -6.9  -16.6\n2 1933-01-20   -6.1  -15.5\n3 1933-01-31   -6.3  -15.6\n4 1933-02-10   -2     -8.6\n5 1933-02-20   -3.3  -12.5\n6 1933-02-28   -0.1   -8.5\n\n\n\nt_dec_filled_wide_tbl %>% tail()\n\n# A tibble: 6 × 3\n  date       T38462 T38471\n  <date>      <dbl>  <dbl>\n1 2015-11-10    2.4   -2.5\n2 2015-11-20    2     -2.2\n3 2015-11-30    4.6   -3.7\n4 2015-12-10   NA     NA  \n5 2015-12-20   NA     NA  \n6 2015-12-31   NA     NA  \n\n\nFinally, we remove these non observations again as above with the function na.omit().\n\nt_dec_filled_wide_tbl <- t_dec_filled_wide_tbl %>% na.omit()\nt_dec_filled_long_tbl <- t_dec_filled_wide_tbl %>% pivot_longer(-date)\n\nTo deal with the missing values at the end of the observational record, we could also have used any other technique. Using the norm values however would have artificially reduced the variance in both cases as explained above. Furthermore and at least in the case of temperature, it is also questionable to what extent a norm calculated over the last 84 years is still representative given global warming. We will look in this important and interesting topic in the next section."
  },
  {
    "objectID": "discharge_station_data.html#sec-anomalies-and-outliers",
    "href": "discharge_station_data.html#sec-anomalies-and-outliers",
    "title": "4  Discharge Station Data",
    "section": "\n4.4 Anomalies and Outliers",
    "text": "4.4 Anomalies and Outliers\nWe use the function timetk::plot_anomaly_diagnostics() to investigate these anomalies in the time series. For discharge, we first log-transform the raw data with the following transformation to reduce the variance of the original data.\n\\[\n\\hat{q}(t) = log(q(t) + 1)\n\\] where \\(\\hat{q}(t)\\) denotes the transformed discharge. Prior to the log transformation, 1 is added so as to avoid cases where discharge would be 0 and the logarithmic transform thus undefined. The transformation can easily be done with the log1p() function in R. Back-transformation is then via the function expm1() simply involves taking the exponent and subtracting 1 from the result. Figure 4.8 shows the result.\nResults are shown in Figure 4.8, Figure 4.9 and Figure 4.10 below.\nThe exceptionally wet year 19169 shows up as anomalous in the Chatkal River Basin and at the downstream Charvak Reservoir inflow gauge.\n\nq_dec_filled_long_tbl %>% \n  plot_anomaly_diagnostics(date,\n                           value %>% log1p(),\n                           .facet_vars  = name,\n                           .frequency = 36,\n                           .interactive = TRUE,\n                           .title = \"\")\n\n\nFigure 4.8: Anomaly diagnostics of discharge data. The transparent grey band shows the width of the normal range. The highly anomalous wet year of 1969 is clearly visible in the discharge record of the Chatkal river basin (Station 16279).\n\n\n\nThe investigation of precipitation anomalies shows a succession of regular anomalous wet events over time. It is interesting to see that the winter 1968/69 regularly anomalous at all three stations (Figure 4.9, zoom in to investigate).\n\np_dec_filled_long_tbl %>% \n  plot_anomaly_diagnostics(date,\n                           value,\n                           .facet_vars  = name,\n                           .interactive = TRUE,\n                           .title = \"\")\n\n\nFigure 4.9: Anomaly diagnostics of precipitation data.\n\n\n\nWhile intuitively, we would have expected an exceptionally mild winter in 1968/69 due to the precipitation excess, the corresponding anomaly does not show up in the temperature record as shown in Figure 4.10.\n\nt_dec_filled_long_tbl %>%  \n  plot_anomaly_diagnostics(date,value,\n                           .facet_vars  = name,\n                           .interactive = TRUE,\n                           .title = \"\")\n\n\nFigure 4.10: Anomaly diagnostics of temperature data.\n\n\n\nApart from the identification of extremal periods since as the 1969 discharge year in the Chatkal river basin, the diagnostics of anomalies also helps to identify likely erroneous data records. In Figure 4.10 for example, when we zoom into the data of the series T38471 in the first decade of the 21st century, problems in relation to positive anomalies during the winter are visible in 4 instances. One explanation would be that in at least some instances, the data are erroneously recorded as positive values when in fact they were negative (see dates ‘2002-01-31’, ‘2005-01-10’ and ‘2007-02-28’, Chatkal Station 38471).\nObvious errors can be spotted like this and corrected. However, non-obvious data errors should be communicated with the data producing agency and replacement strategy jointly defined. If this is not possible, the values could be set to NA and then imputed as shown above.\nThe discharge data is now ready to be used for modelling and we can move on to the next Chapter on Geospatial Data."
  },
  {
    "objectID": "geospatial_data.html#sec-geospatial-data-prerequisites",
    "href": "geospatial_data.html#sec-geospatial-data-prerequisites",
    "title": "5  Geospatial Data",
    "section": "\n5.1 Geospatial Data Prerequisites",
    "text": "5.1 Geospatial Data Prerequisites\nHere, a local installation of QGIS and a basic understanding of Geographic Information Systems are required. Please see ?sec-study-guide-materials for more information how to install QGIS.\n\n\n\n\n\n\nTip\n\n\n\nSection C in the Appendix walks you through in detailed manner of many of the required steps in the Chapter. Therefore, please also consult the resources there.\n\n\nThere are very many online resources that can be consulted for learning QGIS. You can google them or start with a video tutorial like the following one.\n\n\n\n\n\n\n\n\n\nTo process the data for your case study pack, make sure that you downloaded the corresponding files via this link. Depending on the catchment you have chosen, the files are either in the folder ./AMUDARYA, ./CHIRCHIK, ./CHU or ./SYRDAYA."
  },
  {
    "objectID": "geospatial_data.html#sec-catchment-delineation",
    "href": "geospatial_data.html#sec-catchment-delineation",
    "title": "5  Geospatial Data",
    "section": "\n5.2 Catchment Delineation",
    "text": "5.2 Catchment Delineation\nAssuming that you have downloaded the catchment data folder of your choice, the corresponding DEM file should be loaded in a new, empty QGIS project. Prior to do this, make sure that the projects projection is in UTM (How to change the projection of a project). Check the projection of the DEM layer and reproject it if necessary (how to).\nIf you do not have a DEM available to start with, the process of downloading one is straight forward. Probably the easiest way is to install the QGIS SRTM3 plugin (how to). An alternative way is to download it via the USGS Earth Explorer (how to). Both solutions requires you to register an Earth Explorer Account (how to register). Finally and after downloading the DEM tiles for the region of interest, the tiles need to be merged (how to merge DEM tiles).\nTypically, geospatial data from open sources is stored in the standard projection WGS84 (EPSG:4326). The WGS84 is in degrees, minutes and seconds but for hydrological analysis it is more convenient to have spatial layers in the UTM projection. Look up in which UTM Zone your catchment lies and re-project to that zone. In the example of the Chirchiq basin, the preferred UTM zone is 42N, i.e. EPSG:32642.\nAs a next step, we are going to load the shapefile of the discharge gauge station location. This file contains the point location where discharge is measured and from which on we want to delineate the upstream contributing area. The data is stored in the ./GaugeData folder and is called XXXXX_Gauge.shp where the XXXXX are placeholders for the five digit code that uniquely identifies your station. Once the shapefile is loaded, check in Properties/Information that it is correctly projected. If this is not the case, reproject to the relevant UTM zone as described above.\nNow, we are ready to start with the catchment delineation.\nInstead of just loading the existing catchment shapefile from the corresponding ./Basin folder (how to add a vector layer to a QGIS project), it is better to actually go through the steps step-by-step to learning of to derive them. The steps are also described in this online tutorial\n\n\n\n\n\n\n\n\n\nTo derive the the catchment area upstream of the discharge gauge that we have loaded in the previous step, the DEM is traced upstream until elevation values do not increase any more, i.e. until the boundary of the watershed is reached. The SRTM DEM contains the rounded average elevation in each cell of about 25 - 30 meters [m] resolution. Due to the averaging and rounding it may happen, that in a SRTM DEM, an upstream river cell contains lower elevations than the downstream river cell. If the catchment delineation algorithm reaches such a situation, it stops, thinking it has reached the boundary of the watershed. To avoid this, cells that form local depressions should be filled to form a river bed which is continuously increasing in elevation in the upstream direction until it reaches the watershed boundary.\nIn QGIS, there are several methods that can fill terrain depressions. We are using the r.fill.dir algorithm to perform this task. Figure 5.2 shows the process in detail. First, we make sure that we have loaded the DEM in the correct projection. Second, we open the Processing Toolbox and type r.fill.dir in the search bar and then double click on the corresponding algorithm. Third, in the openend window, we ensure that the DEM is selected under the Elevation header. On top of that, the Flow Direction and Problem Areas output files are not needed and their checkboxes unchecked.\n\n\nFigure 5.2: Filling sinks in a DEM with the r.fill.dir algorithm.\n\n\nIf the algorithm has run, a new entry for the Depressionless DEM will appear in the layers panel with the correct DEM. This raster can now be used to delineate the basin area. The following steps need to be carried out for this;\n\nUse the r.watershed algorithm to obtain the flow accumulation and drainage direction rasters.\nEnsure that the gauge is correctly located on the DEM\nUse the r.water.outlet algorithm to delineate the upstream area\nConvert the resulting Basin raster into a polygon via the Conversion/Polygonize method.\n\nWe show the process in detail here.\n\n\nFigure 5.3: Running the r.watershed algorithm. Select the sink-filled DEM, set the minimum size of the exterior watershed to 200’000, check the box in Use positive flow accumulation even for likely underestimation.\n\n\nFigure 5.3 shows step-by-step how to run the r.watershed algorithm. First, make sure that the Depressionless DEM raster is selected in the Elevation field. Fill in 200’000 as a minimum size of the exterior watershed (this is a guessed number that normally yields good results). Also, check the the box to use positive flow accumulation even for likely underestimates. Finally, just select the top two output files, i.e., Number of cell that drain through each cell and Drainage direction. The remaining resulting rasters are not required during the next steps.\nOnce you click run, the resulting raster alyers will be computed. As is evident, the flow accumulation raster (called Number of cells that drain through each cell) contains a large range of values. The largest value is obviously in the one cell that drains the largest area of the DEM.\n\n\n\n\n\n\nTip\n\n\n\nUse the raster calculator to compute the logarithm of the flow accumulation raster. This helps to better visualize the raster. This raster can be used to ensure that the gauge from which the upstream area will be delineated are actually correctly located on the streams. If misplacement is evident, we can edit the gauge shapefile and relocate the gauge to the correct river stretch.\n\n\nOnce it is ensured that the gauge is correctly located on the stream, we are ready to delieate the upstream area by using the r.water.outlet algorithm. For this, zoom in to display a close up of the gauge and select and open the r.water.outlet window. The algorithm only needs two inputs, i.e. the Drainage direction raster and the coordinates of the outlet point. Instead of manually entering them, you can just click on the mapto select precise point of the gauge location which will transfer the coordinates to the algorithm’s interface. After pressing run, the Basin raster will be computed. Figure 5.4 shows the process.\n\n\nFigure 5.4: Basin delineation using the r.water.outlet algorithm. The resulting Basin raster can easily be polygonized.\n\n\nAs a final step, you should polygonize the Basin raster. We are now ready to run the process model which generates the required shapefiles for further processing in RSMinerve. The resulting vectorized basin can be stored in the corresponding folder on the local computer.\nThe process can be repeated for any other basin in a similar manner. Note also that in the case you have several gauges to map the upstream areas from, the Flow accumulation as well as the Drainage direction rasters can be computed once, stored, and reused again for all gauges.\nWe now have all the necessary files to process them in the Graphical Modeler model that is provided with the learning pack."
  },
  {
    "objectID": "snow_and_glacier_data.html",
    "href": "snow_and_glacier_data.html",
    "title": "6  Snow and Glacier Data",
    "section": "",
    "text": "Chapter on snow and glacier data."
  },
  {
    "objectID": "climate_data.html#sec-historical-climate-data",
    "href": "climate_data.html#sec-historical-climate-data",
    "title": "7  Climate Data",
    "section": "\n7.1 Historical Climate Data",
    "text": "7.1 Historical Climate Data\n\n7.1.1 CHELSA V21 High-Resolution Climate Data\nTo obtain information on past precipitation (P) and temperature (T) in the Central Asia region, we use the very high-resolution daily temperature climate product from CHELSA. CHELSA (Climatologies at high resolution for the earth’s land surface areas) is a global downscaled climate data set currently hosted by the Swiss Federal Institute for Forest, Snow and Landscape Research (WSL). It is built to provide free access to high resolution climate data for research and application, and is constantly updated and refined. CHELSA data are in the process of revolutionizing the field of hydrology in data-poor regions, among many other application domains.\nCHELSA includes climate layers for various time periods and variables, ranging from the Last Glacial Maximum, to the present, to several future scenarios. CHELSA is based on a mechanistic statistical downscaling of global reanalysis data for the historical observations (hist_obs) or global circulation model output for future simulations (fut_sim) (see also www.chelsa-climate.org for more information). For more technical information, please consult the following document.\nThe historical observations for CHELSA precipitation and temperature are available from 1979 through 2016 for daily time steps and at 1 km resolution. These are derived from ERA-INTERIM reanalysis model outputs, among other things. ERA-INTERIM is a global atmospheric reanalysis product with a spatial resolution of 80 km, approximately. A good introduction about what a reanalysis product is can be found on this website.\nDaily gridded precipitation fields are generated merging data from the ERA5 reanalysis precipitation and the MODIS monthly cloud cover. The CHELSA algorithm that is used for downscaling takes into account orographic predictors such as wind, topographic exposition and boundary layer height Karger et al. (2021). When compared with other products, the resulting data shows excellent performance, also in complex high mountain terrain. Temperature observations are available over the same period and are taken from (“CHELSA-W5e5 V1.0: W5e5 V1.0 Downscaled with CHELSA V2.0” 2022).\n\n\n\n\n\n\nWarning\n\n\n\nThe data for the Central Asia domain (55 deg. E - 85 deg. E and 30 deg. N - 50 deg. N) is very large and is not provided here for download (total storage requirements > 1 TB). Please contact Tobias Siegfried via siegfried@hydrosolutions.ch for more information on how to obtain access the data of the entire domain.\n\n\nThe high-resolution climate data derived with the CHELSA algorithm is corrected for the problem of snow-undercatch in the high mountain regions as described by (beck2020?). What is snow-undercatch? Measuring precipitation correctly in high altitude regions is complex because of sublimation and blowing snow. An example of this is shown in Figure 7.1 for high elevation gauges in Spain. In a recent inter-comparison project carried out in Spain, it has been shown that undercatch poses significant problems in accurately measuring solid precipitation (Buisán et al. 2017) in mountainous regions. Both, ERA-INTERIM and CHELSA themselves assimilate station data in their models and hence are affected by these erroneous measurements.\n\n\nFigure 7.1: Measured snow undercatch values in high-mountain stations in Spain. The values were determined within the World Meteorological Organization Solid Precipitation Intercomparison Experiment (WMO-SPICE). See text for more information and reference.\n\n\n(beck2020?) has recognized this and released monthly correction factors that are taken into account in the CHELSA algorithm (see Figure 7.2).\n\n\nFigure 7.2: Figure from (beck2020?), Supplementary Material. Plate d): Best estimate of global bias correction factors. Plate e): Lower bound estimate of global bias correction factors. Plate f): Upper bound of global bias correction factors. As is clearly visible, bias correction factors in high-mountain Asia, including the parts of Central Asia are significant.\n\n\nThe annual precipitation climatology, i.e. the long-term mean annual precipitation, from 1979 - 2011 is shown in Figure 7.3. As is easily visible and not further surprising, the mountainous regions receive the bulk of the precipitation, on average.\n\n\nFigure 7.3: The CHELSA V21 Precipitation climatology in 6 large basins Central Asian basins is shown, including Amu Darya, Syr Darya, Talas River, Chu River, Issq Kul and Ily River is shown. Light blue colors indicate very little preciptiation whereas red colors indicate high annual norm precipitation amounts.\n\n\nThe following Figure 7.4 and Figure 7.5 show cold and warm season precipitation amounts. The cold season is defined to encompass the months October (previous year) - March (preceding year) whereas the warm season lasts from April through September.\n\n\nFigure 7.4: The cold season precipitation climatology is shown. This Figure should also be compared with the the warm season precipitation climatology as shown in Figure 7.5.\n\n\n\n\nFigure 7.5: The warm season precipitation climatology is shown. This Figure should also be compared with the the cold season precipitation climatology as shown in Figure 7.4.\n\n\nFigure 7.4 shows that winter precipitation is mainly concentrated on the western fringes of the mountain ranges where moisture gets precipitated via westerly circulations and associated frontal systems. Compared to this, the main warm season precipitation locations move further to the east and to inner mountain range locations where summer convective storms cause this (see Figure 7.5).\nThe climatological data used to produce these Figures is available via this Dropbox link in the climate/chelsa_v21/climatologies/ sub-folder. There data over the historical observation period from 1981 - 2010 has been prepared for the norm annual and cold as well as warm season temperatures (tas_…) has been prepared. Similarly, data on precipitation (pr_…) and potential evapotranspiration (pet_…) is available and on the aridity index which is defined as \\(\\phi = PET/P\\) where \\(PET\\) is the potential evapotranspiration climatology and \\(P\\) is the precipitation climatology.\n\n\n\n\n\n\nTip\n\n\n\nTry it yourself! Download the data for the Central Asia domain here and visualize the climatologies for your case study catchment and extract mean statistics for the basins. As a reminder, the case study basins can be accessed via this link.\n\n\n\n7.1.2 Assessment of CHELSA V21 Data Quality in a Sample Catchment\nHow can the quality of the CHELSA data in the complex Central Asia domain be assessed? With try to answer this question by looking at one of the case study basins provided as part of the Student Case Study Pack. Specifically, we want to answer the following questions for the Gunt River basin in the Pamir mountains:\n\ndoes the magnitude of the precipitation yield physically meaningful results, and\ndoes the climatology adequately reproduce the seasonal cycle observed one at the stations?\n\nLong-term Annual Norm Discharge\nLet us address the first question by investigating long-term norm CHELSA precipitation values and comparing these long-term norm values of the specific discharge of Gunt River. If \\(P >Q\\), where \\(P\\) is the long-term mean precipitation and \\(Q\\) is the long-term mean discharge, we can confidently say that the bias corrected CHELSA precipitation product is meaningful from a water balance perspective. The long-term water balance is simply\n\\[\nQ = P - E\n\\qquad(7.1)\\]\nwhere \\(Q\\) is the specific discharge [mm], \\(P\\) is the long-term mean precipitation [mm] and \\(E\\) is the long-term mean evapotranspiration \\(E\\) [mm] (see also the Chapter on Long-term water balance modeling for more information). Hence, if, over the long run, \\(P>E\\) and under the assumption that storage changes i.e. from glacier melt are not present, the water balance is valid and the product from that perspective validated.\nWe can compute the average long-term precipitation in the catchment in a simple way. The code block below shows how.\n\n# load required libraries\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(exactextractr)\nlibrary(raster)\nlibrary(tmap)\nlibrary(tmaptools)\n\n# load basin shapefile. note that if you want to replicate \n# the code below on your own computer, make sure that you \n# adjust the data paths accordingly, depending on where \n# you stored the downloaded data!\nbasin_shp_path <- \n  \"../caham_data/AmuDarya/17050_Gunt/GIS/17050_basin_latlon.shp\"\nbasin_shp <- sf::st_read(basin_shp_path, quiet = TRUE)\n\n# load climatology and extract values\np_clim_path <- \n  \"../caham_data/central_asia_domain/climate/chelsa_v21/climatologies/hist_obs/pr_chelsa_climatology_ann.tif\"\np_clim <- raster(p_clim_path)\n\n# mask and crop\np_clim_gunt <- p_clim %>% \n  mask(basin_shp) %>% \n  crop(basin_shp)\n\n# plotting climatology\ntmap::tmap_mode(\"view\")\ntmap::tm_basemap(\"Stamen.Terrain\") +\n  tm_shape(p_clim_gunt) +\n  tm_raster(style = \"quantile\", n = 7, palette = get_brewer_pal(\"Blues\", n = 7, plot = FALSE)) +\n  tm_shape(basin_shp) +\n  tm_borders(\"black\")\n\n\nFigure 7.6: CHELSA precipitation climatology in Gunt River Basin. Note, the Figure is interactive and you can zoom in and out to see more details. The above code is shown to demonstrate the generation of such a Figure.\n\n\n\nThe mean norm precipitation can be easily computed as follows.\n\n# extracting mean value over Gunt River basin. \n# note the resulting value is in mm.\np_clim_mean <- p_clim %>% exact_extract(basin_shp,'mean')\n\nWarning in .local(x, y, ...): Polygons transformed to raster CRS (EPSG:NA)\n\np_clim_mean\n\n[1] 384.4959\n\n\nWe thus get 384 mm for the 30 year period from 1981 through 2010. Let us compare this to the long-term discharge at Gunt Khorog discharge station. For this, we load the corresponding data frame from which we extract the corresponding data.\n\n# load discharge data from Gunt Khorog gauging station. note that if you want to replicate the code below on your own computer, make sure that you adjust the data paths accordingly, depending on where you stored the downloaded data!\nstation_data <- \n  readRDS(\"../caham_data/AmuDarya/17050_Gunt/GaugeData/17050_Q.Rds\")\n\n# extract data for station 17050 and discharge between 1981 and 2010\ndischarge_data_17050 <- \n  station_data %>%\n  filter(type == \"Q\") %>% \n  filter(code == \"17050\") %>% \n  filter(date >= ymd(\"1981-01-01\")) %>% \n  filter(date <= ymd(\"2010-12-31\"))\n  \n# compute long-term discharge\ndischarge_data_17050_norm <- \n  discharge_data_17050$data %>% \n  mean(na.rm = TRUE)\ndischarge_data_17050_norm\n\n[1] 108.7257\n\n\nFor the long-term discharge at the station 17050, we thus get 109 m3/s. To compute the annual norm specific discharge, we need to compute the total discharge volume for a year and then divide by the total basin area.\n\nbasin_area <- st_area(basin_shp) %>% as.numeric()\ndischarge_data_17050_norm_vol <- \n  discharge_data_17050_norm * 3600 * 24 * 365 / basin_area * 1000\ndischarge_data_17050_norm_vol\n\n[1] 250.5378\n\n\nHence, for the period 1981 - 2010, we obtain for the specific discharge 250 mm. Clearly, \\(Q<P\\) and we can calculate that, on average, 2 parts of the total precipitation are discharged via the river whereas 1 part is evaporated without contribution to runoff at the gauge 17050 in Khorog town.\nAs an aside, the bias corrected precipitation climatology shows an interesting feature of the Gunt river basin (see Figure 7.6). Namely, there is a stark precipitation gradient between the western part of the basin where the bulk of the precipitation is observed and the hyper-arid Pamir plateau region to the east, where annual precipitation is below 200 mm at a mean altitude of above 4’000 meters above sea level [masl]. This place thus can be classified as alpine desert. This is an orographic effect as most of the moisture is washed out of the atmosphere before it enters the region of the plateau arriving from the westerly direction.\nDischarge Seasonality\nWhat about the seasonality of the CHELSA precipitation? Can it adequately reproduce the observed precipitation seasonality? If this would not be the case, we would have to reject the validity of the product and explore other high-resolution climatologies such as WorldClim V2 or CHPClim V1 (see (beck2020?) for more information on these products). Let us explore again the data of the Gunt River basin to answer this question.\nFirst, we load and prepare all the required station precipitation and geospatial data. Then, we compute the monthly norms of these data for the period 1981-01-01 through 2010-12-31. Note that the meteorological station at Khorog is located at the same place as the discharge gauging station and has the 5-digit index 38954 (see also the dedicated Example Catchments Chapter for more information).\n\nstation_data <- \n  readRDS(\"../caham_data/AmuDarya/17050_Gunt/GaugeData/17050_Q.Rds\")\n\n# extract the precipitation data for station 17050 between 1981 and 2010\npr_data_38954 <- \n  station_data %>%\n  filter(type == \"P\") %>% \n  filter(code == \"38954\") %>% \n  filter(date >= ymd(\"1981-01-01\")) %>% \n  filter(date <= ymd(\"2010-12-31\"))\n\n# add a month identifier to the dataframe and group by months\npr_data_38954 <- pr_data_38954 %>% \n  mutate(month = month(date)) %>% \n  group_by(month) %>% dplyr::select(date, data, month)\n\n# compute monthly mean values\np_monthly_mean_38954 <- pr_data_38954 %>% \n  summarize(data = mean(data, na.rm = TRUE)) %>% \n  add_column(data_source = \"Meteo Station 39954\")\n\n# plot the resulting monthly time series\np_monthly_mean_38954 %>% \n  ggplot(aes(x = month, y = data), color = data_source) + \n  geom_line() + \n  xlab(\"Month\") + \n  ylab(\"mm/month\") +\n  ggtitle(\"Monthly precipitation climatology from 1981 - 2010, Station 38954\")\n\n\n\n\nWe can extract the mean monthly values from the CHELSA SpatRaster data and then compare it to station data.\n\np_clim_monthly_path <- \n  \"../caham_data/central_asia_domain/climate/chelsa_v21/climatologies/hist_obs/monthly/pr_monthly_55_85_30_50.tif\"\np_clim_monthly <- terra::rast(p_clim_monthly_path)\n\np_monthly_mean_CHELSA_data <- \n  p_clim_monthly %>% exact_extract(basin_shp,'mean') %>% \n  as.numeric()\n\nWarning in .local(x, y, ...): Polygons transformed to raster CRS (EPSG:4326)\n\np_monthly_mean_CHELSA <- \n  p_monthly_mean_38954\np_monthly_mean_CHELSA$data <- \n  p_monthly_mean_CHELSA_data\np_monthly_mean_CHELSA$data_source <- \n  \"CHELSA\"\n\np_monthly_mean <- \n  p_monthly_mean_38954 %>% \n  add_row(p_monthly_mean_CHELSA)\n\n# plot the resulting monthly time series\np_monthly_mean %>% \n  ggplot(aes(x = month, y = data, color = data_source)) + \n  geom_line() + \n  xlab(\"Month\") + \n  ylab(\"mm/month\") +\n  ggtitle(\"Comparison of station and CHELSA precipitation climatologies\")\n\n\n\nFigure 7.7: ?(caption)\n\n\n\n\nAs is evident by looking at Figure 7.7, the CHELSA product can adequatly reproduce the seasonality of the local precipitation climatology and is only slightly overestimation absolute values. However, with regard to the later, this argument is not necessarily valied as we compare local point measurements with raster data with a resolution of 1 km2. A more thorow comparsion would generate an interpolated climatology field from station data and then compare these fields. However, as data is very scarce in this large basin, we do not have the means to perform such analysis.\n\n\n\n\n\n\nEXERCISE\n\n\n\nTry it yourself. Conduct the same analysis with the monthly temperature climatologies for the meteorological station and for the CHESLA data. You can very easily carry this out while reusing code blocks from above, also for any of the other basins available in the Case Study pack."
  },
  {
    "objectID": "climate_data.html#sec-climate-projections",
    "href": "climate_data.html#sec-climate-projections",
    "title": "7  Climate Data",
    "section": "\n7.2 Climate Projections",
    "text": "7.2 Climate Projections\nThe daily CHELSA V21 climate forcing data can be using for hydrological modeling from 1979 - 2010. But what about the future? After all, one of the main goals of this course book is to demonstrate how to use hydrological modeling to quantify future climate impacts in the Central Asian river basins. For this purpose, we will demonstrate in this Section how to download and process future climate data for studying hydrological changes.\n\n\nFigure 7.8: The time arrow is shown point from left to the right. The availability of corresponding data is indicated for what we call the historical reference period (hist_…) and the future scenario period (fut_…).\n\n\nTo start with, we need to divide the total period of interest into a historic period and a future period. The historic period in Figure 7.8 is highlighted in orange color and ranges from 1979 through 2010. It is the period for which we have observed climate forcing data from the CHELSA dataset and gauge data available at the same time. As will be discussed in the Chapter on hydrological modeling, it is also the period which we use to calibrate and validate our hydrological model.\nWhat we call the future period is highlighted by the blue arrow in Figure 7.8. Admittedly, the years from 2011 through 2022 are not in our current future (this edition of the book is written and published in 2022), so it really is just a matter of definition. It is this period over which we want to study climate impacts under different scenarios.\n\n7.2.1 Global Circulation Models\nThese scenarios are describing different increasing greenhouse concentration pathways are are computed with large-scale numerical models called General Circulation Models or GCMs. They globally represent physical processes in the atmosphere, the oceans, the cryosphere and the land surface. GCMs compute geographically distributed and physically consistent estimate of regional climate change and thus are the key inputs to different types of impact analyses. A stylized schematic structure of a GCM is shown in Figure 7.9.\n\n\nFigure 7.9: Schematic structure of a GCM model. Source: Penn State University, David Bice.\n\n\nFigure 7.9 shows that GCMs discretize the atmosphere, ocean and land columns into a three dimensional grid with differing numbers of vertical layers that is a function of the model and the compartment under consideration (atmosphere, land, ocean). With a typical horizontal resolution between 250 km - 600 km, the spatial resolution of the GCMs is coarse relative to what is needed for detailed impact studies. Furthermore, many key physical processes such as cloud formation happen at sub-grid resolution. These can thus only indirectly be represented by a process called parameterization and it represents a major source of uncertainty in GCM-based future climate simulations. Furthermore, since every GCM model represents processes and feedback mechanisms in the model in a different way, there is also inter-model uncertainty where different models generate different climate responses despite the same scenario-based forcing. Being cognisant of these uncertainties is important in impact studies. For more information, see also this website.\nFor illustration, ?fig-comparison-chelsa-gcm-resolution shows the temperature fields for 01. January 1979 over the Central Asia domain as provided by the CHELSA V21 data set (left panel) and as computed by the GCM GFDL-ESM4 model under the historic run. Note that there is no particular reason why we choose this model, it is just to serve as an example here. The complete list of models which we use for the climate change impact analysis will be presented and discussed further below.\nThe difference in resolution is striking with the CHELSA data having having a horizontal resolution of 1 km, approx., and the GCM model having a resolution of 1.5 degrees x 1 degrees which corresponds to 166.5 km x 111 km on the equator, approximately. Why is GCM resolution so coarse? It is, simply put, limited by restricitions given by the computational power of the powerful super computers where these models are run on.\n\n\n\n\n\nFigure 7.10: Comparison of CHELSA temperature climatology (left) and the GCM climate field of the historical run of the model GFDL-ESM4 (right).\n\n\n\n\n\n\nFigure 7.11: Comparison of CHELSA temperature climatology (left) and the GCM climate field of the historical run of the model GFDL-ESM4 (right).\n\n\n\n\n\nAs is indicated in Figure 7.9, GCM runs from the historic period from 1979 through 2010 are also available. These historic GCM runs are very important as we shall see in a minute when it comes to bias correcting and downscaling GCM runs onto the spatial units of interest, i.e. hydrological response units (HRUs) in our case.\n\n7.2.2 CMIP6 Climate Scenarios\nWhat are the future scenarios that we are interest in? The global climate science community has worked hard under the within the Phase 6 of the Coupled Model Intercomparison Project (CMIP6) the define relevant future scenarios that are describing different climate forcing trajectories. The paper by (O’Neill et al. 2016) is the refernce source with regard to detailed descriptions of the scenarios. We are interested to cover and study a broad range of possible hydrological future states and thus select 4 distrinct shared socioeconomic pathway (SSP) scenarios that cover this entire possible range.\nThe SSPs are based on five narratives describing broad socioeconomic trends that possibly shape future society. These are intended to span the range of plausible futures. The narratives are (taken from (Riahi et al. 2017)):\n\nSSP1 Sustainability – Taking the Green Road (Low challenges to mitigation and adaptation): The world shifts gradually, but pervasively, toward a more sustainable path, emphasizing more inclusive development that respects perceived environmental boundaries. Management of the global commons slowly improves, educational and health investments accelerate the demographic transition, and the emphasis on economic growth shifts toward a broader emphasis on human well-being. Driven by an increasing commitment to achieving development goals, inequality is reduced both across and within countries. Consumption is oriented toward low material growth and lower resource and energy intensity.\nSSP2 Middle of the Road (Medium challenges to mitigation and adaptation): The world follows a path in which social, economic, and technological trends do not shift markedly from historical patterns. Development and income growth proceeds unevenly, with some countries making relatively good progress while others fall short of expectations. Global and national institutions work toward but make slow progress in achieving sustainable development goals. Environmental systems experience degradation, although there are some improvements and overall the intensity of resource and energy use declines. Global population growth is moderate and levels off in the second half of the century. Income inequality persists or improves only slowly and challenges to reducing vulnerability to societal and environmental changes remain.\nSSP3 Regional Rivalry – A Rocky Road (High challenges to mitigation and adaptation): A resurgent nationalism, concerns about competitiveness and security, and regional conflicts push countries to increasingly focus on domestic or, at most, regional issues. Policies shift over time to become increasingly oriented toward national and regional security issues. Countries focus on achieving energy and food security goals within their own regions at the expense of broader-based development. Investments in education and technological development decline. Economic development is slow, consumption is material-intensive, and inequalities persist or worsen over time. Population growth is low in industrialized and high in developing countries. A low international priority for addressing environmental concerns leads to strong environmental degradation in some regions.\nSSP4 Inequality – A Road Divided (Low challenges to mitigation, high challenges to adaptation): Highly unequal investments in human capital, combined with increasing disparities in economic opportunity and political power, lead to increasing inequalities and stratification both across and within countries. Over time, a gap widens between an internationally-connected society that contributes to knowledge- and capital-intensive sectors of the global economy, and a fragmented collection of lower-income, poorly educated societies that work in a labor intensive, low-tech economy. Social cohesion degrades and conflict and unrest become increasingly common. Technology development is high in the high-tech economy and sectors. The globally connected energy sector diversifies, with investments in both carbon-intensive fuels like coal and unconventional oil, but also low-carbon energy sources. Environmental policies focus on local issues around middle and high income areas.\nSSP5 Fossil-fueled Development – Taking the Highway (High challenges to mitigation, low challenges to adaptation): This world places increasing faith in competitive markets, innovation and participatory societies to produce rapid technological progress and development of human capital as the path to sustainable development. Global markets are increasingly integrated. There are also strong investments in health, education, and institutions to enhance human and social capital. At the same time, the push for economic and social development is coupled with the exploitation of abundant fossil fuel resources and the adoption of resource and energy intensive lifestyles around the world. All these factors lead to rapid growth of the global economy, while global population peaks and declines in the 21st century. Local environmental problems like air pollution are successfully managed. There is faith in the ability to effectively manage social and ecological systems, including by geo-engineering if necessary.\n\nFigure 7.12 shows the underlying population and GDP developments for the corresponding SSPs and Table 7.1 details about the forcing in these scenarios.\n\n\nFigure 7.12: Global population and GDP developments under the CMIP6 shared socioeconomic pathways (taken from this source).\n\n\nThe description of the Tier 1 scenarios below that we are focussing on is taken from the aforementioned publication.\n\n\nTable 7.1: Scenarios, their forcing category and the effective radiative forcing by the year 2100 (O’Neill et al. 2016).\n\n\n\n\n\n\nScenario\nForcing Category\n2100 forcing [W/m2]\n\n\n\nShared Socioeconomic Pathway SSP5-8.5\nHigh\n8.5\n\n\nShared Socioeconomic Pathway SSP3-7.0\nHigh\n7\n\n\nShared Socioeconomic Pathway SSP2-4.5\nMedium\n4.5\n\n\nShared Socioeconomic Pathway SSP1-2.6\nLow\n2.6\n\n\n\n\nFor each scenario, we select 4 high priority GCM models for the preparation of downscaled climate forcings for the basins under consideration. The following Table 7.2 shows overview information about the models which are used in this course. Output of the GCM models shown in the table is available at daily timescales until 2100 and also for the historic runs.\n\n\nTable 7.2: Model names, models and host institution where the GCM models have been developed.\n\n\n\n\n\n\nName\nModel\nInstitution\n\n\n\nGFDL-ESM4\ngfdl- esm4\nNational Oceanic and Atmospheric Administration, Geophysical Fluid Dynamics Laboratory, Princeton, NJ 08540, USA\n\n\nUKESM1-0-LL\nukesm1- 0-ll\nMet Office Hadley Centre, Fitzroy Road, Exeter, Devon, EX1 3PB, UK\n\n\nMPI-ESM1-2-HR\nmpi- esm1-2- hr\nMax Planck Institute for Meteorology, Hamburg 20146, Germany\n\n\nIPSL-CM6A-LR\nipsl- cm6a-lr\nInstitut Pierre Simon Laplace, Paris 75252, France\n\n\nMRI-ESM2-0\nmri-esm2-0\nMeteorological Research Institute, Tsukuba, Ibaraki 305-0052, Japan\n\n\n\n\nGCM Model data can be downloaded from the climate store on the dedicated Copernicus website.\n\n\n\n\n\n\nImportant\n\n\n\nEach GCM model is different from the others. Some model leap days, some don’t. Some assume that each month has 30 days, other don’t. The preprocessing steps required to make the outputs of these models is very time consuming and is not recommendened for beginners. Therefore, please download the pre-processed climate scenarios for the Central Asia domain from this online repository.\n\n\n\n7.2.3 Downscaling and Bias Correction Using Quantile Mapping\nGCM model data can be subject to systematic biases (e.g. Kotlarski et al. 2014) and their coarse resolution does not allow us to directly use these data in hydrological modeling studies. For this reason, a large number of downscaling and bias correction techniques have been developed, inclduing the well-known delta change method (see e.g. Feigenwinter et al. 2018 and references therein).\nThe daily CHELSA V21 data that became available in 2021, together with daily GCM data from CMIP6 now allows a relatively straight forward application of empirical quantile mapping to downscale to and statistically correct GCM for hydrological response units (HRUs).\nFeigenwinter et al. (2018) explains clearly how empirical quantile mapping works. For the historical period (also called calibration period in the context of the discussion here), simulated model output (in our case, GCM hist_sim data, as shown in Figure 7.8) is corrected with a correction function towards an observational reference (here, the high-resolution CHELSA climatology) and systematic model biases are partly removed.\n\n\nFigure 7.13: Overview on the bias correction approach: a bias correction function is calibrated by comparing raw climate model output to observations in a common historical reference period. The calibrated correction function is then applied to the entire raw model output in order to produce a bias-corrected time series out into the future scenario period (taken from (Feigenwinter et al. 2018)).\n\n\nIn a climate change context, the so-called correction function (or transfer function), established in the historical calibration period, can then be applied to the simulated future time series in order to produce bias-corrected scenario time series.\n\n\nFigure 7.14: The nature of empirical quantile mapping is shown (source: (Feigenwinter et al. 2018)). Left panel: Example based on the probability density function (PDF). Right panel: example based on the cumulative distribution function (CDF).\n\n\nFigure 7.14 explains the bias correction approach graphically. A biased simulated distribution (blue) is corrected towards an observed distribution (black). In the example shown the raw simulated distribution is subject to both a bias of the mean and a bias in variance. The resulting bias-corrected distribution (dashed red) approximates the observed one but is typically not identical to it (e.g. due to the sampling uncertainty during the calibration of the correction function or details of the specific quantile mapping implementation).\nAs we explained above, we investigate 4 climate scenarios (ssp126, ssp245, ssp 370 and ssp585) for which we have 4 GCM model runs each (GFDL-ESM4, UKESM1-0-LL, MPI-ESM1-2-HR and MRI-ESM2-0). Hence, we have 16 scenario-model combination and the same amounts of correction functions. The best way to achieve this is show the necessary steps by means of an example catchment. While each catchment is unique, the steps to pre-process and later export the corresponding climate files for modeling in RSMinerve are not and are thus generalizable.\nThe only caveat is that the processing of the CHELSA high-resolution climate files requires the very large raw files to be available locally. Due to the size of these files for the entire Central Asia domain, sharing these files is not easy and we are working on ways to make these files more readily available in the future so that they can be processes locally.\nFor the moment, each case study catchment in the student pack contains the precomputed climate files with which RSMinerve hydrological models can be run in a straight forward manner. These files are stored in the RS_Minerve folder. The following files are available:\n\n\nhist_obs_rsm.csv: This is the .csv-files that contains the CHELSA V21 temperature and precipitation forcing for each of the elevation bands as specified in the /GIS/XXXXX_hru.shp file where XXXXX is the placeholder for the corresponding gauge code.\n\nhist_sim_….csv: For the 4 climate models investigated, the simulated history is stored in these file. They are used for bias correction and not directly used in hydrological modeling.\n\nfut_sim_….csv: We have generated 16 such files which consist of 4 climate scenarios for each of the 4 climate models. These are the future temperature and precipitation time series that were bias corrected using the quantile mapping method as explained above.\n\nfut_sim_bcsd_….csv: These are the 16 final bias corrected and downscaled future climate forcing files. These are the .csv-files that are read into RSMinerve for the study of climate impacts on the river basin under consideration.\n\nThe processes of generating these files is shown in the following at the example of Chon Kemin catchment in Kyrgyzstan. For each catchment in the Students’ Case Study Pack, the code to process the files can be found in the corresponding CODE folder.\nFirst, the necessary libraries need to be loaded.\n\n# Tidy data wrangling\nlibrary(tidyverse) # includes readr and ggplot2\nlibrary(lubridate)\nlibrary(timetk)\n\n# plotting add-ons to ggplot2\nlibrary(patchwork)\n\n# Our own package for load and processing local data\ndevtools::install_github(\"hydrosolutions/riversCentralAsia\")\nlibrary('riversCentralAsia')\n\n# Spatial data processing\nlibrary(raster)\nlibrary(terra)\nlibrary(sf)\nlibrary(stars)\nlibrary(exactextractr)\n\n# quantile mapping\nlibrary(qmap)\n\nThen, we can simply configure the details of the catchment at hand (note the location installation dependent specification of paths!).\n\n# River\nriver_name <- \"ChonKemin\"\nbasin_name <- \"Chu\"\n\n# Gauge\ngauge_name <- '15149_gauge'\ngauge_code <- '15149'\nq_path <- \"../caham_data/student_case_study_basins/15149_ChonKemin/GaugeData/\"\nq_name <- paste0(gauge_code,\"_Q.csv\")\ndata_type_Q <- \"Q\"\nunits_Q <- \"m3/s\"\n\n# GIS \n#Important naming convention. We assume that GIS-files adhere to the following naming convention:\n#- Basin Shapefile: paste0(gauge_code,\"_basin.shp\")\n#- River Shapefile: paste0(gauge_code,\"_river.shp\")\n#- Junctions Shapefile: paste0(gauge_code,\"_junctions.shp\")\n#- HRU Shapefile: paste0(gauge_code,\"_hru.shp\")\n#- DEM Raster: paste0(gauge_code,\"_dem.tif\")\ngis_path <- \"../caham_data/student_case_study_basins/15149_ChonKemin/GIS/\"\ndem_file <- paste0(gauge_code,'_dem.tif')\ncrs_project <- 4326 #latlon WGS84\n\nThere are a number of parameters to be set before the modeling which can be done as follows.\n\n# Time zone\ntz <-  \"UTC\"\n\n# GCM Climate Models and Simulations/Experiments and data paths\nhist_obs_dir <- \"../../../../../../../../../../../../Documents/ca_large_files/CA_CLIMATE_PROJECTIONS/CHELSA_V21_1979_2018/\" \nhist_sim_dir <- \"../../../../central_asia_domain/climate/hist_sim/\"\nfut_sim_dir <-  \"../../../../central_asia_domain/climate/fut_sim/\"\n\ngcm_Models <- c(\"GFDL-ESM4\", \"IPSL-CM6A-LR\", \"MRI-ESM2-0\", \"UKESM1-0-LL\")\ngcm_Scenarios <- c(\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\")\n\ngcm_Models_Scenarios <- base::expand.grid(gcm_Models,gcm_Scenarios) %>%\n        dplyr::mutate(model_scenario_combination = paste0(Var1,\"_\",Var2)) %>%\n        dplyr::select(model_scenario_combination) %>% unlist() %>% as.character()\n\n# Historical Observations\nhist_obs_start <- 1979\nhist_obs_end <- 2011\nhist_obs_dates <- riversCentralAsia::generateSeqDates(hist_obs_start,hist_obs_end,'day')\nhist_obs_dates <- as_date(hist_obs_dates$date) %>% as_tibble() %>% rename(Date = value)\n\n# Historical GCM Simulations\nhist_sim_start <- hist_obs_start\nhist_sim_end <- hist_obs_end\nhist_sim_dates <- hist_obs_dates\n\n# Future GCM Simulations\nfut_sim_start <- 2012\nfut_sim_end <- 2099\nfut_sim_dates <- riversCentralAsia::generateSeqDates(fut_sim_start,fut_sim_end,'day')\nfut_sim_dates <- as_date(fut_sim_dates$date) %>% as_tibble() %>% rename(Date = value)\n\n# Climate Data Observation Frequency\nobs_freq <- \"day\"\n\n# RSMinverve\nmodel_dir <- \"../../RS_MINERVE/\"\n\n## Dates\n\nhist_sim_dates <- hist_obs_dates\nfut_sim_dates <- riversCentralAsia::generateSeqDates(fut_sim_start,fut_sim_end,'day')\nfut_sim_dates <- as_date(fut_sim_dates$date) %>% as_tibble() %>% rename(Date = value)\n\n\n\n\n\n\n\nIMPORTANT\n\n\n\nNote that you would have to set the data_paths in the above code block according to your own local installation. However, since the CHELSA raster stack files are not available in the Students’ Case Study directory, the code here serves just as a demonstration. It should be further noted that this below is an advanced section that requires a good understanding of the R programming language.\n\n\nAfter setting the parameters, we can generate the hydrological response units. As the adept reader realizes, we are creating the elevation bands in R/RStudio and do not resort to QGIS as has been shown in the previous Geospation Data Section.\n\n# Parameter definition for the generation of the elevation bands\nband_interval <- 300 # in meters. Note that normally you want to work with band intervals of 100 m to 200 m. To make the model less computationally demanding, we work with a coarser resolution of 300 m. \nholeSize_km2 <- .1 # cleaning holes smaller than that size\nsmoothFact <- 2 # level of band smoothing\ndemAggFact <- 2 # dem aggregation factor (carefully fine-tune this)\n## Delineation\nhru_shp <- gen_basinElevationBands(gis_path,dem_file,demAggFact,band_interval,holeSize_km2,smoothFact)\n# Control output\nhru_shp %>% plot()\n\n\n\n\nIn other words, the function gen_basinElevationBands() from the riversCentralAsia R Package creates elevation bands (HRUs) as per the parameter values. In the above example, we are generating elevation bands with a 300 meters [m] bands interval. The smaller this number, the higher the number of elevation bands that will be generated and the higher the computational requirements will be of the hydrological model.\nAs a next step, we intersect the subbasins with elevtion bands. In the example of Chon Kemin, the basin corresponds to the one subbasin.\n\npath2subbasin_shp <- paste0(gis_path,gauge_code,\"_basin.shp\")\nsubbasins_shp <- st_read(path2subbasin_shp, quiet = TRUE)\nsubbasins_hru_shp <- st_intersection(hru_shp,subbasins_shp)\n\nWe can extract and add mean elevation data for each HRU in the following way.\n\ndem <- raster::raster(paste0(gis_path,dem_file))\nzonalStat_Z <- exactextractr::exact_extract(dem, subbasins_hru_shp, 'mean', progress = FALSE)\nsubbasins_hru_shp$Z <- zonalStat_Z\n\nIn a final step, we add unique subbasin names and remove the shapefile fields that are no longer needed in the next steps. The plot shows the resulting shapefile.\n\nfor (idxSubBasin in seq(length(subbasins_shp$name))) {\n  \n  subbasin_sel <- subbasins_hru_shp %>% dplyr::filter(name == subbasins_shp$name[idxSubBasin])\n  subbasin_sel <- subbasin_sel %>% dplyr::arrange(Z)\n  subbasin_sel$hru_num <- (1:base::nrow(subbasin_sel))\n  subbasin_sel$name <- paste0(subbasin_sel$name,'_',subbasin_sel$hru_num)\n  \n  if (idxSubBasin == 1) {\n    res_subbasins <- subbasin_sel\n  } else {\n    res_subbasins <- res_subbasins %>% dplyr::add_row(subbasin_sel)\n  }\n  \n}\n\nsubbasins_hru_shp <- res_subbasins %>% dplyr::select(-layer,-hru_num)\nsubbasins_hru_shp %>% plot()\n\n\n\n\nThe resulting shapefile can then be written to local storage.\n\nsf::st_write(subbasins_hru_shp, paste0(gis_path, gauge_code, '_hru', '.shp'), append = FALSE)\n\nThe geometry of the hydrological modeling approach is now defined and we can start to extract and generate the climate forcing data. First, the historical observations (hist_obs) of temperature and precipitation for each HRU need to be defined. We show how this can be done in a straight forward manner using again helper functions from the riversCentralAsia package.\n\n# Parameters\nclimate_data_type <- \"hist_obs\"\n\n# Load HRU shapefile\nsubbasins_hru_shp <- \n  sf::st_read(paste0(gis_path,gauge_code,'_HRU','.shp'))\n\n# List CHELSA climate files\nclimate_files_tas <- \n  list.files(hist_obs_dir,pattern = \"tas_\",full.names = TRUE)\nclimate_files_pr <-  \n  list.files(hist_obs_dir,pattern = \"pr_\",full.names = TRUE)\n\n# Restrict years range\nn_years <- \n  hist_obs_start:hist_obs_end\nclimate_files_tas <- \n  climate_files_tas[1:length(n_years)]\nclimate_files_pr <- \n  climate_files_pr[1:length(n_years)]\n\n# Temperature data processing\ntemp_or_precip <- \"Temperature\"\nhist_obs_tas <- riversCentralAsia::gen_HRU_Climate_CSV_RSMinerve(climate_files_tas,\n                                              river_name,\n                                              temp_or_precip,\n                                              subbasins_hru_shp,\n                                              hist_obs_start,\n                                              hist_obs_end,\n                                              obs_freq,\n                                              climate_data_type,\n                                              crs_project)\n\n# Precipitation data processing\ntemp_or_precip <- \"Precipitation\"\nhist_obs_pr <- riversCentralAsia::gen_HRU_Climate_CSV_RSMinerve(climate_files_pr,\n                                              river_name,\n                                              temp_or_precip,\n                                              subbasins_hru_shp,\n                                              hist_obs_start,\n                                              hist_obs_end,\n                                              obs_freq,\n                                              climate_data_type,\n                                              crs_project)\n\n# Combine extract climate tibbles.\nhist_obs_rsm <- hist_obs_tas %>% add_column(hist_obs_pr %>% \n                                              dplyr::select(-Station),.name_repair = 'unique')\n\n# Add Discharge Data (monthly)\nq_dec <- riversCentralAsia::loadTabularData(q_path,\n                                              q_name,\n                                              gauge_code,\n                                              gauge_name,\n                                              river_name,\n                                              basin_name,\n                                              data_type_Q,\n                                              units_Q)\nfunc_type_lib <- list(mean = \"Q\")\nq_mon <- aggregate_to_monthly(q_dec,func_type_lib)\n\nq_mon <- q_mon %>% \n  mutate(date = floor_date(as.POSIXct(.$date,tz = tz), unit = \"day\")) \n# Note, the function above outputs dates as date class. \n# This causes problems down the road. \n# We address these by converting the date values to dttm class.\n\nq_mon <- q_mon %>% dplyr::select(date, data) %>% \n  dplyr::filter(date >= ymd(paste0(hist_obs_start, \"-01-01\"))) %>% \n  dplyr::filter(date <= ymd(paste0(hist_obs_end, \"-12-31\")))\n\ndates_char_Q <- \n  riversCentralAsia::posixct2rsminerveChar(q_mon$date, tz = \"UTC\") %>% \n  rename(Station = value) %>% \n  tibble::add_column(Q = (q_mon$data %>% as.character))\n\n# Get gauge location and elevation\ngauge_shp <- sf::st_read(paste0(gis_path,gauge_code,\"_gauge.shp\"))\ndem <- raster::raster(paste0(gis_path,dem_file))\ngauge_coord <- gauge_shp %>% sf::st_coordinates()\ngauge_Z <- exactextractr::exact_extract(dem,gauge_shp,'mean')\n\n# Combine everything\nhist_obs_rsm <- dplyr::full_join(hist_obs_rsm,dates_char_Q, by = 'Station') \n\n# now finish off by giving the required attributes in the table for the discharge station\nhist_obs_rsm$Q[1] = data_type_Q\nhist_obs_rsm$Q[2] = gauge_coord[1]\nhist_obs_rsm$Q[3] = gauge_coord[2]\nhist_obs_rsm$Q[4] = 550\nhist_obs_rsm$Q[5] = data_type_Q\nhist_obs_rsm$Q[6] = 'Flow'\nhist_obs_rsm$Q[7] = units_Q\nhist_obs_rsm$Q[8] = 'Constant after'\n\n# Write final file to disk\nreadr::write_csv(hist_obs_rsm,paste0(model_dir,climate_data_type,\"_rsm.csv\"),na = \"NA\",col_names = FALSE)\n\nThe last line of code writes the result back to the disk. Please check in your Case Study Pack the folder /RS_MINERVE/ where the corresponding hist_obs_rsm.csv is stored. The file format corresponds to import requirements from the side of RSMinerve. You can open the text file either with a Spreadsheet program such as Excel or with a text editor. The daily time series of temperature, precipitation for each HRU and the discharge for the gauging station are stored in the columns with a header section. Since observed discharge data is only available on a monthly basis, the time series is filled with NA where there are no obervations available.\nFor the monthly mean discharge values, the following convention is adhered to. Monthly mean discharge values are written/stored in the first day of the month.\nNext, we can process the historical GCM runs in a similar fashion.\n\n# Parameters\noutput_file_dir <- \"../../RS_MINERVE/\"\nclimate_data_type <- \"hist_sim\"\nclimate_files_tas <- list.files(hist_sim_dir,pattern = \"tas_\",full.names = TRUE)\nclimate_files_pr <- list.files(hist_sim_dir,pattern = \"pr_\",full.names = TRUE)\n\n# Extract GCM Model-Specfic Data\nhist_sim_rsm <- vector(mode = \"list\", length = length(gcm_Models))\nnames(hist_sim_rsm) <- gcm_Models\n\nfor (idxGCM in seq(length(gcm_Models))) {\n  \n  temp_or_precip <- \"Temperature\"\n  gcm_model <- gcm_Models[idxGCM]\n  hist_sim_T <- riversCentralAsia::gen_HRU_Climate_CSV_RSMinerve(climate_files_tas[idxGCM],\n                                              river_name,\n                                              temp_or_precip,\n                                              subbasins_hru_shp,\n                                              hist_sim_start,\n                                              hist_sim_end,\n                                              obs_freq,\n                                              climate_data_type,\n                                              crs_project)\n \n  temp_or_precip <- \"Precipitation\"\n  hist_sim_P <- riversCentralAsia::gen_HRU_Climate_CSV_RSMinerve(climate_files_pr[idxGCM],\n                                              river_name,\n                                              temp_or_precip,\n                                              subbasins_hru_shp,\n                                              hist_sim_start,\n                                              hist_sim_end,\n                                              obs_freq,\n                                              climate_data_type,\n                                              crs_project)\n  \n  hist_sim_rsm[[idxGCM]] <- hist_sim_T %>% \n    tibble::add_column(hist_sim_P %>% dplyr::select(-Station),.name_repair = 'unique')\n  write_csv(hist_sim_rsm[[idxGCM]],\n            paste0(model_dir,climate_data_type,\"_\",gcm_model,\"_\",\n                   gauge_code,\"_\",hist_sim_start,\"_\",hist_sim_end,\".csv\"),\n            col_names = FALSE)\n  \n}\n\nThe same applies to the future climate scenario runs.\n\nclimate_data_type <- \"fut_sim\"\n\n# Load HRU shapefile\nsubbasins_hru_shp <- sf::st_read(paste0(gis_path,gauge_code,'_HRU','.shp'))\n\n# Process and Extract GCM Model-Specific Data\nfut_sim_rsm <- base::vector(mode = \"list\", length = length(gcm_Models_Scenarios))\nnames(fut_sim_rsm) <- gcm_Models_Scenarios # now we have a named list\n\ndebug_T <- fut_sim_rsm\n\nfor (idxGCM in seq(length(gcm_Models_Scenarios))) {\n\n  # GCM Model and Scenario\n  str2proc <- gcm_Models_Scenarios[idxGCM]\n  gcm_model <- substr(str2proc, 1, nchar(str2proc) - 7)\n  gcm_scenario <- substr(str2proc, nchar(str2proc) - 6 + 1, nchar(str2proc))\n\n  # Process files - tas\n  temp_or_precip <- \"Temperature\"\n  climate_file_tas <- \n    list.files(fut_sim_dir,pattern = paste0(\"tas_day_\",gcm_Models_Scenarios[idxGCM]),full.names = TRUE)\n  fut_sim_T <- gen_HRU_Climate_CSV_RSMinerve(climate_file_tas,\n                                             river_name,\n                                             temp_or_precip,\n                                             subbasins_hru_shp,\n                                             fut_sim_start,\n                                             fut_sim_end,\n                                             obs_freq,\n                                             climate_data_type,\n                                             crs_project)\n  \n  # Process files - pr\n  temp_or_precip <- \"Precipitation\"\n  climate_file_pr <- \n    list.files(fut_sim_dir,pattern = paste0(\"pr_day_\",gcm_Models_Scenarios[idxGCM]),full.names = TRUE)\n  fut_sim_P <- gen_HRU_Climate_CSV_RSMinerve(climate_file_pr,\n                                             river_name,\n                                             temp_or_precip,\n                                             subbasins_hru_shp,\n                                             fut_sim_start,\n                                             fut_sim_end,\n                                             obs_freq,\n                                             climate_data_type,\n                                             crs_project)\n  \n  # Final dataframe\n  fut_sim_rsm[[idxGCM]] <- fut_sim_T %>% \n    tibble::add_column(fut_sim_P %>% dplyr::select(-Station),.name_repair = 'unique')\n  # Write result to disk\n  readr::write_csv(fut_sim_rsm[[idxGCM]],\n                   paste0(model_dir,climate_data_type,\"_\",gcm_model,\"_\",\n                          gcm_scenario,\"_\",river_name,\"_\",fut_sim_start,\"_\",\n                          fut_sim_end,\".csv\"),\n                   col_names = FALSE)\n}\n\nIn a final step, we can produce the bias corrected future climate scenarios with the following code.\n\n# Preparations\n## HRUs\nsubbasins_hru_shp <- \n  sf::st_read(paste0(gis_path,gauge_code,'_hru','.shp'))\nn_hru <- subbasins_hru_shp %>% nrow()\nhru_names <- subbasins_hru_shp$name\n\n# ================\n# Prepare hist_obs\n# ================\nclimate_data_type <- \"hist_obs\"\nhist_obs_path <- paste0(model_dir,climate_data_type,\"_rsm.csv\")\nhist_obs_orig <- hist_obs_path %>% \n  readr::read_csv(col_types = cols(.default = col_character())) %>%\n  dplyr::select(-Station,-Q)\n\n# Extract data by groups and convert T to deg. K\nhist_obs_T <- hist_obs_orig[,1:n_hru] %>% slice(-1:-7) %>% \n  type_convert() %>% \n  mutate(across(.cols = everything(), ~ . + 273.15))\nhist_obs_P <- \n  hist_obs_orig[, (n_hru + 1):(2 * n_hru)] %>% \n  slice(-1:-7) %>%  \n  type_convert()\n\n# Fix row names\nnames(hist_obs_T) <- hru_names\nnames(hist_obs_P) <- hru_names\n\nhist_obs_T_df <- hist_obs_T %>% as.data.frame()\nrow.names(hist_obs_T_df) <- hist_obs_dates$Date %>% as.character()\nhist_obs_P_df <- hist_obs_P %>% as.data.frame()\nrow.names(hist_obs_P_df) <- hist_obs_dates$Date %>% as.character()\n\n# ================\n# Prepare hist_sim\n# ================\nhist_sim_T_list <- \n  base::vector(mode = \"list\", length = length(gcm_Models))\nhist_sim_P_list <- \n  base::vector(mode = \"list\", length = length(gcm_Models))\nnames(hist_sim_T_list) <- \n  gcm_Models # now we have a named list\nnames(hist_sim_P_list) <- \n  gcm_Models # now we have a named list\n\nfor (idxGCM in 1:length(gcm_Models)) {\n  hist_sim_path <- \n    list.files(model_dir,\n               pattern = paste0(\"hist_sim_\",gcm_Models[idxGCM]),full.names = TRUE)\n  hist_sim_orig <- hist_sim_path %>% \n    readr::read_csv(col_types = cols(.default = col_character())) %>% \n    dplyr::select(-Station)\n  \n  hist_sim_T <- hist_sim_orig[,1:n_hru] %>% slice(-1:-7) %>% \n    type_convert() %>% \n    mutate(across(.cols = everything(), ~ . + 273.15))\n  hist_sim_P <- hist_sim_orig[,(n_hru+1):(2*n_hru)] %>% \n    slice(-1:-7) %>% \n    type.convert()\n  \n  # Fix row names\n  names(hist_sim_T) <- hru_names\n  names(hist_sim_P) <- hru_names\n  \n  hist_sim_T_df <- hist_sim_T %>% as.data.frame()\n  row.names(hist_sim_T_df) <- hist_sim_dates$Date %>% as.character()\n  hist_sim_P_df <- hist_sim_P %>% as.data.frame()\n  row.names(hist_sim_P_df) <- hist_sim_dates$Date %>% as.character()\n  \n  hist_sim_T_list[[idxGCM]] <- hist_sim_T_df\n  hist_sim_P_list[[idxGCM]] <- hist_sim_P_df\n}\n\n# ===============\n# Prepare fut_sim\n# ===============\nfut_sim_T_list <- base::vector(mode = \"list\", length = length(gcm_Models_Scenarios))\nfut_sim_P_list <- base::vector(mode = \"list\", length = length(gcm_Models_Scenarios))\nnames(fut_sim_T_list) <- gcm_Models_Scenarios # now we have a named list\nnames(fut_sim_P_list) <- gcm_Models_Scenarios\nfut_sim_T_list_bcsd <- fut_sim_T_list\nfut_sim_P_list_bcsc <- fut_sim_P_list\n\nfor (idxGCM in 1:length(gcm_Models_Scenarios)) {\n  fut_sim_orig_path <- \n    list.files(model_dir,pattern = \n                 paste0(\"fut_sim_\",gcm_Models_Scenarios[idxGCM]),full.names = TRUE)\n  fut_sim_orig <- fut_sim_orig_path %>% \n    readr::read_csv(col_types = cols(.default = col_character())) %>% \n    dplyr::select(-Station)\n  \n  fut_sim_T <- fut_sim_orig[,1:n_hru] %>% \n    slice(-1:-7) %>% \n    type_convert() %>% \n    mutate(across(.cols = everything(), ~ . + 273.15))\n  fut_sim_P <- fut_sim_orig[,(n_hru+1):(2*n_hru)] %>% \n    slice(-1:-7) %>% \n    type.convert()\n  \n  # Fix row names\n  names(fut_sim_T) <- hru_names\n  names(fut_sim_P) <- hru_names\n  \n  fut_sim_T_df <- fut_sim_T %>% as.data.frame()\n  row.names(fut_sim_T_df) <- fut_sim_dates$Date %>% as.character()\n  fut_sim_P_df <- fut_sim_P %>% as.data.frame()\n  row.names(fut_sim_P_df) <- fut_sim_dates$Date %>% as.character()\n  \n  fut_sim_T_list[[idxGCM]] <- fut_sim_T_df\n  fut_sim_P_list[[idxGCM]] <- fut_sim_P_df\n}\n\n# ===================\n# Do quantile mapping\n# ===================\n\n# --- Debugging\nfut_sim_rsm_qmapped <- \n  base::vector(mode = \"list\", length = length(gcm_Models_Scenarios))\nnames(fut_sim_rsm_qmapped) <- gcm_Models_Scenarios # now we have a named list\n# -----\n\nfor (idxGCM in 1:length(gcm_Models_Scenarios)) {\n  \n  # Preparation\n  str2proc <- gcm_Models_Scenarios[idxGCM]\n  gcm_model <- substr(str2proc, 1, nchar(str2proc) - 7)\n  gcm_scenario <- substr(str2proc, nchar(str2proc) - 6 + 1, nchar(str2proc))\n  \n  # Bias correction\n  hist_sim_T_df_gcmModel <- hist_sim_T_list[[gcm_model]]\n  hist_sim_P_df_gcmModel <- hist_sim_P_list[[gcm_model]]\n  \n  fut_sim_T_df_gcmModel <- fut_sim_T_list[[idxGCM]]\n  fut_sim_P_df_gcmModel <- fut_sim_P_list[[idxGCM]]\n  \n  qmap_param_T_gcm <- fitQmap(hist_obs_T_df, hist_sim_T_df_gcmModel, method = \"QUANT\")\n  qmap_param_P_gcm <- fitQmap(hist_obs_P_df, hist_sim_P_df_gcmModel, method = \"QUANT\")\n  \n  # T bias correction\n  fut_sim_T_df_gcmModel_qmapped <- \n    doQmap(fut_sim_T_df_gcmModel,qmap_param_T_gcm)\n  # Fill 0s where present. Occasionally, there are 0s resulting from the quantile mapping these \n  # are ironed out here by filling the missing values using the ones from the preceeding observation. \n  fut_sim_T_df_gcmModel_qmapped[fut_sim_T_df_gcmModel_qmapped == 0] <- NA\n  fut_sim_T_df_gcmModel_qmapped <- \n    zoo::na.locf(fut_sim_T_df_gcmModel_qmapped)\n  # P bias correction\n  fut_sim_P_df_gcmModel_qmapped <- \n    doQmap(fut_sim_P_df_gcmModel,qmap_param_P_gcm)\n\n  # go back to tibble and convert back to deg. C\n  fut_sim_T_gcmModel_qmapped <- \n    fut_sim_T_df_gcmModel_qmapped %>% as_tibble() %>% \n    add_column(Date = fut_sim_dates$Date,.before = 1)\n  fut_sim_T_gcmModel_qmapped <- \n    fut_sim_T_gcmModel_qmapped %>% mutate(across(-Date, ~ . - 273.15))\n  fut_sim_P_gcmModel_qmapped <- \n    fut_sim_P_df_gcmModel_qmapped %>% as_tibble() %>% \n    add_column(Date = fut_sim_dates$Date,.before = 1)  \n  \n  fut_sim_T_list[[idxGCM]] <- fut_sim_T_gcmModel_qmapped\n  fut_sim_P_list[[idxGCM]] <- fut_sim_P_gcmModel_qmapped\n  \n  # Export to .csv-file\n  fut_sim_qmapped <- \n    fut_sim_T_gcmModel_qmapped %>% #dplyr::select(-Date) %>% \n    tibble::add_column(fut_sim_P_gcmModel_qmapped %>% \n                         dplyr::select(-Date),.name_repair = \"universal\") %>% \n    mutate(across(.cols = everything(),~ as.character(.))) %>% \n    dplyr::select(-Date)\n  \n  # --- Debugging\n  fut_sim_rsm_qmapped[[idxGCM]] <- fut_sim_qmapped\n  # ---\n  \n  fut_sim_orig_path <- \n    list.files(model_dir,pattern = paste0(\"fut_sim_\",gcm_Models_Scenarios[idxGCM]),\n               full.names = TRUE)\n  fut_sim_orig <- \n    fut_sim_orig_path %>% readr::read_csv(col_types = cols(.default = col_character())) \n  \n  fut_sim_orig_header <- fut_sim_orig %>% dplyr::select(-Station) %>% \n    dplyr::slice(1:7,) \n  fut_sim_qmapped <- fut_sim_orig_header  %>% bind_rows(fut_sim_qmapped) %>% \n    add_column(Station = fut_sim_orig$Station,.before = 1)\n\n  # Write result to disk\n  climate_data_type <- \"fut_sim_bcsd\"\n  readr::write_csv(fut_sim_qmapped,\n                   paste0(model_dir,climate_data_type,\"_\",gcm_model,\"_\",\n                          gcm_scenario,\"_\",river_name,\"_\",fut_sim_start,\"_\",\n                          fut_sim_end,\".csv\"),\n                   col_names = FALSE)\n}\n\n\n\n\n\nBuisán, Samuel T., Michael E. Earle, José Luı́s Collado, John Kochendorfer, Javier Alastrué, Mareile Wolff, Craig D. Smith, and Juan I. López-Moreno. 2017. “Assessment of Snowfall Accumulation Underestimation by Tipping Bucket Gauges in the Spanish Operational Network.” Atmospheric Measurement Techniques 10 (3): 1079–91.\n\n\n“CHELSA-W5e5 V1.0: W5e5 V1.0 Downscaled with CHELSA V2.0.” 2022. ISIMIP Repository. https://doi.org/10.48364/ISIMIP.836809.2.\n\n\nFeigenwinter, I., S. Kotlarski, A. Casanueva, A. M. Fischer, C. Schwierz, and M. A. Liniger. 2018. “Exploring Quantile Mapping as a Tool to Produce User-Tailored Climate Scenarios for Switzerland.” Technical Report 270. Federal Office of Meteorology; Climatology MeteoSwiss.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nKarger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-resolution monthly precipitation and temperature time series from 2006 to 2100.” Scientific Data 7 (1): 248. https://doi.org/10.1038/s41597-020-00587-y.\n\n\nKarger, Dirk Nikolaus, Adam M. Wilson, Colin Mahony, Niklaus E. Zimmermann, and Walter Jetz. 2021. “Global daily 1 km land surface precipitation based on cloud cover-informed downscaling.” Scientific Data 8 (1): 307. https://doi.org/10.1038/s41597-021-01084-6.\n\n\nKotlarski, S., K. Keuler, O. B. Christensen, A. Colette, M. Déqué, A. Gobiet, K. Goergen, et al. 2014. “Regional climate modeling on European scales: a joint standard evaluation of the EURO-CORDEX RCM ensemble.” Geoscientific Model Development 7 (4): 1297–1333. https://doi.org/10.5194/gmd-7-1297-2014.\n\n\nO’Neill, Brian C., Claudia Tebaldi, Detlef P. van Vuuren, Veronika Eyring, Pierre Friedlingstein, George Hurtt, Reto Knutti, et al. 2016. “The Scenario Model Intercomparison Project (ScenarioMIP) for CMIP6.” Geoscientific Model Development 9 (9): 3461–82. https://doi.org/10.5194/gmd-9-3461-2016.\n\n\nRiahi, Keywan, Detlef P. van Vuuren, Elmar Kriegler, Jae Edmonds, Brian C. O’Neill, Shinichiro Fujimori, Nico Bauer, et al. 2017. “The Shared Socioeconomic Pathways and their energy, land use, and greenhouse gas emissions implications: An overview.” Global Environmental Change 42: 153–68. https://doi.org/10.1016/j.gloenvcha.2016.05.009."
  },
  {
    "objectID": "hydrological_modeling.html",
    "href": "hydrological_modeling.html",
    "title": "Part III: Hydrological Modeling & Applications",
    "section": "",
    "text": "This part of the book focusses on different types of hydrological modeling approaches and applications. The Chapter on hydrological modeling using rainfall-runoff models introduces in a hands-on manner modeling using the free-tu-use RSMinerve Software Suite. These types of models are foundational for example for basin planning exercises where tradeoffs between water for different sectoral allocations need to be quantified in a specific context.\nSuch models are also important for detailed climate impact studies regularly used to study these. The idea is simple, i.e., to use available climate model output over the 21st century as forcing and investigate changes in the hydrographs at stations of interest over time. When different models and scenarios are run, an band of uncertainty can be specified which is relevant in any decision-making context.\nFinally, the design of hydropower infrastructure depends on hydrological assessments with such types of models. The model outputs, i.e., simulated (modelled) discharge at a particular location, can be used to compute cummulative flow duration curves which are essential for the assessment of the hydropower potential and critically inform infrastructure sizing.\nThe relevance of these types of models for the water planners and managers in the global drylands cannot be overstated and therefore, one of the primary goals of this course is to familiarze the students well with such types of models.\nThe Chapter on long-term hydrological modeling using the Budyko framework looks at the greater semi-arid Central Asia region as compared to individual catchments. It is at this scale and over a large number of smaller catchments where interesting steady-state patterns of the partioning of available water into evporation and runoff can be studied, under current and future climate states. Among other applications, such types of models can help to inform the large-scale questions, also with regard to the current and future inter-state water distribution.\nFinally, the Chapter on time series modeling using predictive inference discusses models"
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-budyko-introduction",
    "href": "long_term_water_balance_modeling.html#sec-budyko-introduction",
    "title": "9  Long-term Water Balance Modeling",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nThe general water balance of a catchment can be written as\n\\[\n\\Delta S = P - E - Q\n\\qquad(9.1)\\]\nwhere \\(\\Delta S\\) is net storage change in millimeter [mm], \\(P\\) is precipitation in mm, \\(E\\) is evaporation in mm, and \\(Q\\) is specific discharge in mm. Evaporation is the phenomenon by which a substance is converted from its liquid into its vapor phase, independently of where it lies in nature (Miralles et al. 2020). This definition of evaporation encompasses evaporation from inside leaves (transpiration), evaporation from bare soils, evaporation from intercepted precipitation (interception loss), evaporation from open water surfaces, and finally, evaporation over ice- and snow-covered surfaces (often referred to as sublimation).\nOver the period of a hydrological year and longer time scales, we expect \\(\\Delta S\\) to be 0 since neither water storage nor destorage happen over longer periods. This would of course not be true for catchments where for example man-made storage infrastructure was built over the period under consideration or for catchments with ongoing glacier melt over a prolonged time. If \\(\\Delta S = 0\\), the above Equation @ref(eq:WB1) can be rewritten as\n\\[\nQ = P - E\n\\qquad(9.2)\\]\nDividing by \\(P\\), we get\n\\[\n\\frac{Q}{P} = 1 - \\frac{E}{P}\n\\qquad(9.3)\\]\nwhere \\(Q/P\\) can be called the runoff index and \\(E/P\\) is the evaporation index or evaporative fraction.\nFor a catchment, annual mean \\(E\\) and \\(Q\\) are governed by total water supply \\(P\\) and the total available energy which is normally expressed as potential evaporation \\(E_{pot}\\) and which denotes the (atmospheric) water demand. If \\(E_{pot}\\) is small, the discharge \\(Q\\) is normally bigger than evaporation \\(E\\). Similarly, if the available radiative energy is very high, the water demand \\(E_{pot}\\) is very large and \\(Q<<E\\) (Arora 2002). \\(E_{pot}\\) and \\(P\\) are thus the key determinants of annual or longer timescale runoff and evaporation rates. Michael Budyko has termed the ratio \\(E_{pot} / P\\) aridity index (Budyko 1974).\nAs explained above, water demand is determined by energy. Solar radiation is the primary energy source for the earth-atmosphere system and the key driver of the hydrological cycle. At the earth’s surface, the net radiative flux \\(R_N\\) is the energy that is available for a) heating and cooling of the soil (ground heat flux), b) changing the phase of water (latent heat flux), and c) heating or cooling air in the boundary layer thus causing atmospheric dynamics (sensible heat flux).\nThis can be formalized with the following relationship\n\\[\nR_{N} = H_{S} + H_{L} + \\Delta H_{G}\n\\qquad(9.4)\\]\nwhere \\(R_{N}\\) is the net radiation [in W/m2 = kg/s3], \\(H_{S}\\) is the upward sensible heat flux, \\(H_{L}\\) is the latent heat flux and \\(\\Delta H_{G}\\) the net ground heat flux. The latent heat flux is directly proportional to evaporation \\(E\\). Thus, \\(H_{L} = L \\cdot E\\) where \\(L = 2.5 \\cdot 10^{6}\\) J/kg [= m2/s2] is the latent heat of vaporization and \\(E\\) is the actual evaporation in [m/s]. As in the case of the water balance, at the annual or longer time scales, we can neglect the heat storage effect in the ground and get\n\\[\nR_{N} = H_{S} + L \\cdot E\n\\qquad(9.5)\\]\nWith the Bowen ratio defined as the fraction of the sensible heat flux divided by the latent heat flux, i.e.\n\\[\n\\gamma = \\frac{H_{S}}{H_{L}} = \\frac{H_{S}}{L \\cdot E }\n\\qquad(9.6)\\]\nand by rearranging the terms, the long-term energy balance in Equation Equation 9.5 can simply be rewritten as\n\\[\nR_{N} = (1 + \\gamma)L E\n\\qquad(9.7)\\]\nUsing the fact that \\(R_{N} = L E_{pot}\\), where \\(E_{pot}\\) is the potential evaporation, and dividing by precipitation, we can rewrite the above Equation 9.7 as\n\\[\n\\frac{E_{pot}}{P} = (1 + \\gamma) \\frac{E}{P}\n\\qquad(9.8)\\]\nwhere the left-hand side is called the aridity index, i.e. \\(\\phi = E_{pot}/P\\) and \\(E/P\\) is called the evaporative fraction or evaporation index as mentioned above. With this, Equation 9.8 from above can be written as a function of the Bowen ratio and the aridity index, i.e.\n\\[\n\\frac{E}{P} = 1 - \\frac{Q}{P} = \\frac{\\phi}{(1 + \\gamma)}\n\\qquad(9.9)\\]\n\\(Q/P\\) is again the runoff index. Since the Bowen ratio is also water supply and energy demand limited, it too is a function of the aridity index and we can thus rewrite Equation 9.9 as\n\\[\n\\frac{E}{P} = \\frac{\\phi}{1 + f(\\phi)} = F[\\phi]\n\\qquad(9.10)\\]\nThe Budyko relationship thus allows for a simple parameterization of how the aridity index \\(\\phi\\) controls the long-term mean partitioning of precipitation into stream-flow and evaporation and it is capable of capturing the behavior of thousands of catchments around the world. This explains its growing popularity over recent years (Berghuijs, Gnann, and Woods 2020).\n\n\n\nFigure @ref(fig:budykoSpace) shows a plot of data from catchments in the US for which consistent long-term hydro-climatological data records are available. Individual catchments’ aridity indices are plotted against evaporative fractions, averaged over many years. The catchment data plots along the Budyko curve in the two-dimensional Budyko space as indicated in the Figure where the Budyko curve is defined as\n\\[\\begin{equation}\n  \\frac{E}{P} = \\left[ \\frac{E_{pot}}{P} \\text{tanh} \\left( \\frac{P}{E_{pot}} \\right) \\left( 1 - \\text{exp} \\left( - \\frac{E_{pot}}{P} \\right) \\right) \\right]^{1/2}\n  (\\#eq:OriginalBudykoCurveEquation)\n\\end{equation}\\]\nThis non-parametric relationship between the aridity index and the evaporative fraction was developed by M. Budyko (Budyko 1951).\nThe Budyko space is delineated by the demand and supply limits. Catchments within the space should theoretically fall below the supply limit (\\(E/P = 1\\)) and the demand limit (\\(E/E_{pot} = 1\\)), but tend to approach these limits under very arid or very wet conditions (Berghuijs, Gnann, and Woods 2020). The data from the US shows that a large percentage of in-between catchment variability can be explained by the Budyko curve. After the seminal work Budyko in the last century, the evidence for a strong universal relationship between aridity and evaporative fraction via the Budyko curve has since grown. As catchment hydrology still lacks a comprehensive theory that could explain this simple behavior across diverse catchments Gentine et al. (2012), the ongoing debate about the the underlying reasons for this relationship continues (see e.g. (Padron et al. 2017; Berghuijs, Gnann, and Woods 2020)).\nWhile almost all catchments plot within a small envelope of the original Budyko curve, systematic deviations are nevertheless observed from the original Budyko curve. Several new expressions for \\(F[\\phi]\\) were therefore developed to describe the long-term catchment water balance with one parameter (see e.g. Budyko (1974); Sposito (2017); Choudhury (1999)). One popular equation using only 1 parameter is the Choudhury equation which relates the aridity index \\(\\phi\\) to the evaporative fraction \\(E/P\\) in the following way\n\\[\\begin{equation}\n  \\frac{E}{P} = \\left[ 1 + \\left( \\frac{E_{pot}}{P} \\right) ^{-n} \\right]^{1/n}\n  (\\#eq:Choudhury1)\n\\end{equation}\\]\nwhere \\(n\\) is a catchment-specific parameter which accounts for factors such as vegetation type and coverage, soil type and topography, etc. (see e.g. Zhang et al. (2015) for more information). In other words, \\(n\\) integrates the net effects of all controls of of the evaporative fraction other than aridity. The Figure @ref(fig:ChoudhuryEquationStateSpace) shows the control of \\(n\\) over the shape of the Budyko Curve."
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-budyko-data-and-methods",
    "href": "long_term_water_balance_modeling.html#sec-budyko-data-and-methods",
    "title": "9  Long-term Water Balance Modeling",
    "section": "9.2 Data and Methods",
    "text": "9.2 Data and Methods\n\n9.2.1 Data\nA large number of geospatial data were collected for the Central Asia region. The domain of interest was defined as 55 deg. E - 85. deg. E and 30 deg. N - 50 deg. N.. Shapefiles from the large river basin were retrieved from the Global Runoff Data Center and extracted for the following basins: Amu Darya, Chu, Issy Kul, Murghab-Harirud, Syr Darya and Talas. Where necessary, the polygons of the downstream flat areas were corrected to account for man-made water transfers via large canal systems and corresponding flow alterations across basins there. These large river basins define the area of interest (AOI).\nFor the selected basins, the WMOBB River Network data was extracted from the layers wmobb_rivnets_Q09_10 (containing line sections representing an upland area above 4’504 km2), wmobb_rivnets_Q08_09 (containing line sections representing an upland area between 1’150 km2 and 4’504 km2) and wmobb_rivnets_Q07_08 (containing line sections representing an upland area above between 487 and 1’150 km2) (GRDC, Koblenz, Germany: Federal Institute of Hydrology (BfG). 2020). Permanent water bodies and courses were taken from the global HydroLakes Database (Messager et al. 2016). Information on land cover were taken from the Copernicus Global Land Service: Land Cover 100m: collection 3: epoch 2019: Globe data (Buchhorn et al. 2019). The NASA SRTM digital elevation model 1 Arc-second (30 m) global product was used as a DEM (“NASA Shuttle Radar Topography Mission (SRTM)(2013)” 2013).\nIn total, data from 277 gauging stations from Afghanistan, Kyrgyzstan, Kazakhstan, Uzbekistan and Tajikistan could be obtained from the local Hydrometeorological Organization, public reports and the Soviet compendia Surface Water Resources, Vol 14 Issues 1 and 3 . Except for the Afghan stations, all stations were manually located in a Geographic Information System (GIS) using the relevant Soviet Military Topographic maps (1:200’000) from the corresponding region. The maps were downloaded from https://maps.vlasenko.net and subsequently geo-referenced in QGIS (QGIS Development Team 2021). Data from northern Afghan rivers’ stream flow characteristics and the location of these gauging stations was taken from (Olson and Williams-Sether 2010).\nFor each gauge, the contributing area was delineated in R with the WhiteboxTools v2.0.0 and long-term norm mean discharge was obtained over variable observation periods between 1900 and 2018 was acquired. For a few selected stations, monthly and decadal time series data are available over the entire observational record. The FLO1K, global maps of mean, maximum and minimum annual stream flow at 1 km resolution from 1960 through 2015 were retrieved (Barbarossa et al. 2018). The goodness of the FLO1K product in the Central Asia domain was validated at the locations of the 277 gauges through linear regression.\nGeospatial information on glaciers was taken from the Randolph Glacier Inventory (RGI) 6.0. Information from 16’617 glaciers was retrieved, together with glacier length, thickness and glacier thinning rates Hugonnet et al. (2021).\nThe CHELSA V21 global daily high-resolution climatology, available from 01-01-1979 until 31-12-2011 was processed over the Central Asia domain to map climate trends, including on temperature, precipitation, snow fraction. The data is available upon request from this site: https://chelsa-climate.org Karger et al. (2021). The CHELSA V21 product is corrected for snow undercatch in the high elevation ranges and thus is able to better represent actual high mountain precipitation than other available global climatologies (beck_2020?). The aridity index (AI) fields were taken from the bio-climate CHELSA V21 data set and compared with the CGIAR AI product (trabucco_2019?). Data on an additional 70 bio-climatic indicators were downloaded from the CHELSA V21 1980 - 2010 climatology and statistics extracted for each of the 277 gauged catchments, together with the AI.\nHigh-resolution crop disaggregated irrigated areas were mapped over the entire Central Asia domain (Ragettli, Herberz, and Siegfried 2018). Like this 30 m crop maps were produced with Google Earth Engine using unsupervised classification for the years 2016 - 2020. Vector information on the irrigation systems in the Chu and Talas River basins as well as from the Uzbek Fergana Oblast, including the land cadaster there, are available.\nFinally, data from the GOODD data set was used to retrieve information from 88 dams in the region of interest (Mulligan, Soesbergen, and Sáenz 2020).\n\n\n9.2.2 Methods\nA strategy for hydrological modeling of the regional Central Asian hydrology using the Budyko framework was devised. The Budyko principle posits that, over the long-run, runoff at a particular location is governed by the long-term availability of water (supply) and energy (demand) there (Budyko M., 1974). Under this assumption, the evaporative fraction of a basin, i.e. the long-term mean actual evaporation divided by long-term mean precipitation, can be expressed as a function of the aridity index (long-term mean potential evaporation divided by long-term mean precipitation)."
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-budyko-results",
    "href": "long_term_water_balance_modeling.html#sec-budyko-results",
    "title": "9  Long-term Water Balance Modeling",
    "section": "9.3 Results",
    "text": "9.3 Results"
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-discussion-and-conclusions",
    "href": "long_term_water_balance_modeling.html#sec-discussion-and-conclusions",
    "title": "9  Long-term Water Balance Modeling",
    "section": "9.4 Discussion and Conclusions",
    "text": "9.4 Discussion and Conclusions\n\n\n\n\nArora, Vivek K. 2002. “The Use of the Aridity Index to Assess Climate Change Effect on Annual Runoff.” Journal of Hydrology 265 (1): 164–77. https://doi.org/https://doi.org/10.1016/S0022-1694(02)00101-4.\n\n\nBarbarossa, Valerio, Mark A. J. Huijbregts, Arthur H. W. Beusen, Hylke E. Beck, Henry King, and Aafke M. Schipper. 2018. “Flo1k, Global Maps of Mean, Maximum and Minimum Annual Streamflow at 1 Km Resolution from 1960 Through 2015.” Scientific Data 5 (1): 180052. https://doi.org/10.1038/sdata.2018.52.\n\n\nBerghuijs, W. R., S. J. Gnann, and R. A. Woods. 2020. “Unanswered Questions on the Budyko Framework.” Hydrological Processes.\n\n\nBuchhorn, M., B. Smets, L. Bertels, B. De Roo, M. Lesiv, N. E. Tsendbazar, M. Herold, and S. Fritz. 2019. “Copernicus Global Land Service: Land Cover 100m: Collection 3: Epoch 2019: Globe.”\n\n\nBudyko, M. I. 1951. “On Climatic Factors of Runof (in Russian).” Problemy Fiz Geeografii 16: 41–48.\n\n\n———. 1974. Climate and Life. Academic Press.\n\n\nChoudhury, B. J. 1999. “Evaluation of an Empirical Equation for Annual Evaporation Using Field Observations and Results from a Biophysical Model.” Journal of Hydrology 216: 99–110.\n\n\nFarinotti, Daniel, Matthias Huss, Johannes J. Fürst, Johannes Landmann, Horst Machguth, Fabien Maussion, and Ankur Pandit. 2019. “A Consensus Estimate for the Ice Thickness Distribution of All Glaciers on Earth.” Nature Geoscience 12 (3): 168–73. https://doi.org/10.1038/s41561-019-0300-3.\n\n\nGentine, P., P. D’Odorico B. R. Lintner, G. Sivandran, and G. Salvucci. 2012. “Interdependence of Climate, Soil, and Vegetation as Constrained by the Budyko Curve.” Geophysical Research Letters 39 (19).\n\n\nGLIMS, and NSIDC. 2005, updated 2018. Global Land Ice Measurements from Space Glacier Database. Compiled and made available by the international GLIMS community and the National Snow and Ice Data Center, Boulder CO, U.S.A. DOI:10.7265/N5V98602.\n\n\nGRDC, Koblenz, Germany: Federal Institute of Hydrology (BfG). 2020. “Major River Basins of the World / Global Runoff Data Centre, GRDC. 2nd, Rev. Ext. Ed.” Shape.\n\n\nHugonnet, Romain, Robert McNabb, Etienne Berthier, Brian Menounos, Christopher Nuth, Luc Girod, Daniel Farinotti, et al. 2021. “Accelerated Global Glacier Mass Loss in the Early Twenty-First Century.” Nature 592 (7856): 726–31. https://doi.org/10.1038/s41586-021-03436-z.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nKarger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-resolution monthly precipitation and temperature time series from 2006 to 2100.” Scientific Data 7 (1): 248. https://doi.org/10.1038/s41597-020-00587-y.\n\n\nKarger, Dirk Nikolaus, Adam M. Wilson, Colin Mahony, Niklaus E. Zimmermann, and Walter Jetz. 2021. “Global daily 1 km land surface precipitation based on cloud cover-informed downscaling.” Scientific Data 8 (1): 307. https://doi.org/10.1038/s41597-021-01084-6.\n\n\nMessager, M. L., B. Lehner, Grill G., I. Nedeva, and O. Schmitt. 2016. “Estimating the Volume and Age of Water Stored in Global Lakes Using a Geo-Statistical Approach.” Nature Communications 13603.\n\n\nMiralles, D. G., W. Brutsaert, A. J. Dolman, and J. H. Gash. 2020. “On the Use of the Term \"Evapotranspiration\".” Water Resources Research 56 (https://doi.org/10.1029/2020WR028055).\n\n\nMulligan, Mark, Arnout van Soesbergen, and Leonardo Sáenz. 2020. “GOODD, a Global Dataset of More Than 38,000 Georeferenced Dams.” Scientific Data 7 (1): 31. https://doi.org/10.1038/s41597-020-0362-5.\n\n\n“NASA Shuttle Radar Topography Mission (SRTM)(2013).” 2013. NASA. https://earthdata.nasa.gov/learn/articles/nasa-shuttle-radar-topography-mission-srtm-version-3-0-global-1-arc-second-data-released-over-asia-and-australia.\n\n\nOlson, S. A., and T. Williams-Sether. 2010. “Streamflow Characteristics at Streamgages in Northern Afghanistan and Selected Locations.” U.S. Geological Survey Data Series 529. USGS.\n\n\nPadron, R. S., L. Gudmundsson, P. Greve, and S. Seneviratne. 2017. “Largescale Controls of the Surface Water Balance over Land: Insights from a Systematic Review and Meta-Analysis.” Water Resources Research.\n\n\nQGIS Development Team. 2021. QGIS Geographic Information System. QGIS Association.\n\n\nRagettli, Silvan, Timo Herberz, and Tobias Siegfried. 2018. “An Unsupervised Classification Algorithm for Multi- Temporal Irrigated Area Mapping in Central Asia.” Remote Sensing 10 (11): 1823. https://doi.org/10.3390/rs10111823.\n\n\nSposito, Garrison. 2017. “Understanding the Budyko Equation.” Water 9 (4): 236. https://doi.org/10.3390/w9040236.\n\n\nZhang, D., Z. Cong, G. Ni, D. Yang, and S. Hu. 2015. “Effects of snow ratio on annual runoff within the Budyko framework.” Hydrology and Earth System Sciences 19 (4): 1977–92. https://doi.org/10.5194/hess-19-1977-2015."
  },
  {
    "objectID": "hydraulic_hydrological_modeling.html",
    "href": "hydraulic_hydrological_modeling.html",
    "title": "8  Hydrological-Hydraulic Modeling",
    "section": "",
    "text": "8.0.1 Pre"
  },
  {
    "objectID": "modeling_using_predictive_inference.html",
    "href": "modeling_using_predictive_inference.html",
    "title": "10  Modeling Using Predictive Inference",
    "section": "",
    "text": "Budyko chapter with critical results there."
  },
  {
    "objectID": "real_world_examples.html#sec-ieasy_hydro",
    "href": "real_world_examples.html#sec-ieasy_hydro",
    "title": "11  Real World Examples",
    "section": "11.1 iEasyHydro Digital Assistant",
    "text": "11.1 iEasyHydro Digital Assistant"
  },
  {
    "objectID": "real_world_examples.html#sec-hydro4u-count4d",
    "href": "real_world_examples.html#sec-hydro4u-count4d",
    "title": "11  Real World Examples",
    "section": "11.2 Hydro4U Count4D Water-Energy-Food Accounting Software",
    "text": "11.2 Hydro4U Count4D Water-Energy-Food Accounting Software"
  },
  {
    "objectID": "real_world_examples.html#sec-challenges-in-operational-deployment",
    "href": "real_world_examples.html#sec-challenges-in-operational-deployment",
    "title": "11  Real World Examples",
    "section": "11.3 Challenges in Operational Model Deployment",
    "text": "11.3 Challenges in Operational Model Deployment"
  },
  {
    "objectID": "appendix_a_free_software.html#literature",
    "href": "appendix_a_free_software.html#literature",
    "title": "Appendix A — Software",
    "section": "A.1 Literature",
    "text": "A.1 Literature\nMany authors of scientific literature are on the web platform researchgate where they can privately share their work with students (users need to register for an account)."
  },
  {
    "objectID": "appendix_a_free_software.html#sec-open-resouces-software-QGIS",
    "href": "appendix_a_free_software.html#sec-open-resouces-software-QGIS",
    "title": "Appendix A — Software",
    "section": "A.2 QGIS",
    "text": "A.2 QGIS\nQGIS is a free and open source Geographical Information System that offers very similar tools as their commercial counterparts. The latest version of QGIS can be downloaded from the QGIS website. We recommend to install the stable long-term support version (installation guide).\n\nA.2.1 Resources for learning QGIS\nA general tutorial for beginners is the QGIS training manual. It includes a short chapter on the use of QGIS for hydrological analysis (Chapter 17.16). For this course you should be familiar with the QGIS window and know the difference between raster and vector data. If you have used QGIS or a similar GIS software before you will not need to do a tutorial prior to this course."
  },
  {
    "objectID": "appendix_a_free_software.html#sec-open-resouces-software-R",
    "href": "appendix_a_free_software.html#sec-open-resouces-software-R",
    "title": "Appendix A — Software",
    "section": "A.3 R and RStudio",
    "text": "A.3 R and RStudio\nR is a free and open source statistical programming language. It’s large user community ensure active development and up-to-date help resources available on the internet. RStudio is a free user interface for R. To install R and RStudio follow the installation guide on ModernDive - Statistical Inference via Data Science.\nFor the bare beginners, also with regard to programming, the book Hands-On Programming with R is an excellent start\n\nA.3.1 Resources for learning R and R studio\n\n“Help! I’m new to R and RStudio and I need to learn them! What do I do?” If you’re asking yourself this, this book is for you: ModernDive - Statistical Inference via Data Science.\nA thorough guide for data science in R: R for Data Science\n\n\n\nA.3.2 RS Minerve\n\nHow to download and install RS Minerve\nGo to the software download page of CREALP’s website https://www.crealp.ch/fr/accueil/outils-services/logiciels/rs-minerve/telechargement-rsm.html (last accessed March 18, 2021) and click on Version actuelle to download the latest installer for Windows as shown in Figure A.1. This will start the download process for the installer RSMinerve-install.exe.\n\n\n\nFigure A.1: Download RS Minerve from the CREALP website https://www.crealp.ch/fr/accueil/outils-services/logiciels/rs-minerve/telechargement-rsm.html (last accessed March 18, 2021).\n\n\nYou should also download the user manual (RS MINERVE user manual, written in English) and the example files used for the tutorials in the user manual (Exemple de fichiers, a zip file with data) as well as the technical manual (RS MINERVE technical manual, written in English).\nOnce the installer is downloaded, install RSMinerve with a double-click on the installer and follow the Setup guide. Open RSMinerve once you have it installed.\nBack to the prerequisites for RS Minerve modelling"
  },
  {
    "objectID": "appendix_b_riverscentralasia_r_toolbox.html",
    "href": "appendix_b_riverscentralasia_r_toolbox.html",
    "title": "Appendix B — R Toolbox riversCentralAsia",
    "section": "",
    "text": "More information can be found on Github, where the package is maintained."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-register-with-earth-explorer",
    "href": "appendix_c_quick_guides.html#sec-register-with-earth-explorer",
    "title": "Appendix C — Quick Guides",
    "section": "Earth Explorer: Register for an Account",
    "text": "Earth Explorer: Register for an Account\nIn your internet browser, navigate to https://earthexplorer.usgs.gov/. The window will look similar as in Figure C.1.\n\n\n\nFigure C.1: Start window of the Earth Explorer interface.\n\n\nClick on Login in the top right corner. This will bring you to a new window where you can click on Create New Account which will open a form where you enter your information. After verifying your account by clicking on the appropriate link that you will be sent, you can download data from the Earth Explorer interface.\n\nEE: Download SRTM Data for a Selected Region\n\nLogin to the Earth Explorer (Register if you haven’t done so before How to).\nNavigate to your area of interest in the map panel of the Earth Explorer interface.\nDraw a polygon around your area of interest by clicking on the map (see Figure C.2).\nIn the Data Set tab, look for the SRTM 1 arc-second global DEM (see Figure C.3) and select it by ticking the box next to the product name in the list.\n\n\n\n\n\nFigure C.2: Define a polygon around your area of interest by clicking on the map.\n\n\n\n\n\nFigure C.3: Search for the SRTM 1 arc-second global DEM product.\n\n\n\nVerify that the result layer(s) cover your area of interest by pressing the foot icon in the Results tab and download if you are satisfied (Figure C.4).\n\n\n\n\nFigure C.4: Verify the results of your search and download the data products you need.\n\n\nBack to the Load DEM section."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-sofware-qgis-installation-guide",
    "href": "appendix_c_quick_guides.html#sec-sofware-qgis-installation-guide",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS Installation Guide",
    "text": "QGIS Installation Guide\nIn your web browser go to https://qgis.org/en/site/forusers/download.html (see Figure C.5).\n\n\n\nFigure C.5: The QGIS download site.\n\n\nChoose the long-term support version of QGIS for your operating system (e.g. if you use a laptop with a 64-bit Windows operating system, open the Download for Windows tab and click on QGIS Standalone Installer Version 3.16 (64 bit), see Figure C.6)). Clicking on the installer will start the download.\n\n\n\nFigure C.6: The QGIS installer if you work on a 64-bit Windows operating system.\n\n\nDouble-click on the downloaded file to start the installation. Typically, SAGA GIS and GRASS GIS are installed alongside QGIS if you choose this installing option."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-qgis-window-overview",
    "href": "appendix_c_quick_guides.html#sec-qgis-window-overview",
    "title": "Appendix C — Quick Guides",
    "section": "The QGIS Window - Overview",
    "text": "The QGIS Window - Overview\nThe main parts of the QGIS window which are referenced in this tutorial, are highlighted in Figure C.7.\n\n\n\nFigure C.7: The QGIS window."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-saving-a-new-qgis-project",
    "href": "appendix_c_quick_guides.html#sec-saving-a-new-qgis-project",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Saving a New QGIS Project",
    "text": "QGIS: Saving a New QGIS Project\nOpen your QGIS and open a new QGIS project by moving your cursor on the white sheet symbol in the upper-left corner of your QGIS window (see Figure C.8).\n\n\n\nFigure C.8: Tutorial project open in QGIS LTS.\n\n\nSave the QGIS project by pressing on the disk icon and selecting a location for the file on your computer and a name for the project. You can add freely available on-line maps as background.\nBack to setting up of QGIS."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-change-project-projection-qgis",
    "href": "appendix_c_quick_guides.html#sec-change-project-projection-qgis",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Change the Projection of the QGIS Project",
    "text": "QGIS: Change the Projection of the QGIS Project\nFor this tutorial, the projection of the QGIS project should be in EPSG:32642 (i.e., UTM 42 N). Change it by clicking on the projects projection in the lower right corner of the QGIS window (see Figure C.7). In the coordinate reference system (CRS) tab of the Project Properties, select WGS84 / UTM zone 42N with ID EPSG:32642 and click OK.\nFor modeling your own sample catchment, a different UTM zone may be applicable. The map in the lower right corner of the Project Properties window shows the area for the projection in red and the area where your data is located in violet so you can visually verify that you choose an appropriate projection.\nGenerally, you want to choose the CRS so that you have minimal distortion through the projection in your area of interest. The CRS with ID EPSG:32462 suits well for all of the student exercise catchments.\nBack to the setting up of QGIS."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-install-and-activate-plugins-in-QGIS3",
    "href": "appendix_c_quick_guides.html#sec-install-and-activate-plugins-in-QGIS3",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Install and activate plugins in QGIS3",
    "text": "QGIS: Install and activate plugins in QGIS3\nNavigate to Plugins in the header toolbar and go to Manage and Install Plugins …. Search for the plugin name and go to Install Plugin to install a plugin or tick the box to the left of the plugin name in the list of plugins to activate it (see Figure C.9).\n\n\n\nFigure C.9: Plugin management window in QGIS 3.16. Install plugin with the Install Plugin button. Activate installed plugins by ticking the box in the list.\n\n\nBack to the setting up of QGIS."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-managing-panels",
    "href": "appendix_c_quick_guides.html#sec-managing-panels",
    "title": "Appendix C — Quick Guides",
    "section": "GQIS: Managing Panels Visibility in QGIS",
    "text": "GQIS: Managing Panels Visibility in QGIS\nShould one of the panels described here not be visible in your QGIS window navigate to View in your header toolbar and then to Panels. The visible panels are marked with a tick. Click on a panel name in the list to activate or deactivate it.\nBack to setting up of QGIS."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-loading-public-background-layers",
    "href": "appendix_c_quick_guides.html#sec-loading-public-background-layers",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Loading Public Background Maps",
    "text": "QGIS: Loading Public Background Maps\nYou can add freely available on-line maps as backgrounds. Note that they will only be available as long as your computer is connected to the internet. In the Browser panel (see What to do if you don’t see the Browser panel), move the cursor to XYZ Tiles, do a right-click and select New Connection. Enter a descriptive name for the map layer and one of the links below and click OK. The new map layer will appear under XYZ Tiles. By double-clicking on the layer you can add it to your Layers panel.\nBack to setting up of QGIS."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-zoom-to-layer",
    "href": "appendix_c_quick_guides.html#sec-zoom-to-layer",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Zoom to Layer",
    "text": "QGIS: Zoom to Layer\nYou can tell QGIS to zoom to the selected layer by selecting a layer in the Layers window and then on the white sheet and magnifying glass icon in the toolbar (see Figure C.10).\n\n\n\nFigure C.10: Zoom to layer.\n\n\nYou can also perform a right-click on the layer name in the Layers panel and select Zoom to layer."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-srtm-plugin",
    "href": "appendix_c_quick_guides.html#sec-srtm-plugin",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Import SRTM layers using the SRTM plugin",
    "text": "QGIS: Import SRTM layers using the SRTM plugin\nMake sure the SRTM plugin is installed (see How to). Navigate to Plugins in the header toolbar and there to SRTM Downloader. Select the SRTM Downloader. Set the boundaries of the SRTM tiles to download and press the Download button. Close the window when done. The SRTM tiles are loaded to the Layers pane.\nBack to Load DEM in QGIS section"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-merge-srtm-tiles",
    "href": "appendix_c_quick_guides.html#sec-merge-srtm-tiles",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Merge SRTM Tiles to a Single Layer",
    "text": "QGIS: Merge SRTM Tiles to a Single Layer\nNavigate to Raster in the header toolbar and then to Miscellaneous and Merge…. In the Merge window that opens, select the input layers by clicking on the … button. Tick the layers that need to be merged and press Run. When the algorithm is done, close the window. You can zoom to the extent of the new layer (see How to). You can change the color of the DEM file.\nBack to Load DEM in QGIS section"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-load-dem",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-load-dem",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Add Raster Layer",
    "text": "QGIS: Add Raster Layer\nIn your QGIS window, navigate to Layer in your header toolbar, there to Add Layer and left-click on Add Raster Layer (see Figure C.11). The Data Source Manager will open in a pop-up window (see Figure C.12).\n\n\n\nFigure C.11: Add raster layer to QGIS project, step 1.: Navigate to the Data Source Manager.\n\n\n\n\n\nFigure C.12: Add raster layer to QGIS project, step 2.: Browse for the raster file to add to the QGIS project by pressing on the box with the three dots (…) to the right of the raster source input field.\n\n\nIn the example above, a DEM for the example of the Nauvalisoy river catchment is loaded. For loading the DEM of your sample catchments, browse for the DEM in the corresponding folder you downloaded from the provided Dropbox directory.\nPress the Add button at the bottom right of the Data Source Manager window to load the raster layer to your QGIS project and close the window by pressing the Close button (to the left of the Add button).\nYour QGIS project now shows a grey-scale version of the raster layer you loaded. If you are satisfied with the change, save your project by pressing the disk icon at the upper left corner of your QGIS window.\nYou can change the color of your raster file (how to). Here is a quick-guide of how to apply a topography-style color band to your DEM."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-change-color-of-raster-layer",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-change-color-of-raster-layer",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Change Color of Raster Layer",
    "text": "QGIS: Change Color of Raster Layer\nYou can change the colors of your cut DEM by double-clicking on the layer name in the Layers window. Go to tab Symbology (with the paint and brush icon) and choose Render type Singleband-pseudocolor (see Figure C.13) and select a Color ramp.\n\n\n\nFigure C.13: Nicely color your DEM, step 1.\n\n\nQGIS comes with a large library of color ramps but you can also create your own. See below a description of how to get your DEM in a topography style color palette."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-topograpy-color-ramp",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-topograpy-color-ramp",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Topography-style color palettes",
    "text": "QGIS: Topography-style color palettes\nFor our DEM we choose an existing topography-style color ramp. Open the Layer Properties window with a double-click on the raster layer in the Layers pane of the QGIS window. In the Symbology tab click the triangle to the right of the color ramp and select Create New Color Ramp… (see Figure C.14).\n\n\n\nFigure C.14: Nicely color your DEM, step 2.\n\n\nIn the drop down menu of the pop-up window choose Catalog: cpt-city and press OK (see Figure C.15).\n\n\n\nFigure C.15: Nicely color your DEM, step 3.\n\n\nA this will open another window containing the catalog of existing color ramps. Under Topography, we choose sd-a and press OK (see Figure C.16).\n\n\n\nFigure C.16: Nicely color your DEM, step 4.\n\n\nGo to tab Transparency, set Global Opacity to 30% and specify the No Data Value 0 (see Figure C.17).\n\n\n\nFigure C.17: Nicely color your DEM, step 5.\n\n\nThen go back to the Symbology tab. The minimum value of the color ramp should now not be 0 but 272. Adapt manually if need be. Then, press classify to get a discrete color ramp for your map and Apply to the map. If you are happy with the colors, quit by pressing OK (see Figure C.18).\n\n\n\nFigure C.18: Nicely color your DEM, step 6.\n\n\nThe result will look like Figure C.19.\n\n\n\nFigure C.19: Nicely color your DEM, step 7.\n\n\nYou can add decorations (e.g. scale and north arrow) to your map (how to)."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-add-map-decorations",
    "href": "appendix_c_quick_guides.html#sec-add-map-decorations",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Add Map Decorations",
    "text": "QGIS: Add Map Decorations\nNavigate to View -> Decorations and choose among the decorations to add. Many options for configuring the decorations are available."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-verify-projection-reproject",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-verify-projection-reproject",
    "title": "Appendix C — Quick Guides",
    "section": "C.1 QGIS: Verify Projection of Layer and Re-project Layer",
    "text": "C.1 QGIS: Verify Projection of Layer and Re-project Layer\nOpen the Layer Properties window with a double-click on the layer in the Layers pane and navigate to the Information tab. Under CRS (coordinate reference system) you see the projection of the layer. For the Chirchiq river basin, the CRS should say “EPSG:32642 - WGS 84 / UTM zone 42N - Projected”. Other river catchments in Central Asia may require a different UTM zone. For the student exercises, it is good to choose this projection for all basins.\nA raster layer can be reprojected to a different CRS: Go to Raster in the header toolbar. From there move the cursor over Projections and click on Warp (Reproject)…. The Warp window will pop up (in fact, it is just an interface where the user can specify the parameters of the wrap algorithm in a convient way). Select the raster layer you wish to re-project in the Input layer and select the Target CRS.\nIf the target CRS you wish to re-project to is not available in the drop-down menu, you can browse for it by clicking on the globe icon to thr right of the Target CRS section. You may re-sample the raster to a coarser resolution by specifying the Output file resolution. You may also specify to save the reprojected raster layer: Scroll to the bottom of the Warp (Reproject) window where you see Reprojected and a white box where you can browse for a location to store the new layer.\n\n\n\n\n\n\nWARNING\n\n\n\nIf you do not specify a target location, only a temporary layer will be loaded to QGIS which will not be available anymore after you close the QGIS project (even if you save the project).\n\n\nYou may decide to load the temporary file and save it later (how to). Click the Run button at the bottom right to start the re-projection algorithm and press Close when the process is done. The reprojected layer will be available in the Layers pane.\nA vector layer can be reprojected by selecting Vector in the header toolbar, moving the cursor to Data Management Tools and clicking on Reproject Layer…. This opens the Reproject Layer window where you can specify a Target CRS and optionally a storage location. As for the reprojection of the raster layer, a temporary layer is loaded to your QGIS project if you do not specify a storage location. However, you can always store temporary layers later (how to).\nBack to the load DEM section."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guide-save-temp-layer",
    "href": "appendix_c_quick_guides.html#sec-quick-guide-save-temp-layer",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Save a Temporary Layer",
    "text": "QGIS: Save a Temporary Layer\nRight-click on a temporary layer in the Layers pane. Temporary layers are indicated by a box to the right of the layer name. From the menu that opens upon right-click, select Export and Save As… (for raster layers) or Save Feature As… (for vector layers). An explorer window will open where you can specify a file name and a location to store the file to."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-add-vector-layer-to-qgis",
    "href": "appendix_c_quick_guides.html#sec-add-vector-layer-to-qgis",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Add Vector Layer",
    "text": "QGIS: Add Vector Layer\nLeft-clicking on Layer, moving your cursor over Add Layer and left-clicking Add Vector Layer (see Figure C.20).\n\n\n\nFigure C.20: Add vector layer to QGIS project, step 1.: Navigate to the Data Source Manager.\n\n\nA window will pop up, asking you to specify the properties of the vector layer to add (see Figure C.21).\n\n\n\nFigure C.21: Add vector layer to QGIS project, step 2: Select vector layers to add. Note that you select the shp file but cpg, dbf and shx need to be present in the same location.\n\n\nPress the box with the three dots on the right of the source field to specify the location of the shape file to be added to your project (see Figure C.22). Click Open and the window will close. The address of your shape files should now stand in the source field as in Figure C.21.\n\n\n\nFigure C.22: Add vector layer to QGIS project, step 3.\n\n\nNote that you load the .shp file but that all the files in the list in Figure C.21 need to be present. In the Add Vector Layer window, click Add and then close the window. QGIS has attributed a random color to your shape file which can be changed manually How to."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-change-color-of-vector-layer",
    "href": "appendix_c_quick_guides.html#sec-change-color-of-vector-layer",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Change Color of a Vector Layer",
    "text": "QGIS: Change Color of a Vector Layer\nYou can change the color of your layer by double-clicking on the layer name in the Layers Window to the left of the map. This will open the properties window. The third tab from the top shows paint and brush (see Figure C.23).\n\n\n\nFigure C.23: dd vector layer to QGIS project, step 5. Change the color of the layer.\n\n\nYou can activate Simple fill by clicking on it and select No brush in the drop-down menu in order to only show the outline of your vector layer (see Figure C.24).\n\n\n\nFigure C.24: Add vector layer to QGIS project, step 6. Only show the outline of the vector layer."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-fill-sinks",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-fill-sinks",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Fill Sinks",
    "text": "QGIS: Fill Sinks\nBrowse for the Fill sinks algorithm in the Processing Toolbox panel (see Figure C.25). If the Processing Toolbox panel is not visible go to Processing in the header toolbar and click on Toolbox to activate it. Alternatively, here is how to manage the visibility of panels in QGIS.\n\n\n\nFigure C.25: Search for the Fill sinks algorithm in the Processing Toolbar panel.\n\n\nOpen the Fill sinks window with a double-click on the name of the algorithm in the Processing Toolbar panel. There, select the DEM you want to process and browse for a location to store the output file (see Figure C.26). You can leave the default minimum slope.\n\n\n\nFigure C.26: Select the raster file to process.\n\n\nWhen the algorithm is done it will load the new layer into your QGIS project. Close the Fill sinks window and save your project.\nBack to catchment delineation."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guide-upslope-area",
    "href": "appendix_c_quick_guides.html#sec-quick-guide-upslope-area",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Calculate the area upslope of a point",
    "text": "QGIS: Calculate the area upslope of a point\nSearch for the SAGA algorithm Upslope Area in the Processing Toolbox panel and open the function window with a double click on the name. Enter the Longitude of your discharge station for the Target X coordinate and the Latitude for the Target Y coordinate for which the upslope area should be calculated. For the Elevation select the sink-filled DEM (see how to fill sinks in a DEM and why we need to fill in sinks).\nAll GIS layers and the coordinates of the discharge gauge need to be in the same UTM projection. Choose a method in the drop-down menue in the Method section and optionally specify a location for the output file. For the example of the Nauvalisoy basin, the Upslope Area window filled in correctly is shown in Figure C.27.\n\n\n\nFigure C.27: The Upslope Area window for the determination of the catchment area of the Nauvalisoy river basin.\n\n\nClick Run and Close after the algorithm is done. A new raster file with the values 0 for outside the catchment area and 100 for inside the catchment area is now loaded into your QGIS project.\nBack to catchment delineation."
  },
  {
    "objectID": "appendix_c_quick_guides.html#appendix-quick-guide-polygonize",
    "href": "appendix_c_quick_guides.html#appendix-quick-guide-polygonize",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Polygonize a Raster",
    "text": "QGIS: Polygonize a Raster\nGot to Raster in the header toolbar, move your cursor over Conversion and click on Polygonize (Raster to Vector)… (see Figure C.28).\n\n\n\nFigure C.28: Open the Polygonize window.\n\n\nSelect the raster layer you wish to polygonize (for example the upslope area raster layer) and run the process. Close the polygonize window after the algorithm is done. A new shape file will be loaded to your GIS project (see Figure C.29).\n\n\n\nFigure C.29: The vector layer generated by a raster to vector conversion.\n\n\nThe new vector layer has 2 features: the watershed boundary and a box around the watershed the same size of the DEM we used. To get rid of the outer shape, open the attribute table with a right-click on the vector layer and selecting Open Attribute Table. In the attribute table, elect the outer shape by clicking on the second row of the attribute table and toggle the edit mode by clicking on the pen icon (see Figure C.30).\n\n\n\nFigure C.30: Select the outer shape to discard by clicking on the second row in the attribute table and toggle the edit mode by clicking on the pen icon.\n\n\nDelete the outer shape by pressing the red bin icon in the attribute table (see Figure C.31).\n\n\n\nFigure C.31: Delete the selected outer shape.\n\n\nSave the edits in the attribute table (see Figure C.32) and press the pen icon again to un-toggle the edit mode.\n\n\n\nFigure C.32: Save your edits in the attribute table.\n\n\nClose the attribute table window and save the boundary of your watershed and your QGIS project.\nBack to catchment delineation."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-graphical-edit-junctions",
    "href": "appendix_c_quick_guides.html#sec-graphical-edit-junctions",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Edit Junctions Layer",
    "text": "QGIS: Edit Junctions Layer\nSelect the Junctions layer and toggle manual editing by clicking on the yellow pen (see Figure C.33).\n\n\n\nFigure C.33: Manually edit the layer with the river junctions, step 1: Toggle layer editing.\n\n\nWhen in editing mode, the yellow pen will appear in the Layers window next to the name of the layer being edited. The edit mode will also activate a button for adding points (i.e. junctions, we don’t need that now) and the vertex tool. Click on the vertex tool icon. It is active when a a boundary appears around the icon and the Vertex Editor windows opens (see Figure C.34).\n\n\n\nFigure C.34: Manually edit the layer with the river junctions, step 1: Toggle layer editing.\n\n\nRight-click on a junction point you would like to delete to activate it (see Figure C.35).\n\n\n\nFigure C.35: Manually edit the layer with the river junctions, step 3: Activate a junction node for editing.\n\n\nSelect the activated point by drawing a rectangle over the point with your mouse. The point will appear blue (see Figure C.36).\n\n\n\nFigure C.36: Manually edit the layer with the river junctions, step 3: Select the activated junction node.\n\n\nDelete the point with the delete key on your keyboard. You can save your edits by pressing the blue-white Save Layer Edits button that is decorated with an orange pen (see Figure C.37). This saves your changes without exiting the edit mode.\n\n\n\nFigure C.37: Manually edit the layer with the river junctions, step 3: Save edits.\n\n\nIf you have many points to remove, as in our case, it may be faster to identify the IDs of the features you want to keep, select these and delete all others. To start, you activate the Identify Features mode by clicking on the icon with the white i on the blue circle (see Figure C.38).\n\n\n\nFigure C.38: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 1.\n\n\nA black i will appear next to your cursor. You then click on the first of your nodes that you want to keep. This will highlight it in red and a list with information on the selected feature appears on the right in the Identify Results window. You will see the attribute NODE_ID with value 1 for the outflow node (see Figure C.39). Note down the ID of the feature.\n\n\n\nFigure C.39: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 2.\n\n\nYou then press on the node at the confluence of the two tributaries in the center of the catchment. The Identify Results window shows 2 results, that means, that two junction nodes are close to each other (see Figure C.40).\n\n\n\nFigure C.40: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 3.\n\n\nZoom in in your map window with your mouse to see the two nodes (see Figure C.41).\n\n\n\nFigure C.41: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 4.\n\n\nSelect the node that should be kept and not the ID of the node (NODE_ID 11) (see Figure C.42).\n\n\n\nFigure C.42: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 5.\n\n\nZoom back to the entire Junctions layer (see Figure C.10) and go to Select Features by Values… in the toolbar (see Figure C.43).\n\n\n\nFigure C.43: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 1.\n\n\nAdd NODE_ID 1 to your selection as demonstrated in Figure C.44.\n\n\n\nFigure C.44: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 2.\n\n\nThe outflow node with ID 1 will change color in your map as shown in Figure C.45.\n\n\n\nFigure C.45: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 3.\n\n\nAdd node 11 to your selection in the same way and close the Select Node by Value window. Invert the feature selection as shown in Figure C.46.\n\n\n\nFigure C.46: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 4.\n\n\nAll other nodes will now be yellow and the ones to keep will appear in the layer color (see Figure C.47).\n\n\n\nFigure C.47: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 5.\n\n\nDelete the features (nodes) by pressing the Delete Selected button in the edit features toolbar as shown in Figure C.48.\n\n\n\nFigure C.48: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 6.\n\n\nSave your edits (see Figure C.37). To verify that, indeed, all superfluous nodes are deleted from the Junctions file, open the Attribute table shown in Figure C.49.\n\n\n\nFigure C.49: Edit Attribute table. Step 1: Open the attribute table with a click on the Attribute Table icon in the QGIS toolbar.\n\n\nOnly 2 features should be listed under each attribute. We will now edit the attribute table to prepare it for the RSMinerve model. RSMinerve needs an identifier to differentiate between the junctions. We can use the attribute TYPE to uniquely identify the two junctions needed. RSMinerve further needs the ID of the downstream river. Add a column to the attribute table by pressing the Add Field button (see Figure C.50).\n\n\n\nFigure C.50: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 1.\n\n\nDefine a name for the attribute, a type and admissible length of each entry in the Add Field window. In our case, we choose a string consisting of letters as ID and allow it to be 20 characters long as shown in Figure C.51.\n\n\n\nFigure C.51: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 2.\n\n\nClose the window by pressing OK. By clicking in the newly created NULL fields, you can now type names for the downstream rivers and save your edits by pressing the save edits icon (3rd from the left in the toolbar of the attribute table window). As the outlet of the catchment goes directly into Charvak reservoir, we can type Charvak as the downstream river ID for this example. The river stretch between junction and outlet is called Pskem (see Figure C.52).\n\n\n\nFigure C.52: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 3.\n\n\nWe are done editing the Junctions layer. Deactivate the edit mode by clicking on the yellow pen in the attribute table window as demonstrated in Figure C.53 and close the window.\n\n\n\nFigure C.53: Save your edits.\n\n\nNow save the Junctions layer in an appropriate place on your drive. Time to save your QGIS project.\nIn the same way you can also edit the river channels layer."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-create-hbv-model",
    "href": "appendix_c_quick_guides.html#sec-quickguides-create-hbv-model",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Create HBV model and edit parameters",
    "text": "RSM: Create HBV model and edit parameters\nClick on the HBV model icon in the model selection panel (see Figure C.54).\n\n\n\nFigure C.54: To generate an HBV model, click on the HBV model icon in the model selection panel.\n\n\nMove your cursor to the white area in the center and create a HBV model with a click. The HBV model icon will now appear in the white model panel (see Figure C.55).\n\n\n\nFigure C.55: A HBV model has been generated.\n\n\nEdit the parameters of the model by double-clicking on the model icon and changing parameter values in the table that appears to the right of the RSMinerve window (see Figure C.56).\n\n\n\nFigure C.56: Double-click on the HBV model to open and edit the parameter table.\n\n\nAlternatively (especially if you have to change parameters for several models), activate the Parameters panel in the Model Properties toolbar (see Figure C.57).\n\n\n\nFigure C.57: Activate the Parameters panel in the Model Properties toolbar.\n\n\nSelect the HBV model in the Parameters panel as shown in Figure C.58.\n\n\n\nFigure C.58: Select HBV in the Parameters panel.\n\n\nIf you have several HBV models, you can select models by zone and apply edits in the parameter table in the left of the Parameters panel to all selected models (marked with tick). You can also edit paramers for individual models in the right-hand table of the Parameters panel as is visible in Figure C.59.\n\n\n\nFigure C.59: Edit parameters for groups of models (left parameter table) or for individual models (right parameter table).\n\n\nSave your model by clicking the floppy disk icon in the toolbar.\nYou can also export parameters to a text file via the button Export P in the Model Properties toolbar, edit the text file and import the edited parameter file through Import P.\nNote that for the Nauvalisoy demonstration case study, the area of the HBV model should be 99’000’000 m2.\nBack to the Nauvalisoy model guide."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-add-climate-station",
    "href": "appendix_c_quick_guides.html#sec-quickguides-add-climate-station",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Add and link climate station",
    "text": "RSM: Add and link climate station\nMove your cursor to the V-Station icon in the models panel (the first icon in Figure C.54). Click on the icon to activate it and click next to the HBV model in your model pane to place the V-Station. With a double-click on the newly created V-Station, you can visualize the parameters of the virtual weather station.\nThen, connect the station to the HBV model by activating the Connections Mode in the Editing tools toolbar as shown in Figure C.60.\n\n\n\nFigure C.60: Create a virtual weather station.\n\n\nClick on the station, hold down the finger move your cursor to the HBV model, then release the hold. A grey line appears between the station and the model and a pop-up window asks you to verify the data links between the two components (see Figure C.61).\n\n\n\nFigure C.61: Link the virtual weather station to the HBV model.\n\n\nClick Ok to accept the suggested data links and a blue arrow will appear between the weather station and the model. Click on the black arrow in the Editing tools toolbar to leave the Connections Mode.\nBack to the Nauvalisoy model guide."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-import-climate-data",
    "href": "appendix_c_quick_guides.html#sec-quickguides-import-climate-data",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Import climate data",
    "text": "RSM: Import climate data\nMake sure that you have downloaded storede the following climate data file ERA5_Nauvalisoy_1981_2013.csv for Nauvalisoy River on your computer.\nGo to the database tab and click on Open in the File database toolbar (see Figure C.62).\n\n\n\nFigure C.62: The database tab.\n\n\nSelect the file that you downloaded before. If you don’t see the file in your file browser, make sure the file ending .csv is selected in the file browser (see Figure C.63).\n\n\n\nFigure C.63: Make sure the file ending is .csv in the file browser.\n\n\nPress open to load the data into RSMinerve. Depending on your computer this may take a few seconds. Once the data is loaded, click through the database browser in the left pane to explore the data as shown in Figure C.64.\n\n\n\nFigure C.64: Explore the data base.\n\n\nTo connect the data base to the model you will need to make the data consistent with the model. To do this change the name “New group” to “Measurements” and select Inputs for the Category selection (see Figure C.65).\n\n\n\nFigure C.65: Change \"New group\" to \"Measurements\" and select Input as category.\n\n\nThen, browse the name of the station as done in Figure C.66.\n\n\n\nFigure C.66: The name of the station is \"Nauvalisoy\".\n\n\nAs our weather data is representative for the entire Nauvalisoy catchment, the station name in the data base needs to be consistent with the station name of the V-Station in the model pane. Change the name of the station in the model pane from V-Station to Nauvalisoy by clicking on the name and then editing it.\nChoose the nauvalisoy data set as source and adapt the simulation period as shown in Figure C.67.\n\n\n\nFigure C.67: Choose the nauvalisoy data set to link the weather station data to the virtual weather station. The simulation times should not extend the period of the input data. Simulation time step is 1 hour and the recording time step is 1 month.\n\n\nClick on Validation to verify that the model has been set up correctly. Once you have adapted the model settings to calculate evaporation based on temperature (how to), no errors should be reported. You can now run the model. A warning tells you that the number of meteo stations is not sufficient. Ignore the warning for now.\nBack to the Nauvalisoy model guide"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-atapt-model-settings",
    "href": "appendix_c_quick_guides.html#sec-quickguides-atapt-model-settings",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Adapt Model Settings",
    "text": "RSM: Adapt Model Settings\nNavigate to Edit in the Settings toolbar (see Figure C.68).\n\n\n\nFigure C.68: Open the models settings tab.\n\n\nOpen the Settings tab and choose an ET model. Adapt the coordinates of the project (choose the station coordinates mentioned in the Introduction Section) (see Figure C.69).\n\n\n\nFigure C.69: Edit the evaporation calculation method and the coordinates of the project.\n\n\nBack to the model guide"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-export-sim-results-to-db",
    "href": "appendix_c_quick_guides.html#sec-export-sim-results-to-db",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Export Simulation Results to Data Base",
    "text": "RSM: Export Simulation Results to Data Base\nGo to Export in the Database toolbar (see Figure C.70). Define a name for the simulation results and choose a data base group to save the data to. Create a new group if you haven’t already done so.\n\n\n\nFigure C.70: Export simulation results to the data base.\n\n\nThe data sets are now available in the data base tab and can be visualized in the Selection and Plots tab.\nBack to the model guide"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-add-comparator-and-discharge-data",
    "href": "appendix_c_quick_guides.html#sec-quickguides-add-comparator-and-discharge-data",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Add Comparator and Load Discharge Measurements",
    "text": "RSM: Add Comparator and Load Discharge Measurements\nThe comparator object allows the user to compare a simulated variable to a reference. In our case, we want to compare the simulated discharge of the Nauvalisoy river to the one actually measured at the only gauge in the catchment. The measured discharge can be imported to RSMinerve via the database tab.\nMove your cursor to the comparator icon as shown in Figure C.71 in the model components panel.\n\n\n\nFigure C.71: Comparator icon in RS Minerve.\n\n\nActivate it with a left-click and move your cursor next to the HBV model in the model pane. Place the comparator object with another click.\nAdd a source object to your model following the same procedure as for the comparator object. Figure C.72 shows the icon of the source object.\n\n\n\nFigure C.72: Source icon in RS Minerve.\n\n\nYou can now optionally rename your model objects.\nConnect the source and the HBV model to the comparator by activating the Connections mode in the Editing Tools toolbar. Right-click on the HBV model, hold the click and drag the cursor to the comparator object where you release the cursor. You will be asked in a pop-up window to specify the flow you wish to compare and if the data is to be viewed as simulation results or reference (measured) data. Connect the total discharge computed by the HBV model component as simulation result to the comparator (see Figure C.73) and close the pop-up window by pressing OK.\n\n\n\nFigure C.73: Connecting a Source to Comparater Object in RSMinerve.\n\n\nConnect the source object to the comparator in the same way as the HBV object. The source will be connected as reference (see Figure C.74).\n\n\n\nFigure C.74: Final, connected objects.\n\n\nNext, we have to load the discharge data into the database. Navigate to the database tab. In the database, go to Measurements -> Nauvalisoy and click Add and enter a name for the discharge station (see Figure C.75). For this example, we do not need to bother with the coordinates of the station (later, in more complex models, we will have to though!).\n\n\n\nFigure C.75: Connect the outflow of the HBV model as simulation result to the comparator.\n\n\nUnder the new discharge station, add a sensor and rename it to the source object in your model as shown in Figure C.76.\n\n\n\nFigure C.76: Connect the discharge measurements (source) as reference to the comparator.\n\n\nOpen the tab with the values. Here we need to import the discharge data. You can do that by opening the discharge data in a spreadsheet (e.g. Excel) and copy-pasting the dates and discharge values into the Values table in RSMinerve (here is how to do this step-by-step.\nNow we need to link the discharge data in the database to the source object in the model. Navigate to the model do the following.\n\n\n\nFigure C.77: Select the data source for the source object (under Data Source in the left window pane) and select the sensor for discharge data for the source (under Source, Series identifier in the right window pane).\n\n\nValidate the model to see if the model setup went correctly. Run the model if the validation did not throw an error.\nBack to the practical model calibration and validation Section"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-copy-paste-database-values",
    "href": "appendix_c_quick_guides.html#sec-quickguides-copy-paste-database-values",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Copy-paste Data to Database",
    "text": "RSM: Copy-paste Data to Database\nMake sure that the following file synthetic_discharge_Nauvalisoy_river_for_calibration_exercise.csv is available on your computer.\nNow, open the file in a spreadsheet software, e.g. Excel, Numbers, Google Sheets or OpenOffice Calc and select the rows containing dates and values (Figure C.78). Press Control+C or perform a right-click with the mouse and choose copy.\n\n\n\nFigure C.78: Select the rows and columns containing the dates and data to be copied. Press Control+C to copy the selected cells.\n\n\nThen navigate to the discharge sensor on the database tab in RSMinerve where you want to add the data to. Click in the small square to the left of the first row in the Values tab and click the keyboard keys Control+P. Alternatively perform a right-click in the small square and select Paste. The dates and values from the excel sheet will now appear in the table (Figure C.79). Save the database.\n\n\n\nFigure C.79: Paste dates and values to the database table.\n\n\nBack to the practical model calibration and validation Section"
  },
  {
    "objectID": "appendix_d_exercise_solutions.html#sec-appendix-solutions-exercise1",
    "href": "appendix_d_exercise_solutions.html#sec-appendix-solutions-exercise1",
    "title": "Appendix D — Exercise Solutions",
    "section": "\nD.1 Exercise on Linear Reservoir modelling",
    "text": "D.1 Exercise on Linear Reservoir modelling\nTask 1\nWhat will determine the flow through your bucket?\nThe flow through the bucket will be influenced by the volume to bottom area fraction of the bucket, the amount and speed of water added to the bucket and the size of the outlet hole.\nWhat do you need to measure?\nYou will need to measure:\n\nthe discharge from your bucket over time,\n\nthe recharge volume (how much water you put into the bucket over a given time),\n\nthe time when you start pouring water and when you stop pouring water into the bucket, and\nthe approximate volume of your bucket.\nHow can you measure it?\nThis depends on what you have available. You can draw a water level line outside of your outflow receptacle every 10 seconds and then determine the volume change over time. Maybe you have a scale and a smart phone so you can put your outflow receptacle on the scale and make a movie of the weight change over time (note, 1 kg of water is approximately 1 liter of water).\nFor the inflow pour a well defined volume over a well defined time interval. You can do this manually unless of course you have pipes and valves lying around that you can use.\nYou will need a watch for measuring time and a receptacle with known volume to measure volumes (or a scale).\nA couple of notes on measurement accuracy:\n\nGenerally, the larger the volumes, the smaller the relative measurement error. Say you measure a discharge of 50 ml in 1 s (i.e. 50 ml/s) and you can read your volume with an accuracy of 5 ml and the time with an accuracy of 0.2 s. Your measurement uncertainty becomes 11 ml/s which is more than 20 % of your discharge. If on the other hand, you measure 500 ml over 10 s (which is the same discharge of 50 ml/s) with the same inaccuracies for volume and time your measurement uncertainty for discharge becomes 1 ml/s which is only 2 % of your discharge.\n\nHow do you estimate measurement uncertainties: Measure several times, compute the average and the standard deviation of your measurements assuming a student-t distribution.\n\nHow do you combine uncertainties of volume and time to the uncertainty of discharge: By applying Gaussian error propagation.\nWhat materials you will need to set up the experiment?\n\nFor the bucket (the linear reservoir): A plastic bottle, a box or a can that is no longer used. It should have an opening at the top and the material should repel water and be thin enough that you can drill a hole into the wall.\nA pair of pointy scissors or a knife to drill a hole into the bucket.\n\nA water source (a tap, hose or a water container larger than the one above). This will be your rain machine.\nA watch to measure time.\n\nNote paper and pen.\n\nA receptacle for measuring the outflow.\n\nAdditional material to facilitate measurement according to availability.\n\n\n\nFigure D.1: Example for material needed and example setup to perform the linear reservoir experiment. Add a minion with a stop watch or a smart phone to help you logging the discharge from the bucket.\n\n\nTask 2\nThe video was recorded with a smart phone. The weight of the outflow receptacle was noted down every second. The discharge is computed as the change of volume in the outflow receptacle over time.\n\n\nTask 3\nThe height of the measured discharge peak can be best reproduced with k = 0.42. However, the measured discharge peaks 1s later than the simulated discharge peak.\nReasons for the discrepancy can be the shape of the linear reservoir, non-linear pouring speed, and measurement uncertainties."
  },
  {
    "objectID": "appendix_d_exercise_solutions.html#sec-appendix-solutions-hbv-exercises",
    "href": "appendix_d_exercise_solutions.html#sec-appendix-solutions-hbv-exercises",
    "title": "Appendix D — Exercise Solutions",
    "section": "\nD.2 Exercises on the HBV Model",
    "text": "D.2 Exercises on the HBV Model\nExercise: Driving Forces of the HBV Model\nThe model drivers are precipitation, temperature and evaporation (P, T and ET in ?fig-overview-hbv-model). You need to provide time series of the model drivers to the model. Evaporation is typically not measured at climate stations but many empirical functions are available in the literature to estimate evaporation. RSMinerve offers the possibility to calculate evaporation based on temperature measurements and catchment location (you will do that later in this tutorial).\nExercise - HBV Model States\nThe model states are the snow water equivalent height (SWE), the relative water content in the snow pack (WH), the humidity (Hum), the upper reservoir water level (SU) and the lower reservoir water level (SL). The model states are initialized using the initial conditions.\nExercise: Data Visualization in RSMinerve\nSimulate from the 01/01/1981 01:00:00 to 31/12/1983 23:00:00, then choose data from 31/12/1983 23:00:00 as the initial conditions and run the model from 01/01/1984 01:00:00 to 31/12/1984 23:00:00. Choose hourly output for the simulation results.\nOpen the Selection and plots tab by clicking on the Selection and plots button in the Modules toolbar and select simulated P and T from the Nauvalisoy station as shown in Figure D.2.\n\n\nFigure D.2: Hourly precipitation and temperature at the virtual Nauvalisoy weather station.\n\n\nNote: If you want to repeat a simulation with specific initial conditions, you can store them through Export IC in the Model Properties toolbar.\nThe approximate temperature range is -13 deg. C. in December to 34 deg. C. in August. The annual precipitation is about 1.4 m (visualize Pcum and click on the last value of the time series). No precipitation falls during the summer months.\nExercise: Compare Evaporation Methods\nFigure Figure D.3 shows the evaporation computed with various methods and the resulting discharge. Uniform evaporation should not be used for sub-annual modeling time steps for obvious reasons that ET shows a strong seasonality. The difference between the different methods by Turc, McGuinness and Oudin are within 5 % of total discharge which is negligible for a regional model.\n\n\nFigure D.3: Hourly precipitation and temperature at the virtual Nauvalisoy weather station.\n\n\nFor advanced modeling, the choice of the evaporation model may be relevant but only if a validation with measured data is possible.\nExercise: Common Difficulties in Model Calibration\n\nEspecially fully and semi-distributed hydrological models are typically over-parameterized, i.e. the number of model parameters is much larger than the number of observations for the model states. The true parameter values of the system cannot be uniquely identified based on a discharge time series alone.\n\nThe outcome of the calibration depends on the measure of similarity between the simulated and the measured discharge.\nThe water balance is often forgotten during model calibration. A nice fit of the discharge curve can for example be achieved by increasing the volume of water in the model over time.\nThe model is calibrated against historical data. Its ability to predict future discharge may be limited.\n\nThe model is not perfect, it remains an approximation of the real system and may not incorporate all relevant processes of the hydrological cycle (e.g. water storage and transport in glaciers for the case of the HBV model or significant sub-surface water fluxes that very difficult to capture as for example in Karst regions).\n\nDischarge measurements typically have uncertainties of 20 %. Particularly measurements at the lower and upper ends of the rating curve (i.e. the water table - discharge relationship) are typically prone to larger uncertainties (bonus question: think about why this is so!). Is the measurement location or the equipment not properly maintained, biases may grow over time. On the other hand, if the measurement method is updated and changed, the measured discharge may display a different pattern as was for example discussed in the data from Gunt River basin (Section 2.1).\nExercise: Strategies to Overcome some of the Model Calibration Difficulties\n\nOver-parameterization:\n\n\n\nConsider simplifying the model, i.e. reducing the number of parameters. If the model complexity is required, try adding additional measured variables, e.g. snow cover from MODIS data (see Chapter on snow cover data) to validate individual components of the HBV model.\n\nCollect data to verify individual fluxes of the model components (e.g. soil parameters, snow water equivalent, etc.). As physical measurements in the field are not always possible you may have to become creative here, e.g. use MODIS snow cover data to validate the snow/no snow partitioning of the HBV model. Also consult the literature for parameterizations of similar catchments.\n\n\n\nUse a combination of similarity measures. This will be demonstrated later on in the model calibration section.\n\nDuring model calibration, look at the components of the model as well as the total discharge time series. Make sure that the storage of water changes within reasonable bounds and that the partitioning of the water in your system is physically reasonable (e.g. comparatively small storage compartments for rocky mountain catchments).\n\nExclude part of your data set from model calibration and use it for model validation. If the model does not perform well in the validation period, its parameters are too specific for the calibration period (you have over-fitted the model) and the model is said to not generalize well. If this happens you should try to reduce the number of parameters in your model. To understand better which parameters are responsible for the over-fit of the historical discharge, use different calibration and validation periods and compare the resulting parameters. Through sensitivity analysis, identify the model components that are most sensitive to predicted changes of model forcings, geometry or parameterization and perform scenario analysis.\n\nImplement and validate multiple possible conceptual models. All of the models must be calibrated and validated individually. It is further recommended to calculate at the highest possible temporal resolution and to try and compare the model outcome at different spatial resolution.\n\nSquare-root filters or data assimilation algorithms are able to account for non-correlated measurement errors (they are not implemented in RSMinerve and not topic of this course). Error bands for the measurements should be adapted when communicating model results.\n\nMost of the above points will be discussed in more detail during this course.\n\nD.2.1 Exercise: Calibrate a Simple HBV Model {#sec-appendix-solutions-calibrated parameters .unnumbered}\nThe parameter set of the calibrated model are: ::: {.cell} ::: {.cell-output-stdout}\n\n\n|                |      |\n|:---------------|-----:|\n|CFMax (mm/°C/d) |  0.50|\n|CFR (-)         |  0.05|\n|CWH (-)         |  0.10|\n|TT (°C)         |  3.00|\n|TTInt (°C)      |  3.00|\n|TTSM (°C)       |  0.00|\n|Beta (-)        |  2.50|\n|FC (mm)         | 20.00|\n|PWP (-)         |  0.50|\n|SUMax (mm)      | 10.00|\n|Kr (1/d)        |  0.09|\n|Ku (1/d)        |  0.02|\n|Kl (1/d)        |  0.00|\n|Kperc (1/d)     |  0.00|\n::: :::\nYou can import the calibrated parameters via Import P in the Model Properties toolbar (note that the calibrated parameters are available for download here."
  },
  {
    "objectID": "geospatial_data.html#sec-preparation-of-rsminerve-input-files",
    "href": "geospatial_data.html#sec-preparation-of-rsminerve-input-files",
    "title": "5  Geospatial Data",
    "section": "\n5.3 Preparation of RSMinerve Input files",
    "text": "5.3 Preparation of RSMinerve Input files\nYou can download the Graphical Modeler model from the Students_CaseStudyPacks online repository. The model file is called rsminerve_gis_files_preparation_2022_win_os.model3 and should be downloaded in your local working directory. The model can then be loaded via the Processing/Graphical Modeler menu in QGIS. Once you have loaded the model, you should see the model pop up on your screen as is shown in a similar fashion in Figure 5.5.\n\n\nFigure 5.5: The Graphical Modeler model rsminerve_gis_files_preparation_2022_win_os.model3 is shown in a graphical manner in the right window (1). When you click Run, the parameters specification window on the left will pop up (2). See text for further explanations\n\n\nFigure 5.5 shows nothing more than a graphical representation of a script. This script is like a recipie to execute algorithms in QGIS in a sequential manner where an output of one algorithm feeds as input into the next algorithm. The yellow highlighted elements in window (1) are input parameters that are specified in the window (2) prior to the execution of the script. The green elements in the window (1) are the results that are stored during and after the execution of the algorithm and available for further processing then.\nAs explained above, the rsminerve_gis_files_preparation_2022_win_os.model3 model, in a nutshell, prepares input files for RSMinerve using the DEM, the basin shapefile as well as the location of the gauge as input. The key output elements are called SUBBASINS, RIVERS and JUNCTIONS.\nPrior to the execution of the model (script), the parameters have to be set in a careful manner. For some of the parameter values, a first best guess might produce outcomes that are not a the required level of detail or, alternatively, too detailed. Depending on the basin under consideration, an iterative approach normally must be followed to arrive at the desired output as is explained in the following.\nIn Figure 5.6, we show a meaningful parameter selection for the Chatkal River basin in the Case Studies pack (Gauge 16279). The basin and gauge shapefiles as well as the DEM are the geospatial assets that need to be specified are shown in (1), (5) and (4). The following parameters are important to set correctly\n\n\nFigure 5.6: Parameter selection menu of the Graphical Modeler rsminerve_gis_files_preparation_2022_win_os.model3 model. The detailed description of the input elements as well as parameter values chosen is explained in the main text.\n\n\n\nBasinShapeBuffer_meters (2): This is buffer value to be specified in meters. The default value of 500 m is a value that can be used for a wide range of different catchment sizes and does not need to be changed.\nChannel Network Cutoff Value (3): This is a crucial parameter for the determination of the subbasin granularity. If many the main basin is to be subdivided into a larger number of subbtains, this value needs to be chosen to be around 107 - 108. For Chatkal, as our example shows, a perfect value for the subdivision into the main 4 subbasins is 109.\nElevation Bands Table (4): Figure 5.7 shows the table than can be customized according to user needs and wants. The table specifies that granuarity with which the basin domain will be subdivided into elevation bands (hydrological response units, i.e. HRUs). As a rule of thumb, in climate change impact studies and for snow melt dominated basins, a spacing of 100 m - 200 m is an optimal choice for use with HBV models in RSMinerve. Note however, the finer the domain is, the larger the number of HRUs will need to be modeled which proportionally increases the computational demand of the hydrological model.\nRiver Network Level (8): Here you set a number between 1 and 8. The lower the number, the higher the number of tributaries and subtributaries that will be delineated for the RIVERS output shapefile. If you choose 8, this means that normally only the river’s main stem will be delineated. This is relevant for the case where not further subdivisions into small subbasins is desired for modeling in RSMinerve. 7 is a robust choice if only the main tributaries should be delineated.\n\n\n\nFigure 5.7: The elevation bands table is shown. It shows the user customizable elevation band intervals. The example shows the specification of 9 elevation bands (HRUs) of 500 m spacing between each. For each HRU and for each subbasin, a corrsponding hydrological model will be configured in RSMinerve\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou will very likely have to iterate and run the model a couple of times to achieve the desired results. Try it yourself! Note that if you get an error message that POLYGONS.shp was not found, try increasing the Channel Network Cutoff Value by 50 % - 70 %.\n\n\nSample output results for our demonstration catchment are shown in Figure 5.8.\n\n\nFigure 5.8: Resulting output of the Graphical Modeler model for Chatkal River basin. The next step is to clean up manually the SUBBASINS, RIVERS and JUNCTIONS shapefiles."
  },
  {
    "objectID": "geospatial_data.html#sec-post-processing-results-shapefiles",
    "href": "geospatial_data.html#sec-post-processing-results-shapefiles",
    "title": "5  Geospatial Data",
    "section": "\n5.4 Post-Processing Results Shapefiles",
    "text": "5.4 Post-Processing Results Shapefiles\nAs mentioned in the previous Section, we need to post-process the SUBBASINS, RIVERS and JUNCTIONS shapefiles so that they correctly contain the required fields as shown in Figure 5.1. A good way to post-process the files is to start with the RIVERS layer, the continue with the JUNCTIONS layer and to end with the SUBBASINS layer.\n\n5.4.1 JUNCTIONS and RIVER Layers\nIn editing the junctions layer, the basic ideas are to edit the junctions layer in a way to remove junctions that are not needed, given the desired subdivision of the larger basin into subbasins. For the demo catchment shown in Figure 5.9, the siutation is explained in greater detail.\n\n\nFigure 5.9: The Figure shows the output files of the Graphical Modeler model. For this demonstration catchment, the only junction that we want to keep (apart from the outflow junction located at the gauge) is the one highlighted with the red arrow which marks the confluence of the two major upstream tributaries.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you struggle with shapefile editing, please consult the detailed step-by-step guide as shown in the cooresponding Quick Guide in the Appendix.\n\n\nOnce we have removed the junctions that are not needed, we can very easily also remote the river lines that are not needed. Generally, we do not need river sections above the most upstream junctions for any given subbasin within the river basin or the river basin itself.\nIf we follow these guides, we end up with the results shown in Figure 5.10. After editing the attribute tables of these shapefiles, the files are ready for import in RSMinerve.\n\n\nFigure 5.10: Final edited shapefiles with the unnecessary junctions and river stretches removed. The two red arrows point to the two remaining junctions and the blue arrow shows the river stretch that is left.\n\n\nWhat is left now is to ensure that the attribute tables contain the correct fields and that these fields contain the correct labels. Figure 5.1 shows the required fields. These are\n\nJUNCTIONS: Junctions Name (junct_name), Junctions ID (junct_id), Rivers ID (riv_id)\nRIVERS: Rivers Name (riv_name), Rivers ID (riv_id), Junctions ID (junct_id)\n\nThe abbreviated field names are used in QGIS since field names are limited in shapefiles. The fields and values of the final junctions shapefile are shown in Figure 5.11. Compare this with the edited final attribute table of the rivers shapefile as shown in Figure 5.12.\n\n\nFigure 5.11: The final attribute table of the demo junctions shapefile is shown. (1) denotes the fields for the donwstream outlet junction and (2) shows the values for the upstream junction. The naming convention has to be understood in the context of the naming convention using in the RIVERS shapefile\n\n\n\n\nFigure 5.12: The final rivers shapefile attribute table is shown with the only river highlighted (1). Very importantly, the junt_id field referes to the downstream junction where the river drains into.\n\n\n\n5.4.2 SUBBASINS Layer {sec-subbasins-layer}\nNow, we are working on the subbasins. The major subbasins are highlighted in the following Figure 5.13. We call them Chatkal River downstream, Chatkal River upstream and Sandalash River. These big subbasins can be understood as hydrological response units (HRUs) for hydrological modeling.\n\n\nFigure 5.13: The major subbasins of Chatkal River basin above Gauge 16279 are shown. (1) is Chatkal downstream, (2) is Sandalash River and (3) is Chatkal upstream.\n\n\nHowever, do not forget that we have classified the entire basin into zones of elevation bands. These are shown in the Figure 5.13 as polygons with black outlines from the shapefile layer BasinElevationBands_poly_fixed.shp. Exactly 7 elevation bands (or, in other words, HRUs) were delineated with the Graphical Modeler like this. We need to somehow intersect the subbasin HRUs with the elevation bands to generate elevation band HRUs for each subbasin. We will show the steps required to achieve this in the following.\nFirst, we prepare the attribute table of the subbasins shapefile. The way this is done correctly for the demonstration catchment is shown in Figure 5.14. The attribute table contains two fields, i.e., basin_name and junct_id. We name the downstream and upstream sections of the Chatkal River with chatkal_ds and chatkal_us correspondingly. The cells of the field junct_id are populated with the name of the corresponding downstream junction where the subbasin drains into. Since both, the subbasin chatkal_us and sandalash drain into the junction upstream_junct, the later appears in both cells of the junct_id field for these rivers.\n\n\nFigure 5.14: Subbasins attribute table after adding the required fields and entires as per the requirements of RSMinerve. (1) is for the downstream part of Chatkal River, (2) is for the upstream part of Chatkal River and (3) is for Sandalash River.\n\n\nSecond, we clean up and prepare the attribute table for intersecting with the subbasins layer in a thrid step. The attribute table of the layer BasinElevationbands_Poly_fixed contains three fields, i.e., ID, VALUE and NAME from which you can safely delete the ID and VALUE fields. What remains is the NAME field that contains a unique identifier for the elevation bands in ascending order, starting from the lowest. Figure 5.15 shows the attribute table.\n\n\nFigure 5.15: The attribute table of the elevation bands shapefile is shown after the fields ID and VALUE have been removed (1). When an elevation band is selected in te edit mode, the corresponding geometry is highlighted on the map as is shown by the arrows (2).\n\n\nThird, we need to intersect the elevation bands shapefile with the subbasins shapefile. This can easily be accomplished using the Vector/Geoprocessing/Intersection tool in QGIS. When you select the Intersection Algorithm, make sure that you set the elevation bands shapefile as the Input layer and set subbasins shapefile as the Overlay Layer. When executed, you receive an attribute table that looks as the one shown in Figure 5.16.\n\n\nFigure 5.16: The attribute table resulting from the Intersection Algorithm. You now have to manually add the numbers of the Name field as suffix to the existing names of the basin_name field.\n\n\nAs a final step, you have to manually add the numbers stored in the NAME field to the basin names in the field basin_name. After this is done, you can safely delete the NAME field and have proudly completed the required steps to setup the GIS shapefiles that can be used as input to the RSMinerve hydrological-hydraulic modeling tool. Figure 5.17 shows the result.\n\n\nFigure 5.17: The final attribute table of the subbasins file is shown. The elevation band identifiers have been added as suffixes with “_x” to the subbasin names where x denotes the number of the subbasin-specific elevation band."
  },
  {
    "objectID": "climate_data.html#exporting-climate-data-to-.csv-files-for-use-in-rsminerve",
    "href": "climate_data.html#exporting-climate-data-to-.csv-files-for-use-in-rsminerve",
    "title": "7  Climate Data",
    "section": "\n7.3 Exporting Climate Data to .csv-Files for Use in RSMinerve",
    "text": "7.3 Exporting Climate Data to .csv-Files for Use in RSMinerve\n\n\n\n\nBeck, Hylke E., Eric F. Wood, Tim R. McVicar, Mauricio Zambrano-Bigiarini, Camila Alvarez-Garreton, Oscar M. Baez-Villanueva, Justin Sheffield, and Dirk N. Karger. 2020. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1.\n\n\nBuisán, Samuel T., Michael E. Earle, José Luı́s Collado, John Kochendorfer, Javier Alastrué, Mareile Wolff, Craig D. Smith, and Juan I. López-Moreno. 2017. “Assessment of Snowfall Accumulation Underestimation by Tipping Bucket Gauges in the Spanish Operational Network.” Atmospheric Measurement Techniques 10 (3): 1079–91.\n\n\n“CHELSA-W5e5 V1.0: W5e5 V1.0 Downscaled with CHELSA V2.0.” 2022. ISIMIP Repository. https://doi.org/10.48364/ISIMIP.836809.2.\n\n\nFeigenwinter, I., S. Kotlarski, A. Casanueva, A. M. Fischer, C. Schwierz, and M. A. Liniger. 2018. “Exploring Quantile Mapping as a Tool to Produce User-Tailored Climate Scenarios for Switzerland.” Technical Report 270. Federal Office of Meteorology; Climatology MeteoSwiss.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nKarger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-resolution monthly precipitation and temperature time series from 2006 to 2100.” Scientific Data 7 (1): 248. https://doi.org/10.1038/s41597-020-00587-y.\n\n\nKarger, Dirk Nikolaus, Adam M. Wilson, Colin Mahony, Niklaus E. Zimmermann, and Walter Jetz. 2021. “Global daily 1 km land surface precipitation based on cloud cover-informed downscaling.” Scientific Data 8 (1): 307. https://doi.org/10.1038/s41597-021-01084-6.\n\n\nKotlarski, S., K. Keuler, O. B. Christensen, A. Colette, M. Déqué, A. Gobiet, K. Goergen, et al. 2014. “Regional climate modeling on European scales: a joint standard evaluation of the EURO-CORDEX RCM ensemble.” Geoscientific Model Development 7 (4): 1297–1333. https://doi.org/10.5194/gmd-7-1297-2014.\n\n\nO’Neill, Brian C., Claudia Tebaldi, Detlef P. van Vuuren, Veronika Eyring, Pierre Friedlingstein, George Hurtt, Reto Knutti, et al. 2016. “The Scenario Model Intercomparison Project (ScenarioMIP) for CMIP6.” Geoscientific Model Development 9 (9): 3461–82. https://doi.org/10.5194/gmd-9-3461-2016.\n\n\nRiahi, Keywan, Detlef P. van Vuuren, Elmar Kriegler, Jae Edmonds, Brian C. O’Neill, Shinichiro Fujimori, Nico Bauer, et al. 2017. “The Shared Socioeconomic Pathways and their energy, land use, and greenhouse gas emissions implications: An overview.” Global Environmental Change 42: 153–68. https://doi.org/10.1016/j.gloenvcha.2016.05.009."
  },
  {
    "objectID": "climate_data.html#export-climate-data-to-rsminerve",
    "href": "climate_data.html#export-climate-data-to-rsminerve",
    "title": "7  Climate Data",
    "section": "\n7.3 Export Climate Data to RSMinerve",
    "text": "7.3 Export Climate Data to RSMinerve\n\n\n\n\nBeck, Hylke E., Eric F. Wood, Tim R. McVicar, Mauricio Zambrano-Bigiarini, Camila Alvarez-Garreton, Oscar M. Baez-Villanueva, Justin Sheffield, and Dirk N. Karger. 2020. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1.\n\n\nBuisán, Samuel T., Michael E. Earle, José Luı́s Collado, John Kochendorfer, Javier Alastrué, Mareile Wolff, Craig D. Smith, and Juan I. López-Moreno. 2017. “Assessment of Snowfall Accumulation Underestimation by Tipping Bucket Gauges in the Spanish Operational Network.” Atmospheric Measurement Techniques 10 (3): 1079–91.\n\n\n“CHELSA-W5e5 V1.0: W5e5 V1.0 Downscaled with CHELSA V2.0.” 2022. ISIMIP Repository. https://doi.org/10.48364/ISIMIP.836809.2.\n\n\nFeigenwinter, I., S. Kotlarski, A. Casanueva, A. M. Fischer, C. Schwierz, and M. A. Liniger. 2018. “Exploring Quantile Mapping as a Tool to Produce User-Tailored Climate Scenarios for Switzerland.” Technical Report 270. Federal Office of Meteorology; Climatology MeteoSwiss.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nKarger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-resolution monthly precipitation and temperature time series from 2006 to 2100.” Scientific Data 7 (1): 248. https://doi.org/10.1038/s41597-020-00587-y.\n\n\nKarger, Dirk Nikolaus, Adam M. Wilson, Colin Mahony, Niklaus E. Zimmermann, and Walter Jetz. 2021. “Global daily 1 km land surface precipitation based on cloud cover-informed downscaling.” Scientific Data 8 (1): 307. https://doi.org/10.1038/s41597-021-01084-6.\n\n\nKotlarski, S., K. Keuler, O. B. Christensen, A. Colette, M. Déqué, A. Gobiet, K. Goergen, et al. 2014. “Regional climate modeling on European scales: a joint standard evaluation of the EURO-CORDEX RCM ensemble.” Geoscientific Model Development 7 (4): 1297–1333. https://doi.org/10.5194/gmd-7-1297-2014.\n\n\nO’Neill, Brian C., Claudia Tebaldi, Detlef P. van Vuuren, Veronika Eyring, Pierre Friedlingstein, George Hurtt, Reto Knutti, et al. 2016. “The Scenario Model Intercomparison Project (ScenarioMIP) for CMIP6.” Geoscientific Model Development 9 (9): 3461–82. https://doi.org/10.5194/gmd-9-3461-2016.\n\n\nRiahi, Keywan, Detlef P. van Vuuren, Elmar Kriegler, Jae Edmonds, Brian C. O’Neill, Shinichiro Fujimori, Nico Bauer, et al. 2017. “The Shared Socioeconomic Pathways and their energy, land use, and greenhouse gas emissions implications: An overview.” Global Environmental Change 42: 153–68. https://doi.org/10.1016/j.gloenvcha.2016.05.009."
  },
  {
    "objectID": "real_world_examples.html",
    "href": "real_world_examples.html",
    "title": "11  Real World Examples",
    "section": "",
    "text": "Here, we provide a tour of model applications in the domain of the management of water resources in Central Asia. This Chapter is currently under active development.\nPlease check back later."
  }
]