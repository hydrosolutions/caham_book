[
  {
    "objectID": "snow_and_glacier_data.html#introduction",
    "href": "snow_and_glacier_data.html#introduction",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.1 Introduction",
    "text": "6.1 Introduction\nThe runoff of the rivers Syr Darya and Amu Darya consists of 65%-75% snow melt, 23% precipitation and 2-8% glacier melt approximately (Armstrong et al. 2019). In smaller, highly glaciated catchments, the glacier contribution to discharge can be more important (Khanal et al. 2021). Generally, the cryosphere is a major contributor to the water balance in Central Asia (Barandun et al. 2020). While glacier runoff is a small contributor to the annual runoff, it is seasonally important as it covers the irrigation demand in summer, when snow melt is over (Kaser, Grosshauser, and Marzeion 2010).\nAmong other things, climate impacts translate into long-term changes of runoff formation fractions and the distribution of runoff formation within the hydrological year. Typical rainfall-runoff models such as the HBV Model simulate the fractionation of precipitation into snow and rain with a temperature threshold method. Snow and liquid water reservoirs and corresponding fluxes are then accounted for. However, these models have only a limited understanding of glacier processes which are normally inadequate at best to estimate glacier contributions to discharge.\nThe following section gives a brief overview over the available regional open source data regarding Central Asias cryosphere. A later chapter [TODO LINK TO CHAPTER] will then focus on the modelling of the cryosphere.\nPlease note that new (highly relevant and public) glacier data are released ever more frequently. The summary provided here refers to the latest data sets at the time of writing in February 2022.\nWe use the catchment of the gauging station on the Atabshy river, a tributary to the Naryn river in Central Asia as a demo site. If you’d like to reproduce the examples presented in this chapter you can download the zipped data in the example data set available here. You can extract the the downloaded data into a location of your choice and adapt the reference path below. The rest of the code will run as it is, provided you have the required r packages installed. The size of the data package is XX GB.\n\nlibrary(tmap)\nlibrary(sf)\nlibrary(raster)\nlibrary(tidyverse)\nlibrary(lubridate)\n\ndevtools::install_github(\"hydrosolutions/riversCentralAsia\")\nlibrary(riversCentralAsia)\n\n# Path to the data directory downloaded from the download link provided above. \n# Here the data is extracted to a folder called atbashy_glacier_demo_data\ndata_path <- \"../caham_data/SyrDarya/Atbashy/\""
  },
  {
    "objectID": "snow_and_glacier_data.html#high-mountain-asia-snow-reanalysis-product",
    "href": "snow_and_glacier_data.html#high-mountain-asia-snow-reanalysis-product",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.2 High Mountain Asia Snow Reanalysis Product",
    "text": "6.2 High Mountain Asia Snow Reanalysis Product\nYufei Liu, Fang, and Margulis (2021) provide a reanalysis product for snow covered area and snow water equivalent (SWE) in High Mountain Asia. The data is available via NSIDC (Y. Liu, Fang, and Margulis 2021). Their SWE can directly be compared to the SWE computed in hydrological models like HBV.\nFrom the downloaded data, only the SWE and the validity mask (showing the pixels where the snow water equivalent product is valid) is required.\n\ndem <- raster(paste0(data_path, \"GIS/16076_DEM.tif\"))\nbasin <- st_read(paste0(data_path, \"GIS/16076_Basin_outline.shp\"), quiet = TRUE)\n\n# Load one example file and display SWE for a random date in the cold season. \nfilespath <- paste0(data_path, \"SNOW/\")\nyear <- 1999\n\n# Load non-seasonal snow mask\nfilepart <- \"_MASK.nc\"\nindex = sprintf(\"%02d\", (year - 1999))\n\n# The Atbashy basin is covered by two raster stacks\nmask_w <- raster::brick(paste0(filespath, \n                               \"HMA_SR_D_v01_N41_0E76_0_agg_16_WY\", \n                               year, \"_\", index, filepart), \n                     varname = \"Non_seasonal_snow_mask\")\nraster::crs(mask_w) = raster::crs(\"+proj=longlat +datum=WGS84 +no_defs\")\nmask_e <- raster::brick(paste0(filespath,\n                               \"HMA_SR_D_v01_N41_0E77_0_agg_16_WY\", \n                               year, \"_\", index, filepart), \n                     varname = \"Non_seasonal_snow_mask\")\nraster::crs(mask_e) = raster::crs(\"+proj=longlat +datum=WGS84 +no_defs\")\n\n# The rasters need to be rotated\ntemplate <- raster::projectRaster(from = mask_e, to= mask_w, alignOnly = TRUE)\n\n# template is an empty raster that has the projected extent of r2 but is \n# aligned with r1 (i.e. same resolution, origin, and crs of r1)\nmask_e_aligned <- raster::projectRaster(from = mask_e, to = template)\nmask_w <- flip(t(mask_w), direction = 'x')\nmask_e_aligned <- flip(t(mask_e_aligned), direction = 'x')\nmask <- merge(mask_w, mask_e_aligned, tolerance = 0.1) \nmask = raster::projectRaster(from = mask, \n                             crs = crs(\"+proj=utm +zone=42 +datum=WGS84 +units=m +no_defs\"))\n\n# Load snow data\nvarname = \"SWE_Post\"\nfilepart <- \"_SWE_SCA_POST.nc\"\nsca_w <- raster::brick(paste0(filespath, \n                              \"HMA_SR_D_v01_N41_0E76_0_agg_16_WY\", \n                              year, \"_\", index, filepart), \n                       varname = varname)\n\n[1] \"vobjtovarid4: **** WARNING **** I was asked to get a varid for dimension named Day BUT this dimension HAS NO DIMVAR! Code will probably fail at this point\"\n\nraster::crs(sca_w) = raster::crs(\"+proj=longlat +datum=WGS84 +no_defs\")\nsca_e <- raster::brick(paste0(filespath,\n                              \"HMA_SR_D_v01_N41_0E77_0_agg_16_WY\", \n                              year, \"_\", index, filepart), \n                       varname = varname)\n\n[1] \"vobjtovarid4: **** WARNING **** I was asked to get a varid for dimension named Day BUT this dimension HAS NO DIMVAR! Code will probably fail at this point\"\n\nraster::crs(sca_e) = raster::crs(\"+proj=longlat +datum=WGS84 +no_defs\")\ntemplate <- raster::projectRaster(from = sca_e, to = sca_w, alignOnly = TRUE)\n# template is an empty raster that has the projected extent of r2 but is \n# aligned with r1 (i.e. same resolution, origin, and crs of r1)\nsca_e_aligned<- raster::projectRaster(from = sca_e, to = template)\nsca_w <- flip(t(sca_w), direction = 'x')\nsca_e_aligned <- flip(t(sca_e_aligned), direction = 'x')\nsca <- raster::merge(sca_w, sca_e_aligned, tolerance = 0.1)\nsca <- projectRaster(from = sca, \n                     crs = crs(\"+proj=utm +zone=42 +datum=WGS84 +units=m +no_defs\"))\n\nsca_masked <- mask(sca, mask, maskvalue = 1)\nsca_masked <- mask(sca_masked, basin)\n\n# Visualize snow water equivalent\ntmap_mode(\"view\")\ntm_shape(sca_masked$layer.1) + \n  tm_raster(n = 6,\n            palette = \"Blues\",\n            alpha = 0.8,\n            legend.show = TRUE, \n            title = \"SWE (-)\") + \n  tm_shape(basin) + \n  tm_borders(col = \"black\", lwd = 0.6)\n\n\nSnow water equivalent map of October 1, 1998 (source; HMASR)."
  },
  {
    "objectID": "snow_and_glacier_data.html#randolph-glacier-inventory-rgi",
    "href": "snow_and_glacier_data.html#randolph-glacier-inventory-rgi",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.3 Randolph Glacier Inventory (RGI)",
    "text": "6.3 Randolph Glacier Inventory (RGI)\nThe Randolph Glacier Inventory (RGI) v6.0 (RGI Consortium 2017) makes a consistent global glacier data base publicly available. It includes geo-located glacier geometry and some additional parameters like elevation, length, slope and aspect. A new version (v7) is under review at the time of writing beginning of 2022. For Central Asian water resources modelling, RGI regions 13 (Central Asia) and 14 (South Asia West) are relevant. You can download the glacier geometries for all RGI regions from the GLIMS RGI v6.0 web site. For this demo, the data for the Atbashy basin is available from the data download link given above.\n\n# Loading the data\nrgi <- st_read(paste0(data_path, \"GIS/16076_Glaciers_per_subbasin.shp\"), \n               quiet = TRUE) |> \n  st_transform(crs = crs(dem))\n\n# Generation of figure\ntmap_mode(\"view\")\ntm_shape(dem) +\n  tm_raster(n = 6, \n            palette = terrain.colors(6),\n            alpha = 0.8,\n            legend.show = TRUE, \n            title = \"Elevation (masl)\") + \n  tm_shape(rgi) + \n  tm_polygons(col = \"lightgray\", lwd = 0.2)\n\n\nDEM & Glaciers (light gray) of the demo basin."
  },
  {
    "objectID": "snow_and_glacier_data.html#glacier-thickness",
    "href": "snow_and_glacier_data.html#glacier-thickness",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.4 Glacier thickness",
    "text": "6.4 Glacier thickness\nFarinotti et al. (2019) make distributed glacier thickness maps available for each glacier in the RGI v6 data set. We have downloaded the required maps of glacier thickness for you and made them available in the download link above. We will refer to this data set as the glacier thickness data set or the Farinotti data set.\nThe original glacier thickness data set is available from the data collection of Farinotti et al. (2019) which is available from the data section of their online article.\nThe following code chunk demonstrates how to extract glacier thickness data from the Farinotti data set.\n\n6.4.1 How to extract glacier thickness\n\n# Get a list of all files in the glacier thickness data set. The files are named \n# after the glacier ID in the RGI v6.0 data set (variable RGIId).  \nglacier_thickness_dir <- paste0(data_path, \"GLACIERS/Farinotti/\") \nfilelist <- list.files(path = glacier_thickness_dir, pattern = \".tif$\", \n                       full.names = TRUE)\n\n# Filter the glacier thickness file list for the glacier ids in the catchment of \n# interest. \nfilelist <- filelist[sapply(rgi$RGIId, grep, filelist)]\n\n# Get the maximum glacier thickness for each of the glaciers in filelist. \n# Note: this works only for small catchments as the origin of the rasters to be \n# mosaiced needs to be consistent. For a larger data set you will need to implement \n# a loop over all glaciers to extract the thickness per glacier or per elevation \n# band. This operation can take a while. \nglacier_thickness <- Reduce(function(x, y) raster::mosaic(x, y, fun = max),\n                            lapply(filelist, raster::raster)) \n\n# For plotting, clip the glacier thickness raster of the basin to the basin boundary\nglacier_thickness <- mask(glacier_thickness |> \n                            projectRaster(crs = crs(dem)), basin)\n\n\ntmap_mode(\"view\")\ntm_shape(dem) + \n  tm_raster(n = 6, \n            palette = terrain.colors(6),\n            alpha = 0.8,\n            legend.show = TRUE, \n            title = \"Elevation (masl)\") + \n  tm_shape(glacier_thickness) +\n  tm_raster(n = 6, \n            palette = \"Blues\",\n            legend.show = TRUE, \n            title = \"Glacier thickness\\n(m)\") + \n  tm_shape(rgi) + \n  tm_borders(col = \"gray\", lwd = 0.4) + \n  tm_shape(basin) + \n  tm_borders(col = \"black\", lwd = 0.6)\n\n\nGlacier thickness by Farinotti et al., 2019\n\n\nA more recent glacier thickness data set by Millan et al. (2022) estimates much larger ice reservoirs in the Himalayan region but similar goodness of fit for the glaciers in the Central Asian region as the Farinotti data set. The Millan et al. (2022) data set is not included in the present workflow yet but could be an alternative for the Farinotti data set."
  },
  {
    "objectID": "snow_and_glacier_data.html#glacier-thinning-rates",
    "href": "snow_and_glacier_data.html#glacier-thinning-rates",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.5 Glacier thinning rates",
    "text": "6.5 Glacier thinning rates\nHugonnet et al. (2021) provide annual estimates of glacier thinning rates for each glacier in the RGI v6.0 data set. It is advised to not to rely on the annual data but rather on an average over at least 5 years to get reliable thinning rates for individual glaciers. We compare trends in glacier thinning rates to trends in computed glacier balance components. We will refer to this data set as the thinning rates data set or the Hugonnet data set. A copy of the Hugonnet thinning rates is included in the download link above.\nThe original per-glacier time series of thinning rates can be downloaded from the data repository as described in the github site linked under the code availability section of the online paper of Hugonnet et al. (2021).\n\nhugonnet <- read_csv(paste0(data_path, \"/GLACIERS/Hugonnet/dh_13_rgi60_pergla_rates.csv\"))\n# Explanation of variables:\n# - dhdt is the elevation change rate in meters per year,\n# - dvoldt is the volume change rate in meters cube per year,\n# - dmdt is the mass change rate in gigatons per year,\n# - dmdtda is the specific-mass change rate in meters water-equivalent per year.\n\n# Filter the basin glaciers from the Hugonnet data set. \nhugonnet <- hugonnet |> \n  dplyr::filter(rgiid %in% rgi$RGIId) |> \n  tidyr::separate(period, c(\"start\", \"end\"), sep = \"_\") |> \n  mutate(start = as_date(start, format = \"%Y-%m-%d\"), \n         end = as_date(end, format = \"%Y-%m-%d\"), \n         period = round(as.numeric(end - start, units = \"days\")/366))\n\n# Join the Hugonnet data set to the RGI data set to be able to plot the thinning \n# rates on the glacier geometry. \nglaciers_hugonnet <- rgi |> \n  left_join(hugonnet |> dplyr::select(rgiid, area, start, end, dhdt, err_dhdt, \n                                      dvoldt, err_dvoldt, dmdt, err_dmdt, \n                                      dmdtda, err_dmdtda, period),  \n            by = c(\"RGIId\" = \"rgiid\")) \n\n# Visualization of data\ntmap_mode(\"view\")\ntm_shape(dem) + \n  tm_raster(n = 6, \n            palette = terrain.colors(6),\n            alpha = 0.8, \n            legend.show = TRUE, \n            title = \"Elevation (masl)\") + \n  tm_shape(glaciers_hugonnet |> dplyr::filter(period == 20)) +\n  tm_fill(col = \"dmdtda\", \n          n = 6, \n          palette = \"RdBu\",\n          midpoint = 0, \n          legend.show = TRUE, \n          title = \"Glacier thinning\\n(m weq/a)\") + \n  tm_shape(rgi) + \n  tm_borders(col = \"gray\", lwd = 0.4) + \n  tm_shape(basin) + \n  tm_borders(col = \"black\", lwd = 0.6)\n\n\nAverage glacier mass change by Hugonnet et al., 2021."
  },
  {
    "objectID": "snow_and_glacier_data.html#glacier-discharge",
    "href": "snow_and_glacier_data.html#glacier-discharge",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.6 Glacier discharge",
    "text": "6.6 Glacier discharge\nMiles et al. (2021) ran specific mass balance calculations over many glaciers larger than 2 km2 of High Mountain Asia. They provide the average glacier discharge between 2000 and 2016. The package includes an empirical relationship based on a regression between glacier thinning rates and glacier discharge which allows the estimation of glacier discharge. We will refer to this data set as the glacier discharge data set or the Miles data set. A copy of the glacier discharge data is available from the data download link provided above.\nThe original data is available from the data repository linked in the online version of the paper.\n\n# Calculate glacier discharge using the glacierDischarge_HM function of the \n# riversCentralAsia package. An empirical relationship between glacier thinning \n# rates by Hugonnet et al., 2021 and glacier discharge by Miles et al., 2021.  \nglaciers_hugonnet <- glaciers_hugonnet |> \n  mutate(Qgl_m3a = glacierDischarge_HM(dhdt))\n\n# Data visualization\ntmap_mode(\"view\")\ntm_shape(dem) + \n  tm_raster(n = 6, \n            palette = terrain.colors(6),\n            alpha = 0.8, \n            legend.show = TRUE, \n            title = \"Elevation (masl)\") + \n  tm_shape(glaciers_hugonnet |> dplyr::filter(period == 20)) +\n  tm_fill(col = \"Qgl_m3a\", \n          n = 6, \n          palette = \"RdBu\",\n          midpoint = 0, \n          legend.show = TRUE, \n          title = \"Glacier discharge\\n(m3/a)\") + \n  tm_shape(rgi) + \n  tm_borders(col = \"gray\", lwd = 0.4) + \n  tm_shape(basin) + \n  tm_borders(col = \"black\", lwd = 0.6)\n\n\nGlacier discharge derived from Miles et al., 2021."
  },
  {
    "objectID": "snow_and_glacier_data.html#a-note-on-the-uncertainties-of-glacier-data-sets",
    "href": "snow_and_glacier_data.html#a-note-on-the-uncertainties-of-glacier-data-sets",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.7 A note on the uncertainties of glacier data sets",
    "text": "6.7 A note on the uncertainties of glacier data sets\nThe geometries of the RGI v6.0 data set are generally very good. If you simulate glacier discharge in a small catchment with few glaciers it is advisable to visually check the glacier geometries and make sure, all relevant glaciers in the basin are included in the RGI data set. You may have to manually add missing glaciers or correct the geometry.\nFor some regions in Central Asia, OpenStreetMap is an excellent reference for glacier locations and names in Central Asia. You can import the map layer in QGIS or also download individual GIS layers.\nThe glacier thickness data set is validated only at few locations as measurements of glacier thickness are typically not available. Farinotti et al. (2019) list an uncertainty range for the volume estimate in regions RGI 13 and 14 of 26% each.\nHugonnet et al. (2021) & Miles et al. (2021) provide the uncertainties of their estimates for per-glacier glacier thinning & discharge rates in the data set itself. They typically lie around p/m 150%.\n## References\n\n\n\n\nArmstrong, Richard L., Karl Rittger, Mary J. Brodzik, Adina Racoviteanu, Andrew P. Barrett, Siri-Jodha Singh Khalsa, Bruce Raup, et al. 2019. “Runoff from Glacier Ice and Seasonal Snow in High Asia: Separating Melt Water Sources in River Flow.” Regional Environmental Change 19 (5): 1249–61. https://doi.org/10.1007/s10113-018-1429-0.\n\n\nBarandun, Martina, Joel Fiddes, Martin Scherler, Tamara Mathys, Tomas Saks, Dimitry Petrakov, and Martin Hoelzle. 2020. “The State and Future of the Cryosphere in Central Asia.” Water Security 11. https://doi.org/10.1016/j.wasec.2020.100072.\n\n\nFarinotti, Daniel, Matthias Huss, Johannes J. Fürst, Johannes Landmann, Horst Machguth, Fabien Maussion, and Ankur Pandit. 2019. “A Consensus Estimate for the Ice Thickness Distribution of All Glaciers on Earth.” Nature Geoscience 12 (3): 168–73. https://doi.org/10.1038/s41561-019-0300-3.\n\n\nHugonnet, Romain, Robert McNabb, Etienne Berthier, Brian Menounos, Christopher Nuth, Luc Girod, Daniel Farinotti, et al. 2021. “Accelerated Global Glacier Mass Loss in the Early Twenty-First Century.” Nature 592 (7856): 726–31. https://doi.org/10.1038/s41586-021-03436-z.\n\n\nKaser, G., M. Grosshauser, and B. Marzeion. 2010. “Contribution Potential of Glaciers to Water Availability in Different Climate Regimes.” Proceedings of the National Academy of Sciences 107 (47): 20223–27. https://doi.org/10.1073/pnas.1008162107.\n\n\nKhanal, S., A. F. Lutz, P. D. A. Kraaijenbrink, B. van den Hurk, T. Yao, and W. W. Immerzeel. 2021. “Variable 21st Century Climate Change Response for Rivers in High Mountain Asia at Seasonal to Decadal Time Scales.” Water Resources Research 57 (5). https://doi.org/10.1029/2020WR029266.\n\n\nLiu, Y., Y. Fang, and S. A. Margulis. 2021. High Mountain Asia UCLA Daily Snow Reanalysis (version Version 1). Boulder, Colorado USA: NASA National Snow; Ice Data Center Distributed Active Archive Center. doi: https://doi.org/10.5067/HNAUGJQXSCVU.\n\n\nLiu, Yufei, Yiwen Fang, and Steven A. Margulis. 2021. “Spatiotemporal Distribution of Seasonal Snow Water Equivalent in High-Mountain Asia from an 18-Year Landsat-MODIS Era Snow Reanalysis Dataset.” The Cryosphere 15: 5261–80. https://doi.org/10.5194/tc-2021-139.\n\n\nMiles, Evan, Michael McCarthy, Amaury Dehecq, Marin Kneib, Stefan Fugger, and Francesca Pellicciotti. 2021. “Health and Sustainability of Glaciers in High Mountain Asia.” Nature Communications 12 (2868): 10. https://doi.org/https://doi.org/10.1038/s41467-021-23073-4.\n\n\nMillan, Romain, Jérémie Mouginot, Antoine Rabatel, and Mathieu Morlighem. 2022. “Ice Velocity and Thickness of the World’s Glaciers.” Nature Geoscience 15 (2): 124–29. https://doi.org/10.1038/s41561-021-00885-z.\n\n\nRGI Consortium. 2017. “Randolph Glacier Inventory – a Dataset of Global Glacier Outlines: Version 6.0: Technical Report.” Global Land Ice Measurements from Space, Colorado, USA. Digital Media. https://doi.org/https://doi.org/10.7265/N5-RGI-60."
  },
  {
    "objectID": "climate_data.html#sec-historical-climate-data",
    "href": "climate_data.html#sec-historical-climate-data",
    "title": "7  Climate Data",
    "section": "\n7.1 Historical Climate Data",
    "text": "7.1 Historical Climate Data\n\n7.1.1 CHELSA V21 High-Resolution Climate Data\nTo obtain information on past precipitation (P) and temperature (T) in the Central Asia region, we use the very high-resolution daily temperature climate product from CHELSA. CHELSA (Climatologies at high resolution for the earth’s land surface areas) is a global downscaled climate data set currently hosted by the Swiss Federal Institute for Forest, Snow and Landscape Research (WSL). It is built to provide free access to high resolution climate data for research and application, and is constantly updated and refined. CHELSA data are in the process of revolutionizing the field of hydrology in data-poor regions, among many other application domains.\nCHELSA includes climate layers for various time periods and variables, ranging from the Last Glacial Maximum, to the present, to several future scenarios. CHELSA is based on a mechanistic statistical downscaling of global reanalysis data for the historical observations (hist_obs) or global circulation model output for future simulations (fut_sim) (see also www.chelsa-climate.org for more information). For more technical information, please consult the following document.\nThe historical observations for CHELSA precipitation and temperature are available from 1979 through 2016 for daily time steps and at 1 km resolution. These are derived from ERA-INTERIM reanalysis model outputs, among other things. ERA-INTERIM is a global atmospheric reanalysis product with a spatial resolution of 80 km, approximately. A good introduction about what a reanalysis product is can be found on this website.\nDaily gridded precipitation fields are generated merging data from the ERA5 reanalysis precipitation and the MODIS monthly cloud cover. The CHELSA algorithm that is used for downscaling takes into account orographic predictors such as wind, topographic exposition and boundary layer height Karger et al. (2021). When compared with other products, the resulting data shows excellent performance, also in complex high mountain terrain. Temperature observations are available over the same period and are taken from (“CHELSA-W5e5 V1.0: W5e5 V1.0 Downscaled with CHELSA V2.0” 2022).\n\n\n\n\n\n\nWarning\n\n\n\nThe data for the Central Asia domain (55 deg. E - 85 deg. E and 30 deg. N - 50 deg. N) is very large and is not provided here for download (total storage requirements > 1 TB). Please contact Tobias Siegfried via siegfried@hydrosolutions.ch for more information on how to obtain access the data of the entire domain.\n\n\nThe high-resolution climate data derived with the CHELSA algorithm is corrected for the problem of snow-undercatch in the high mountain regions as described by (beck2020?). What is snow-undercatch? Measuring precipitation correctly in high altitude regions is complex because of sublimation and blowing snow. An example of this is shown in Figure 7.1 for high elevation gauges in Spain. In a recent inter-comparison project carried out in Spain, it has been shown that undercatch poses significant problems in accurately measuring solid precipitation (Buisán et al. 2017) in mountainous regions. Both, ERA-INTERIM and CHELSA themselves assimilate station data in their models and hence are affected by these erroneous measurements.\n\n\nFigure 7.1: Measured snow undercatch values in high-mountain stations in Spain. The values were determined within the World Meteorological Organization Solid Precipitation Intercomparison Experiment (WMO-SPICE). See text for more information and reference.\n\n\n(beck2020?) has recognized this and released monthly correction factors that are taken into account in the CHELSA algorithm (see Figure 7.2).\n\n\nFigure 7.2: Figure from (beck2020?), Supplementary Material. Plate d): Best estimate of global bias correction factors. Plate e): Lower bound estimate of global bias correction factors. Plate f): Upper bound of global bias correction factors. As is clearly visible, bias correction factors in high-mountain Asia, including the parts of Central Asia are significant.\n\n\nThe annual precipitation climatology, i.e. the long-term mean annual precipitation, from 1979 - 2011 is shown in Figure 7.3. As is easily visible and not further surprising, the mountainous regions receive the bulk of the precipitation, on average.\n\n\nFigure 7.3: The CHELSA V21 Precipitation climatology in 6 large basins Central Asian basins is shown, including Amu Darya, Syr Darya, Talas River, Chu River, Issq Kul and Ily River is shown. Light blue colors indicate very little preciptiation whereas red colors indicate high annual norm precipitation amounts.\n\n\nThe following Figure 7.4 and Figure 7.5 show cold and warm season precipitation amounts. The cold season is defined to encompass the months October (previous year) - March (preceding year) whereas the warm season lasts from April through September.\n\n\nFigure 7.4: The cold season precipitation climatology is shown. This Figure should also be compared with the the warm season precipitation climatology as shown in Figure 7.5.\n\n\n\n\nFigure 7.5: The warm season precipitation climatology is shown. This Figure should also be compared with the the cold season precipitation climatology as shown in Figure 7.4.\n\n\nFigure 7.4 shows that winter precipitation is mainly concentrated on the western fringes of the mountain ranges where moisture gets precipitated via westerly circulations and associated frontal systems. Compared to this, the main warm season precipitation locations move further to the east and to inner mountain range locations where summer convective storms cause this (see Figure 7.5).\nThe climatological data used to produce these Figures is available via this Dropbox link in the climate/chelsa_v21/climatologies/ sub-folder. There data over the historical observation period from 1981 - 2010 has been prepared for the norm annual and cold as well as warm season temperatures (tas_…) has been prepared. Similarly, data on precipitation (pr_…) and potential evapotranspiration (pet_…) is available and on the aridity index which is defined as \\(\\phi = PET/P\\) where \\(PET\\) is the potential evapotranspiration climatology and \\(P\\) is the precipitation climatology.\n\n\n\n\n\n\nTip\n\n\n\nTry it yourself! Download the data for the Central Asia domain here and visualize the climatologies for your case study catchment and extract mean statistics for the basins. As a reminder, the case study basins can be accessed via this link.\n\n\n\n7.1.2 Assessment of CHELSA V21 Data Quality in a Sample Catchment\nHow can the quality of the CHELSA data in the complex Central Asia domain be assessed? With try to answer this question by looking at one of the case study basins provided as part of the Student Case Study Pack. Specifically, we want to answer the following questions for the Gunt River basin in the Pamir mountains:\n\ndoes the magnitude of the precipitation yield physically meaningful results, and\ndoes the climatology adequately reproduce the seasonal cycle observed one at the stations?\n\nLong-term Annual Norm Discharge\nLet us address the first question by investigating long-term norm CHELSA precipitation values and comparing these long-term norm values of the specific discharge of Gunt River. If \\(P >Q\\), where \\(P\\) is the long-term mean precipitation and \\(Q\\) is the long-term mean discharge, we can confidently say that the bias corrected CHELSA precipitation product is meaningful from a water balance perspective. The long-term water balance is simply\n\\[\nQ = P - E\n\\qquad(7.1)\\]\nwhere \\(Q\\) is the specific discharge [mm], \\(P\\) is the long-term mean precipitation [mm] and \\(E\\) is the long-term mean evapotranspiration \\(E\\) [mm] (see also the Chapter on Long-term water balance modeling for more information). Hence, if, over the long run, \\(P>E\\) and under the assumption that storage changes i.e. from glacier melt are not present, the water balance is valid and the product from that perspective validated.\nWe can compute the average long-term precipitation in the catchment in a simple way. The code block below shows how.\n\n# load required libraries\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(exactextractr)\nlibrary(raster)\nlibrary(tmap)\nlibrary(tmaptools)\n\n# load basin shapefile. note that if you want to replicate \n# the code below on your own computer, make sure that you \n# adjust the data paths accordingly, depending on where \n# you stored the downloaded data!\nbasin_shp_path <- \n  \"../caham_data/AmuDarya/17050_Gunt/GIS/17050_basin_latlon.shp\"\nbasin_shp <- sf::st_read(basin_shp_path, quiet = TRUE)\n\n# load climatology and extract values\np_clim_path <- \n  \"../caham_data/central_asia_domain/climate/chelsa_v21/climatologies/hist_obs/pr_chelsa_climatology_ann.tif\"\np_clim <- raster(p_clim_path)\n\n# mask and crop\np_clim_gunt <- p_clim %>% \n  mask(basin_shp) %>% \n  crop(basin_shp)\n\n# plotting climatology\ntmap::tmap_mode(\"view\")\ntmap::tm_basemap(\"Stamen.Terrain\") +\n  tm_shape(p_clim_gunt) +\n  tm_raster(style = \"quantile\", n = 7, palette = get_brewer_pal(\"Blues\", n = 7, plot = FALSE)) +\n  tm_shape(basin_shp) +\n  tm_borders(\"black\")\n\n\nFigure 7.6: CHELSA precipitation climatology in Gunt River Basin. Note, the Figure is interactive and you can zoom in and out to see more details. The above code is shown to demonstrate the generation of such a Figure.\n\n\n\nThe mean norm precipitation can be easily computed as follows.\n\n# extracting mean value over Gunt River basin. \n# note the resulting value is in mm.\np_clim_mean <- p_clim %>% exact_extract(basin_shp,'mean')\n\nWarning in .local(x, y, ...): Polygons transformed to raster CRS (EPSG:NA)\n\np_clim_mean\n\n[1] 384.4959\n\n\nWe thus get 384 mm for the 30 year period from 1981 through 2010. Let us compare this to the long-term discharge at Gunt Khorog discharge station. For this, we load the corresponding data frame from which we extract the corresponding data.\n\n# load discharge data from Gunt Khorog gauging station. note that if you want to replicate the code below on your own computer, make sure that you adjust the data paths accordingly, depending on where you stored the downloaded data!\nstation_data <- \n  readRDS(\"../caham_data/AmuDarya/17050_Gunt/GaugeData/17050_Q.Rds\")\n\n# extract data for station 17050 and discharge between 1981 and 2010\ndischarge_data_17050 <- \n  station_data %>%\n  filter(type == \"Q\") %>% \n  filter(code == \"17050\") %>% \n  filter(date >= ymd(\"1981-01-01\")) %>% \n  filter(date <= ymd(\"2010-12-31\"))\n  \n# compute long-term discharge\ndischarge_data_17050_norm <- \n  discharge_data_17050$data %>% \n  mean(na.rm = TRUE)\ndischarge_data_17050_norm\n\n[1] 108.7257\n\n\nFor the long-term discharge at the station 17050, we thus get 109 m3/s. To compute the annual norm specific discharge, we need to compute the total discharge volume for a year and then divide by the total basin area.\n\nbasin_area <- st_area(basin_shp) %>% as.numeric()\ndischarge_data_17050_norm_vol <- \n  discharge_data_17050_norm * 3600 * 24 * 365 / basin_area * 1000\ndischarge_data_17050_norm_vol\n\n[1] 250.5378\n\n\nHence, for the period 1981 - 2010, we obtain for the specific discharge 250 mm. Clearly, \\(Q<P\\) and we can calculate that, on average, 2 parts of the total precipitation are discharged via the river whereas 1 part is evaporated without contribution to runoff at the gauge 17050 in Khorog town.\nAs an aside, the bias corrected precipitation climatology shows an interesting feature of the Gunt river basin (see Figure 7.6). Namely, there is a stark precipitation gradient between the western part of the basin where the bulk of the precipitation is observed and the hyper-arid Pamir plateau region to the east, where annual precipitation is below 200 mm at a mean altitude of above 4’000 meters above sea level [masl]. This place thus can be classified as alpine desert. This is an orographic effect as most of the moisture is washed out of the atmosphere before it enters the region of the plateau arriving from the westerly direction.\nDischarge Seasonality\nWhat about the seasonality of the CHELSA precipitation? Can it adequately reproduce the observed precipitation seasonality? If this would not be the case, we would have to reject the validity of the product and explore other high-resolution climatologies such as WorldClim V2 or CHPClim V1 (see (beck2020?) for more information on these products). Let us explore again the data of the Gunt River basin to answer this question.\nFirst, we load and prepare all the required station precipitation and geospatial data. Then, we compute the monthly norms of these data for the period 1981-01-01 through 2010-12-31. Note that the meteorological station at Khorog is located at the same place as the discharge gauging station and has the 5-digit index 38954 (see also the dedicated Example Catchments Chapter for more information).\n\nstation_data <- \n  readRDS(\"../caham_data/AmuDarya/17050_Gunt/GaugeData/17050_Q.Rds\")\n\n# extract the precipitation data for station 17050 between 1981 and 2010\npr_data_38954 <- \n  station_data %>%\n  filter(type == \"P\") %>% \n  filter(code == \"38954\") %>% \n  filter(date >= ymd(\"1981-01-01\")) %>% \n  filter(date <= ymd(\"2010-12-31\"))\n\n# add a month identifier to the dataframe and group by months\npr_data_38954 <- pr_data_38954 %>% \n  mutate(month = month(date)) %>% \n  group_by(month) %>% dplyr::select(date, data, month)\n\n# compute monthly mean values\np_monthly_mean_38954 <- pr_data_38954 %>% \n  summarize(data = mean(data, na.rm = TRUE)) %>% \n  add_column(data_source = \"Meteo Station 39954\")\n\n# plot the resulting monthly time series\np_monthly_mean_38954 %>% \n  ggplot(aes(x = month, y = data), color = data_source) + \n  geom_line() + \n  xlab(\"Month\") + \n  ylab(\"mm/month\") +\n  ggtitle(\"Monthly precipitation climatology from 1981 - 2010, Station 38954\")\n\n\n\n\nWe can extract the mean monthly values from the CHELSA SpatRaster data and then compare it to station data.\n\np_clim_monthly_path <- \n  \"../caham_data/central_asia_domain/climate/chelsa_v21/climatologies/hist_obs/monthly/pr_monthly_55_85_30_50.tif\"\np_clim_monthly <- terra::rast(p_clim_monthly_path)\n\np_monthly_mean_CHELSA_data <- \n  p_clim_monthly %>% exact_extract(basin_shp,'mean') %>% \n  as.numeric()\n\nWarning in .local(x, y, ...): Polygons transformed to raster CRS (EPSG:4326)\n\np_monthly_mean_CHELSA <- \n  p_monthly_mean_38954\np_monthly_mean_CHELSA$data <- \n  p_monthly_mean_CHELSA_data\np_monthly_mean_CHELSA$data_source <- \n  \"CHELSA\"\n\np_monthly_mean <- \n  p_monthly_mean_38954 %>% \n  add_row(p_monthly_mean_CHELSA)\n\n# plot the resulting monthly time series\np_monthly_mean %>% \n  ggplot(aes(x = month, y = data, color = data_source)) + \n  geom_line() + \n  xlab(\"Month\") + \n  ylab(\"mm/month\") +\n  ggtitle(\"Comparison of station and CHELSA precipitation climatologies\")\n\n\n\nFigure 7.7: ?(caption)\n\n\n\n\nAs is evident by looking at Figure 7.7, the CHELSA product can adequatly reproduce the seasonality of the local precipitation climatology and is only slightly overestimation absolute values. However, with regard to the later, this argument is not necessarily valied as we compare local point measurements with raster data with a resolution of 1 km2. A more thorow comparsion would generate an interpolated climatology field from station data and then compare these fields. However, as data is very scarce in this large basin, we do not have the means to perform such analysis.\n\n\n\n\n\n\nEXERCISE\n\n\n\nTry it yourself. Conduct the same analysis with the monthly temperature climatologies for the meteorological station and for the CHESLA data. You can very easily carry this out while reusing code blocks from above, also for any of the other basins available in the Case Study pack."
  },
  {
    "objectID": "climate_data.html#sec-climate-projections",
    "href": "climate_data.html#sec-climate-projections",
    "title": "7  Climate Data",
    "section": "\n7.2 Climate Projections",
    "text": "7.2 Climate Projections\nThe daily CHELSA V21 climate forcing data can be using for hydrological modeling from 1979 - 2010. But what about the future? After all, one of the main goals of this course book is to demonstrate how to use hydrological modeling to quantify future climate impacts in the Central Asian river basins. For this purpose, we will demonstrate in this Section how to download and process future climate data for studying hydrological changes.\n\n\nFigure 7.8: The time arrow is shown point from left to the right. The availability of corresponding data is indicated for what we call the historical reference period (hist_…) and the future scenario period (fut_…).\n\n\nTo start with, we need to divide the total period of interest into a historic period and a future period. The historic period in Figure 7.8 is highlighted in orange color and ranges from 1979 through 2010. It is the period for which we have observed climate forcing data from the CHELSA dataset and gauge data available at the same time. As will be discussed in the Chapter on hydrological modeling, it is also the period which we use to calibrate and validate our hydrological model.\nWhat we call the future period is highlighted by the blue arrow in Figure 7.8. Admittedly, the years from 2011 through 2022 are not in our current future (this edition of the book is written and published in 2022), so it really is just a matter of definition. It is this period over which we want to study climate impacts under different scenarios.\n\n7.2.1 Global Circulation Models\nThese scenarios are describing different increasing greenhouse concentration pathways are are computed with large-scale numerical models called General Circulation Models or GCMs. They globally represent physical processes in the atmosphere, the oceans, the cryosphere and the land surface. GCMs compute geographically distributed and physically consistent estimate of regional climate change and thus are the key inputs to different types of impact analyses. A stylized schematic structure of a GCM is shown in Figure 7.9.\n\n\nFigure 7.9: Schematic structure of a GCM model. Source: Penn State University, David Bice.\n\n\nFigure 7.9 shows that GCMs discretize the atmosphere, ocean and land columns into a three dimensional grid with differing numbers of vertical layers that is a function of the model and the compartment under consideration (atmosphere, land, ocean). With a typical horizontal resolution between 250 km - 600 km, the spatial resolution of the GCMs is coarse relative to what is needed for detailed impact studies. Furthermore, many key physical processes such as cloud formation happen at sub-grid resolution. These can thus only indirectly be represented by a process called parameterization and it represents a major source of uncertainty in GCM-based future climate simulations. Furthermore, since every GCM model represents processes and feedback mechanisms in the model in a different way, there is also inter-model uncertainty where different models generate different climate responses despite the same scenario-based forcing. Being cognisant of these uncertainties is important in impact studies. For more information, see also this website.\nFor illustration, ?fig-comparison-chelsa-gcm-resolution shows the temperature fields for 01. January 1979 over the Central Asia domain as provided by the CHELSA V21 data set (left panel) and as computed by the GCM GFDL-ESM4 model under the historic run. Note that there is no particular reason why we choose this model, it is just to serve as an example here. The complete list of models which we use for the climate change impact analysis will be presented and discussed further below.\nThe difference in resolution is striking with the CHELSA data having having a horizontal resolution of 1 km, approx., and the GCM model having a resolution of 1.5 degrees x 1 degrees which corresponds to 166.5 km x 111 km on the equator, approximately. Why is GCM resolution so coarse? It is, simply put, limited by restricitions given by the computational power of the powerful super computers where these models are run on.\n\n\n\n\n\nFigure 7.10: Comparison of CHELSA temperature climatology (left) and the GCM climate field of the historical run of the model GFDL-ESM4 (right).\n\n\n\n\n\n\nFigure 7.11: Comparison of CHELSA temperature climatology (left) and the GCM climate field of the historical run of the model GFDL-ESM4 (right).\n\n\n\n\n\nAs is indicated in Figure 7.9, GCM runs from the historic period from 1979 through 2010 are also available. These historic GCM runs are very important as we shall see in a minute when it comes to bias correcting and downscaling GCM runs onto the spatial units of interest, i.e. hydrological response units (HRUs) in our case.\n\n7.2.2 CMIP6 Climate Scenarios\nWhat are the future scenarios that we are interest in? The global climate science community has worked hard under the within the Phase 6 of the Coupled Model Intercomparison Project (CMIP6) the define relevant future scenarios that are describing different climate forcing trajectories. The paper by (O’Neill et al. 2016) is the refernce source with regard to detailed descriptions of the scenarios. We are interested to cover and study a broad range of possible hydrological future states and thus select 4 distrinct shared socioeconomic pathway (SSP) scenarios that cover this entire possible range.\nThe SSPs are based on five narratives describing broad socioeconomic trends that possibly shape future society. These are intended to span the range of plausible futures. The narratives are (taken from (Riahi et al. 2017)):\n\nSSP1 Sustainability – Taking the Green Road (Low challenges to mitigation and adaptation): The world shifts gradually, but pervasively, toward a more sustainable path, emphasizing more inclusive development that respects perceived environmental boundaries. Management of the global commons slowly improves, educational and health investments accelerate the demographic transition, and the emphasis on economic growth shifts toward a broader emphasis on human well-being. Driven by an increasing commitment to achieving development goals, inequality is reduced both across and within countries. Consumption is oriented toward low material growth and lower resource and energy intensity.\nSSP2 Middle of the Road (Medium challenges to mitigation and adaptation): The world follows a path in which social, economic, and technological trends do not shift markedly from historical patterns. Development and income growth proceeds unevenly, with some countries making relatively good progress while others fall short of expectations. Global and national institutions work toward but make slow progress in achieving sustainable development goals. Environmental systems experience degradation, although there are some improvements and overall the intensity of resource and energy use declines. Global population growth is moderate and levels off in the second half of the century. Income inequality persists or improves only slowly and challenges to reducing vulnerability to societal and environmental changes remain.\nSSP3 Regional Rivalry – A Rocky Road (High challenges to mitigation and adaptation): A resurgent nationalism, concerns about competitiveness and security, and regional conflicts push countries to increasingly focus on domestic or, at most, regional issues. Policies shift over time to become increasingly oriented toward national and regional security issues. Countries focus on achieving energy and food security goals within their own regions at the expense of broader-based development. Investments in education and technological development decline. Economic development is slow, consumption is material-intensive, and inequalities persist or worsen over time. Population growth is low in industrialized and high in developing countries. A low international priority for addressing environmental concerns leads to strong environmental degradation in some regions.\nSSP4 Inequality – A Road Divided (Low challenges to mitigation, high challenges to adaptation): Highly unequal investments in human capital, combined with increasing disparities in economic opportunity and political power, lead to increasing inequalities and stratification both across and within countries. Over time, a gap widens between an internationally-connected society that contributes to knowledge- and capital-intensive sectors of the global economy, and a fragmented collection of lower-income, poorly educated societies that work in a labor intensive, low-tech economy. Social cohesion degrades and conflict and unrest become increasingly common. Technology development is high in the high-tech economy and sectors. The globally connected energy sector diversifies, with investments in both carbon-intensive fuels like coal and unconventional oil, but also low-carbon energy sources. Environmental policies focus on local issues around middle and high income areas.\nSSP5 Fossil-fueled Development – Taking the Highway (High challenges to mitigation, low challenges to adaptation): This world places increasing faith in competitive markets, innovation and participatory societies to produce rapid technological progress and development of human capital as the path to sustainable development. Global markets are increasingly integrated. There are also strong investments in health, education, and institutions to enhance human and social capital. At the same time, the push for economic and social development is coupled with the exploitation of abundant fossil fuel resources and the adoption of resource and energy intensive lifestyles around the world. All these factors lead to rapid growth of the global economy, while global population peaks and declines in the 21st century. Local environmental problems like air pollution are successfully managed. There is faith in the ability to effectively manage social and ecological systems, including by geo-engineering if necessary.\n\nFigure 7.12 shows the underlying population and GDP developments for the corresponding SSPs and Table 7.1 details about the forcing in these scenarios.\n\n\nFigure 7.12: Global population and GDP developments under the CMIP6 shared socioeconomic pathways (taken from this source).\n\n\nThe description of the Tier 1 scenarios below that we are focussing on is taken from the aforementioned publication.\n\n\nTable 7.1: Scenarios, their forcing category and the effective radiative forcing by the year 2100 (O’Neill et al. 2016).\n\n\n\n\n\n\nScenario\nForcing Category\n2100 forcing [W/m2]\n\n\n\nShared Socioeconomic Pathway SSP5-8.5\nHigh\n8.5\n\n\nShared Socioeconomic Pathway SSP3-7.0\nHigh\n7\n\n\nShared Socioeconomic Pathway SSP2-4.5\nMedium\n4.5\n\n\nShared Socioeconomic Pathway SSP1-2.6\nLow\n2.6\n\n\n\n\nFor each scenario, we select 4 high priority GCM models for the preparation of downscaled climate forcings for the basins under consideration. The following Table 7.2 shows overview information about the models which are used in this course. Output of the GCM models shown in the table is available at daily timescales until 2100 and also for the historic runs.\n\n\nTable 7.2: Model names, models and host institution where the GCM models have been developed.\n\n\n\n\n\n\nName\nModel\nInstitution\n\n\n\nGFDL-ESM4\ngfdl- esm4\nNational Oceanic and Atmospheric Administration, Geophysical Fluid Dynamics Laboratory, Princeton, NJ 08540, USA\n\n\nUKESM1-0-LL\nukesm1- 0-ll\nMet Office Hadley Centre, Fitzroy Road, Exeter, Devon, EX1 3PB, UK\n\n\nMPI-ESM1-2-HR\nmpi- esm1-2- hr\nMax Planck Institute for Meteorology, Hamburg 20146, Germany\n\n\nIPSL-CM6A-LR\nipsl- cm6a-lr\nInstitut Pierre Simon Laplace, Paris 75252, France\n\n\nMRI-ESM2-0\nmri-esm2-0\nMeteorological Research Institute, Tsukuba, Ibaraki 305-0052, Japan\n\n\n\n\nGCM Model data can be downloaded from the climate store on the dedicated Copernicus website.\n\n\n\n\n\n\nImportant\n\n\n\nEach GCM model is different from the others. Some model leap days, some don’t. Some assume that each month has 30 days, other don’t. The preprocessing steps required to make the outputs of these models is very time consuming and is not recommendened for beginners. Therefore, please download the pre-processed climate scenarios for the Central Asia domain from this online repository.\n\n\n\n7.2.3 Downscaling and Bias Correction Using Quantile Mapping\nGCM model data can be subject to systematic biases (e.g. Kotlarski et al. 2014) and their coarse resolution does not allow us to directly use these data in hydrological modeling studies. For this reason, a large number of downscaling and bias correction techniques have been developed, inclduing the well-known delta change method (see e.g. Feigenwinter et al. 2018 and references therein).\nThe daily CHELSA V21 data that became available in 2021, together with daily GCM data from CMIP6 now allows a relatively straight forward application of empirical quantile mapping to downscale to and statistically correct GCM for hydrological response units (HRUs).\nFeigenwinter et al. (2018) explains clearly how empirical quantile mapping works. For the historical period (also called calibration period in the context of the discussion here), simulated model output (in our case, GCM hist_sim data, as shown in Figure 7.8) is corrected with a correction function towards an observational reference (here, the high-resolution CHELSA climatology) and systematic model biases are partly removed.\n\n\nFigure 7.13: Overview on the bias correction approach: a bias correction function is calibrated by comparing raw climate model output to observations in a common historical reference period. The calibrated correction function is then applied to the entire raw model output in order to produce a bias-corrected time series out into the future scenario period (taken from (Feigenwinter et al. 2018)).\n\n\nIn a climate change context, the so-called correction function (or transfer function), established in the historical calibration period, can then be applied to the simulated future time series in order to produce bias-corrected scenario time series.\n\n\nFigure 7.14: The nature of empirical quantile mapping is shown (source: (Feigenwinter et al. 2018)). Left panel: Example based on the probability density function (PDF). Right panel: example based on the cumulative distribution function (CDF).\n\n\nFigure 7.14 explains the bias correction approach graphically. A biased simulated distribution (blue) is corrected towards an observed distribution (black). In the example shown the raw simulated distribution is subject to both a bias of the mean and a bias in variance. The resulting bias-corrected distribution (dashed red) approximates the observed one but is typically not identical to it (e.g. due to the sampling uncertainty during the calibration of the correction function or details of the specific quantile mapping implementation).\nAs we explained above, we investigate 4 climate scenarios (ssp126, ssp245, ssp 370 and ssp585) for which we have 4 GCM model runs each (GFDL-ESM4, UKESM1-0-LL, MPI-ESM1-2-HR and MRI-ESM2-0). Hence, we have 16 scenario-model combination and the same amounts of correction functions. The best way to achieve this is show the necessary steps by means of an example catchment. While each catchment is unique, the steps to pre-process and later export the corresponding climate files for modeling in RSMinerve are not and are thus generalizable.\nThe only caveat is that the processing of the CHELSA high-resolution climate files requires the very large raw files to be available locally. Due to the size of these files for the entire Central Asia domain, sharing these files is not easy and we are working on ways to make these files more readily available in the future so that they can be processes locally.\nFor the moment, each case study catchment in the student pack contains the precomputed climate files with which RSMinerve hydrological models can be run in a straight forward manner. These files are stored in the RS_Minerve folder. The following files are available:\n\n\nhist_obs_rsm.csv: This is the .csv-files that contains the CHELSA V21 temperature and precipitation forcing for each of the elevation bands as specified in the /GIS/XXXXX_hru.shp file where XXXXX is the placeholder for the corresponding gauge code.\n\nhist_sim_….csv: For the 4 climate models investigated, the simulated history is stored in these file. They are used for bias correction and not directly used in hydrological modeling.\n\nfut_sim_….csv: We have generated 16 such files which consist of 4 climate scenarios for each of the 4 climate models. These are the future temperature and precipitation time series that were bias corrected using the quantile mapping method as explained above.\n\nfut_sim_bcsd_….csv: These are the 16 final bias corrected and downscaled future climate forcing files. These are the .csv-files that are read into RSMinerve for the study of climate impacts on the river basin under consideration.\n\nThe processes of generating these files is shown in the following at the example of Chon Kemin catchment in Kyrgyzstan. For each catchment in the Students’ Case Study Pack, the code to process the files can be found in the corresponding CODE folder.\nFirst, the necessary libraries need to be loaded.\n\n# Tidy data wrangling\nlibrary(tidyverse) # includes readr and ggplot2\nlibrary(lubridate)\nlibrary(timetk)\n\n# plotting add-ons to ggplot2\nlibrary(patchwork)\n\n# Our own package for load and processing local data\ndevtools::install_github(\"hydrosolutions/riversCentralAsia\")\nlibrary('riversCentralAsia')\n\n# Spatial data processing\nlibrary(raster)\nlibrary(terra)\nlibrary(sf)\nlibrary(stars)\nlibrary(exactextractr)\n\n# quantile mapping\nlibrary(qmap)\n\nThen, we can simply configure the details of the catchment at hand (note the location installation dependent specification of paths!).\n\n# River\nriver_name <- \"ChonKemin\"\nbasin_name <- \"Chu\"\n\n# Gauge\ngauge_name <- '15149_gauge'\ngauge_code <- '15149'\nq_path <- \"../caham_data/student_case_study_basins/15149_ChonKemin/GaugeData/\"\nq_name <- paste0(gauge_code,\"_Q.csv\")\ndata_type_Q <- \"Q\"\nunits_Q <- \"m3/s\"\n\n# GIS \n#Important naming convention. We assume that GIS-files adhere to the following naming convention:\n#- Basin Shapefile: paste0(gauge_code,\"_basin.shp\")\n#- River Shapefile: paste0(gauge_code,\"_river.shp\")\n#- Junctions Shapefile: paste0(gauge_code,\"_junctions.shp\")\n#- HRU Shapefile: paste0(gauge_code,\"_hru.shp\")\n#- DEM Raster: paste0(gauge_code,\"_dem.tif\")\ngis_path <- \"../caham_data/student_case_study_basins/15149_ChonKemin/GIS/\"\ndem_file <- paste0(gauge_code,'_dem.tif')\ncrs_project <- 4326 #latlon WGS84\n\nThere are a number of parameters to be set before the modeling which can be done as follows.\n\n# Time zone\ntz <-  \"UTC\"\n\n# GCM Climate Models and Simulations/Experiments and data paths\nhist_obs_dir <- \"../../../../../../../../../../../../Documents/ca_large_files/CA_CLIMATE_PROJECTIONS/CHELSA_V21_1979_2018/\" \nhist_sim_dir <- \"../../../../central_asia_domain/climate/hist_sim/\"\nfut_sim_dir <-  \"../../../../central_asia_domain/climate/fut_sim/\"\n\ngcm_Models <- c(\"GFDL-ESM4\", \"IPSL-CM6A-LR\", \"MRI-ESM2-0\", \"UKESM1-0-LL\")\ngcm_Scenarios <- c(\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\")\n\ngcm_Models_Scenarios <- base::expand.grid(gcm_Models,gcm_Scenarios) %>%\n        dplyr::mutate(model_scenario_combination = paste0(Var1,\"_\",Var2)) %>%\n        dplyr::select(model_scenario_combination) %>% unlist() %>% as.character()\n\n# Historical Observations\nhist_obs_start <- 1979\nhist_obs_end <- 2011\nhist_obs_dates <- riversCentralAsia::generateSeqDates(hist_obs_start,hist_obs_end,'day')\nhist_obs_dates <- as_date(hist_obs_dates$date) %>% as_tibble() %>% rename(Date = value)\n\n# Historical GCM Simulations\nhist_sim_start <- hist_obs_start\nhist_sim_end <- hist_obs_end\nhist_sim_dates <- hist_obs_dates\n\n# Future GCM Simulations\nfut_sim_start <- 2012\nfut_sim_end <- 2099\nfut_sim_dates <- riversCentralAsia::generateSeqDates(fut_sim_start,fut_sim_end,'day')\nfut_sim_dates <- as_date(fut_sim_dates$date) %>% as_tibble() %>% rename(Date = value)\n\n# Climate Data Observation Frequency\nobs_freq <- \"day\"\n\n# RSMinverve\nmodel_dir <- \"../../RS_MINERVE/\"\n\n## Dates\n\nhist_sim_dates <- hist_obs_dates\nfut_sim_dates <- riversCentralAsia::generateSeqDates(fut_sim_start,fut_sim_end,'day')\nfut_sim_dates <- as_date(fut_sim_dates$date) %>% as_tibble() %>% rename(Date = value)\n\n\n\n\n\n\n\nIMPORTANT\n\n\n\nNote that you would have to set the data_paths in the above code block according to your own local installation. However, since the CHELSA raster stack files are not available in the Students’ Case Study directory, the code here serves just as a demonstration. It should be further noted that this below is an advanced section that requires a good understanding of the R programming language.\n\n\nAfter setting the parameters, we can generate the hydrological response units. As the adept reader realizes, we are creating the elevation bands in R/RStudio and do not resort to QGIS as has been shown in the previous Geospation Data Section.\n\n# Parameter definition for the generation of the elevation bands\nband_interval <- 300 # in meters. Note that normally you want to work with band intervals of 100 m to 200 m. To make the model less computationally demanding, we work with a coarser resolution of 300 m. \nholeSize_km2 <- .1 # cleaning holes smaller than that size\nsmoothFact <- 2 # level of band smoothing\ndemAggFact <- 2 # dem aggregation factor (carefully fine-tune this)\n## Delineation\nhru_shp <- gen_basinElevationBands(gis_path,dem_file,demAggFact,band_interval,holeSize_km2,smoothFact)\n# Control output\nhru_shp %>% plot()\n\n\n\n\nIn other words, the function gen_basinElevationBands() from the riversCentralAsia R Package creates elevation bands (HRUs) as per the parameter values. In the above example, we are generating elevation bands with a 300 meters [m] bands interval. The smaller this number, the higher the number of elevation bands that will be generated and the higher the computational requirements will be of the hydrological model.\nAs a next step, we intersect the subbasins with elevtion bands. In the example of Chon Kemin, the basin corresponds to the one subbasin.\n\npath2subbasin_shp <- paste0(gis_path,gauge_code,\"_basin.shp\")\nsubbasins_shp <- st_read(path2subbasin_shp, quiet = TRUE)\nsubbasins_hru_shp <- st_intersection(hru_shp,subbasins_shp)\n\nWe can extract and add mean elevation data for each HRU in the following way.\n\ndem <- raster::raster(paste0(gis_path,dem_file))\nzonalStat_Z <- exactextractr::exact_extract(dem, subbasins_hru_shp, 'mean', progress = FALSE)\nsubbasins_hru_shp$Z <- zonalStat_Z\n\nIn a final step, we add unique subbasin names and remove the shapefile fields that are no longer needed in the next steps. The plot shows the resulting shapefile.\n\nfor (idxSubBasin in seq(length(subbasins_shp$name))) {\n  \n  subbasin_sel <- subbasins_hru_shp %>% dplyr::filter(name == subbasins_shp$name[idxSubBasin])\n  subbasin_sel <- subbasin_sel %>% dplyr::arrange(Z)\n  subbasin_sel$hru_num <- (1:base::nrow(subbasin_sel))\n  subbasin_sel$name <- paste0(subbasin_sel$name,'_',subbasin_sel$hru_num)\n  \n  if (idxSubBasin == 1) {\n    res_subbasins <- subbasin_sel\n  } else {\n    res_subbasins <- res_subbasins %>% dplyr::add_row(subbasin_sel)\n  }\n  \n}\n\nsubbasins_hru_shp <- res_subbasins %>% dplyr::select(-layer,-hru_num)\nsubbasins_hru_shp %>% plot()\n\n\n\n\nThe resulting shapefile can then be written to local storage.\n\nsf::st_write(subbasins_hru_shp, paste0(gis_path, gauge_code, '_hru', '.shp'), append = FALSE)\n\nThe geometry of the hydrological modeling approach is now defined and we can start to extract and generate the climate forcing data. First, the historical observations (hist_obs) of temperature and precipitation for each HRU need to be defined. We show how this can be done in a straight forward manner using again helper functions from the riversCentralAsia package.\n\n# Parameters\nclimate_data_type <- \"hist_obs\"\n\n# Load HRU shapefile\nsubbasins_hru_shp <- \n  sf::st_read(paste0(gis_path,gauge_code,'_HRU','.shp'))\n\n# List CHELSA climate files\nclimate_files_tas <- \n  list.files(hist_obs_dir,pattern = \"tas_\",full.names = TRUE)\nclimate_files_pr <-  \n  list.files(hist_obs_dir,pattern = \"pr_\",full.names = TRUE)\n\n# Restrict years range\nn_years <- \n  hist_obs_start:hist_obs_end\nclimate_files_tas <- \n  climate_files_tas[1:length(n_years)]\nclimate_files_pr <- \n  climate_files_pr[1:length(n_years)]\n\n# Temperature data processing\ntemp_or_precip <- \"Temperature\"\nhist_obs_tas <- riversCentralAsia::gen_HRU_Climate_CSV_RSMinerve(climate_files_tas,\n                                              river_name,\n                                              temp_or_precip,\n                                              subbasins_hru_shp,\n                                              hist_obs_start,\n                                              hist_obs_end,\n                                              obs_freq,\n                                              climate_data_type,\n                                              crs_project)\n\n# Precipitation data processing\ntemp_or_precip <- \"Precipitation\"\nhist_obs_pr <- riversCentralAsia::gen_HRU_Climate_CSV_RSMinerve(climate_files_pr,\n                                              river_name,\n                                              temp_or_precip,\n                                              subbasins_hru_shp,\n                                              hist_obs_start,\n                                              hist_obs_end,\n                                              obs_freq,\n                                              climate_data_type,\n                                              crs_project)\n\n# Combine extract climate tibbles.\nhist_obs_rsm <- hist_obs_tas %>% add_column(hist_obs_pr %>% \n                                              dplyr::select(-Station),.name_repair = 'unique')\n\n# Add Discharge Data (monthly)\nq_dec <- riversCentralAsia::loadTabularData(q_path,\n                                              q_name,\n                                              gauge_code,\n                                              gauge_name,\n                                              river_name,\n                                              basin_name,\n                                              data_type_Q,\n                                              units_Q)\nfunc_type_lib <- list(mean = \"Q\")\nq_mon <- aggregate_to_monthly(q_dec,func_type_lib)\n\nq_mon <- q_mon %>% \n  mutate(date = floor_date(as.POSIXct(.$date,tz = tz), unit = \"day\")) \n# Note, the function above outputs dates as date class. \n# This causes problems down the road. \n# We address these by converting the date values to dttm class.\n\nq_mon <- q_mon %>% dplyr::select(date, data) %>% \n  dplyr::filter(date >= ymd(paste0(hist_obs_start, \"-01-01\"))) %>% \n  dplyr::filter(date <= ymd(paste0(hist_obs_end, \"-12-31\")))\n\ndates_char_Q <- \n  riversCentralAsia::posixct2rsminerveChar(q_mon$date, tz = \"UTC\") %>% \n  rename(Station = value) %>% \n  tibble::add_column(Q = (q_mon$data %>% as.character))\n\n# Get gauge location and elevation\ngauge_shp <- sf::st_read(paste0(gis_path,gauge_code,\"_gauge.shp\"))\ndem <- raster::raster(paste0(gis_path,dem_file))\ngauge_coord <- gauge_shp %>% sf::st_coordinates()\ngauge_Z <- exactextractr::exact_extract(dem,gauge_shp,'mean')\n\n# Combine everything\nhist_obs_rsm <- dplyr::full_join(hist_obs_rsm,dates_char_Q, by = 'Station') \n\n# now finish off by giving the required attributes in the table for the discharge station\nhist_obs_rsm$Q[1] = data_type_Q\nhist_obs_rsm$Q[2] = gauge_coord[1]\nhist_obs_rsm$Q[3] = gauge_coord[2]\nhist_obs_rsm$Q[4] = 550\nhist_obs_rsm$Q[5] = data_type_Q\nhist_obs_rsm$Q[6] = 'Flow'\nhist_obs_rsm$Q[7] = units_Q\nhist_obs_rsm$Q[8] = 'Constant after'\n\n# Write final file to disk\nreadr::write_csv(hist_obs_rsm,paste0(model_dir,climate_data_type,\"_rsm.csv\"),na = \"NA\",col_names = FALSE)\n\nThe last line of code writes the result back to the disk. Please check in your Case Study Pack the folder /RS_MINERVE/ where the corresponding hist_obs_rsm.csv is stored. The file format corresponds to import requirements from the side of RSMinerve. You can open the text file either with a Spreadsheet program such as Excel or with a text editor. The daily time series of temperature, precipitation for each HRU and the discharge for the gauging station are stored in the columns with a header section. Since observed discharge data is only available on a monthly basis, the time series is filled with NA where there are no obervations available.\nFor the monthly mean discharge values, the following convention is adhered to. Monthly mean discharge values are written/stored in the first day of the month.\nNext, we can process the historical GCM runs in a similar fashion.\n\n# Parameters\noutput_file_dir <- \"../../RS_MINERVE/\"\nclimate_data_type <- \"hist_sim\"\nclimate_files_tas <- list.files(hist_sim_dir,pattern = \"tas_\",full.names = TRUE)\nclimate_files_pr <- list.files(hist_sim_dir,pattern = \"pr_\",full.names = TRUE)\n\n# Extract GCM Model-Specfic Data\nhist_sim_rsm <- vector(mode = \"list\", length = length(gcm_Models))\nnames(hist_sim_rsm) <- gcm_Models\n\nfor (idxGCM in seq(length(gcm_Models))) {\n  \n  temp_or_precip <- \"Temperature\"\n  gcm_model <- gcm_Models[idxGCM]\n  hist_sim_T <- riversCentralAsia::gen_HRU_Climate_CSV_RSMinerve(climate_files_tas[idxGCM],\n                                              river_name,\n                                              temp_or_precip,\n                                              subbasins_hru_shp,\n                                              hist_sim_start,\n                                              hist_sim_end,\n                                              obs_freq,\n                                              climate_data_type,\n                                              crs_project)\n \n  temp_or_precip <- \"Precipitation\"\n  hist_sim_P <- riversCentralAsia::gen_HRU_Climate_CSV_RSMinerve(climate_files_pr[idxGCM],\n                                              river_name,\n                                              temp_or_precip,\n                                              subbasins_hru_shp,\n                                              hist_sim_start,\n                                              hist_sim_end,\n                                              obs_freq,\n                                              climate_data_type,\n                                              crs_project)\n  \n  hist_sim_rsm[[idxGCM]] <- hist_sim_T %>% \n    tibble::add_column(hist_sim_P %>% dplyr::select(-Station),.name_repair = 'unique')\n  write_csv(hist_sim_rsm[[idxGCM]],\n            paste0(model_dir,climate_data_type,\"_\",gcm_model,\"_\",\n                   gauge_code,\"_\",hist_sim_start,\"_\",hist_sim_end,\".csv\"),\n            col_names = FALSE)\n  \n}\n\nThe same applies to the future climate scenario runs.\n\nclimate_data_type <- \"fut_sim\"\n\n# Load HRU shapefile\nsubbasins_hru_shp <- sf::st_read(paste0(gis_path,gauge_code,'_HRU','.shp'))\n\n# Process and Extract GCM Model-Specific Data\nfut_sim_rsm <- base::vector(mode = \"list\", length = length(gcm_Models_Scenarios))\nnames(fut_sim_rsm) <- gcm_Models_Scenarios # now we have a named list\n\ndebug_T <- fut_sim_rsm\n\nfor (idxGCM in seq(length(gcm_Models_Scenarios))) {\n\n  # GCM Model and Scenario\n  str2proc <- gcm_Models_Scenarios[idxGCM]\n  gcm_model <- substr(str2proc, 1, nchar(str2proc) - 7)\n  gcm_scenario <- substr(str2proc, nchar(str2proc) - 6 + 1, nchar(str2proc))\n\n  # Process files - tas\n  temp_or_precip <- \"Temperature\"\n  climate_file_tas <- \n    list.files(fut_sim_dir,pattern = paste0(\"tas_day_\",gcm_Models_Scenarios[idxGCM]),full.names = TRUE)\n  fut_sim_T <- gen_HRU_Climate_CSV_RSMinerve(climate_file_tas,\n                                             river_name,\n                                             temp_or_precip,\n                                             subbasins_hru_shp,\n                                             fut_sim_start,\n                                             fut_sim_end,\n                                             obs_freq,\n                                             climate_data_type,\n                                             crs_project)\n  \n  # Process files - pr\n  temp_or_precip <- \"Precipitation\"\n  climate_file_pr <- \n    list.files(fut_sim_dir,pattern = paste0(\"pr_day_\",gcm_Models_Scenarios[idxGCM]),full.names = TRUE)\n  fut_sim_P <- gen_HRU_Climate_CSV_RSMinerve(climate_file_pr,\n                                             river_name,\n                                             temp_or_precip,\n                                             subbasins_hru_shp,\n                                             fut_sim_start,\n                                             fut_sim_end,\n                                             obs_freq,\n                                             climate_data_type,\n                                             crs_project)\n  \n  # Final dataframe\n  fut_sim_rsm[[idxGCM]] <- fut_sim_T %>% \n    tibble::add_column(fut_sim_P %>% dplyr::select(-Station),.name_repair = 'unique')\n  # Write result to disk\n  readr::write_csv(fut_sim_rsm[[idxGCM]],\n                   paste0(model_dir,climate_data_type,\"_\",gcm_model,\"_\",\n                          gcm_scenario,\"_\",river_name,\"_\",fut_sim_start,\"_\",\n                          fut_sim_end,\".csv\"),\n                   col_names = FALSE)\n}\n\nIn a final step, we can produce the bias corrected future climate scenarios with the following code.\n\n# Preparations\n## HRUs\nsubbasins_hru_shp <- \n  sf::st_read(paste0(gis_path,gauge_code,'_hru','.shp'))\nn_hru <- subbasins_hru_shp %>% nrow()\nhru_names <- subbasins_hru_shp$name\n\n# ================\n# Prepare hist_obs\n# ================\nclimate_data_type <- \"hist_obs\"\nhist_obs_path <- paste0(model_dir,climate_data_type,\"_rsm.csv\")\nhist_obs_orig <- hist_obs_path %>% \n  readr::read_csv(col_types = cols(.default = col_character())) %>%\n  dplyr::select(-Station,-Q)\n\n# Extract data by groups and convert T to deg. K\nhist_obs_T <- hist_obs_orig[,1:n_hru] %>% slice(-1:-7) %>% \n  type_convert() %>% \n  mutate(across(.cols = everything(), ~ . + 273.15))\nhist_obs_P <- \n  hist_obs_orig[, (n_hru + 1):(2 * n_hru)] %>% \n  slice(-1:-7) %>%  \n  type_convert()\n\n# Fix row names\nnames(hist_obs_T) <- hru_names\nnames(hist_obs_P) <- hru_names\n\nhist_obs_T_df <- hist_obs_T %>% as.data.frame()\nrow.names(hist_obs_T_df) <- hist_obs_dates$Date %>% as.character()\nhist_obs_P_df <- hist_obs_P %>% as.data.frame()\nrow.names(hist_obs_P_df) <- hist_obs_dates$Date %>% as.character()\n\n# ================\n# Prepare hist_sim\n# ================\nhist_sim_T_list <- \n  base::vector(mode = \"list\", length = length(gcm_Models))\nhist_sim_P_list <- \n  base::vector(mode = \"list\", length = length(gcm_Models))\nnames(hist_sim_T_list) <- \n  gcm_Models # now we have a named list\nnames(hist_sim_P_list) <- \n  gcm_Models # now we have a named list\n\nfor (idxGCM in 1:length(gcm_Models)) {\n  hist_sim_path <- \n    list.files(model_dir,\n               pattern = paste0(\"hist_sim_\",gcm_Models[idxGCM]),full.names = TRUE)\n  hist_sim_orig <- hist_sim_path %>% \n    readr::read_csv(col_types = cols(.default = col_character())) %>% \n    dplyr::select(-Station)\n  \n  hist_sim_T <- hist_sim_orig[,1:n_hru] %>% slice(-1:-7) %>% \n    type_convert() %>% \n    mutate(across(.cols = everything(), ~ . + 273.15))\n  hist_sim_P <- hist_sim_orig[,(n_hru+1):(2*n_hru)] %>% \n    slice(-1:-7) %>% \n    type.convert()\n  \n  # Fix row names\n  names(hist_sim_T) <- hru_names\n  names(hist_sim_P) <- hru_names\n  \n  hist_sim_T_df <- hist_sim_T %>% as.data.frame()\n  row.names(hist_sim_T_df) <- hist_sim_dates$Date %>% as.character()\n  hist_sim_P_df <- hist_sim_P %>% as.data.frame()\n  row.names(hist_sim_P_df) <- hist_sim_dates$Date %>% as.character()\n  \n  hist_sim_T_list[[idxGCM]] <- hist_sim_T_df\n  hist_sim_P_list[[idxGCM]] <- hist_sim_P_df\n}\n\n# ===============\n# Prepare fut_sim\n# ===============\nfut_sim_T_list <- base::vector(mode = \"list\", length = length(gcm_Models_Scenarios))\nfut_sim_P_list <- base::vector(mode = \"list\", length = length(gcm_Models_Scenarios))\nnames(fut_sim_T_list) <- gcm_Models_Scenarios # now we have a named list\nnames(fut_sim_P_list) <- gcm_Models_Scenarios\nfut_sim_T_list_bcsd <- fut_sim_T_list\nfut_sim_P_list_bcsc <- fut_sim_P_list\n\nfor (idxGCM in 1:length(gcm_Models_Scenarios)) {\n  fut_sim_orig_path <- \n    list.files(model_dir,pattern = \n                 paste0(\"fut_sim_\",gcm_Models_Scenarios[idxGCM]),full.names = TRUE)\n  fut_sim_orig <- fut_sim_orig_path %>% \n    readr::read_csv(col_types = cols(.default = col_character())) %>% \n    dplyr::select(-Station)\n  \n  fut_sim_T <- fut_sim_orig[,1:n_hru] %>% \n    slice(-1:-7) %>% \n    type_convert() %>% \n    mutate(across(.cols = everything(), ~ . + 273.15))\n  fut_sim_P <- fut_sim_orig[,(n_hru+1):(2*n_hru)] %>% \n    slice(-1:-7) %>% \n    type.convert()\n  \n  # Fix row names\n  names(fut_sim_T) <- hru_names\n  names(fut_sim_P) <- hru_names\n  \n  fut_sim_T_df <- fut_sim_T %>% as.data.frame()\n  row.names(fut_sim_T_df) <- fut_sim_dates$Date %>% as.character()\n  fut_sim_P_df <- fut_sim_P %>% as.data.frame()\n  row.names(fut_sim_P_df) <- fut_sim_dates$Date %>% as.character()\n  \n  fut_sim_T_list[[idxGCM]] <- fut_sim_T_df\n  fut_sim_P_list[[idxGCM]] <- fut_sim_P_df\n}\n\n# ===================\n# Do quantile mapping\n# ===================\n\n# --- Debugging\nfut_sim_rsm_qmapped <- \n  base::vector(mode = \"list\", length = length(gcm_Models_Scenarios))\nnames(fut_sim_rsm_qmapped) <- gcm_Models_Scenarios # now we have a named list\n# -----\n\nfor (idxGCM in 1:length(gcm_Models_Scenarios)) {\n  \n  # Preparation\n  str2proc <- gcm_Models_Scenarios[idxGCM]\n  gcm_model <- substr(str2proc, 1, nchar(str2proc) - 7)\n  gcm_scenario <- substr(str2proc, nchar(str2proc) - 6 + 1, nchar(str2proc))\n  \n  # Bias correction\n  hist_sim_T_df_gcmModel <- hist_sim_T_list[[gcm_model]]\n  hist_sim_P_df_gcmModel <- hist_sim_P_list[[gcm_model]]\n  \n  fut_sim_T_df_gcmModel <- fut_sim_T_list[[idxGCM]]\n  fut_sim_P_df_gcmModel <- fut_sim_P_list[[idxGCM]]\n  \n  qmap_param_T_gcm <- fitQmap(hist_obs_T_df, hist_sim_T_df_gcmModel, method = \"QUANT\")\n  qmap_param_P_gcm <- fitQmap(hist_obs_P_df, hist_sim_P_df_gcmModel, method = \"QUANT\")\n  \n  # T bias correction\n  fut_sim_T_df_gcmModel_qmapped <- \n    doQmap(fut_sim_T_df_gcmModel,qmap_param_T_gcm)\n  # Fill 0s where present. Occasionally, there are 0s resulting from the quantile mapping these \n  # are ironed out here by filling the missing values using the ones from the preceeding observation. \n  fut_sim_T_df_gcmModel_qmapped[fut_sim_T_df_gcmModel_qmapped == 0] <- NA\n  fut_sim_T_df_gcmModel_qmapped <- \n    zoo::na.locf(fut_sim_T_df_gcmModel_qmapped)\n  # P bias correction\n  fut_sim_P_df_gcmModel_qmapped <- \n    doQmap(fut_sim_P_df_gcmModel,qmap_param_P_gcm)\n\n  # go back to tibble and convert back to deg. C\n  fut_sim_T_gcmModel_qmapped <- \n    fut_sim_T_df_gcmModel_qmapped %>% as_tibble() %>% \n    add_column(Date = fut_sim_dates$Date,.before = 1)\n  fut_sim_T_gcmModel_qmapped <- \n    fut_sim_T_gcmModel_qmapped %>% mutate(across(-Date, ~ . - 273.15))\n  fut_sim_P_gcmModel_qmapped <- \n    fut_sim_P_df_gcmModel_qmapped %>% as_tibble() %>% \n    add_column(Date = fut_sim_dates$Date,.before = 1)  \n  \n  fut_sim_T_list[[idxGCM]] <- fut_sim_T_gcmModel_qmapped\n  fut_sim_P_list[[idxGCM]] <- fut_sim_P_gcmModel_qmapped\n  \n  # Export to .csv-file\n  fut_sim_qmapped <- \n    fut_sim_T_gcmModel_qmapped %>% #dplyr::select(-Date) %>% \n    tibble::add_column(fut_sim_P_gcmModel_qmapped %>% \n                         dplyr::select(-Date),.name_repair = \"universal\") %>% \n    mutate(across(.cols = everything(),~ as.character(.))) %>% \n    dplyr::select(-Date)\n  \n  # --- Debugging\n  fut_sim_rsm_qmapped[[idxGCM]] <- fut_sim_qmapped\n  # ---\n  \n  fut_sim_orig_path <- \n    list.files(model_dir,pattern = paste0(\"fut_sim_\",gcm_Models_Scenarios[idxGCM]),\n               full.names = TRUE)\n  fut_sim_orig <- \n    fut_sim_orig_path %>% readr::read_csv(col_types = cols(.default = col_character())) \n  \n  fut_sim_orig_header <- fut_sim_orig %>% dplyr::select(-Station) %>% \n    dplyr::slice(1:7,) \n  fut_sim_qmapped <- fut_sim_orig_header  %>% bind_rows(fut_sim_qmapped) %>% \n    add_column(Station = fut_sim_orig$Station,.before = 1)\n\n  # Write result to disk\n  climate_data_type <- \"fut_sim_bcsd\"\n  readr::write_csv(fut_sim_qmapped,\n                   paste0(model_dir,climate_data_type,\"_\",gcm_model,\"_\",\n                          gcm_scenario,\"_\",river_name,\"_\",fut_sim_start,\"_\",\n                          fut_sim_end,\".csv\"),\n                   col_names = FALSE)\n}\n\n\n\n\n\nBuisán, Samuel T., Michael E. Earle, José Luı́s Collado, John Kochendorfer, Javier Alastrué, Mareile Wolff, Craig D. Smith, and Juan I. López-Moreno. 2017. “Assessment of Snowfall Accumulation Underestimation by Tipping Bucket Gauges in the Spanish Operational Network.” Atmospheric Measurement Techniques 10 (3): 1079–91.\n\n\n“CHELSA-W5e5 V1.0: W5e5 V1.0 Downscaled with CHELSA V2.0.” 2022. ISIMIP Repository. https://doi.org/10.48364/ISIMIP.836809.2.\n\n\nFeigenwinter, I., S. Kotlarski, A. Casanueva, A. M. Fischer, C. Schwierz, and M. A. Liniger. 2018. “Exploring Quantile Mapping as a Tool to Produce User-Tailored Climate Scenarios for Switzerland.” Technical Report 270. Federal Office of Meteorology; Climatology MeteoSwiss.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nKarger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-resolution monthly precipitation and temperature time series from 2006 to 2100.” Scientific Data 7 (1): 248. https://doi.org/10.1038/s41597-020-00587-y.\n\n\nKarger, Dirk Nikolaus, Adam M. Wilson, Colin Mahony, Niklaus E. Zimmermann, and Walter Jetz. 2021. “Global daily 1 km land surface precipitation based on cloud cover-informed downscaling.” Scientific Data 8 (1): 307. https://doi.org/10.1038/s41597-021-01084-6.\n\n\nKotlarski, S., K. Keuler, O. B. Christensen, A. Colette, M. Déqué, A. Gobiet, K. Goergen, et al. 2014. “Regional climate modeling on European scales: a joint standard evaluation of the EURO-CORDEX RCM ensemble.” Geoscientific Model Development 7 (4): 1297–1333. https://doi.org/10.5194/gmd-7-1297-2014.\n\n\nO’Neill, Brian C., Claudia Tebaldi, Detlef P. van Vuuren, Veronika Eyring, Pierre Friedlingstein, George Hurtt, Reto Knutti, et al. 2016. “The Scenario Model Intercomparison Project (ScenarioMIP) for CMIP6.” Geoscientific Model Development 9 (9): 3461–82. https://doi.org/10.5194/gmd-9-3461-2016.\n\n\nRiahi, Keywan, Detlef P. van Vuuren, Elmar Kriegler, Jae Edmonds, Brian C. O’Neill, Shinichiro Fujimori, Nico Bauer, et al. 2017. “The Shared Socioeconomic Pathways and their energy, land use, and greenhouse gas emissions implications: An overview.” Global Environmental Change 42: 153–68. https://doi.org/10.1016/j.gloenvcha.2016.05.009."
  },
  {
    "objectID": "hydrological_modeling.html",
    "href": "hydrological_modeling.html",
    "title": "Part III: Hydrological Modeling & Applications",
    "section": "",
    "text": "This part of the book focusses on different types of hydrological modeling approaches and applications. The Chapter on hydrological modeling using rainfall-runoff models introduces in a hands-on manner modeling using the free-tu-use RSMinerve Software Suite. These types of models are foundational for example for basin planning exercises where tradeoffs between water for different sectoral allocations need to be quantified in a specific context.\nSuch models are also important for detailed climate impact studies regularly used to study these. The idea is simple, i.e., to use available climate model output over the 21st century as forcing and investigate changes in the hydrographs at stations of interest over time. When different models and scenarios are run, an band of uncertainty can be specified which is relevant in any decision-making context.\nFinally, the design of hydropower infrastructure depends on hydrological assessments with such types of models. The model outputs, i.e., simulated (modelled) discharge at a particular location, can be used to compute cummulative flow duration curves which are essential for the assessment of the hydropower potential and critically inform infrastructure sizing.\nThe relevance of these types of models for the water planners and managers in the global drylands cannot be overstated and therefore, one of the primary goals of this course is to familiarze the students well with such types of models.\nThe Chapter on long-term hydrological modeling using the Budyko framework looks at the greater semi-arid Central Asia region as compared to individual catchments. It is at this scale and over a large number of smaller catchments where interesting steady-state patterns of the partioning of available water into evporation and runoff can be studied, under current and future climate states. Among other applications, such types of models can help to inform the large-scale questions, also with regard to the current and future inter-state water distribution.\nFinally, the Chapter on time series modeling using predictive inference discusses models that learn from past patterns to predict the future, without explicit water balance constraints. Through learning patterns in time-ordered data, possibly also with the help of auxilliary data such as preseason snow cover, it has been shown that time series models can be powerful to predict discharge at certain specific location for different lead times, from hours to seasons. The section will present such type approaches in the context of the seasonal forcasting of river flows in Central Asia."
  },
  {
    "objectID": "hydraulic_hydrological_modeling.html",
    "href": "hydraulic_hydrological_modeling.html",
    "title": "8  Hydrological-Hydraulic Modeling",
    "section": "",
    "text": "8.0.1 Pre"
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-budyko-introduction",
    "href": "long_term_water_balance_modeling.html#sec-budyko-introduction",
    "title": "9  Long-term Water Balance Modeling",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nThe general water balance of a catchment can be written as\n\\[\n\\Delta S = P - E - Q\n\\qquad(9.1)\\]\nwhere \\(\\Delta S\\) is net storage change in millimeter [mm], \\(P\\) is precipitation in mm, \\(E\\) is evaporation in mm, and \\(Q\\) is specific discharge in mm. Evaporation is the phenomenon by which a substance is converted from its liquid into its vapor phase, independently of where it lies in nature (Miralles et al. 2020). This definition of evaporation encompasses evaporation from inside leaves (transpiration), evaporation from bare soils, evaporation from intercepted precipitation (interception loss), evaporation from open water surfaces, and finally, evaporation over ice- and snow-covered surfaces (often referred to as sublimation).\nOver the period of a hydrological year and longer time scales, we expect \\(\\Delta S\\) to be 0 since neither water storage nor destorage happen over longer periods. This would of course not be true for catchments where for example man-made storage infrastructure was built over the period under consideration or for catchments with ongoing glacier melt over a prolonged time. If \\(\\Delta S = 0\\), the above Equation @ref(eq:WB1) can be rewritten as\n\\[\nQ = P - E\n\\qquad(9.2)\\]\nDividing by \\(P\\), we get\n\\[\n\\frac{Q}{P} = 1 - \\frac{E}{P}\n\\qquad(9.3)\\]\nwhere \\(Q/P\\) can be called the runoff index and \\(E/P\\) is the evaporation index or evaporative fraction.\nFor a catchment, annual mean \\(E\\) and \\(Q\\) are governed by total water supply \\(P\\) and the total available energy which is normally expressed as potential evaporation \\(E_{pot}\\) and which denotes the (atmospheric) water demand. If \\(E_{pot}\\) is small, the discharge \\(Q\\) is normally bigger than evaporation \\(E\\). Similarly, if the available radiative energy is very high, the water demand \\(E_{pot}\\) is very large and \\(Q<<E\\) (Arora 2002). \\(E_{pot}\\) and \\(P\\) are thus the key determinants of annual or longer timescale runoff and evaporation rates. Michael Budyko has termed the ratio \\(E_{pot} / P\\) aridity index (Budyko 1974).\nAs explained above, water demand is determined by energy. Solar radiation is the primary energy source for the earth-atmosphere system and the key driver of the hydrological cycle. At the earth’s surface, the net radiative flux \\(R_N\\) is the energy that is available for a) heating and cooling of the soil (ground heat flux), b) changing the phase of water (latent heat flux), and c) heating or cooling air in the boundary layer thus causing atmospheric dynamics (sensible heat flux).\nThis can be formalized with the following relationship\n\\[\nR_{N} = H_{S} + H_{L} + \\Delta H_{G}\n\\qquad(9.4)\\]\nwhere \\(R_{N}\\) is the net radiation [in W/m2 = kg/s3], \\(H_{S}\\) is the upward sensible heat flux, \\(H_{L}\\) is the latent heat flux and \\(\\Delta H_{G}\\) the net ground heat flux. The latent heat flux is directly proportional to evaporation \\(E\\). Thus, \\(H_{L} = L \\cdot E\\) where \\(L = 2.5 \\cdot 10^{6}\\) J/kg [= m2/s2] is the latent heat of vaporization and \\(E\\) is the actual evaporation in [m/s]. As in the case of the water balance, at the annual or longer time scales, we can neglect the heat storage effect in the ground and get\n\\[\nR_{N} = H_{S} + L \\cdot E\n\\qquad(9.5)\\]\nWith the Bowen ratio defined as the fraction of the sensible heat flux divided by the latent heat flux, i.e.\n\\[\n\\gamma = \\frac{H_{S}}{H_{L}} = \\frac{H_{S}}{L \\cdot E }\n\\qquad(9.6)\\]\nand by rearranging the terms, the long-term energy balance in Equation Equation 9.5 can simply be rewritten as\n\\[\nR_{N} = (1 + \\gamma)L E\n\\qquad(9.7)\\]\nUsing the fact that \\(R_{N} = L E_{pot}\\), where \\(E_{pot}\\) is the potential evaporation, and dividing by precipitation, we can rewrite the above Equation 9.7 as\n\\[\n\\frac{E_{pot}}{P} = (1 + \\gamma) \\frac{E}{P}\n\\qquad(9.8)\\]\nwhere the left-hand side is called the aridity index, i.e. \\(\\phi = E_{pot}/P\\) and \\(E/P\\) is called the evaporative fraction or evaporation index as mentioned above. With this, Equation 9.8 from above can be written as a function of the Bowen ratio and the aridity index, i.e.\n\\[\n\\frac{E}{P} = 1 - \\frac{Q}{P} = \\frac{\\phi}{(1 + \\gamma)}\n\\qquad(9.9)\\]\n\\(Q/P\\) is again the runoff index. Since the Bowen ratio is also water supply and energy demand limited, it too is a function of the aridity index and we can thus rewrite Equation 9.9 as\n\\[\n\\frac{E}{P} = \\frac{\\phi}{1 + f(\\phi)} = F[\\phi]\n\\qquad(9.10)\\]\nThe Budyko relationship thus allows for a simple parameterization of how the aridity index \\(\\phi\\) controls the long-term mean partitioning of precipitation into stream-flow and evaporation and it is capable of capturing the behavior of thousands of catchments around the world. This explains its growing popularity over recent years (Berghuijs, Gnann, and Woods 2020).\n\n\n\nFigure @ref(fig:budykoSpace) shows a plot of data from catchments in the US for which consistent long-term hydro-climatological data records are available. Individual catchments’ aridity indices are plotted against evaporative fractions, averaged over many years. The catchment data plots along the Budyko curve in the two-dimensional Budyko space as indicated in the Figure where the Budyko curve is defined as\n\\[\\begin{equation}\n  \\frac{E}{P} = \\left[ \\frac{E_{pot}}{P} \\text{tanh} \\left( \\frac{P}{E_{pot}} \\right) \\left( 1 - \\text{exp} \\left( - \\frac{E_{pot}}{P} \\right) \\right) \\right]^{1/2}\n  (\\#eq:OriginalBudykoCurveEquation)\n\\end{equation}\\]\nThis non-parametric relationship between the aridity index and the evaporative fraction was developed by M. Budyko (Budyko 1951).\nThe Budyko space is delineated by the demand and supply limits. Catchments within the space should theoretically fall below the supply limit (\\(E/P = 1\\)) and the demand limit (\\(E/E_{pot} = 1\\)), but tend to approach these limits under very arid or very wet conditions (Berghuijs, Gnann, and Woods 2020). The data from the US shows that a large percentage of in-between catchment variability can be explained by the Budyko curve. After the seminal work Budyko in the last century, the evidence for a strong universal relationship between aridity and evaporative fraction via the Budyko curve has since grown. As catchment hydrology still lacks a comprehensive theory that could explain this simple behavior across diverse catchments Gentine et al. (2012), the ongoing debate about the the underlying reasons for this relationship continues (see e.g. (Padron et al. 2017; Berghuijs, Gnann, and Woods 2020)).\nWhile almost all catchments plot within a small envelope of the original Budyko curve, systematic deviations are nevertheless observed from the original Budyko curve. Several new expressions for \\(F[\\phi]\\) were therefore developed to describe the long-term catchment water balance with one parameter (see e.g. Budyko (1974); Sposito (2017); Choudhury (1999)). One popular equation using only 1 parameter is the Choudhury equation which relates the aridity index \\(\\phi\\) to the evaporative fraction \\(E/P\\) in the following way\n\\[\\begin{equation}\n  \\frac{E}{P} = \\left[ 1 + \\left( \\frac{E_{pot}}{P} \\right) ^{-n} \\right]^{1/n}\n  (\\#eq:Choudhury1)\n\\end{equation}\\]\nwhere \\(n\\) is a catchment-specific parameter which accounts for factors such as vegetation type and coverage, soil type and topography, etc. (see e.g. Zhang et al. (2015) for more information). In other words, \\(n\\) integrates the net effects of all controls of of the evaporative fraction other than aridity. The Figure @ref(fig:ChoudhuryEquationStateSpace) shows the control of \\(n\\) over the shape of the Budyko Curve."
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-budyko-data-and-methods",
    "href": "long_term_water_balance_modeling.html#sec-budyko-data-and-methods",
    "title": "9  Long-term Water Balance Modeling",
    "section": "9.2 Data and Methods",
    "text": "9.2 Data and Methods\n\n9.2.1 Data\nA large number of geospatial data were collected for the Central Asia region. The domain of interest was defined as 55 deg. E - 85. deg. E and 30 deg. N - 50 deg. N.. Shapefiles from the large river basin were retrieved from the Global Runoff Data Center and extracted for the following basins: Amu Darya, Chu, Issy Kul, Murghab-Harirud, Syr Darya and Talas. Where necessary, the polygons of the downstream flat areas were corrected to account for man-made water transfers via large canal systems and corresponding flow alterations across basins there. These large river basins define the area of interest (AOI).\nFor the selected basins, the WMOBB River Network data was extracted from the layers wmobb_rivnets_Q09_10 (containing line sections representing an upland area above 4’504 km2), wmobb_rivnets_Q08_09 (containing line sections representing an upland area between 1’150 km2 and 4’504 km2) and wmobb_rivnets_Q07_08 (containing line sections representing an upland area above between 487 and 1’150 km2) (GRDC, Koblenz, Germany: Federal Institute of Hydrology (BfG). 2020). Permanent water bodies and courses were taken from the global HydroLakes Database (Messager et al. 2016). Information on land cover were taken from the Copernicus Global Land Service: Land Cover 100m: collection 3: epoch 2019: Globe data (Buchhorn et al. 2019). The NASA SRTM digital elevation model 1 Arc-second (30 m) global product was used as a DEM (“NASA Shuttle Radar Topography Mission (SRTM)(2013)” 2013).\nIn total, data from 277 gauging stations from Afghanistan, Kyrgyzstan, Kazakhstan, Uzbekistan and Tajikistan could be obtained from the local Hydrometeorological Organization, public reports and the Soviet compendia Surface Water Resources, Vol 14 Issues 1 and 3 . Except for the Afghan stations, all stations were manually located in a Geographic Information System (GIS) using the relevant Soviet Military Topographic maps (1:200’000) from the corresponding region. The maps were downloaded from https://maps.vlasenko.net and subsequently geo-referenced in QGIS (QGIS Development Team 2021). Data from northern Afghan rivers’ stream flow characteristics and the location of these gauging stations was taken from (Olson and Williams-Sether 2010).\nFor each gauge, the contributing area was delineated in R with the WhiteboxTools v2.0.0 and long-term norm mean discharge was obtained over variable observation periods between 1900 and 2018 was acquired. For a few selected stations, monthly and decadal time series data are available over the entire observational record. The FLO1K, global maps of mean, maximum and minimum annual stream flow at 1 km resolution from 1960 through 2015 were retrieved (Barbarossa et al. 2018). The goodness of the FLO1K product in the Central Asia domain was validated at the locations of the 277 gauges through linear regression.\nGeospatial information on glaciers was taken from the Randolph Glacier Inventory (RGI) 6.0. Information from 16’617 glaciers was retrieved, together with glacier length, thickness and glacier thinning rates Hugonnet et al. (2021).\nThe CHELSA V21 global daily high-resolution climatology, available from 01-01-1979 until 31-12-2011 was processed over the Central Asia domain to map climate trends, including on temperature, precipitation, snow fraction. The data is available upon request from this site: https://chelsa-climate.org Karger et al. (2021). The CHELSA V21 product is corrected for snow undercatch in the high elevation ranges and thus is able to better represent actual high mountain precipitation than other available global climatologies (Beck et al. 2020). The aridity index (AI) fields were taken from the bio-climate CHELSA V21 data set and compared with the CGIAR AI product (Trabucco and Zomer 2019). Data on an additional 70 bio-climatic indicators were downloaded from the CHELSA V21 1980 - 2010 climatology and statistics extracted for each of the 277 gauged catchments, together with the AI.\nHigh-resolution crop disaggregated irrigated areas were mapped over the entire Central Asia domain (Ragettli, Herberz, and Siegfried 2018). Like this 30 m crop maps were produced with Google Earth Engine using unsupervised classification for the years 2016 - 2020. Vector information on the irrigation systems in the Chu and Talas River basins as well as from the Uzbek Fergana Oblast, including the land cadaster there, are available.\nFinally, data from the GOODD data set was used to retrieve information from 88 dams in the region of interest (Mulligan, Soesbergen, and Sáenz 2020).\n\n\n9.2.2 Methods\nA strategy for hydrological modeling of the regional Central Asian hydrology using the Budyko framework was devised. The Budyko principle posits that, over the long-run, runoff at a particular location is governed by the long-term availability of water (supply) and energy (demand) there (Budyko M., 1974). Under this assumption, the evaporative fraction of a basin, i.e. the long-term mean actual evaporation divided by long-term mean precipitation, can be expressed as a function of the aridity index (long-term mean potential evaporation divided by long-term mean precipitation)."
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-budyko-results",
    "href": "long_term_water_balance_modeling.html#sec-budyko-results",
    "title": "9  Long-term Water Balance Modeling",
    "section": "9.3 Results",
    "text": "9.3 Results"
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-discussion-and-conclusions",
    "href": "long_term_water_balance_modeling.html#sec-discussion-and-conclusions",
    "title": "9  Long-term Water Balance Modeling",
    "section": "9.4 Discussion and Conclusions",
    "text": "9.4 Discussion and Conclusions\n\n\n\n\nArora, Vivek K. 2002. “The Use of the Aridity Index to Assess Climate Change Effect on Annual Runoff.” Journal of Hydrology 265 (1): 164–77. https://doi.org/https://doi.org/10.1016/S0022-1694(02)00101-4.\n\n\nBarbarossa, Valerio, Mark A. J. Huijbregts, Arthur H. W. Beusen, Hylke E. Beck, Henry King, and Aafke M. Schipper. 2018. “Flo1k, Global Maps of Mean, Maximum and Minimum Annual Streamflow at 1 Km Resolution from 1960 Through 2015.” Scientific Data 5 (1): 180052. https://doi.org/10.1038/sdata.2018.52.\n\n\nBeck, Hylke E., Eric F. Wood, Tim R. McVicar, Mauricio Zambrano-Bigiarini, Camila Alvarez-Garreton, Oscar M. Baez-Villanueva, Justin Sheffield, and Dirk N. Karger. 2020. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1.\n\n\nBerghuijs, W. R., S. J. Gnann, and R. A. Woods. 2020. “Unanswered Questions on the Budyko Framework.” Hydrological Processes.\n\n\nBuchhorn, M., B. Smets, L. Bertels, B. De Roo, M. Lesiv, N. E. Tsendbazar, M. Herold, and S. Fritz. 2019. “Copernicus Global Land Service: Land Cover 100m: Collection 3: Epoch 2019: Globe.”\n\n\nBudyko, M. I. 1951. “On Climatic Factors of Runof (in Russian).” Problemy Fiz Geeografii 16: 41–48.\n\n\n———. 1974. Climate and Life. Academic Press.\n\n\nChoudhury, B. J. 1999. “Evaluation of an Empirical Equation for Annual Evaporation Using Field Observations and Results from a Biophysical Model.” Journal of Hydrology 216: 99–110.\n\n\nFarinotti, Daniel, Matthias Huss, Johannes J. Fürst, Johannes Landmann, Horst Machguth, Fabien Maussion, and Ankur Pandit. 2019. “A Consensus Estimate for the Ice Thickness Distribution of All Glaciers on Earth.” Nature Geoscience 12 (3): 168–73. https://doi.org/10.1038/s41561-019-0300-3.\n\n\nGentine, P., P. D’Odorico B. R. Lintner, G. Sivandran, and G. Salvucci. 2012. “Interdependence of Climate, Soil, and Vegetation as Constrained by the Budyko Curve.” Geophysical Research Letters 39 (19).\n\n\nGLIMS, and NSIDC. 2005, updated 2018. Global Land Ice Measurements from Space Glacier Database. Compiled and made available by the international GLIMS community and the National Snow and Ice Data Center, Boulder CO, U.S.A. DOI:10.7265/N5V98602.\n\n\nGRDC, Koblenz, Germany: Federal Institute of Hydrology (BfG). 2020. “Major River Basins of the World / Global Runoff Data Centre, GRDC. 2nd, Rev. Ext. Ed.” Shape.\n\n\nHugonnet, Romain, Robert McNabb, Etienne Berthier, Brian Menounos, Christopher Nuth, Luc Girod, Daniel Farinotti, et al. 2021. “Accelerated Global Glacier Mass Loss in the Early Twenty-First Century.” Nature 592 (7856): 726–31. https://doi.org/10.1038/s41586-021-03436-z.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nKarger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-resolution monthly precipitation and temperature time series from 2006 to 2100.” Scientific Data 7 (1): 248. https://doi.org/10.1038/s41597-020-00587-y.\n\n\nKarger, Dirk Nikolaus, Adam M. Wilson, Colin Mahony, Niklaus E. Zimmermann, and Walter Jetz. 2021. “Global daily 1 km land surface precipitation based on cloud cover-informed downscaling.” Scientific Data 8 (1): 307. https://doi.org/10.1038/s41597-021-01084-6.\n\n\nMessager, M. L., B. Lehner, Grill G., I. Nedeva, and O. Schmitt. 2016. “Estimating the Volume and Age of Water Stored in Global Lakes Using a Geo-Statistical Approach.” Nature Communications 13603.\n\n\nMiralles, D. G., W. Brutsaert, A. J. Dolman, and J. H. Gash. 2020. “On the Use of the Term \"Evapotranspiration\".” Water Resources Research 56 (https://doi.org/10.1029/2020WR028055).\n\n\nMulligan, Mark, Arnout van Soesbergen, and Leonardo Sáenz. 2020. “GOODD, a Global Dataset of More Than 38,000 Georeferenced Dams.” Scientific Data 7 (1): 31. https://doi.org/10.1038/s41597-020-0362-5.\n\n\n“NASA Shuttle Radar Topography Mission (SRTM)(2013).” 2013. NASA. https://earthdata.nasa.gov/learn/articles/nasa-shuttle-radar-topography-mission-srtm-version-3-0-global-1-arc-second-data-released-over-asia-and-australia.\n\n\nOlson, S. A., and T. Williams-Sether. 2010. “Streamflow Characteristics at Streamgages in Northern Afghanistan and Selected Locations.” U.S. Geological Survey Data Series 529. USGS.\n\n\nPadron, R. S., L. Gudmundsson, P. Greve, and S. Seneviratne. 2017. “Largescale Controls of the Surface Water Balance over Land: Insights from a Systematic Review and Meta-Analysis.” Water Resources Research.\n\n\nQGIS Development Team. 2021. QGIS Geographic Information System. QGIS Association.\n\n\nRagettli, Silvan, Timo Herberz, and Tobias Siegfried. 2018. “An Unsupervised Classification Algorithm for Multi- Temporal Irrigated Area Mapping in Central Asia.” Remote Sensing 10 (11): 1823. https://doi.org/10.3390/rs10111823.\n\n\nSposito, Garrison. 2017. “Understanding the Budyko Equation.” Water 9 (4): 236. https://doi.org/10.3390/w9040236.\n\n\nTrabucco, Antonio, and Robert Zomer. 2019. “Global Aridity Index and Potential Evapotranspiration (ET0) Climate Database v2,” January. https://doi.org/10.6084/m9.figshare.7504448.v3.\n\n\nZhang, D., Z. Cong, G. Ni, D. Yang, and S. Hu. 2015. “Effects of snow ratio on annual runoff within the Budyko framework.” Hydrology and Earth System Sciences 19 (4): 1977–92. https://doi.org/10.5194/hess-19-1977-2015."
  },
  {
    "objectID": "real_world_examples.html",
    "href": "real_world_examples.html",
    "title": "11  Real World Examples",
    "section": "",
    "text": "Here, we provide a tour of model applications in the domain of the management of water resources in Central Asia. This Chapter is currently under active development.\nPlease check back later."
  },
  {
    "objectID": "appendix_a_free_software.html#literature",
    "href": "appendix_a_free_software.html#literature",
    "title": "Appendix A — Software",
    "section": "A.1 Literature",
    "text": "A.1 Literature\nMany authors of scientific literature are on the web platform researchgate where they can privately share their work with students (users need to register for an account)."
  },
  {
    "objectID": "appendix_a_free_software.html#sec-open-resouces-software-QGIS",
    "href": "appendix_a_free_software.html#sec-open-resouces-software-QGIS",
    "title": "Appendix A — Software",
    "section": "A.2 QGIS",
    "text": "A.2 QGIS\nQGIS is a free and open source Geographical Information System that offers very similar tools as their commercial counterparts. The latest version of QGIS can be downloaded from the QGIS website. We recommend to install the stable long-term support version (installation guide).\n\nA.2.1 Resources for learning QGIS\nA general tutorial for beginners is the QGIS training manual. It includes a short chapter on the use of QGIS for hydrological analysis (Chapter 17.16). For this course you should be familiar with the QGIS window and know the difference between raster and vector data. If you have used QGIS or a similar GIS software before you will not need to do a tutorial prior to this course."
  },
  {
    "objectID": "appendix_a_free_software.html#sec-open-resouces-software-R",
    "href": "appendix_a_free_software.html#sec-open-resouces-software-R",
    "title": "Appendix A — Software",
    "section": "A.3 R and RStudio",
    "text": "A.3 R and RStudio\nR is a free and open source statistical programming language. It’s large user community ensure active development and up-to-date help resources available on the internet. RStudio is a free user interface for R. To install R and RStudio follow the installation guide on ModernDive - Statistical Inference via Data Science.\nFor the bare beginners, also with regard to programming, the book Hands-On Programming with R is an excellent start\n\nA.3.1 Resources for learning R and R studio\n\n“Help! I’m new to R and RStudio and I need to learn them! What do I do?” If you’re asking yourself this, this book is for you: ModernDive - Statistical Inference via Data Science.\nA thorough guide for data science in R: R for Data Science\n\n\n\nA.3.2 RS Minerve\n\nHow to download and install RS Minerve\nGo to the software download page of CREALP’s website https://www.crealp.ch/fr/accueil/outils-services/logiciels/rs-minerve/telechargement-rsm.html (last accessed March 18, 2021) and click on Version actuelle to download the latest installer for Windows as shown in Figure A.1. This will start the download process for the installer RSMinerve-install.exe.\n\n\n\nFigure A.1: Download RS Minerve from the CREALP website https://www.crealp.ch/fr/accueil/outils-services/logiciels/rs-minerve/telechargement-rsm.html (last accessed March 18, 2021).\n\n\nYou should also download the user manual (RS MINERVE user manual, written in English) and the example files used for the tutorials in the user manual (Exemple de fichiers, a zip file with data) as well as the technical manual (RS MINERVE technical manual, written in English).\nOnce the installer is downloaded, install RSMinerve with a double-click on the installer and follow the Setup guide. Open RSMinerve once you have it installed.\nBack to the prerequisites for RS Minerve modelling"
  },
  {
    "objectID": "appendix_b_riverscentralasia_r_toolbox.html",
    "href": "appendix_b_riverscentralasia_r_toolbox.html",
    "title": "Appendix B — R Toolbox riversCentralAsia",
    "section": "",
    "text": "More information can be found on Github, where the package is maintained."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-register-with-earth-explorer",
    "href": "appendix_c_quick_guides.html#sec-register-with-earth-explorer",
    "title": "Appendix C — Quick Guides",
    "section": "Earth Explorer: Register for an Account",
    "text": "Earth Explorer: Register for an Account\nIn your internet browser, navigate to https://earthexplorer.usgs.gov/. The window will look similar as in Figure C.1.\n\n\n\nFigure C.1: Start window of the Earth Explorer interface.\n\n\nClick on Login in the top right corner. This will bring you to a new window where you can click on Create New Account which will open a form where you enter your information. After verifying your account by clicking on the appropriate link that you will be sent, you can download data from the Earth Explorer interface.\n\nEE: Download SRTM Data for a Selected Region\n\nLogin to the Earth Explorer (Register if you haven’t done so before How to).\nNavigate to your area of interest in the map panel of the Earth Explorer interface.\nDraw a polygon around your area of interest by clicking on the map (see Figure C.2).\nIn the Data Set tab, look for the SRTM 1 arc-second global DEM (see Figure C.3) and select it by ticking the box next to the product name in the list.\n\n\n\n\n\nFigure C.2: Define a polygon around your area of interest by clicking on the map.\n\n\n\n\n\nFigure C.3: Search for the SRTM 1 arc-second global DEM product.\n\n\n\nVerify that the result layer(s) cover your area of interest by pressing the foot icon in the Results tab and download if you are satisfied (Figure C.4).\n\n\n\n\nFigure C.4: Verify the results of your search and download the data products you need.\n\n\nBack to the Load DEM section."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-sofware-qgis-installation-guide",
    "href": "appendix_c_quick_guides.html#sec-sofware-qgis-installation-guide",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS Installation Guide",
    "text": "QGIS Installation Guide\nIn your web browser go to https://qgis.org/en/site/forusers/download.html (see Figure C.5).\n\n\n\nFigure C.5: The QGIS download site.\n\n\nChoose the long-term support version of QGIS for your operating system (e.g. if you use a laptop with a 64-bit Windows operating system, open the Download for Windows tab and click on QGIS Standalone Installer Version 3.16 (64 bit), see Figure C.6)). Clicking on the installer will start the download.\n\n\n\nFigure C.6: The QGIS installer if you work on a 64-bit Windows operating system.\n\n\nDouble-click on the downloaded file to start the installation. Typically, SAGA GIS and GRASS GIS are installed alongside QGIS if you choose this installing option."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-qgis-window-overview",
    "href": "appendix_c_quick_guides.html#sec-qgis-window-overview",
    "title": "Appendix C — Quick Guides",
    "section": "The QGIS Window - Overview",
    "text": "The QGIS Window - Overview\nThe main parts of the QGIS window which are referenced in this tutorial, are highlighted in Figure C.7.\n\n\n\nFigure C.7: The QGIS window."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-saving-a-new-qgis-project",
    "href": "appendix_c_quick_guides.html#sec-saving-a-new-qgis-project",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Saving a New QGIS Project",
    "text": "QGIS: Saving a New QGIS Project\nOpen your QGIS and open a new QGIS project by moving your cursor on the white sheet symbol in the upper-left corner of your QGIS window (see Figure C.8).\n\n\n\nFigure C.8: Tutorial project open in QGIS LTS.\n\n\nSave the QGIS project by pressing on the disk icon and selecting a location for the file on your computer and a name for the project. You can add freely available on-line maps as background.\nBack to setting up of QGIS."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-change-project-projection-qgis",
    "href": "appendix_c_quick_guides.html#sec-change-project-projection-qgis",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Change the Projection of the QGIS Project",
    "text": "QGIS: Change the Projection of the QGIS Project\nFor this tutorial, the projection of the QGIS project should be in EPSG:32642 (i.e., UTM 42 N). Change it by clicking on the projects projection in the lower right corner of the QGIS window (see Figure C.7). In the coordinate reference system (CRS) tab of the Project Properties, select WGS84 / UTM zone 42N with ID EPSG:32642 and click OK.\nFor modeling your own sample catchment, a different UTM zone may be applicable. The map in the lower right corner of the Project Properties window shows the area for the projection in red and the area where your data is located in violet so you can visually verify that you choose an appropriate projection.\nGenerally, you want to choose the CRS so that you have minimal distortion through the projection in your area of interest. The CRS with ID EPSG:32462 suits well for all of the student exercise catchments.\nBack to the setting up of QGIS."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-install-and-activate-plugins-in-QGIS3",
    "href": "appendix_c_quick_guides.html#sec-install-and-activate-plugins-in-QGIS3",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Install and activate plugins in QGIS3",
    "text": "QGIS: Install and activate plugins in QGIS3\nNavigate to Plugins in the header toolbar and go to Manage and Install Plugins …. Search for the plugin name and go to Install Plugin to install a plugin or tick the box to the left of the plugin name in the list of plugins to activate it (see Figure C.9).\n\n\n\nFigure C.9: Plugin management window in QGIS 3.16. Install plugin with the Install Plugin button. Activate installed plugins by ticking the box in the list.\n\n\nBack to the setting up of QGIS."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-managing-panels",
    "href": "appendix_c_quick_guides.html#sec-managing-panels",
    "title": "Appendix C — Quick Guides",
    "section": "GQIS: Managing Panels Visibility in QGIS",
    "text": "GQIS: Managing Panels Visibility in QGIS\nShould one of the panels described here not be visible in your QGIS window navigate to View in your header toolbar and then to Panels. The visible panels are marked with a tick. Click on a panel name in the list to activate or deactivate it.\nBack to setting up of QGIS."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-loading-public-background-layers",
    "href": "appendix_c_quick_guides.html#sec-loading-public-background-layers",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Loading Public Background Maps",
    "text": "QGIS: Loading Public Background Maps\nYou can add freely available on-line maps as backgrounds. Note that they will only be available as long as your computer is connected to the internet. In the Browser panel (see What to do if you don’t see the Browser panel), move the cursor to XYZ Tiles, do a right-click and select New Connection. Enter a descriptive name for the map layer and one of the links below and click OK. The new map layer will appear under XYZ Tiles. By double-clicking on the layer you can add it to your Layers panel.\nBack to setting up of QGIS."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-zoom-to-layer",
    "href": "appendix_c_quick_guides.html#sec-zoom-to-layer",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Zoom to Layer",
    "text": "QGIS: Zoom to Layer\nYou can tell QGIS to zoom to the selected layer by selecting a layer in the Layers window and then on the white sheet and magnifying glass icon in the toolbar (see Figure C.10).\n\n\n\nFigure C.10: Zoom to layer.\n\n\nYou can also perform a right-click on the layer name in the Layers panel and select Zoom to layer."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-srtm-plugin",
    "href": "appendix_c_quick_guides.html#sec-srtm-plugin",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Import SRTM layers using the SRTM plugin",
    "text": "QGIS: Import SRTM layers using the SRTM plugin\nMake sure the SRTM plugin is installed (see How to). Navigate to Plugins in the header toolbar and there to SRTM Downloader. Select the SRTM Downloader. Set the boundaries of the SRTM tiles to download and press the Download button. Close the window when done. The SRTM tiles are loaded to the Layers pane.\nBack to Load DEM in QGIS section"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-merge-srtm-tiles",
    "href": "appendix_c_quick_guides.html#sec-merge-srtm-tiles",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Merge SRTM Tiles to a Single Layer",
    "text": "QGIS: Merge SRTM Tiles to a Single Layer\nNavigate to Raster in the header toolbar and then to Miscellaneous and Merge…. In the Merge window that opens, select the input layers by clicking on the … button. Tick the layers that need to be merged and press Run. When the algorithm is done, close the window. You can zoom to the extent of the new layer (see How to). You can change the color of the DEM file.\nBack to Load DEM in QGIS section"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-load-dem",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-load-dem",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Add Raster Layer",
    "text": "QGIS: Add Raster Layer\nIn your QGIS window, navigate to Layer in your header toolbar, there to Add Layer and left-click on Add Raster Layer (see Figure C.11). The Data Source Manager will open in a pop-up window (see Figure C.12).\n\n\n\nFigure C.11: Add raster layer to QGIS project, step 1.: Navigate to the Data Source Manager.\n\n\n\n\n\nFigure C.12: Add raster layer to QGIS project, step 2.: Browse for the raster file to add to the QGIS project by pressing on the box with the three dots (…) to the right of the raster source input field.\n\n\nIn the example above, a DEM for the example of the Nauvalisoy river catchment is loaded. For loading the DEM of your sample catchments, browse for the DEM in the corresponding folder you downloaded from the provided Dropbox directory.\nPress the Add button at the bottom right of the Data Source Manager window to load the raster layer to your QGIS project and close the window by pressing the Close button (to the left of the Add button).\nYour QGIS project now shows a grey-scale version of the raster layer you loaded. If you are satisfied with the change, save your project by pressing the disk icon at the upper left corner of your QGIS window.\nYou can change the color of your raster file (how to). Here is a quick-guide of how to apply a topography-style color band to your DEM."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-change-color-of-raster-layer",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-change-color-of-raster-layer",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Change Color of Raster Layer",
    "text": "QGIS: Change Color of Raster Layer\nYou can change the colors of your cut DEM by double-clicking on the layer name in the Layers window. Go to tab Symbology (with the paint and brush icon) and choose Render type Singleband-pseudocolor (see Figure C.13) and select a Color ramp.\n\n\n\nFigure C.13: Nicely color your DEM, step 1.\n\n\nQGIS comes with a large library of color ramps but you can also create your own. See below a description of how to get your DEM in a topography style color palette."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-topograpy-color-ramp",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-topograpy-color-ramp",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Topography-style color palettes",
    "text": "QGIS: Topography-style color palettes\nFor our DEM we choose an existing topography-style color ramp. Open the Layer Properties window with a double-click on the raster layer in the Layers pane of the QGIS window. In the Symbology tab click the triangle to the right of the color ramp and select Create New Color Ramp… (see Figure C.14).\n\n\n\nFigure C.14: Nicely color your DEM, step 2.\n\n\nIn the drop down menu of the pop-up window choose Catalog: cpt-city and press OK (see Figure C.15).\n\n\n\nFigure C.15: Nicely color your DEM, step 3.\n\n\nA this will open another window containing the catalog of existing color ramps. Under Topography, we choose sd-a and press OK (see Figure C.16).\n\n\n\nFigure C.16: Nicely color your DEM, step 4.\n\n\nGo to tab Transparency, set Global Opacity to 30% and specify the No Data Value 0 (see Figure C.17).\n\n\n\nFigure C.17: Nicely color your DEM, step 5.\n\n\nThen go back to the Symbology tab. The minimum value of the color ramp should now not be 0 but 272. Adapt manually if need be. Then, press classify to get a discrete color ramp for your map and Apply to the map. If you are happy with the colors, quit by pressing OK (see Figure C.18).\n\n\n\nFigure C.18: Nicely color your DEM, step 6.\n\n\nThe result will look like Figure C.19.\n\n\n\nFigure C.19: Nicely color your DEM, step 7.\n\n\nYou can add decorations (e.g. scale and north arrow) to your map (how to)."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-add-map-decorations",
    "href": "appendix_c_quick_guides.html#sec-add-map-decorations",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Add Map Decorations",
    "text": "QGIS: Add Map Decorations\nNavigate to View -> Decorations and choose among the decorations to add. Many options for configuring the decorations are available."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-verify-projection-reproject",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-verify-projection-reproject",
    "title": "Appendix C — Quick Guides",
    "section": "C.1 QGIS: Verify Projection of Layer and Re-project Layer",
    "text": "C.1 QGIS: Verify Projection of Layer and Re-project Layer\nOpen the Layer Properties window with a double-click on the layer in the Layers pane and navigate to the Information tab. Under CRS (coordinate reference system) you see the projection of the layer. For the Chirchiq river basin, the CRS should say “EPSG:32642 - WGS 84 / UTM zone 42N - Projected”. Other river catchments in Central Asia may require a different UTM zone. For the student exercises, it is good to choose this projection for all basins.\nA raster layer can be reprojected to a different CRS: Go to Raster in the header toolbar. From there move the cursor over Projections and click on Warp (Reproject)…. The Warp window will pop up (in fact, it is just an interface where the user can specify the parameters of the wrap algorithm in a convient way). Select the raster layer you wish to re-project in the Input layer and select the Target CRS.\nIf the target CRS you wish to re-project to is not available in the drop-down menu, you can browse for it by clicking on the globe icon to thr right of the Target CRS section. You may re-sample the raster to a coarser resolution by specifying the Output file resolution. You may also specify to save the reprojected raster layer: Scroll to the bottom of the Warp (Reproject) window where you see Reprojected and a white box where you can browse for a location to store the new layer.\n\n\n\n\n\n\nWARNING\n\n\n\nIf you do not specify a target location, only a temporary layer will be loaded to QGIS which will not be available anymore after you close the QGIS project (even if you save the project).\n\n\nYou may decide to load the temporary file and save it later (how to). Click the Run button at the bottom right to start the re-projection algorithm and press Close when the process is done. The reprojected layer will be available in the Layers pane.\nA vector layer can be reprojected by selecting Vector in the header toolbar, moving the cursor to Data Management Tools and clicking on Reproject Layer…. This opens the Reproject Layer window where you can specify a Target CRS and optionally a storage location. As for the reprojection of the raster layer, a temporary layer is loaded to your QGIS project if you do not specify a storage location. However, you can always store temporary layers later (how to).\nBack to the load DEM section."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guide-save-temp-layer",
    "href": "appendix_c_quick_guides.html#sec-quick-guide-save-temp-layer",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Save a Temporary Layer",
    "text": "QGIS: Save a Temporary Layer\nRight-click on a temporary layer in the Layers pane. Temporary layers are indicated by a box to the right of the layer name. From the menu that opens upon right-click, select Export and Save As… (for raster layers) or Save Feature As… (for vector layers). An explorer window will open where you can specify a file name and a location to store the file to."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-add-vector-layer-to-qgis",
    "href": "appendix_c_quick_guides.html#sec-add-vector-layer-to-qgis",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Add Vector Layer",
    "text": "QGIS: Add Vector Layer\nLeft-clicking on Layer, moving your cursor over Add Layer and left-clicking Add Vector Layer (see Figure C.20).\n\n\n\nFigure C.20: Add vector layer to QGIS project, step 1.: Navigate to the Data Source Manager.\n\n\nA window will pop up, asking you to specify the properties of the vector layer to add (see Figure C.21).\n\n\n\nFigure C.21: Add vector layer to QGIS project, step 2: Select vector layers to add. Note that you select the shp file but cpg, dbf and shx need to be present in the same location.\n\n\nPress the box with the three dots on the right of the source field to specify the location of the shape file to be added to your project (see Figure C.22). Click Open and the window will close. The address of your shape files should now stand in the source field as in Figure C.21.\n\n\n\nFigure C.22: Add vector layer to QGIS project, step 3.\n\n\nNote that you load the .shp file but that all the files in the list in Figure C.21 need to be present. In the Add Vector Layer window, click Add and then close the window. QGIS has attributed a random color to your shape file which can be changed manually How to."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-change-color-of-vector-layer",
    "href": "appendix_c_quick_guides.html#sec-change-color-of-vector-layer",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Change Color of a Vector Layer",
    "text": "QGIS: Change Color of a Vector Layer\nYou can change the color of your layer by double-clicking on the layer name in the Layers Window to the left of the map. This will open the properties window. The third tab from the top shows paint and brush (see Figure C.23).\n\n\n\nFigure C.23: dd vector layer to QGIS project, step 5. Change the color of the layer.\n\n\nYou can activate Simple fill by clicking on it and select No brush in the drop-down menu in order to only show the outline of your vector layer (see Figure C.24).\n\n\n\nFigure C.24: Add vector layer to QGIS project, step 6. Only show the outline of the vector layer."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-fill-sinks",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-fill-sinks",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Fill Sinks",
    "text": "QGIS: Fill Sinks\nBrowse for the Fill sinks algorithm in the Processing Toolbox panel (see Figure C.25). If the Processing Toolbox panel is not visible go to Processing in the header toolbar and click on Toolbox to activate it. Alternatively, here is how to manage the visibility of panels in QGIS.\n\n\n\nFigure C.25: Search for the Fill sinks algorithm in the Processing Toolbar panel.\n\n\nOpen the Fill sinks window with a double-click on the name of the algorithm in the Processing Toolbar panel. There, select the DEM you want to process and browse for a location to store the output file (see Figure C.26). You can leave the default minimum slope.\n\n\n\nFigure C.26: Select the raster file to process.\n\n\nWhen the algorithm is done it will load the new layer into your QGIS project. Close the Fill sinks window and save your project.\nBack to catchment delineation."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guide-upslope-area",
    "href": "appendix_c_quick_guides.html#sec-quick-guide-upslope-area",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Calculate the area upslope of a point",
    "text": "QGIS: Calculate the area upslope of a point\nSearch for the SAGA algorithm Upslope Area in the Processing Toolbox panel and open the function window with a double click on the name. Enter the Longitude of your discharge station for the Target X coordinate and the Latitude for the Target Y coordinate for which the upslope area should be calculated. For the Elevation select the sink-filled DEM (see how to fill sinks in a DEM and why we need to fill in sinks).\nAll GIS layers and the coordinates of the discharge gauge need to be in the same UTM projection. Choose a method in the drop-down menue in the Method section and optionally specify a location for the output file. For the example of the Nauvalisoy basin, the Upslope Area window filled in correctly is shown in Figure C.27.\n\n\n\nFigure C.27: The Upslope Area window for the determination of the catchment area of the Nauvalisoy river basin.\n\n\nClick Run and Close after the algorithm is done. A new raster file with the values 0 for outside the catchment area and 100 for inside the catchment area is now loaded into your QGIS project.\nBack to catchment delineation."
  },
  {
    "objectID": "appendix_c_quick_guides.html#appendix-quick-guide-polygonize",
    "href": "appendix_c_quick_guides.html#appendix-quick-guide-polygonize",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Polygonize a Raster",
    "text": "QGIS: Polygonize a Raster\nGot to Raster in the header toolbar, move your cursor over Conversion and click on Polygonize (Raster to Vector)… (see Figure C.28).\n\n\n\nFigure C.28: Open the Polygonize window.\n\n\nSelect the raster layer you wish to polygonize (for example the upslope area raster layer) and run the process. Close the polygonize window after the algorithm is done. A new shape file will be loaded to your GIS project (see Figure C.29).\n\n\n\nFigure C.29: The vector layer generated by a raster to vector conversion.\n\n\nThe new vector layer has 2 features: the watershed boundary and a box around the watershed the same size of the DEM we used. To get rid of the outer shape, open the attribute table with a right-click on the vector layer and selecting Open Attribute Table. In the attribute table, elect the outer shape by clicking on the second row of the attribute table and toggle the edit mode by clicking on the pen icon (see Figure C.30).\n\n\n\nFigure C.30: Select the outer shape to discard by clicking on the second row in the attribute table and toggle the edit mode by clicking on the pen icon.\n\n\nDelete the outer shape by pressing the red bin icon in the attribute table (see Figure C.31).\n\n\n\nFigure C.31: Delete the selected outer shape.\n\n\nSave the edits in the attribute table (see Figure C.32) and press the pen icon again to un-toggle the edit mode.\n\n\n\nFigure C.32: Save your edits in the attribute table.\n\n\nClose the attribute table window and save the boundary of your watershed and your QGIS project.\nBack to catchment delineation."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-graphical-edit-junctions",
    "href": "appendix_c_quick_guides.html#sec-graphical-edit-junctions",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Edit Junctions Layer",
    "text": "QGIS: Edit Junctions Layer\nSelect the Junctions layer and toggle manual editing by clicking on the yellow pen (see Figure C.33).\n\n\n\nFigure C.33: Manually edit the layer with the river junctions, step 1: Toggle layer editing.\n\n\nWhen in editing mode, the yellow pen will appear in the Layers window next to the name of the layer being edited. The edit mode will also activate a button for adding points (i.e. junctions, we don’t need that now) and the vertex tool. Click on the vertex tool icon. It is active when a a boundary appears around the icon and the Vertex Editor windows opens (see Figure C.34).\n\n\n\nFigure C.34: Manually edit the layer with the river junctions, step 1: Toggle layer editing.\n\n\nRight-click on a junction point you would like to delete to activate it (see Figure C.35).\n\n\n\nFigure C.35: Manually edit the layer with the river junctions, step 3: Activate a junction node for editing.\n\n\nSelect the activated point by drawing a rectangle over the point with your mouse. The point will appear blue (see Figure C.36).\n\n\n\nFigure C.36: Manually edit the layer with the river junctions, step 3: Select the activated junction node.\n\n\nDelete the point with the delete key on your keyboard. You can save your edits by pressing the blue-white Save Layer Edits button that is decorated with an orange pen (see Figure C.37). This saves your changes without exiting the edit mode.\n\n\n\nFigure C.37: Manually edit the layer with the river junctions, step 3: Save edits.\n\n\nIf you have many points to remove, as in our case, it may be faster to identify the IDs of the features you want to keep, select these and delete all others. To start, you activate the Identify Features mode by clicking on the icon with the white i on the blue circle (see Figure C.38).\n\n\n\nFigure C.38: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 1.\n\n\nA black i will appear next to your cursor. You then click on the first of your nodes that you want to keep. This will highlight it in red and a list with information on the selected feature appears on the right in the Identify Results window. You will see the attribute NODE_ID with value 1 for the outflow node (see Figure C.39). Note down the ID of the feature.\n\n\n\nFigure C.39: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 2.\n\n\nYou then press on the node at the confluence of the two tributaries in the center of the catchment. The Identify Results window shows 2 results, that means, that two junction nodes are close to each other (see Figure C.40).\n\n\n\nFigure C.40: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 3.\n\n\nZoom in in your map window with your mouse to see the two nodes (see Figure C.41).\n\n\n\nFigure C.41: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 4.\n\n\nSelect the node that should be kept and not the ID of the node (NODE_ID 11) (see Figure C.42).\n\n\n\nFigure C.42: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 5.\n\n\nZoom back to the entire Junctions layer (see Figure C.10) and go to Select Features by Values… in the toolbar (see Figure C.43).\n\n\n\nFigure C.43: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 1.\n\n\nAdd NODE_ID 1 to your selection as demonstrated in Figure C.44.\n\n\n\nFigure C.44: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 2.\n\n\nThe outflow node with ID 1 will change color in your map as shown in Figure C.45.\n\n\n\nFigure C.45: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 3.\n\n\nAdd node 11 to your selection in the same way and close the Select Node by Value window. Invert the feature selection as shown in Figure C.46.\n\n\n\nFigure C.46: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 4.\n\n\nAll other nodes will now be yellow and the ones to keep will appear in the layer color (see Figure C.47).\n\n\n\nFigure C.47: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 5.\n\n\nDelete the features (nodes) by pressing the Delete Selected button in the edit features toolbar as shown in Figure C.48.\n\n\n\nFigure C.48: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 6.\n\n\nSave your edits (see Figure C.37). To verify that, indeed, all superfluous nodes are deleted from the Junctions file, open the Attribute table shown in Figure C.49.\n\n\n\nFigure C.49: Edit Attribute table. Step 1: Open the attribute table with a click on the Attribute Table icon in the QGIS toolbar.\n\n\nOnly 2 features should be listed under each attribute. We will now edit the attribute table to prepare it for the RSMinerve model. RSMinerve needs an identifier to differentiate between the junctions. We can use the attribute TYPE to uniquely identify the two junctions needed. RSMinerve further needs the ID of the downstream river. Add a column to the attribute table by pressing the Add Field button (see Figure C.50).\n\n\n\nFigure C.50: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 1.\n\n\nDefine a name for the attribute, a type and admissible length of each entry in the Add Field window. In our case, we choose a string consisting of letters as ID and allow it to be 20 characters long as shown in Figure C.51.\n\n\n\nFigure C.51: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 2.\n\n\nClose the window by pressing OK. By clicking in the newly created NULL fields, you can now type names for the downstream rivers and save your edits by pressing the save edits icon (3rd from the left in the toolbar of the attribute table window). As the outlet of the catchment goes directly into Charvak reservoir, we can type Charvak as the downstream river ID for this example. The river stretch between junction and outlet is called Pskem (see Figure C.52).\n\n\n\nFigure C.52: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 3.\n\n\nWe are done editing the Junctions layer. Deactivate the edit mode by clicking on the yellow pen in the attribute table window as demonstrated in Figure C.53 and close the window.\n\n\n\nFigure C.53: Save your edits.\n\n\nNow save the Junctions layer in an appropriate place on your drive. Time to save your QGIS project.\nIn the same way you can also edit the river channels layer."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-create-hbv-model",
    "href": "appendix_c_quick_guides.html#sec-quickguides-create-hbv-model",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Create HBV model and edit parameters",
    "text": "RSM: Create HBV model and edit parameters\nClick on the HBV model icon in the model selection panel (see Figure C.54).\n\n\n\nFigure C.54: To generate an HBV model, click on the HBV model icon in the model selection panel.\n\n\nMove your cursor to the white area in the center and create a HBV model with a click. The HBV model icon will now appear in the white model panel (see Figure C.55).\n\n\n\nFigure C.55: A HBV model has been generated.\n\n\nEdit the parameters of the model by double-clicking on the model icon and changing parameter values in the table that appears to the right of the RSMinerve window (see Figure C.56).\n\n\n\nFigure C.56: Double-click on the HBV model to open and edit the parameter table.\n\n\nAlternatively (especially if you have to change parameters for several models), activate the Parameters panel in the Model Properties toolbar (see Figure C.57).\n\n\n\nFigure C.57: Activate the Parameters panel in the Model Properties toolbar.\n\n\nSelect the HBV model in the Parameters panel as shown in Figure C.58.\n\n\n\nFigure C.58: Select HBV in the Parameters panel.\n\n\nIf you have several HBV models, you can select models by zone and apply edits in the parameter table in the left of the Parameters panel to all selected models (marked with tick). You can also edit paramers for individual models in the right-hand table of the Parameters panel as is visible in Figure C.59.\n\n\n\nFigure C.59: Edit parameters for groups of models (left parameter table) or for individual models (right parameter table).\n\n\nSave your model by clicking the floppy disk icon in the toolbar.\nYou can also export parameters to a text file via the button Export P in the Model Properties toolbar, edit the text file and import the edited parameter file through Import P.\nNote that for the Nauvalisoy demonstration case study, the area of the HBV model should be 99’000’000 m2.\nBack to the Nauvalisoy model guide."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-add-climate-station",
    "href": "appendix_c_quick_guides.html#sec-quickguides-add-climate-station",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Add and link climate station",
    "text": "RSM: Add and link climate station\nMove your cursor to the V-Station icon in the models panel (the first icon in Figure C.54). Click on the icon to activate it and click next to the HBV model in your model pane to place the V-Station. With a double-click on the newly created V-Station, you can visualize the parameters of the virtual weather station.\nThen, connect the station to the HBV model by activating the Connections Mode in the Editing tools toolbar as shown in Figure C.60.\n\n\n\nFigure C.60: Create a virtual weather station.\n\n\nClick on the station, hold down the finger move your cursor to the HBV model, then release the hold. A grey line appears between the station and the model and a pop-up window asks you to verify the data links between the two components (see Figure C.61).\n\n\n\nFigure C.61: Link the virtual weather station to the HBV model.\n\n\nClick Ok to accept the suggested data links and a blue arrow will appear between the weather station and the model. Click on the black arrow in the Editing tools toolbar to leave the Connections Mode.\nBack to the Nauvalisoy model guide."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-import-climate-data",
    "href": "appendix_c_quick_guides.html#sec-quickguides-import-climate-data",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Import climate data",
    "text": "RSM: Import climate data\nMake sure that you have downloaded storede the following climate data file ERA5_Nauvalisoy_1981_2013.csv for Nauvalisoy River on your computer.\nGo to the database tab and click on Open in the File database toolbar (see Figure C.62).\n\n\n\nFigure C.62: The database tab.\n\n\nSelect the file that you downloaded before. If you don’t see the file in your file browser, make sure the file ending .csv is selected in the file browser (see Figure C.63).\n\n\n\nFigure C.63: Make sure the file ending is .csv in the file browser.\n\n\nPress open to load the data into RSMinerve. Depending on your computer this may take a few seconds. Once the data is loaded, click through the database browser in the left pane to explore the data as shown in Figure C.64.\n\n\n\nFigure C.64: Explore the data base.\n\n\nTo connect the data base to the model you will need to make the data consistent with the model. To do this change the name “New group” to “Measurements” and select Inputs for the Category selection (see Figure C.65).\n\n\n\nFigure C.65: Change \"New group\" to \"Measurements\" and select Input as category.\n\n\nThen, browse the name of the station as done in Figure C.66.\n\n\n\nFigure C.66: The name of the station is \"Nauvalisoy\".\n\n\nAs our weather data is representative for the entire Nauvalisoy catchment, the station name in the data base needs to be consistent with the station name of the V-Station in the model pane. Change the name of the station in the model pane from V-Station to Nauvalisoy by clicking on the name and then editing it.\nChoose the nauvalisoy data set as source and adapt the simulation period as shown in Figure C.67.\n\n\n\nFigure C.67: Choose the nauvalisoy data set to link the weather station data to the virtual weather station. The simulation times should not extend the period of the input data. Simulation time step is 1 hour and the recording time step is 1 month.\n\n\nClick on Validation to verify that the model has been set up correctly. Once you have adapted the model settings to calculate evaporation based on temperature (how to), no errors should be reported. You can now run the model. A warning tells you that the number of meteo stations is not sufficient. Ignore the warning for now.\nBack to the Nauvalisoy model guide"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-atapt-model-settings",
    "href": "appendix_c_quick_guides.html#sec-quickguides-atapt-model-settings",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Adapt Model Settings",
    "text": "RSM: Adapt Model Settings\nNavigate to Edit in the Settings toolbar (see Figure C.68).\n\n\n\nFigure C.68: Open the models settings tab.\n\n\nOpen the Settings tab and choose an ET model. Adapt the coordinates of the project (choose the station coordinates mentioned in the Introduction Section) (see Figure C.69).\n\n\n\nFigure C.69: Edit the evaporation calculation method and the coordinates of the project.\n\n\nBack to the model guide"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-export-sim-results-to-db",
    "href": "appendix_c_quick_guides.html#sec-export-sim-results-to-db",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Export Simulation Results to Data Base",
    "text": "RSM: Export Simulation Results to Data Base\nGo to Export in the Database toolbar (see Figure C.70). Define a name for the simulation results and choose a data base group to save the data to. Create a new group if you haven’t already done so.\n\n\n\nFigure C.70: Export simulation results to the data base.\n\n\nThe data sets are now available in the data base tab and can be visualized in the Selection and Plots tab.\nBack to the model guide"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-add-comparator-and-discharge-data",
    "href": "appendix_c_quick_guides.html#sec-quickguides-add-comparator-and-discharge-data",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Add Comparator and Load Discharge Measurements",
    "text": "RSM: Add Comparator and Load Discharge Measurements\nThe comparator object allows the user to compare a simulated variable to a reference. In our case, we want to compare the simulated discharge of the Nauvalisoy river to the one actually measured at the only gauge in the catchment. The measured discharge can be imported to RSMinerve via the database tab.\nMove your cursor to the comparator icon as shown in Figure C.71 in the model components panel.\n\n\n\nFigure C.71: Comparator icon in RS Minerve.\n\n\nActivate it with a left-click and move your cursor next to the HBV model in the model pane. Place the comparator object with another click.\nAdd a source object to your model following the same procedure as for the comparator object. Figure C.72 shows the icon of the source object.\n\n\n\nFigure C.72: Source icon in RS Minerve.\n\n\nYou can now optionally rename your model objects.\nConnect the source and the HBV model to the comparator by activating the Connections mode in the Editing Tools toolbar. Right-click on the HBV model, hold the click and drag the cursor to the comparator object where you release the cursor. You will be asked in a pop-up window to specify the flow you wish to compare and if the data is to be viewed as simulation results or reference (measured) data. Connect the total discharge computed by the HBV model component as simulation result to the comparator (see Figure C.73) and close the pop-up window by pressing OK.\n\n\n\nFigure C.73: Connecting a Source to Comparater Object in RSMinerve.\n\n\nConnect the source object to the comparator in the same way as the HBV object. The source will be connected as reference (see Figure C.74).\n\n\n\nFigure C.74: Final, connected objects.\n\n\nNext, we have to load the discharge data into the database. Navigate to the database tab. In the database, go to Measurements -> Nauvalisoy and click Add and enter a name for the discharge station (see Figure C.75). For this example, we do not need to bother with the coordinates of the station (later, in more complex models, we will have to though!).\n\n\n\nFigure C.75: Connect the outflow of the HBV model as simulation result to the comparator.\n\n\nUnder the new discharge station, add a sensor and rename it to the source object in your model as shown in Figure C.76.\n\n\n\nFigure C.76: Connect the discharge measurements (source) as reference to the comparator.\n\n\nOpen the tab with the values. Here we need to import the discharge data. You can do that by opening the discharge data in a spreadsheet (e.g. Excel) and copy-pasting the dates and discharge values into the Values table in RSMinerve (here is how to do this step-by-step.\nNow we need to link the discharge data in the database to the source object in the model. Navigate to the model do the following.\n\n\n\nFigure C.77: Select the data source for the source object (under Data Source in the left window pane) and select the sensor for discharge data for the source (under Source, Series identifier in the right window pane).\n\n\nValidate the model to see if the model setup went correctly. Run the model if the validation did not throw an error.\nBack to the practical model calibration and validation Section"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-copy-paste-database-values",
    "href": "appendix_c_quick_guides.html#sec-quickguides-copy-paste-database-values",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Copy-paste Data to Database",
    "text": "RSM: Copy-paste Data to Database\nMake sure that the following file synthetic_discharge_Nauvalisoy_river_for_calibration_exercise.csv is available on your computer.\nNow, open the file in a spreadsheet software, e.g. Excel, Numbers, Google Sheets or OpenOffice Calc and select the rows containing dates and values (Figure C.78). Press Control+C or perform a right-click with the mouse and choose copy.\n\n\n\nFigure C.78: Select the rows and columns containing the dates and data to be copied. Press Control+C to copy the selected cells.\n\n\nThen navigate to the discharge sensor on the database tab in RSMinerve where you want to add the data to. Click in the small square to the left of the first row in the Values tab and click the keyboard keys Control+P. Alternatively perform a right-click in the small square and select Paste. The dates and values from the excel sheet will now appear in the table (Figure C.79). Save the database.\n\n\n\nFigure C.79: Paste dates and values to the database table.\n\n\nBack to the practical model calibration and validation Section"
  },
  {
    "objectID": "appendix_d_exercise_solutions.html#sec-appendix-solutions-exercise1",
    "href": "appendix_d_exercise_solutions.html#sec-appendix-solutions-exercise1",
    "title": "Appendix D — Exercise Solutions",
    "section": "\nD.1 Exercise on Linear Reservoir modelling",
    "text": "D.1 Exercise on Linear Reservoir modelling\nTask 1\nWhat will determine the flow through your bucket?\nThe flow through the bucket will be influenced by the volume to bottom area fraction of the bucket, the amount and speed of water added to the bucket and the size of the outlet hole.\nWhat do you need to measure?\nYou will need to measure:\n\nthe discharge from your bucket over time,\n\nthe recharge volume (how much water you put into the bucket over a given time),\n\nthe time when you start pouring water and when you stop pouring water into the bucket, and\nthe approximate volume of your bucket.\nHow can you measure it?\nThis depends on what you have available. You can draw a water level line outside of your outflow receptacle every 10 seconds and then determine the volume change over time. Maybe you have a scale and a smart phone so you can put your outflow receptacle on the scale and make a movie of the weight change over time (note, 1 kg of water is approximately 1 liter of water).\nFor the inflow pour a well defined volume over a well defined time interval. You can do this manually unless of course you have pipes and valves lying around that you can use.\nYou will need a watch for measuring time and a receptacle with known volume to measure volumes (or a scale).\nA couple of notes on measurement accuracy:\n\nGenerally, the larger the volumes, the smaller the relative measurement error. Say you measure a discharge of 50 ml in 1 s (i.e. 50 ml/s) and you can read your volume with an accuracy of 5 ml and the time with an accuracy of 0.2 s. Your measurement uncertainty becomes 11 ml/s which is more than 20 % of your discharge. If on the other hand, you measure 500 ml over 10 s (which is the same discharge of 50 ml/s) with the same inaccuracies for volume and time your measurement uncertainty for discharge becomes 1 ml/s which is only 2 % of your discharge.\n\nHow do you estimate measurement uncertainties: Measure several times, compute the average and the standard deviation of your measurements assuming a student-t distribution.\n\nHow do you combine uncertainties of volume and time to the uncertainty of discharge: By applying Gaussian error propagation.\nWhat materials you will need to set up the experiment?\n\nFor the bucket (the linear reservoir): A plastic bottle, a box or a can that is no longer used. It should have an opening at the top and the material should repel water and be thin enough that you can drill a hole into the wall.\nA pair of pointy scissors or a knife to drill a hole into the bucket.\n\nA water source (a tap, hose or a water container larger than the one above). This will be your rain machine.\nA watch to measure time.\n\nNote paper and pen.\n\nA receptacle for measuring the outflow.\n\nAdditional material to facilitate measurement according to availability.\n\n\n\nFigure D.1: Example for material needed and example setup to perform the linear reservoir experiment. Add a minion with a stop watch or a smart phone to help you logging the discharge from the bucket.\n\n\nTask 2\nThe video was recorded with a smart phone. The weight of the outflow receptacle was noted down every second. The discharge is computed as the change of volume in the outflow receptacle over time.\n\n\nTask 3\nThe height of the measured discharge peak can be best reproduced with k = 0.42. However, the measured discharge peaks 1s later than the simulated discharge peak.\nReasons for the discrepancy can be the shape of the linear reservoir, non-linear pouring speed, and measurement uncertainties."
  },
  {
    "objectID": "appendix_d_exercise_solutions.html#sec-appendix-solutions-hbv-exercises",
    "href": "appendix_d_exercise_solutions.html#sec-appendix-solutions-hbv-exercises",
    "title": "Appendix D — Exercise Solutions",
    "section": "\nD.2 Exercises on the HBV Model",
    "text": "D.2 Exercises on the HBV Model\nExercise: Driving Forces of the HBV Model\nThe model drivers are precipitation, temperature and evaporation (P, T and ET in ?fig-overview-hbv-model). You need to provide time series of the model drivers to the model. Evaporation is typically not measured at climate stations but many empirical functions are available in the literature to estimate evaporation. RSMinerve offers the possibility to calculate evaporation based on temperature measurements and catchment location (you will do that later in this tutorial).\nExercise - HBV Model States\nThe model states are the snow water equivalent height (SWE), the relative water content in the snow pack (WH), the humidity (Hum), the upper reservoir water level (SU) and the lower reservoir water level (SL). The model states are initialized using the initial conditions.\nExercise: Data Visualization in RSMinerve\nSimulate from the 01/01/1981 01:00:00 to 31/12/1983 23:00:00, then choose data from 31/12/1983 23:00:00 as the initial conditions and run the model from 01/01/1984 01:00:00 to 31/12/1984 23:00:00. Choose hourly output for the simulation results.\nOpen the Selection and plots tab by clicking on the Selection and plots button in the Modules toolbar and select simulated P and T from the Nauvalisoy station as shown in Figure D.2.\n\n\nFigure D.2: Hourly precipitation and temperature at the virtual Nauvalisoy weather station.\n\n\nNote: If you want to repeat a simulation with specific initial conditions, you can store them through Export IC in the Model Properties toolbar.\nThe approximate temperature range is -13 deg. C. in December to 34 deg. C. in August. The annual precipitation is about 1.4 m (visualize Pcum and click on the last value of the time series). No precipitation falls during the summer months.\nExercise: Compare Evaporation Methods\nFigure Figure D.3 shows the evaporation computed with various methods and the resulting discharge. Uniform evaporation should not be used for sub-annual modeling time steps for obvious reasons that ET shows a strong seasonality. The difference between the different methods by Turc, McGuinness and Oudin are within 5 % of total discharge which is negligible for a regional model.\n\n\nFigure D.3: Hourly precipitation and temperature at the virtual Nauvalisoy weather station.\n\n\nFor advanced modeling, the choice of the evaporation model may be relevant but only if a validation with measured data is possible.\nExercise: Common Difficulties in Model Calibration\n\nEspecially fully and semi-distributed hydrological models are typically over-parameterized, i.e. the number of model parameters is much larger than the number of observations for the model states. The true parameter values of the system cannot be uniquely identified based on a discharge time series alone.\n\nThe outcome of the calibration depends on the measure of similarity between the simulated and the measured discharge.\nThe water balance is often forgotten during model calibration. A nice fit of the discharge curve can for example be achieved by increasing the volume of water in the model over time.\nThe model is calibrated against historical data. Its ability to predict future discharge may be limited.\n\nThe model is not perfect, it remains an approximation of the real system and may not incorporate all relevant processes of the hydrological cycle (e.g. water storage and transport in glaciers for the case of the HBV model or significant sub-surface water fluxes that very difficult to capture as for example in Karst regions).\n\nDischarge measurements typically have uncertainties of 20 %. Particularly measurements at the lower and upper ends of the rating curve (i.e. the water table - discharge relationship) are typically prone to larger uncertainties (bonus question: think about why this is so!). Is the measurement location or the equipment not properly maintained, biases may grow over time. On the other hand, if the measurement method is updated and changed, the measured discharge may display a different pattern as was for example discussed in the data from Gunt River basin (Section 2.1).\nExercise: Strategies to Overcome some of the Model Calibration Difficulties\n\nOver-parameterization:\n\n\n\nConsider simplifying the model, i.e. reducing the number of parameters. If the model complexity is required, try adding additional measured variables, e.g. snow cover from MODIS data (see Chapter on snow cover data) to validate individual components of the HBV model.\n\nCollect data to verify individual fluxes of the model components (e.g. soil parameters, snow water equivalent, etc.). As physical measurements in the field are not always possible you may have to become creative here, e.g. use MODIS snow cover data to validate the snow/no snow partitioning of the HBV model. Also consult the literature for parameterizations of similar catchments.\n\n\n\nUse a combination of similarity measures. This will be demonstrated later on in the model calibration section.\n\nDuring model calibration, look at the components of the model as well as the total discharge time series. Make sure that the storage of water changes within reasonable bounds and that the partitioning of the water in your system is physically reasonable (e.g. comparatively small storage compartments for rocky mountain catchments).\n\nExclude part of your data set from model calibration and use it for model validation. If the model does not perform well in the validation period, its parameters are too specific for the calibration period (you have over-fitted the model) and the model is said to not generalize well. If this happens you should try to reduce the number of parameters in your model. To understand better which parameters are responsible for the over-fit of the historical discharge, use different calibration and validation periods and compare the resulting parameters. Through sensitivity analysis, identify the model components that are most sensitive to predicted changes of model forcings, geometry or parameterization and perform scenario analysis.\n\nImplement and validate multiple possible conceptual models. All of the models must be calibrated and validated individually. It is further recommended to calculate at the highest possible temporal resolution and to try and compare the model outcome at different spatial resolution.\n\nSquare-root filters or data assimilation algorithms are able to account for non-correlated measurement errors (they are not implemented in RSMinerve and not topic of this course). Error bands for the measurements should be adapted when communicating model results.\n\nMost of the above points will be discussed in more detail during this course.\n\nD.2.1 Exercise: Calibrate a Simple HBV Model {#sec-appendix-solutions-calibrated parameters .unnumbered}\nThe parameter set of the calibrated model are: ::: {.cell} ::: {.cell-output-stdout}\n\n\n|                |      |\n|:---------------|-----:|\n|CFMax (mm/°C/d) |  0.50|\n|CFR (-)         |  0.05|\n|CWH (-)         |  0.10|\n|TT (°C)         |  3.00|\n|TTInt (°C)      |  3.00|\n|TTSM (°C)       |  0.00|\n|Beta (-)        |  2.50|\n|FC (mm)         | 20.00|\n|PWP (-)         |  0.50|\n|SUMax (mm)      | 10.00|\n|Kr (1/d)        |  0.09|\n|Ku (1/d)        |  0.02|\n|Kl (1/d)        |  0.00|\n|Kperc (1/d)     |  0.00|\n::: :::\nYou can import the calibrated parameters via Import P in the Model Properties toolbar (note that the calibrated parameters are available for download here."
  },
  {
    "objectID": "glacier_modeling.html#temperature-index-model",
    "href": "glacier_modeling.html#temperature-index-model",
    "title": "11  Modeling of discharge from glacier melt",
    "section": "\n11.1 Temperature index model",
    "text": "11.1 Temperature index model\n(hock_temperature_2003?) describes several variants of the temperature index model for simulating glacier melt. The riversCentralAsia package implements the temperature index melt model described in (hock_temperature_2003?) in the function glacierMelt_TI (Equation 11.1).\n\\[\nM = \\biggl\\{ \\begin{array}{l, l}\n0, & T < T_{threshold} \\\\\nf_{M} \\cdot \\left( T - T_{threshold} \\right), & T >= T_{threshold}\n\\end{array}\n\\qquad(11.1)\\]\nwhere \\(M\\) is the glacier melt in \\(mm/d\\), \\(T\\) is the daily average temperature in \\(^{\\circ} C\\). The two parameters \\(f_{M}\\) and \\(T_{\\text{threshold}}\\) refer to the melt factor and the threshold temperature above which glacier melt occurs and need to be calibrated. They have the units \\(\\frac{mm}{^{\\circ} C \\cdot d}\\) and \\(^{\\circ} C\\) respectively. Glacier melt is calculated in daily time steps."
  },
  {
    "objectID": "glacier_modeling.html#glacier-mass-balance",
    "href": "glacier_modeling.html#glacier-mass-balance",
    "title": "11  Modeling of discharge from glacier melt",
    "section": "\n11.2 Glacier mass balance",
    "text": "11.2 Glacier mass balance\nThe glacier mass balance is simplified to Equation 11.2:\n\\[\n\\Delta S = P-M = A_{\\text{imbal}}\n\\qquad(11.2)\\]\nwhere the change of water storage (\\(\\Delta S\\)) is equal to the precipitation (\\(P\\)) minus the glacier melt (\\(M\\)). Typically melt excesses precipitation and we have negative \\(\\Delta S\\), that is imbalance ablation, indicating glacier storage loss. The glacier mass balance is calculated in annual time steps. We thereby refer to the hydrological year starting on October 1st of the previous year to take a full accumulation and ablation season into account."
  },
  {
    "objectID": "glacier_modeling.html#glacier-volume-development",
    "href": "glacier_modeling.html#glacier-volume-development",
    "title": "11  Modeling of discharge from glacier melt",
    "section": "\n11.3 Glacier volume development",
    "text": "11.3 Glacier volume development\nAs glaciers melt, their volume changes. This has to be taken into account for the long-term simulation of glacier discharge. To determine the initial glacier volume the area of the geometry of the RGI v6.0 data set is multiplied with the average thickness of the glacier (the Farinotti data set). Please note that the data and the retrieval of the data are described in Part II of this book. Glaciers larger than 1km2 are sub-divided into elevation bands of 100m altitude to account for elevation dependent temperature forcing. The large glaciers melt from the lowest elevation band to the highest elevation band whereby the glacier melt is subtracted from the glacier volume of lowest elevation band that is still glacierized. The small glaciers are not spatially discretized and thus they are melted homogeneously.\nFor annual time step \\(t\\), the evolution of the glacier volume is calculated as follows:\n\\[\nA(t) = \\text{glacierArea RGIF}\\bigl(V(t)\\bigr)\n\\qquad(11.3)\\]\n\\[\nQ_{glacier}(t) = M(t) \\cdot A(t)\n\\qquad(11.4)\\]\n\\[\nV(t+1) = V(t) + \\Delta S = V(t) + \\text{glacierImbalAbl}\\bigl(M(t)\\bigr)\n\\qquad(11.5)\\]\n\\(Q_{\\text{glacier}}\\) can be calibrated against glacier discharge derived from the Miles & Hugonnet data sets. The automated calibration is currently not included in the riversCentralAsia package.\nThe function glacierArea_RGIF() is an empirical scaling function analogue the inverse of the scaling function derived by Erasov (1968) but based on the modern RGI v6.0 glacier geometries and the glacier thickness data set by Farinotti et al. (2019). The package riversCentralAsia implements volume-area and area-volume scaling functions based on both, Erashov and RGI-Farinotti data, allowing the estimation of glacier areas based on glacier volumes (glacierArea_Erasov and glacierArea_RGIF) and estimations of glacier volumes based on glacier areas (glacierVolume_Erasov and glacierVolume_RGIF).\nThe function glacierImbalAbl is an empirical scaling function relating glacier imbalance ablation, i.e. the glacier storage loss glacier melt. It is derived from the glacier discharge data set by Miles et al. (2021) and the glacier thinning data set by Hugonnet et al. (2021)."
  },
  {
    "objectID": "glacier_modeling.html#summary-of-workflow",
    "href": "glacier_modeling.html#summary-of-workflow",
    "title": "11  Modeling of discharge from glacier melt",
    "section": "\n11.4 Summary of workflow",
    "text": "11.4 Summary of workflow\nTo model the contribution to discharge from glacier melt in a basin, the following steps are required:\nPre-processing of GIS layers\nPre-processing of climate forcing data\nCalculation of daily glacier melt\nCalculation of annual glacier masss balance\nScaling of annual glacier contribution to daily values\nAggregation of per glacier contribution to sub-basins (optional)\nWriting of RSMinerve source intput filesIntegration of glacier discharge sources in RSMinerve"
  },
  {
    "objectID": "glacier_modeling.html#propagation-of-uncertainty",
    "href": "glacier_modeling.html#propagation-of-uncertainty",
    "title": "11  Modeling of discharge from glacier melt",
    "section": "\n11.5 Propagation of uncertainty",
    "text": "11.5 Propagation of uncertainty\nThe initial glacier volume of each glacier is attributed an uncertainty of p/m 26%. This number is based on the average uncertainty of the glacier volume per RGI region reported in Farinotti et al. (2019). The uncertainty of the area of the RGI v6.0 glacier outlines is not known. It is therefore assumed that the error of the glacier volume stems to 50% from the estimation of the glacier thickness and to 50% from the glacier area. We further assume that the errors of the glacier area and glacier thickness are un-correlated and can thus estimate the uncertainties of the glacier area data and the glacier thickness data to be p/m 13% each.\nFor the non-linear relationships in glacierVolume_RGIF and glacierArea_RGIF, the standard deviation of the residuals of the fit was computed. The estimated error of the fit is assumed to be equal plus/minus twice the standard deviation of the residuals and yields 31% and 53% respectively. The residuals are not normally distributed and their actual distribution is unknown. Further, uncorrelated errors and the applicability of linear error propagation are assumed. Therefore, the error of the function outputs is simply computed by adding the error of the function input to the error of the fit.\n\\[\n\\varepsilon_{V} = \\varepsilon_{A} + \\varepsilon_{\\text{glacierVolume RGIF}} = 0.26 + 0.31 = 0.57\n\\qquad(11.6)\\]\n\\[\n\\varepsilon_{A} = \\varepsilon_{V} + \\varepsilon_{\\text{glacierArea RGIF}} = 0.26 + 0.53 = 0.79\n\\qquad(11.7)\\]\nError estimates for the temperature index model are not available. A conservative relative error of 2 is therefore assumed, indicating that the estimated glacier melt is within a range of plus/minus 2 times it’s value.\nThe error of the imbalance ablation amounts to 73%. Assuming independent errors from the temperature index model for glacier melt and the scaling function between imbalance ablation and glacier melt, the error of the imbalance ablation amounts to approximately:\n\\[\n\\varepsilon_{Q_{\\text{imb,melt}}} = \\varepsilon_{Q_{\\text{M}}} + \\varepsilon_{\\text{glacierImbalAbl}} = 2 + 0.73 = 2.73\n\\qquad(11.8)\\]\nThe errors stated above relate to initial estimates of the glacier areas, volumes, total ablation and imbalance ablation. The errors of the glacier model variables of each subsequent time step depend on the errors of the previous time steps, i.e. they are not uncorrelated over time. For simplicity reason, this non-linear propagation of errors is neglected. In any case, the uncertainty margins for any glacier melt modelling done with the presented method are large. The following table summarises the estimated errors for each variable\n\n# Relative error estimates for the initial state in %\nerror_stats = tibble::tibble(\n  sV = 0.26,  # Glacier volume\n  sA = 0.13,  # Glacier area\n  sTh = 0.13,  # Glacier thickness\n  sMelt = 2)  # Glacier melt"
  },
  {
    "objectID": "glacier_modeling.html#demonstration",
    "href": "glacier_modeling.html#demonstration",
    "title": "11  Modeling of discharge from glacier melt",
    "section": "\n11.6 Demonstration",
    "text": "11.6 Demonstration\nWe demonstrate the above described method with the data from the Atbashy basin. Al required data is available from the downloadable data package.\n\nlibrary(tmap)\nlibrary(sf)\nlibrary(raster)\nlibrary(exactextractr)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(riversCentralAsia)\n# Path to the data directory downloaded from the download link provided above. \n# Here the data is extracted to a folder called atbashy_glacier_demo_data\ndata_path <- \"../caham_data/SyrDarya/Atbashy/\"\n\n\n11.6.1 Forcing\nThere will be a separate Section to demonstrate how to prepare the forcing. For now, we provide you with the pre-processed forcing data and give you just a brief overview over the data source.\nHistorical temperatures\nAs meteorological data for high elevations in Central Asia is very scarce we use the CHELSA v2.1 data set (karger_climatologies_2017?). This is a global data set of forcing data for hydrolgical models based on ERA5 but corrected for biases and especially suited for high elevations. The daily CHELSA forcing has been cut to the Central Region by the originator of the data set, D. Karger, WSL and extracted to the hydrological response units of the glaciers in the Atbashy basin by T. Siegfried.\n\nhist_obs <- readRDS(file = paste0(data_path, \"CLIMATE/hist_obs_glacier_tas.rds\"))\n# Plot the temperature time series for a given glacier/elevation band\nglacier <- \"RGI60-13.08930_1\"\nggplot(hist_obs) + \n  geom_line(aes(date, get(glacier))) + \n  labs(x = \"Date\", y = \"T [deg C]\") +\n  theme_bw()\n\n\n\nDaily temperature time series of a small glacier (RGI60-13.08930) in the Atbashy basin. Data source: CHELSA v2.1.\n\n\n\n\nFuture temperatures\nFuture temperature development per glacier or elevation band is extracted from the 3 CMIP6 GCM models with highest priorities for the region downloaded from COPERNICUS. We take 4 socioeconomic scenarios into account, covering 4 different emission scenarios. The temperatures of the climate models are bias corrected using the CHELSA data and a quantile mapping method. More details in the climate data preparation section.\n\nfut_sim <- readRDS(file = paste0(data_path, \"CLIMATE/fut_sim_glacier_tas_qmapped.RDS\"))\n# Plot the temperature time series for a given glacier/elevation band\nglacier <- \"RGI60-13.08930_1\"\n# Extract the temperature for the selected glaciers for all GCMs and SSPs\nscenarios <- names(fut_sim)\nfut_temp <- NULL\nfor (scenario in scenarios) {\n  fut_temp <- rbind(fut_temp, \n                    fut_sim[[scenario]] |> \n                      dplyr::select(Date, all_of(glacier)) |> \n                      mutate(Scenario = scenario))\n}\nfut_temp <- fut_temp |> \n  mutate(Hyear = hyear(Date)) |> \n  group_by(Hyear, Scenario) |> \n  summarise(Date = first(Date), \n            Temp = mean(get(glacier))) |> \n  separate(Scenario, into = c(\"GCM\", \"SSP\"), sep = \"_\") |> \n  ungroup() |>\n  dplyr::filter(Hyear > min(Hyear) & Hyear < max(Hyear), \n                GCM != \"IPSL-CM6A-LR\")\n# Plot annual data for better readability\nggplot() + \n  geom_line(data = hist_obs |> \n              mutate(Hyear = hyear(date)) |> \n              group_by(Hyear) |> \n              summarise(date = first(date), \n                        Temp = mean(get(glacier))) |> \n              ungroup() |> \n              dplyr::filter(Hyear > min(Hyear) & Hyear < max(Hyear)), \n            aes(date, Temp)) + \n  geom_line(data = fut_temp, aes(Date, Temp, colour = GCM, \n            linetype = SSP)) + \n  scale_colour_viridis_d() + \n  labs(x = \"Date\", y = \"T [deg C]\") +\n  theme_bw()\n\n\n\nFigure 11.1: Annual historical (CHELSA) and future (CMIP6, Copernicus) temperature for glacier RGI60-13.08930.\n\n\n\n\n\n11.6.2 Glacier geometry\n\n# Load data\n## Digital elevation model (DEM)\ndem <- raster(paste0(data_path, \"GIS/16076_DEM.tif\"))\n\n## Glacier outlines from the Randolph Glacier Inventory (RGI) v6.0 \nrgi <- st_read(paste0(data_path, \"GIS/16076_Glaciers_per_subbasin.shp\"), \n               quiet = TRUE) |> \n  st_transform(crs = crs(dem))\n\n## Outlines of hydrological response units for the modelling of glacier discharge.  \nrgi_elbands <- st_read(paste0(data_path, \n                              \"GIS/rgi_glaciers_atbaschy_el_bands.shp\"), \n               quiet = TRUE) |> \n  st_transform(crs = crs(dem))\n\n# Load a pre-processed raster file with glacier thickness in the Atbashy basin\n# (see vignette [glaciers 01]{glaciers-01-intro.html} for details on the pre\n# -processing)\nglacier_thickness <- raster(\n  paste0(data_path, \n  \"GLACIERS/Farinotti/pre-processed_glacier_thickness.tif\"))\n\n# Load the glacier thickness data set and filter it to the glaciers in the \n# catchment of interest.  \nhugonnet <- read_csv(paste0(\n  data_path, \"/GLACIERS/Hugonnet/dh_13_rgi60_pergla_rates.csv\"))\nhugonnet <- hugonnet |> \n  dplyr::filter(rgiid %in% rgi$RGIId) |> \n  tidyr::separate(period, c(\"start\", \"end\"), sep = \"_\") |> \n  mutate(start = as_date(start, format = \"%Y-%m-%d\"), \n         end = as_date(end, format = \"%Y-%m-%d\"), \n         period = round(as.numeric(end - start, units = \"days\")/366))\nglaciers_hugonnet <- rgi |> \n  left_join(hugonnet |> dplyr::select(rgiid, area, start, end, dhdt, err_dhdt, \n                                      dvoldt, err_dvoldt, dmdt, err_dmdt, \n                                      dmdtda, err_dmdtda, period),  \n            by = c(\"RGIId\" = \"rgiid\")) \n# Only keep the variables we need for this analysis\nrgi_elbands <- rgi_elbands |> \n  dplyr::select(RGIId, Area, elvtn_b) |> \n  rename(Area_tot_glacier_km2 = Area) |> \n  mutate(ID = paste0(RGIId, \"_\", elvtn_b))\n# Get mean elevation of each glacier/elevation band from DEM\nrgi_elbands$z_masl <- exact_extract(dem, rgi_elbands, \"mean\", progress = FALSE)\n# Update the glacier area within the basin boundaries\nrgi_elbands$A_km2 <- as.numeric(st_area(rgi_elbands))*10^(-6)\nglaciers <- unique(rgi_elbands$RGIId)\n# Calculate the average glacier thickness for each elevation band. \nrgi_elbands$thickness_m = exact_extract(glacier_thickness, \n                                        rgi_elbands, \"mean\", progress = FALSE)\n\n\n11.6.3 Processing of forcing\nTemperature time series for each glacier/elevation band are available. Figure 11.2} shows temperature time series extracted from the CHELSA data set on the example of one of the larger glaciers in the Atbashy basin. The data is aggregated to the hydrological year to better illustrate the temperature gradient over the elevation. The highest temperatures are measured at the lowest elevations and vice versa.\n\nggplot(hist_obs[, 1:9] |> \n         pivot_longer(-date, names_to = \"ID\", values_to = \"Temp\") |> \n         mutate(Hyear = hyear(date)) |> \n         group_by(Hyear, ID) |> \n         summarise(date = first(date), \n                   Temp = mean(Temp))|> \n         ungroup() |> \n         dplyr::filter(Hyear>min(Hyear) & Hyear<max(Hyear)) |> \n         left_join(rgi_elbands |> \n                     st_drop_geometry() |> \n                     dplyr::select(ID, z_masl), by = \"ID\") |> \n         mutate(\"Elevation [masl]\" = as.factor(round(z_masl)))) + \n  geom_line(aes(date, Temp, colour = `Elevation [masl]`)) + \n  scale_colour_viridis_d() + \n  labs(x = \"Date\", y = \"T [deg C]\") + \n  theme_bw()\n\n\n\nFigure 11.2: Temperature forcing for the elevation bands of a glacier.\n\n\n\n\n\n11.6.4 Calculating glacier melt\nThe code snippet below illustrates how to calculate the annual glacier melt of the catchment. Figure 11.3 shows daily melt rates for the different elevation bands of one of the larger glaciers (same as in the figure above). The highest melt occurs at low elevation bands.\n\nMF_small = 1\nMF_large = 0.5\nthreshold_temperature = 0\nArea <- rgi_elbands |> \n  st_drop_geometry() |> \n  dplyr::select(ID, A_km2) |> \n  pivot_wider(names_from = ID, values_from = A_km2)\n\n# Assign different melt factors to large and small glaciers. \nMF <- Area |> \n  mutate(across(everything(), ~MF_large), \n         across(ends_with(\"_1\"), ~MF_small))\nmelt <- glacierMelt_TI(temperature = hist_obs |> dplyr::select(-date),\n                       MF = MF,\n                       threshold_temperature = threshold_temperature)\nmelt <- as_tibble(melt) |> \n  mutate(date = hist_obs$date) |> \n  relocate(date, .before =  where(is.numeric))\nggplot(melt[, 1:9] |> \n         pivot_longer(-date, names_to = \"ID\", values_to = \"Melt\") |> \n         left_join(rgi_elbands |> \n                     st_drop_geometry() |> \n                     dplyr::select(ID, z_masl), by = \"ID\") |> \n         mutate(\"Elevation [masl]\" = as.factor(round(z_masl)))) + \n  geom_line(aes(date, Melt, colour = `Elevation [masl]`)) + \n  scale_colour_viridis_d() + \n  labs(x = \"Date\", y = \"Melt [mm/d]\") + \n  theme_bw()\n\n\n\nFigure 11.3: Daily glacier melt per elevation band.\n\n\n\n\n\n11.6.5 Compare to measured glacier melt\nAggregate the daily glacier melt to annual data and compare it to the observed glacier melt. Note that the glacier melt simulated with the temperature index model (sim) is never negative whereas the glacier mass change derived from Hugonnet et al. (2021) and Miles et al. (2021) can be negative, indicating glacier growth.\n\nmelt_a_eb <- melt |> \n  pivot_longer(-date, names_to = \"ID\", values_to = \"Melt\") |> \n  mutate(Hyear = hyear(date)) |> \n  group_by(Hyear, ID) |> \n  summarise(date = first(date), \n            Melt = sum(Melt), \n            .lb_Melt = ifelse(Melt*(1-error_stats$sMelt)<0, 0, \n                              Melt*(1-error_stats$sMelt)), \n            .ub_Melt = Melt*(1+error_stats$sMelt))|> \n  ungroup() \nmelt_a <- melt_a_eb |> \n  separate(ID, into = c(\"RGIId\", \"elB\"), sep = \"_\") |> \n  group_by(Hyear, RGIId) |> \n  summarise(date = as_date(first(date)), \n            Melt = sum(Melt), \n            .lb_Melt = ifelse(Melt*(1-error_stats$sMelt)<0, 0, \n                              Melt*(1-error_stats$sMelt)), \n            .ub_Melt = Melt*(1+error_stats$sMelt)) |> \n  ungroup() |> \n  dplyr::filter(Hyear > min(Hyear) & Hyear < max(Hyear))\nglaciers_hugonnet <- glaciers_hugonnet |> \n  mutate(Qgl_m3a = glacierDischarge_HM(dhdt), \n         .lb_Qgl_m3a = ifelse(\n           dhdt > 0, \n           ifelse(Qgl_m3a*(1-error_stats$sQglgrowth)<0, 0, \n                  Qgl_m3a*(1-error_stats$sQglgrowth)), \n           ifelse(Qgl_m3a*(1-error_stats$sQglmelt)<0, 0, \n                  Qgl_m3a*(1-error_stats$sQglmelt))),\n         .ub_Qgl_m3a = ifelse(dhdt > 0, \n                              Qgl_m3a*(1+error_stats$sQglgrowth), \n                              Qgl_m3a*(1+error_stats$sQglmelt)))\nmelt_obs_a <- glaciers_hugonnet |> \n  dplyr::filter(RGIId %in% glaciers[6:9], \n                period == 1)\n  \nggplot() + \n  geom_ribbon(data = melt_a |> \n                dplyr::filter(RGIId %in% glaciers[6:9]), \n              aes(date, Melt/1000, ymin = .lb_Melt/1000, ymax = .ub_Melt/1000, \n                  fill = RGIId), colour = NA, alpha = 0.2) + \n  geom_ribbon(data = melt_obs_a |> \n                dplyr::filter(RGIId %in% glaciers[6:9]), \n              aes(start, -dmdtda, \n                  ymin = -dmdtda-err_dmdtda, \n                  ymax = -dmdtda+err_dmdtda, \n                  colour = RGIId, linetype = \"obs\", fill = RGIId), \n              size = 0.2, alpha = 0.2) + \n  geom_line(data = melt_a |> \n              dplyr::filter(RGIId %in% glaciers[6:9]), \n            aes(date, Melt/1000, colour = RGIId, linetype = \"sim\")) + \n  geom_line(data = melt_obs_a, aes(start, -dmdtda, colour = RGIId, \n                                   linetype = \"obs\")) + \n  labs(x = \"Date\", y = \"Melt [m weq/a]\") + \n  scale_linetype_manual(name = \"Source\", \n                        values = c(\"sim\" = 1, \"obs\" = 2)) + \n  scale_colour_viridis_d() + \n  scale_fill_viridis_d() + \n  ggtitle(paste0(\"MF: \", MF, \"Tth: \", threshold_temperature)) + \n  theme_bw()\n\n\n\nFigure 11.4: Daily glacier melt per elevation band.\n\n\n\n\nThe temperature index model can be calibrated with the specific glacier volume change provided by Hugonnet et al. (2021) (see Figure 11.4}). For the moment, manual calibration of the parameters is required.\n\n11.6.6 Glacier mass balance\nThe following figure shows the components of the glacier mass balance for a few glaciers in the Atbashy basin.\n\nglacier_balance <- glacierBalance(melt_a_eb = melt_a_eb, \n                                  rgi_elbands = rgi_elbands)\nglacier_balance <- glacier_balance |> \n  mutate(.lb = ifelse(Variable == \"A_km2\", \n                      ifelse(Value*(1-error_stats$sA)>0, \n                             Value*(1-error_stats$sA), 0), \n                      ifelse(Variable == \"V_km3\", \n                             ifelse(Value*(1-error_stats$sV)>0, \n                                    Value*(1-error_stats$sV), 0), \n                             ifelse(Variable == \"Q_m3a\", \n                                    ifelse(Value*(1-error_stats$sQglmelt)>0,\n                                           Value*(1-error_stats$sQglmelt), 0), \n                                    ifelse(Variable == \"Qimb_m3a\", \n                                           Value*(1-error_stats$sImbal), NA)))), \n         .ub = ifelse(Variable == \"A_km2\", \n                      Value*(1+error_stats$sA), \n                      ifelse(Variable == \"V_km3\", \n                             Value*(1+error_stats$sV), \n                             ifelse(Variable == \"Q_m3a\", \n                                    Value*(1+error_stats$sQglmelt), \n                                    ifelse(Variable == \"Qimb_m3a\", \n                                           Value*(1+error_stats$sImbal), NA)))))\nggplot(glacier_balance |> \n         dplyr::filter(RGIId %in% glaciers[6:9], \n                       Hyear > min(Hyear) & Hyear < max(Hyear), \n                       Variable %in% c(\"A_km2\", \"V_km3\", \"Q_m3a\", \"Qimb_m3a\"))) + \n  geom_ribbon(aes(Hyear, ymin = .lb, ymax = .ub, fill = RGIId), \n              alpha = 0.2, colour = NA) + \n  geom_line(aes(Hyear, Value, colour = RGIId)) + \n  facet_wrap(\"Variable\", scales = \"free_y\") + \n  scale_colour_viridis_d() + \n  scale_fill_viridis_d() + \n  theme_bw()\n\n\n\nFigure 11.5: Glacier area and volume development and total and imbalance discharge.\n\n\n\n\nWe now have glacier discharge (Q_m3s) and the unsustainable contribution to glacier discharge, the imbalance ablation (Qimb_m3a) which is negative for glacier loss and positive for growing glaciers. We are only interested in the contribution of imbalance ablation to river discharge, that is, only the negative part of Qimb_m3a is relevant to us."
  },
  {
    "objectID": "glacier_modeling.html#from-annual-to-daily-melt",
    "href": "glacier_modeling.html#from-annual-to-daily-melt",
    "title": "11  Modeling of discharge from glacier melt",
    "section": "\n11.7 From annual to daily melt",
    "text": "11.7 From annual to daily melt\nThe glacier mass balance is done on a yearly basis (if not at lower frequency). Hydrological models, however, typically run at higher frequency, for example monthly or daily time steps. One simple method to distribute glacier discharge on a year is to scale it according to the daily melt computed.\n\n# Aggregate the daily melt per glacier\nmelt_mmd <- melt |> \n  pivot_longer(-date, names_to = \"ID\", values_to = \"melt\") |> \n  separate(ID, into = c(\"RGIId\", \"elB\"), sep = \"_\") |> \n  group_by(date, RGIId) |> \n  summarize(melt = sum(melt)) |> \n  ungroup() |> \n  rename(M_mmd = melt)\n\n# Compute the annual melt per glacier \nmelt_mma <- melt_mmd |> \n  mutate(Hyear = hyear(date)) |> \n  group_by(Hyear, RGIId) |> \n  summarise(M_mma = sum(M_mmd)) |> \n  ungroup()\n\n# Rescale the annual imbalance glacier ablation with the daily melt rates\nimbalAbl_m3s <- melt_mmd |> \n  mutate(Hyear = hyear(date)) |> \n  left_join(melt_mma, by = c(\"RGIId\", \"Hyear\")) |> \n  left_join(glacier_balance |> \n              dplyr::filter(Variable == \"Qimb_m3a\") |> \n              transmute(Hyear = Hyear, \n                        RGIId = RGIId, \n                        Qimba_m3s = -1* Value/(60*60*24*365)) |> \n              mutate(Qimba_m3s = ifelse(Qimba_m3s < 0, 0, Qimba_m3s)), \n            by = c(\"RGIId\", \"Hyear\")) |> \n  mutate(Qimb_m3s = Qimba_m3s * (M_mmd/M_mma), \n         Qimb_m3s = ifelse(is.na(Qimb_m3s), 0, Qimb_m3s))\n\n# Visualise daily imbalance ablation\nggplot(imbalAbl_m3s |> \n         dplyr::filter(RGIId %in% unique(glacier_balance$RGIId)[1:7])) + \n  geom_line(aes(date, Qimb_m3s, colour = RGIId)) + \n  scale_colour_viridis_d() + \n  labs(x = \"Date\", y = \"Glacier discharge from imbalance ablation [m3/s]\") + \n  theme_bw()\n\n\n\nFigure 11.6: Daily glacier discharge from imbalance ablation."
  },
  {
    "objectID": "glacier_modeling.html#writing-input-file-for-rs-minerve",
    "href": "glacier_modeling.html#writing-input-file-for-rs-minerve",
    "title": "11  Modeling of discharge from glacier melt",
    "section": "\n12.1 Writing input file for RS Minerve",
    "text": "12.1 Writing input file for RS Minerve\nThe input file format for RS Minerve is described in Garcia Hernandez et al. (2020), page 136.\n\nQ <- Qimb_m3s_sub |>\n  mutate(Qimb_m3s = round(Qimb_m3s, digits = 7))\ntemp_wide <- Q |>\n  pivot_wider(names_from = name_2, values_from = Qimb_m3s) |> \n  rename(Station = date) \ndatechar <- posixct2rsminerveChar(temp_wide$Station)$value\ndatechar <- gsub(\" 01:00:00\", \" 00:00:00\", datechar)\ndatechar <- gsub(\" 02:00:00\", \" 00:00:00\", datechar)\noutput <- rbind(colnames(temp_wide),\n                c(\"X\", \"1\", \"1\", \"1\", \"1\"),  # Random coordinates, not relevant\n                c(\"Y\", \"2\", \"2\", \"2\", \"3\"), \n                c(\"Z\", \"3\", \"3\", \"3\", \"3\"), \n                c(\"Sensor\", \"Q\", \"Q\", \"Q\", \"Q\"), \n                c(\"Category\", \"Flow\", \"Flow\", \"Flow\", \"Flow\"), \n                c(\"Unit\", \"m3/s\", \"m3/s\", \"m3/s\", \"m3/s\"), \n                c(\"Interpolation\", \"Linear\", \"Linear\", \"Linear\", \n                  \"Linear\"), \n                cbind(datechar, \n                      as.character(temp_wide$Dzhaldzhur_Subbasin), \n                      as.character(temp_wide$Ulak_Subbasin), \n                      as.character(temp_wide$Atbaschy_Midstream_Subbasin), \n                      as.character(temp_wide$Atbaschy_Downstream_Subbasin)))\n\nFinally, the prepared data is written to a csv file which can be read into RSMinerve.\n\nwritefilename <- paste0(data_path, \"RSM_demo_glacier_source.csv\")\nwrite.table(output, file = writefilename, col.names = FALSE, \n            row.names = FALSE, append = FALSE, quote = FALSE, \n            sep = \",\", dec = \".\")"
  },
  {
    "objectID": "glacier_modeling.html#references",
    "href": "glacier_modeling.html#references",
    "title": "11  Modeling of discharge from glacier melt",
    "section": "\n12.2 References",
    "text": "12.2 References\n\n\n\n\nErasov, N. V. 1968. “Method for Determining of Volume of Mountain Glaciers.” MGI, no. 14: 307–8.\n\n\nFarinotti, Daniel, Matthias Huss, Johannes J. Fürst, Johannes Landmann, Horst Machguth, Fabien Maussion, and Ankur Pandit. 2019. “A Consensus Estimate for the Ice Thickness Distribution of All Glaciers on Earth.” Nature Geoscience 12 (3): 168–73. https://doi.org/10.1038/s41561-019-0300-3.\n\n\nGarcia Hernandez, J., A. Foehn, J. Fluixa-Sanmartin, B. Roquier, T. Brauchli, J. Paredes Arquiola, and De Cesare G. 2020. “RS MINERVE - Technical Manual, V2.25.” ISSN 2673-2661. Switzerland: Ed. CREALP.\n\n\nHugonnet, Romain, Robert McNabb, Etienne Berthier, Brian Menounos, Christopher Nuth, Luc Girod, Daniel Farinotti, et al. 2021. “Accelerated Global Glacier Mass Loss in the Early Twenty-First Century.” Nature 592 (7856): 726–31. https://doi.org/10.1038/s41586-021-03436-z.\n\n\nMiles, Evan, Michael McCarthy, Amaury Dehecq, Marin Kneib, Stefan Fugger, and Francesca Pellicciotti. 2021. “Health and Sustainability of Glaciers in High Mountain Asia.” Nature Communications 12 (2868): 10. https://doi.org/https://doi.org/10.1038/s41467-021-23073-4."
  },
  {
    "objectID": "study_guide_materials.html#sec-study-guide",
    "href": "study_guide_materials.html#sec-study-guide",
    "title": "Study Guide and Materials",
    "section": "Study Guide",
    "text": "Study Guide\nOver the duration of the course and as part of the Applied Modeling track, students are guided through implementing their own conceptual hydrological rainfall-runoff model of one of the Central Asian sample catchments that they can choose from the Case Studies Pack.\nStudents are required to work through the Chapters, including the occasional tasks that serve to deepen reflection on the course material and to do their daily homework assignments. As the final exam, the homework results are presented in a final student conference for which the students have to submit a conference abstract prior to the conference.\nThis Chapter explains how to use this course book.\nDifferent callout blocks appear throughout the text. These include Exercise, Tasks and Take Home Messages. Caution and Warning callouts highlight possibly problematic issues.\n\n\n\n\n\n\nEXERCISE\n\n\n\nExercise boxes are highlighted in blue color. With the description of the exercise, hints and a link to the solution are provided. Wherever they appear in the text, exercises should be completed before starting the next course chapter.\n\n\n\n\n\n\n\n\nTASK\n\n\n\n\n\n\n\n\n\n\n\n\nTAKE HOME MESSAGE\n\n\n\n\n\n\n\n\n\n\n\n\nCAUTION\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING\n\n\n\n\n\n\nCode blocks of R code with corresponding output are regularly shown throughout the text and look like this. Note that in grayed-out code cell, the code can be copied and the pasted into RStudio locally. Note that code blocks in Chapters are executed sequentially.\n\na <- 1 + 1\nprint(paste(\"a is set to\", a))\n\n[1] \"a is set to 2\""
  },
  {
    "objectID": "study_guide_materials.html#sec-materials",
    "href": "study_guide_materials.html#sec-materials",
    "title": "Study Guide and Materials",
    "section": "Materials",
    "text": "Materials\nIn the highly intensive hydrological modeling course at GKU, students have to pass 4 GRADED EXERCISEs to be admitted to the final presentation in addidion to preparatory home work. The following section describes the daily course content as well as the homework and the GRADED EXERCISEs with links to the relevant supporting chapters in the course book. The descriptions of the GRADED EXERCISEs are highlighted with exercise boxes.\nDay 1: Introduction & Installation of Software\nRead Chapter 1: A short history of Water in Central Asia and Chapter 2: Hydrological Systems in Semi-Arid Central Asia in the course book. Then make sure the required software for this course is installed on your computer. Section Open-source resources of the Appendix includes installation instructions and the on-line learning material that can get you started with the software. Below is a quick summary:\n\n\nQGIS\n\nR\n\nRStudio\n\nRS Minerve\n\nIf you have not used the software above before we recommend the following resources to get your started (remember, more detailed instructions for most tasks are available in the Appendix):\n\n\nQGIS training manual\n\n\nModern Dive for getting started with R and RStudio\n\nRS Minerve User Manual\n\nInevitably, you will also perform a lot of geocomputations with R in the future. After all, a GIS system like QGIS is nothing more than a nicely packed bunch of geocomputation algorithms and a window for visualizing geospatial assets. Well, rest assured, all of this can be done inside R. It is recommended therefore that you also consult the following excellent online resource Geocomputation with R.\n\n\n\n\n\n\nHOMEWORK\n\n\n\nDay 1 involves a lot of preparatory homework:\n- Reading the introductory chapters linked above and\n- Downloading and installing the required software linked above.\nThe homework is not graded but completion is a requirement for being able to work through the course.\n\n\nDay 2: Hydrological modelling and processes\nDay 2 involves a continued introduction to the hydrological modelling process and a deepending of the understanding what hydrological models are used for as well as a first part on hydrological processes (the partitioning of rainfall, transfer of water through the hydrological compartments).\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\nRoleplay on model uses. Read the role play exercise. You will be assigned a role. With your study colleagues, discuss the questions and take notes (about 15 min). One person per group will briefly (1 min) present the answers to the questions.\nIn preparation of tomorrows lecture and GRADED EXERCISE: Read the chapter on the case studies of Central Asian river basins and on [Hydraulic-Hydrological Modeling] (#sec-hydraulic-hydrological-modeling).\n\nThe homework is not graded but supports reflection about the use of hydrological models and how to judge the quality of hydrological models on day 3.\n\n\nDay 3: Hydrological modelling concepts and Catchment Characterization\nWatch the [tutorial video] explaining one method for the catchment characterization. Familiarize yourself with the Geospatial Data. Do the catchment characterization of the basin that you selected to work on by filling in the Table Table 1 below. If you have downloaded the entire folder on your local drive, you already have all the data available for the analysis.\n\n\nTable 1: As an example, key relevant basin statistics for Gunt river basin are shown with individual data sources indicated. Using the data available in the data pack, you should characterize your case study basin in a similar way.\n\n\n\n\n\nATTRIBUTE\nVALUE\n\n\n\n\nGeography (srtmgl12020?)\n\n\n\n\nBasin Area \\(A\\)\n\n13’693 km2\n\n\n\nMinimum Elevation \\(h_{min}\\)\n\n2’068 masl\n\n\nMaximum Elevation \\(h_{max}\\)\n\n6’652 masl\n\n\nMean Elevation \\(h_{mean}\\)\n\n4’267 masl\n\n\nHydrology [Source: Tajik Hydromet Service]\n\n\n\nNorm hydrological year discharge \\(Q_{norm}\\)\n\n103.8 m3/s\n\n\nNorm cold season discharge (Oct. - Mar., Q4/Q1)\n19.8 m3/s\n\n\nNorm warm season discharge (Apr. - Sept., Q2/Q3)\n84.2 m3/s\n\n\nAnnual norm discharge volume\n3.28 km3\n\n\n\nAnnual norm specific discharge\n239 mm\n\n\nClimate\n\n\n\nMean basin temperature \\(T\\) (Karger et al. 2017)\n\n-5.96 deg. Celsius\n\n\nMean basin precipitation \\(P\\) (Beck et al. 2020)\n\n351 mm\n\n\nPotential Evaporation \\(E_{pot}\\) (Trabucco and Zomer 2019)\n\n929 mm\n\n\nAridity Index \\(\\phi = E_{pot} / P\\)\n\n2.7\n\n\nAridity Index (Trabucco and Zomer 2019)\n\n3.6\n\n\nLand Cover (Buchhorn et al. 2019)\n\n\n\nShrubland\n8 km2\n\n\n\nHerbaceous Vegetation\n4’241 km2\n\n\n\nCrop Land\n0.5 km2\n\n\n\nBuilt up\n4 km2\n\n\n\nBare / Sparse Vegetation\n8’410 km2\n\n\n\nSnow and Ice\n969 km2\n\n\n\nPermanent Water Bodies\n80 km2\n\n\n\nLand Ice\n\n\n\nTotal glacier area (RGI Consortium 2017)\n\n875 km2\n\n\n\nTotal glacier volume (calculated with (Erasov 1968))\n699 km3\n\n\n\n\n\n\n\n\n\n\n\nGRADED EXERCISE : Catchment characterization\n\n\n\nFollowing the video tutorial, fill in the table above with the characteristic numbers of your catchment together with your colleague. Compare your numbers to the ones of the Gunt catchment (table above). Submit your table via email to the lecturer before the start of tomorrows lecture.\n\n\nDay 4: Discharge and climate data\nYet more data preparation is required before you can start modelling: The review of the basins discharge and climate forcing.\n\n\n\n\n\n\nGRADED EXERCISE : Discharge characterization\n\n\n\nRead the chapters on discharge station data and climate data and, together with your colleague, perform a discharge characterization of your basin following the video tutorial.\nSubmit your discharge characterization via email to your lecturer before the start of tomorrows lecture.\n\n\nDay 5: Discussion of Types of Hydrological Models\nHydrological models in general are discussed. Consult the introductory Section of Part III: Hydrological Modeling and Applications. All three types of modeling approaches will be presented but with a focus on hydraulic-hydrological rainfall-runoff modeling.\n\n\n\n\n\n\nHOMEWORK : RS Minerve tutorial\n\n\n\n\nRead the modelling chapter\n\nGo through the RS Minerve tutorial (TODO LINK)\n\nThis homework is not graded but basic knowledge of RS Minerve is required for the second part of the course.\n\n\nDay 6 & 7: Model Calibration and Validation\nRead the chapter on Model calibration and validation and go through the example of the Nauvalisoy catchment which illustrates the iterative model refinement process.\nStudents will implement a hydrological model of their study catchment and calibrate it.\n\n\n\n\n\n\nGRADED EXERCISE : Model implementation and calibration in RS Minerve\n\n\n\n\nRead the modelling chapter\n\nImplementing a hydrological model of your study basin in RS Minerve.\n\nShow your working model and calibration results to your lecturer during group work session on day 7.\n\n\n\n\n\n\n\n\nGRADED EXERCISE : Abstract submission for student conference\n\n\n\n\nCarefully read the abstract submission guidelines and write an abstract for your model.\n\nSubmitt your abstract by Saturday, 10 a.m. Almaty time to your lecturer by email.\n\n\nDay 8: Student Conference & Course Wrap Up\nThe last day of the course is organized as a student conference where students present their modeling work on their respective case study catchment. The groups need to prepare a presentation of 12 minutes duration. Each presentation will be followed by a 3 minutes Q&A session. After all the groups have presented, impressions and feedback will be shared by the teachers followed by a larger group discussion.\nOnly students that have passed the GRADED EXERCISEs will be admitted to the student conference which consists the final exam.\nAt the end, students are invited to provide feedback with regard to their impression of the course. A key question will be hoe the course can be further improved to reach future students even more effectively.\n\n\n\n\n\n\nFINAL EXAM : Model presentation\n\n\n\n\nPresent an overview of your catchment, discharge characterization and your model implementation and results at the students conference."
  },
  {
    "objectID": "study_guide_materials.html#references",
    "href": "study_guide_materials.html#references",
    "title": "Study Guide and Materials",
    "section": "References",
    "text": "References\n\n\n\n\nBeck, Hylke E., Eric F. Wood, Tim R. McVicar, Mauricio Zambrano-Bigiarini, Camila Alvarez-Garreton, Oscar M. Baez-Villanueva, Justin Sheffield, and Dirk N. Karger. 2020. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1.\n\n\nBuchhorn, M., B. Smets, L. Bertels, B. De Roo, M. Lesiv, N. E. Tsendbazar, M. Herold, and S. Fritz. 2019. “Copernicus Global Land Service: Land Cover 100m: Collection 3: Epoch 2019: Globe.”\n\n\nErasov, N. V. 1968. “Method for Determining of Volume of Mountain Glaciers.” MGI, no. 14: 307–8.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nRGI Consortium. 2017. “Randolph Glacier Inventory – a Dataset of Global Glacier Outlines: Version 6.0: Technical Report.” Global Land Ice Measurements from Space, Colorado, USA. Digital Media. https://doi.org/https://doi.org/10.7265/N5-RGI-60.\n\n\nTrabucco, Antonio, and Robert Zomer. 2019. “Global Aridity Index and Potential Evapotranspiration (ET0) Climate Database v2,” January. https://doi.org/10.6084/m9.figshare.7504448.v3."
  }
]