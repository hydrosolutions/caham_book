[
  {
    "objectID": "hydrology_of_central_asia.html",
    "href": "hydrology_of_central_asia.html",
    "title": "",
    "section": "",
    "text": "Part I: Hydrology of Semi-Arid Central Asia\nThe arid plains of Central Asia have been teeming with life thanks to the rivers cutting through them and bringing water to the parched plains when its most needed, in the hot and dry summer months. Why is it that the natural, undisturbed rivers swelled exactly during that time? How do we explain this phenomenon that has enabled for the Silk Road and unique cultural centers to emerge over the course of history and have brought riches to the communities there?\nThis Chapter describes the key elements of the regional hydrology of semi-arid Central Asia that will help to understand this coincidence of nature. Leaving the large scale perspective, 2 example river basins are discussed in greater detail with a special focus on the runoff generation mechanisms there.\nApart from looking at past data for explanation, a forward view will also be taken to discuss the recent and anticipated future hydrological changes. It will be highlighted how our current understanding of these changes will impact societies and the environment alike."
  },
  {
    "objectID": "hydrological_modeling.html",
    "href": "hydrological_modeling.html",
    "title": "",
    "section": "",
    "text": "Part III: Hydrological Modeling & Applications\nThis part of the book focusses on different types of hydrological modeling approaches and applications. The Chapter on hydrological modeling using rainfall-runoff models introduces in a hands-on manner modeling using the free-tu-use RSMinerve Software Suite. These types of models are foundational for example for basin planning exercises where tradeoffs between water for different sectoral allocations need to be quantified in a specific context.\nSuch models are also important for detailed climate impact studies regularly used to study these. The idea is simple, i.e., to use available climate model output over the 21st century as forcing and investigate changes in the hydrographs at stations of interest over time. When different models and scenarios are run, an band of uncertainty can be specified which is relevant in any decision-making context.\nFinally, the design of hydropower infrastructure depends on hydrological assessments with such types of models. The model outputs, i.e., simulated (modelled) discharge at a particular location, can be used to compute cummulative flow duration curves which are essential for the assessment of the hydropower potential and critically inform infrastructure sizing.\nThe relevance of these types of models for the water planners and managers in the global drylands cannot be overstated and therefore, one of the primary goals of this course is to familiarze the students well with such types of models.\nThe Chapter on long-term hydrological modeling using the Budyko framework looks at the greater semi-arid Central Asia region as compared to individual catchments. It is at this scale and over a large number of smaller catchments where interesting steady-state patterns of the partioning of available water into evporation and runoff can be studied, under current and future climate states. Among other applications, such types of models can help to inform the large-scale questions, also with regard to the current and future inter-state water distribution.\nFinally, the Chapter on time series modeling using predictive inference discusses models that learn from past patterns to predict the future, without explicit water balance constraints. Through learning patterns in time-ordered data, possibly also with the help of auxilliary data such as preseason snow cover, it has been shown that time series models can be powerful to predict discharge at certain specific location for different lead times, from hours to seasons. The section will present such type approaches in the context of the seasonal forcasting of river flows in Central Asia."
  },
  {
    "objectID": "short_history_of_central_asia_water.html",
    "href": "short_history_of_central_asia_water.html",
    "title": "",
    "section": "",
    "text": "Historical Context\nHighlight the importance of water in semi-arid Central Asia in the historical development of the region."
  },
  {
    "objectID": "intro.html#welcome",
    "href": "intro.html#welcome",
    "title": "",
    "section": "Welcome",
    "text": "Welcome\nThis handbook on hydrological modeling of Central Asian river basins is geared towards young water professionals in Central Asia. They inherit fascinatingly complex natural and man-made hydrological systems. They face work where opportunities for modernization abound after decades of limited investments in the water sectors of the countries and where continuous population growth and a changing climate pose emerging challenges. At the same time, they face work in a field that has enabled the region to prosper and flourish over hundreds if not thousands of years.\nThe authors hope that this textbook provides a source of inspiration for these students and that the text and the methods presented will also be used by teachers and integrated in university curricula locally.\nThe book is dedicated to colleagues at the Central Asian Hydrometeorological Agencies whose tireless work in collecting and analyzing hydro-meteorological data in Central Asia has helped to significantly improve our understanding of the complex runoff generation processes at work in the region.\nThe authors are grateful for the support by the Global Water Programme of the Swiss Agency for Development and Cooperation who greatly helped to push the envelop further with regard to modern water education in the Central Asia region. Finally, Mr. Andrey Yakovlev and his tremendous knowledge of the region is acknowledged."
  },
  {
    "objectID": "intro.html#license",
    "href": "intro.html#license",
    "title": "",
    "section": "License",
    "text": "License\n\n\nDOI\n\n\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\nThe development of this book was supported by the Swiss Agency for Development and Cooperation."
  },
  {
    "objectID": "climate_data.html#sec-historical-climate-data",
    "href": "climate_data.html#sec-historical-climate-data",
    "title": "",
    "section": "Historical Climate Data",
    "text": "Historical Climate Data\nCHELSA V21 High-Resolution Climate Data\nTo obtain information on past precipitation (P) and temperature (T) in the Central Asia region, we use the very high-resolution daily temperature climate product from CHELSA. CHELSA (Climatologies at high resolution for the earth’s land surface areas) is a global downscaled climate data set currently hosted by the Swiss Federal Institute for Forest, Snow and Landscape Research (WSL). It is built to provide free access to high resolution climate data for research and application, and is constantly updated and refined. CHELSA data are in the process of revolutionizing the field of hydrology in data-poor regions, among many other application domains.\nCHELSA includes climate layers for various time periods and variables, ranging from the Last Glacial Maximum, to the present, to several future scenarios. CHELSA is based on a mechanistic statistical downscaling of global reanalysis data for the historical observations (hist_obs) or global circulation model output for future simulations (fut_sim) (see also www.chelsa-climate.org for more information). For more technical information, please consult the following document.\nThe historical observations for CHELSA precipitation and temperature are available from 1979 through 2016 for daily time steps and at 1 km resolution. These are derived from ERA-INTERIM reanalysis model outputs, among other things. ERA-INTERIM is a global atmospheric reanalysis product with a spatial resolution of 80 km, approximately. A good introduction about what a reanalysis product is can be found on this website.\nDaily gridded precipitation fields are generated merging data from the ERA5 reanalysis precipitation and the MODIS monthly cloud cover. The CHELSA algorithm that is used for downscaling takes into account orographic predictors such as wind, topographic exposition and boundary layer height Karger et al. (2021). When compared with other products, the resulting data shows excellent performance, also in complex high mountain terrain. Temperature observations are available over the same period and are taken from (“CHELSA-W5e5 V1.0: W5e5 V1.0 Downscaled with CHELSA V2.0” 2022).\n\n\n\n\n\n\nWarning\n\n\n\nThe data for the Central Asia domain (55 deg. E - 85 deg. E and 30 deg. N - 50 deg. N) is very large and is not provided here for download (total storage requirements > 1 TB). Please contact Tobias Siegfried via siegfried@hydrosolutions.ch for more information on how to obtain access the data of the entire domain.\n\n\nThe high-resolution climate data derived with the CHELSA algorithm is corrected for the problem of snow-undercatch in the high mountain regions as described by (beck2020?). What is snow-undercatch? Measuring precipitation correctly in high altitude regions is complex because of sublimation and blowing snow. An example of this is shown in Figure 1 for high elevation gauges in Spain. In a recent inter-comparison project carried out in Spain, it has been shown that undercatch poses significant problems in accurately measuring solid precipitation (Buisán et al. 2017) in mountainous regions. Both, ERA-INTERIM and CHELSA themselves assimilate station data in their models and hence are affected by these erroneous measurements.\n\n\nFigure 1: Measured snow undercatch values in high-mountain stations in Spain. The values were determined within the World Meteorological Organization Solid Precipitation Intercomparison Experiment (WMO-SPICE). See text for more information and reference.\n\n\n(beck2020?) has recognized this and released monthly correction factors that are taken into account in the CHELSA algorithm (see Figure 2).\n\n\nFigure 2: Figure from (beck2020?), Supplementary Material. Plate d): Best estimate of global bias correction factors. Plate e): Lower bound estimate of global bias correction factors. Plate f): Upper bound of global bias correction factors. As is clearly visible, bias correction factors in high-mountain Asia, including the parts of Central Asia are significant.\n\n\nThe annual precipitation climatology, i.e. the long-term mean annual precipitation, from 1979 - 2011 is shown in Figure 3. As is easily visible and not further surprising, the mountainous regions receive the bulk of the precipitation, on average.\n\n\nFigure 3: The CHELSA V21 Precipitation climatology in 6 large basins Central Asian basins is shown, including Amu Darya, Syr Darya, Talas River, Chu River, Issq Kul and Ily River is shown. Light blue colors indicate very little preciptiation whereas red colors indicate high annual norm precipitation amounts.\n\n\nThe following Figure 4 and Figure 5 show cold and warm season precipitation amounts. The cold season is defined to encompass the months October (previous year) - March (preceding year) whereas the warm season lasts from April through September.\n\n\nFigure 4: The cold season precipitation climatology is shown. This Figure should also be compared with the the warm season precipitation climatology as shown in Figure 5.\n\n\n\n\nFigure 5: The warm season precipitation climatology is shown. This Figure should also be compared with the the cold season precipitation climatology as shown in Figure 4.\n\n\nFigure 4 shows that winter precipitation is mainly concentrated on the western fringes of the mountain ranges where moisture gets precipitated via westerly circulations and associated frontal systems. Compared to this, the main warm season precipitation locations move further to the east and to inner mountain range locations where summer convective storms cause this (see Figure 5).\nThe climatological data used to produce these Figures is available via this Dropbox link in the climate/chelsa_v21/climatologies/ sub-folder. There data over the historical observation period from 1981 - 2010 has been prepared for the norm annual and cold as well as warm season temperatures (tas_…) has been prepared. Similarly, data on precipitation (pr_…) and potential evapotranspiration (pet_…) is available and on the aridity index which is defined as \\(\\phi = PET/P\\) where \\(PET\\) is the potential evapotranspiration climatology and \\(P\\) is the precipitation climatology.\n\n\n\n\n\n\nTip\n\n\n\nTry it yourself! Download the data for the Central Asia domain here and visualize the climatologies for your case study catchment and extract mean statistics for the basins. As a reminder, the case study basins can be accessed via this link.\n\n\nAssessment of CHELSA V21 Data Quality in a Sample Catchment\nHow can the quality of the CHELSA data in the complex Central Asia domain be assessed? With try to answer this question by looking at one of the case study basins provided as part of the Student Case Study Pack. Specifically, we want to answer the following questions for the Gunt River basin in the Pamir mountains:\n\ndoes the magnitude of the precipitation yield physically meaningful results, and\ndoes the climatology adequately reproduce the seasonal cycle observed one at the stations?\n\nLong-term Annual Norm Discharge\nLet us address the first question by investigating long-term norm CHELSA precipitation values and comparing these long-term norm values of the specific discharge of Gunt River. If \\(P >Q\\), where \\(P\\) is the long-term mean precipitation and \\(Q\\) is the long-term mean discharge, we can confidently say that the bias corrected CHELSA precipitation product is meaningful from a water balance perspective. The long-term water balance is simply\n\\[\nQ = P - E\n\\qquad(1)\\]\nwhere \\(Q\\) is the specific discharge [mm], \\(P\\) is the long-term mean precipitation [mm] and \\(E\\) is the long-term mean evapotranspiration \\(E\\) [mm] (see also the Chapter on Long-term water balance modeling for more information). Hence, if, over the long run, \\(P>E\\) and under the assumption that storage changes i.e. from glacier melt are not present, the water balance is valid and the product from that perspective validated.\nWe can compute the average long-term precipitation in the catchment in a simple way. The code block below shows how.\n\n# load required libraries\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(exactextractr)\nlibrary(raster)\nlibrary(tmap)\nlibrary(tmaptools)\n\n# load basin shapefile. note that if you want to replicate \n# the code below on your own computer, make sure that you \n# adjust the data paths accordingly, depending on where \n# you stored the downloaded data!\nbasin_shp_path <- \n  \"../caham_data/AmuDarya/17050_Gunt/GIS/17050_basin_latlon.shp\"\nbasin_shp <- sf::st_read(basin_shp_path, quiet = TRUE)\n\n# load climatology and extract values\np_clim_path <- \n  \"../caham_data/central_asia_domain/climate/chelsa_v21/climatologies/hist_obs/pr_chelsa_climatology_ann.tif\"\np_clim <- raster(p_clim_path)\n\n# mask and crop\np_clim_gunt <- p_clim %>% \n  mask(basin_shp) %>% \n  crop(basin_shp)\n\n# plotting climatology\ntmap::tmap_mode(\"view\")\ntmap::tm_basemap(\"Stamen.Terrain\") +\n  tm_shape(p_clim_gunt) +\n  tm_raster(style = \"quantile\", n = 7, palette = get_brewer_pal(\"Blues\", n = 7, plot = FALSE)) +\n  tm_shape(basin_shp) +\n  tm_borders(\"black\")\n\n\nFigure 6: CHELSA precipitation climatology in Gunt River Basin. Note, the Figure is interactive and you can zoom in and out to see more details. The above code is shown to demonstrate the generation of such a Figure.\n\n\n\nThe mean norm precipitation can be easily computed as follows.\n\n# extracting mean value over Gunt River basin. \n# note the resulting value is in mm.\np_clim_mean <- p_clim %>% exact_extract(basin_shp,'mean')\n\nWarning in .local(x, y, ...): Polygons transformed to raster CRS (EPSG:NA)\n\np_clim_mean\n\n[1] 384.4959\n\n\nWe thus get 384 mm for the 30 year period from 1981 through 2010. Let us compare this to the long-term discharge at Gunt Khorog discharge station. For this, we load the corresponding data frame from which we extract the corresponding data.\n\n# load discharge data from Gunt Khorog gauging station. note that if you want to replicate the code below on your own computer, make sure that you adjust the data paths accordingly, depending on where you stored the downloaded data!\nstation_data <- \n  readRDS(\"../caham_data/AmuDarya/17050_Gunt/GaugeData/17050_Q.Rds\")\n\n# extract data for station 17050 and discharge between 1981 and 2010\ndischarge_data_17050 <- \n  station_data %>%\n  filter(type == \"Q\") %>% \n  filter(code == \"17050\") %>% \n  filter(date >= ymd(\"1981-01-01\")) %>% \n  filter(date <= ymd(\"2010-12-31\"))\n  \n# compute long-term discharge\ndischarge_data_17050_norm <- \n  discharge_data_17050$data %>% \n  mean(na.rm = TRUE)\ndischarge_data_17050_norm\n\n[1] 108.7257\n\n\nFor the long-term discharge at the station 17050, we thus get 109 m3/s. To compute the annual norm specific discharge, we need to compute the total discharge volume for a year and then divide by the total basin area.\n\nbasin_area <- st_area(basin_shp) %>% as.numeric()\ndischarge_data_17050_norm_vol <- \n  discharge_data_17050_norm * 3600 * 24 * 365 / basin_area * 1000\ndischarge_data_17050_norm_vol\n\n[1] 250.5378\n\n\nHence, for the period 1981 - 2010, we obtain for the specific discharge 250 mm. Clearly, \\(Q<P\\) and we can calculate that, on average, 2 parts of the total precipitation are discharged via the river whereas 1 part is evaporated without contribution to runoff at the gauge 17050 in Khorog town.\nAs an aside, the bias corrected precipitation climatology shows an interesting feature of the Gunt river basin (see Figure 6). Namely, there is a stark precipitation gradient between the western part of the basin where the bulk of the precipitation is observed and the hyper-arid Pamir plateau region to the east, where annual precipitation is below 200 mm at a mean altitude of above 4’000 meters above sea level [masl]. This place thus can be classified as alpine desert. This is an orographic effect as most of the moisture is washed out of the atmosphere before it enters the region of the plateau arriving from the westerly direction.\nDischarge Seasonality\nWhat about the seasonality of the CHELSA precipitation? Can it adequately reproduce the observed precipitation seasonality? If this would not be the case, we would have to reject the validity of the product and explore other high-resolution climatologies such as WorldClim V2 or CHPClim V1 (see (beck2020?) for more information on these products). Let us explore again the data of the Gunt River basin to answer this question.\nFirst, we load and prepare all the required station precipitation and geospatial data. Then, we compute the monthly norms of these data for the period 1981-01-01 through 2010-12-31. Note that the meteorological station at Khorog is located at the same place as the discharge gauging station and has the 5-digit index 38954 (see also the dedicated Example Catchments Chapter for more information).\n\nstation_data <- \n  readRDS(\"../caham_data/AmuDarya/17050_Gunt/GaugeData/17050_Q.Rds\")\n\n# extract the precipitation data for station 17050 between 1981 and 2010\npr_data_38954 <- \n  station_data %>%\n  filter(type == \"P\") %>% \n  filter(code == \"38954\") %>% \n  filter(date >= ymd(\"1981-01-01\")) %>% \n  filter(date <= ymd(\"2010-12-31\"))\n\n# add a month identifier to the dataframe and group by months\npr_data_38954 <- pr_data_38954 %>% \n  mutate(month = month(date)) %>% \n  group_by(month) %>% dplyr::select(date, data, month)\n\n# compute monthly mean values\np_monthly_mean_38954 <- pr_data_38954 %>% \n  summarize(data = mean(data, na.rm = TRUE)) %>% \n  add_column(data_source = \"Meteo Station 39954\")\n\n# plot the resulting monthly time series\np_monthly_mean_38954 %>% \n  ggplot(aes(x = month, y = data), color = data_source) + \n  geom_line() + \n  xlab(\"Month\") + \n  ylab(\"mm/month\") +\n  ggtitle(\"Monthly precipitation climatology from 1981 - 2010, Station 38954\")\n\n\n\n\nWe can extract the mean monthly values from the CHELSA SpatRaster data and then compare it to station data.\n\np_clim_monthly_path <- \n  \"../caham_data/central_asia_domain/climate/chelsa_v21/climatologies/hist_obs/monthly/pr_monthly_55_85_30_50.tif\"\np_clim_monthly <- terra::rast(p_clim_monthly_path)\n\np_monthly_mean_CHELSA_data <- \n  p_clim_monthly %>% exact_extract(basin_shp,'mean') %>% \n  as.numeric()\n\nWarning in .local(x, y, ...): Polygons transformed to raster CRS (EPSG:4326)\n\np_monthly_mean_CHELSA <- \n  p_monthly_mean_38954\np_monthly_mean_CHELSA$data <- \n  p_monthly_mean_CHELSA_data\np_monthly_mean_CHELSA$data_source <- \n  \"CHELSA\"\n\np_monthly_mean <- \n  p_monthly_mean_38954 %>% \n  add_row(p_monthly_mean_CHELSA)\n\n# plot the resulting monthly time series\np_monthly_mean %>% \n  ggplot(aes(x = month, y = data, color = data_source)) + \n  geom_line() + \n  xlab(\"Month\") + \n  ylab(\"mm/month\") +\n  ggtitle(\"Comparison of station and CHELSA precipitation climatologies\")\n\n\n\nFigure 7: ?(caption)\n\n\n\n\nAs is evident by looking at Figure 7, the CHELSA product can adequatly reproduce the seasonality of the local precipitation climatology and is only slightly overestimation absolute values. However, with regard to the later, this argument is not necessarily valied as we compare local point measurements with raster data with a resolution of 1 km2. A more thorow comparsion would generate an interpolated climatology field from station data and then compare these fields. However, as data is very scarce in this large basin, we do not have the means to perform such analysis.\n\n\n\n\n\n\nEXERCISE\n\n\n\nTry it yourself. Conduct the same analysis with the monthly temperature climatologies for the meteorological station and for the CHESLA data. You can very easily carry this out while reusing code blocks from above, also for any of the other basins available in the Case Study pack."
  },
  {
    "objectID": "climate_data.html#sec-climate-projections",
    "href": "climate_data.html#sec-climate-projections",
    "title": "",
    "section": "Climate Projections",
    "text": "Climate Projections\nThe daily CHELSA V21 climate forcing data can be using for hydrological modeling from 1979 - 2010. But what about the future? After all, one of the main goals of this course book is to demonstrate how to use hydrological modeling to quantify future climate impacts in the Central Asian river basins. For this purpose, we will demonstrate in this Section how to download and process future climate data for studying hydrological changes.\n\n\nFigure 8: The time arrow is shown point from left to the right. The availability of corresponding data is indicated for what we call the historical reference period (hist_…) and the future scenario period (fut_…).\n\n\nTo start with, we need to divide the total period of interest into a historic period and a future period. The historic period in Figure 8 is highlighted in orange color and ranges from 1979 through 2010. It is the period for which we have observed climate forcing data from the CHELSA dataset and gauge data available at the same time. As will be discussed in the Chapter on hydrological modeling, it is also the period which we use to calibrate and validate our hydrological model.\nWhat we call the future period is highlighted by the blue arrow in Figure 8. Admittedly, the years from 2011 through 2022 are not in our current future (this edition of the book is written and published in 2022), so it really is just a matter of definition. It is this period over which we want to study climate impacts under different scenarios.\nGlobal Circulation Models\nThese scenarios are describing different increasing greenhouse concentration pathways are are computed with large-scale numerical models called General Circulation Models or GCMs. They globally represent physical processes in the atmosphere, the oceans, the cryosphere and the land surface. GCMs compute geographically distributed and physically consistent estimate of regional climate change and thus are the key inputs to different types of impact analyses. A stylized schematic structure of a GCM is shown in Figure 9.\n\n\nFigure 9: Schematic structure of a GCM model. Source: Penn State University, David Bice.\n\n\nFigure 9 shows that GCMs discretize the atmosphere, ocean and land columns into a three dimensional grid with differing numbers of vertical layers that is a function of the model and the compartment under consideration (atmosphere, land, ocean). With a typical horizontal resolution between 250 km - 600 km, the spatial resolution of the GCMs is coarse relative to what is needed for detailed impact studies. Furthermore, many key physical processes such as cloud formation happen at sub-grid resolution. These can thus only indirectly be represented by a process called parameterization and it represents a major source of uncertainty in GCM-based future climate simulations. Furthermore, since every GCM model represents processes and feedback mechanisms in the model in a different way, there is also inter-model uncertainty where different models generate different climate responses despite the same scenario-based forcing. Being cognisant of these uncertainties is important in impact studies. For more information, see also this website.\nFor illustration, ?@fig-comparison-chelsa-gcm-resolution shows the temperature fields for 01. January 1979 over the Central Asia domain as provided by the CHELSA V21 data set (left panel) and as computed by the GCM GFDL-ESM4 model under the historic run. Note that there is no particular reason why we choose this model, it is just to serve as an example here. The complete list of models which we use for the climate change impact analysis will be presented and discussed further below.\nThe difference in resolution is striking with the CHELSA data having having a horizontal resolution of 1 km, approx., and the GCM model having a resolution of 1.5 degrees x 1 degrees which corresponds to 166.5 km x 111 km on the equator, approximately. Why is GCM resolution so coarse? It is, simply put, limited by restricitions given by the computational power of the powerful super computers where these models are run on.\n\n\n\n\n\nFigure 10: Comparison of CHELSA temperature climatology (left) and the GCM climate field of the historical run of the model GFDL-ESM4 (right).\n\n\n\n\n\n\nFigure 11: Comparison of CHELSA temperature climatology (left) and the GCM climate field of the historical run of the model GFDL-ESM4 (right).\n\n\n\n\n\nAs is indicated in Figure 9, GCM runs from the historic period from 1979 through 2010 are also available. These historic GCM runs are very important as we shall see in a minute when it comes to bias correcting and downscaling GCM runs onto the spatial units of interest, i.e. hydrological response units (HRUs) in our case.\nCMIP6 Climate Scenarios\nWhat are the future scenarios that we are interest in? The global climate science community has worked hard under the within the Phase 6 of the Coupled Model Intercomparison Project (CMIP6) the define relevant future scenarios that are describing different climate forcing trajectories. The paper by (O’Neill et al. 2016) is the refernce source with regard to detailed descriptions of the scenarios. We are interested to cover and study a broad range of possible hydrological future states and thus select 4 distrinct shared socioeconomic pathway (SSP) scenarios that cover this entire possible range.\nThe SSPs are based on five narratives describing broad socioeconomic trends that possibly shape future society. These are intended to span the range of plausible futures. The narratives are (taken from (Riahi et al. 2017)):\n\nSSP1 Sustainability – Taking the Green Road (Low challenges to mitigation and adaptation): The world shifts gradually, but pervasively, toward a more sustainable path, emphasizing more inclusive development that respects perceived environmental boundaries. Management of the global commons slowly improves, educational and health investments accelerate the demographic transition, and the emphasis on economic growth shifts toward a broader emphasis on human well-being. Driven by an increasing commitment to achieving development goals, inequality is reduced both across and within countries. Consumption is oriented toward low material growth and lower resource and energy intensity.\nSSP2 Middle of the Road (Medium challenges to mitigation and adaptation): The world follows a path in which social, economic, and technological trends do not shift markedly from historical patterns. Development and income growth proceeds unevenly, with some countries making relatively good progress while others fall short of expectations. Global and national institutions work toward but make slow progress in achieving sustainable development goals. Environmental systems experience degradation, although there are some improvements and overall the intensity of resource and energy use declines. Global population growth is moderate and levels off in the second half of the century. Income inequality persists or improves only slowly and challenges to reducing vulnerability to societal and environmental changes remain.\nSSP3 Regional Rivalry – A Rocky Road (High challenges to mitigation and adaptation): A resurgent nationalism, concerns about competitiveness and security, and regional conflicts push countries to increasingly focus on domestic or, at most, regional issues. Policies shift over time to become increasingly oriented toward national and regional security issues. Countries focus on achieving energy and food security goals within their own regions at the expense of broader-based development. Investments in education and technological development decline. Economic development is slow, consumption is material-intensive, and inequalities persist or worsen over time. Population growth is low in industrialized and high in developing countries. A low international priority for addressing environmental concerns leads to strong environmental degradation in some regions.\nSSP4 Inequality – A Road Divided (Low challenges to mitigation, high challenges to adaptation): Highly unequal investments in human capital, combined with increasing disparities in economic opportunity and political power, lead to increasing inequalities and stratification both across and within countries. Over time, a gap widens between an internationally-connected society that contributes to knowledge- and capital-intensive sectors of the global economy, and a fragmented collection of lower-income, poorly educated societies that work in a labor intensive, low-tech economy. Social cohesion degrades and conflict and unrest become increasingly common. Technology development is high in the high-tech economy and sectors. The globally connected energy sector diversifies, with investments in both carbon-intensive fuels like coal and unconventional oil, but also low-carbon energy sources. Environmental policies focus on local issues around middle and high income areas.\nSSP5 Fossil-fueled Development – Taking the Highway (High challenges to mitigation, low challenges to adaptation): This world places increasing faith in competitive markets, innovation and participatory societies to produce rapid technological progress and development of human capital as the path to sustainable development. Global markets are increasingly integrated. There are also strong investments in health, education, and institutions to enhance human and social capital. At the same time, the push for economic and social development is coupled with the exploitation of abundant fossil fuel resources and the adoption of resource and energy intensive lifestyles around the world. All these factors lead to rapid growth of the global economy, while global population peaks and declines in the 21st century. Local environmental problems like air pollution are successfully managed. There is faith in the ability to effectively manage social and ecological systems, including by geo-engineering if necessary.\n\nFigure 12 shows the underlying population and GDP developments for the corresponding SSPs and Table 1 details about the forcing in these scenarios.\n\n\nFigure 12: Global population and GDP developments under the CMIP6 shared socioeconomic pathways (taken from this source).\n\n\nThe description of the Tier 1 scenarios below that we are focussing on is taken from the aforementioned publication.\n\n\nTable 1: Scenarios, their forcing category and the effective radiative forcing by the year 2100 (O’Neill et al. 2016).\n\n\n\n\n\n\nScenario\nForcing Category\n2100 forcing [W/m2]\n\n\n\nShared Socioeconomic Pathway SSP5-8.5\nHigh\n8.5\n\n\nShared Socioeconomic Pathway SSP3-7.0\nHigh\n7\n\n\nShared Socioeconomic Pathway SSP2-4.5\nMedium\n4.5\n\n\nShared Socioeconomic Pathway SSP1-2.6\nLow\n2.6\n\n\n\n\nFor each scenario, we select 4 high priority GCM models for the preparation of downscaled climate forcings for the basins under consideration. The following Table 2 shows overview information about the models which are used in this course. Output of the GCM models shown in the table is available at daily timescales until 2100 and also for the historic runs.\n\n\nTable 2: Model names, models and host institution where the GCM models have been developed.\n\n\n\n\n\n\nName\nModel\nInstitution\n\n\n\nGFDL-ESM4\ngfdl- esm4\nNational Oceanic and Atmospheric Administration, Geophysical Fluid Dynamics Laboratory, Princeton, NJ 08540, USA\n\n\nUKESM1-0-LL\nukesm1- 0-ll\nMet Office Hadley Centre, Fitzroy Road, Exeter, Devon, EX1 3PB, UK\n\n\nMPI-ESM1-2-HR\nmpi- esm1-2- hr\nMax Planck Institute for Meteorology, Hamburg 20146, Germany\n\n\nIPSL-CM6A-LR\nipsl- cm6a-lr\nInstitut Pierre Simon Laplace, Paris 75252, France\n\n\nMRI-ESM2-0\nmri-esm2-0\nMeteorological Research Institute, Tsukuba, Ibaraki 305-0052, Japan\n\n\n\n\nGCM Model data can be downloaded from the climate store on the dedicated Copernicus website.\n\n\n\n\n\n\nImportant\n\n\n\nEach GCM model is different from the others. Some model leap days, some don’t. Some assume that each month has 30 days, other don’t. The preprocessing steps required to make the outputs of these models is very time consuming and is not recommendened for beginners. Therefore, please download the pre-processed climate scenarios for the Central Asia domain from this online repository.\n\n\nDownscaling and Bias Correction Using Quantile Mapping\nGCM model data can be subject to systematic biases (e.g. Kotlarski et al. 2014) and their coarse resolution does not allow us to directly use these data in hydrological modeling studies. For this reason, a large number of downscaling and bias correction techniques have been developed, inclduing the well-known delta change method (see e.g. Feigenwinter et al. 2018 and references therein).\nThe daily CHELSA V21 data that became available in 2021, together with daily GCM data from CMIP6 now allows a relatively straight forward application of empirical quantile mapping to downscale to and statistically correct GCM for hydrological response units (HRUs).\nFeigenwinter et al. (2018) explains clearly how empirical quantile mapping works. For the historical period (also called calibration period in the context of the discussion here), simulated model output (in our case, GCM hist_sim data, as shown in Figure 8) is corrected with a correction function towards an observational reference (here, the high-resolution CHELSA climatology) and systematic model biases are partly removed.\n\n\nFigure 13: Overview on the bias correction approach: a bias correction function is calibrated by comparing raw climate model output to observations in a common historical reference period. The calibrated correction function is then applied to the entire raw model output in order to produce a bias-corrected time series out into the future scenario period (taken from (Feigenwinter et al. 2018)).\n\n\nIn a climate change context, the so-called correction function (or transfer function), established in the historical calibration period, can then be applied to the simulated future time series in order to produce bias-corrected scenario time series.\n\n\nFigure 14: The nature of empirical quantile mapping is shown (source: (Feigenwinter et al. 2018)). Left panel: Example based on the probability density function (PDF). Right panel: example based on the cumulative distribution function (CDF).\n\n\nFigure 14 explains the bias correction approach graphically. A biased simulated distribution (blue) is corrected towards an observed distribution (black). In the example shown the raw simulated distribution is subject to both a bias of the mean and a bias in variance. The resulting bias-corrected distribution (dashed red) approximates the observed one but is typically not identical to it (e.g. due to the sampling uncertainty during the calibration of the correction function or details of the specific quantile mapping implementation).\nAs we explained above, we investigate 4 climate scenarios (ssp126, ssp245, ssp 370 and ssp585) for which we have 4 GCM model runs each (GFDL-ESM4, UKESM1-0-LL, MPI-ESM1-2-HR and MRI-ESM2-0). Hence, we have 16 scenario-model combination and the same amounts of correction functions. The best way to achieve this is show the necessary steps by means of an example catchment. While each catchment is unique, the steps to pre-process and later export the corresponding climate files for modeling in RSMinerve are not and are thus generalizable.\nThe only caveat is that the processing of the CHELSA high-resolution climate files requires the very large raw files to be available locally. Due to the size of these files for the entire Central Asia domain, sharing these files is not easy and we are working on ways to make these files more readily available in the future so that they can be processes locally.\nFor the moment, each case study catchment in the student pack contains the precomputed climate files with which RSMinerve hydrological models can be run in a straight forward manner. These files are stored in the RS_Minerve folder. The following files are available:\n\n\nhist_obs_rsm.csv: This is the .csv-files that contains the CHELSA V21 temperature and precipitation forcing for each of the elevation bands as specified in the /GIS/XXXXX_hru.shp file where XXXXX is the placeholder for the corresponding gauge code.\n\nhist_sim_….csv: For the 4 climate models investigated, the simulated history is stored in these file. They are used for bias correction and not directly used in hydrological modeling.\n\nfut_sim_….csv: We have generated 16 such files which consist of 4 climate scenarios for each of the 4 climate models. These are the future temperature and precipitation time series that were bias corrected using the quantile mapping method as explained above.\n\nfut_sim_bcsd_….csv: These are the 16 final bias corrected and downscaled future climate forcing files. These are the .csv-files that are read into RSMinerve for the study of climate impacts on the river basin under consideration.\n\nThe processes of generating these files is shown in the following at the example of Chon Kemin catchment in Kyrgyzstan. For each catchment in the Students’ Case Study Pack, the code to process the files can be found in the corresponding CODE folder.\nFirst, the necessary libraries need to be loaded.\n\n# Tidy data wrangling\nlibrary(tidyverse) # includes readr and ggplot2\nlibrary(lubridate)\nlibrary(timetk)\n\n# plotting add-ons to ggplot2\nlibrary(patchwork)\n\n# Our own package for load and processing local data\ndevtools::install_github(\"hydrosolutions/riversCentralAsia\")\nlibrary('riversCentralAsia')\n\n# Spatial data processing\nlibrary(raster)\nlibrary(terra)\nlibrary(sf)\nlibrary(stars)\nlibrary(exactextractr)\n\n# quantile mapping\nlibrary(qmap)\n\nThen, we can simply configure the details of the catchment at hand (note the location installation dependent specification of paths!).\n\n# River\nriver_name <- \"ChonKemin\"\nbasin_name <- \"Chu\"\n\n# Gauge\ngauge_name <- '15149_gauge'\ngauge_code <- '15149'\nq_path <- \"../caham_data/student_case_study_basins/15149_ChonKemin/GaugeData/\"\nq_name <- paste0(gauge_code,\"_Q.csv\")\ndata_type_Q <- \"Q\"\nunits_Q <- \"m3/s\"\n\n# GIS \n#Important naming convention. We assume that GIS-files adhere to the following naming convention:\n#- Basin Shapefile: paste0(gauge_code,\"_basin.shp\")\n#- River Shapefile: paste0(gauge_code,\"_river.shp\")\n#- Junctions Shapefile: paste0(gauge_code,\"_junctions.shp\")\n#- HRU Shapefile: paste0(gauge_code,\"_hru.shp\")\n#- DEM Raster: paste0(gauge_code,\"_dem.tif\")\ngis_path <- \"../caham_data/student_case_study_basins/15149_ChonKemin/GIS/\"\ndem_file <- paste0(gauge_code,'_dem.tif')\ncrs_project <- 4326 #latlon WGS84\n\nThere are a number of parameters to be set before the modeling which can be done as follows.\n\n# Time zone\ntz <-  \"UTC\"\n\n# GCM Climate Models and Simulations/Experiments and data paths\nhist_obs_dir <- \"../../../../../../../../../../../../Documents/ca_large_files/CA_CLIMATE_PROJECTIONS/CHELSA_V21_1979_2018/\" \nhist_sim_dir <- \"../../../../central_asia_domain/climate/hist_sim/\"\nfut_sim_dir <-  \"../../../../central_asia_domain/climate/fut_sim/\"\n\ngcm_Models <- c(\"GFDL-ESM4\", \"IPSL-CM6A-LR\", \"MRI-ESM2-0\", \"UKESM1-0-LL\")\ngcm_Scenarios <- c(\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\")\n\ngcm_Models_Scenarios <- base::expand.grid(gcm_Models,gcm_Scenarios) %>%\n        dplyr::mutate(model_scenario_combination = paste0(Var1,\"_\",Var2)) %>%\n        dplyr::select(model_scenario_combination) %>% unlist() %>% as.character()\n\n# Historical Observations\nhist_obs_start <- 1979\nhist_obs_end <- 2011\nhist_obs_dates <- riversCentralAsia::generateSeqDates(hist_obs_start,hist_obs_end,'day')\nhist_obs_dates <- as_date(hist_obs_dates$date) %>% as_tibble() %>% rename(Date = value)\n\n# Historical GCM Simulations\nhist_sim_start <- hist_obs_start\nhist_sim_end <- hist_obs_end\nhist_sim_dates <- hist_obs_dates\n\n# Future GCM Simulations\nfut_sim_start <- 2012\nfut_sim_end <- 2099\nfut_sim_dates <- riversCentralAsia::generateSeqDates(fut_sim_start,fut_sim_end,'day')\nfut_sim_dates <- as_date(fut_sim_dates$date) %>% as_tibble() %>% rename(Date = value)\n\n# Climate Data Observation Frequency\nobs_freq <- \"day\"\n\n# RSMinverve\nmodel_dir <- \"../../RS_MINERVE/\"\n\n## Dates\n\nhist_sim_dates <- hist_obs_dates\nfut_sim_dates <- riversCentralAsia::generateSeqDates(fut_sim_start,fut_sim_end,'day')\nfut_sim_dates <- as_date(fut_sim_dates$date) %>% as_tibble() %>% rename(Date = value)\n\n\n\n\n\n\n\nIMPORTANT\n\n\n\nNote that you would have to set the data_paths in the above code block according to your own local installation. However, since the CHELSA raster stack files are not available in the Students’ Case Study directory, the code here serves just as a demonstration. It should be further noted that this below is an advanced section that requires a good understanding of the R programming language.\n\n\nAfter setting the parameters, we can generate the hydrological response units. As the adept reader realizes, we are creating the elevation bands in R/RStudio and do not resort to QGIS as has been shown in the previous Geospation Data Section.\n\n# Parameter definition for the generation of the elevation bands\nband_interval <- 300 # in meters. Note that normally you want to work with band intervals of 100 m to 200 m. To make the model less computationally demanding, we work with a coarser resolution of 300 m. \nholeSize_km2 <- .1 # cleaning holes smaller than that size\nsmoothFact <- 2 # level of band smoothing\ndemAggFact <- 2 # dem aggregation factor (carefully fine-tune this)\n## Delineation\nhru_shp <- gen_basinElevationBands(gis_path,dem_file,demAggFact,band_interval,holeSize_km2,smoothFact)\n# Control output\nhru_shp %>% plot()\n\n\n\n\nIn other words, the function gen_basinElevationBands() from the riversCentralAsia R Package creates elevation bands (HRUs) as per the parameter values. In the above example, we are generating elevation bands with a 300 meters [m] bands interval. The smaller this number, the higher the number of elevation bands that will be generated and the higher the computational requirements will be of the hydrological model.\nAs a next step, we intersect the subbasins with elevtion bands. In the example of Chon Kemin, the basin corresponds to the one subbasin.\n\npath2subbasin_shp <- paste0(gis_path,gauge_code,\"_basin.shp\")\nsubbasins_shp <- st_read(path2subbasin_shp, quiet = TRUE)\nsubbasins_hru_shp <- st_intersection(hru_shp,subbasins_shp)\n\nWe can extract and add mean elevation data for each HRU in the following way.\n\ndem <- raster::raster(paste0(gis_path,dem_file))\nzonalStat_Z <- exactextractr::exact_extract(dem, subbasins_hru_shp, 'mean', progress = FALSE)\nsubbasins_hru_shp$Z <- zonalStat_Z\n\nIn a final step, we add unique subbasin names and remove the shapefile fields that are no longer needed in the next steps. The plot shows the resulting shapefile.\n\nfor (idxSubBasin in seq(length(subbasins_shp$name))) {\n  \n  subbasin_sel <- subbasins_hru_shp %>% dplyr::filter(name == subbasins_shp$name[idxSubBasin])\n  subbasin_sel <- subbasin_sel %>% dplyr::arrange(Z)\n  subbasin_sel$hru_num <- (1:base::nrow(subbasin_sel))\n  subbasin_sel$name <- paste0(subbasin_sel$name,'_',subbasin_sel$hru_num)\n  \n  if (idxSubBasin == 1) {\n    res_subbasins <- subbasin_sel\n  } else {\n    res_subbasins <- res_subbasins %>% dplyr::add_row(subbasin_sel)\n  }\n  \n}\n\nsubbasins_hru_shp <- res_subbasins %>% dplyr::select(-layer,-hru_num)\nsubbasins_hru_shp %>% plot()\n\n\n\n\nThe resulting shapefile can then be written to local storage.\n\nsf::st_write(subbasins_hru_shp, paste0(gis_path, gauge_code, '_hru', '.shp'), append = FALSE)\n\nThe geometry of the hydrological modeling approach is now defined and we can start to extract and generate the climate forcing data. First, the historical observations (hist_obs) of temperature and precipitation for each HRU need to be defined. We show how this can be done in a straight forward manner using again helper functions from the riversCentralAsia package.\n\n# Parameters\nclimate_data_type <- \"hist_obs\"\n\n# Load HRU shapefile\nsubbasins_hru_shp <- \n  sf::st_read(paste0(gis_path,gauge_code,'_HRU','.shp'))\n\n# List CHELSA climate files\nclimate_files_tas <- \n  list.files(hist_obs_dir,pattern = \"tas_\",full.names = TRUE)\nclimate_files_pr <-  \n  list.files(hist_obs_dir,pattern = \"pr_\",full.names = TRUE)\n\n# Restrict years range\nn_years <- \n  hist_obs_start:hist_obs_end\nclimate_files_tas <- \n  climate_files_tas[1:length(n_years)]\nclimate_files_pr <- \n  climate_files_pr[1:length(n_years)]\n\n# Temperature data processing\ntemp_or_precip <- \"Temperature\"\nhist_obs_tas <- riversCentralAsia::gen_HRU_Climate_CSV_RSMinerve(climate_files_tas,\n                                              river_name,\n                                              temp_or_precip,\n                                              subbasins_hru_shp,\n                                              hist_obs_start,\n                                              hist_obs_end,\n                                              obs_freq,\n                                              climate_data_type,\n                                              crs_project)\n\n# Precipitation data processing\ntemp_or_precip <- \"Precipitation\"\nhist_obs_pr <- riversCentralAsia::gen_HRU_Climate_CSV_RSMinerve(climate_files_pr,\n                                              river_name,\n                                              temp_or_precip,\n                                              subbasins_hru_shp,\n                                              hist_obs_start,\n                                              hist_obs_end,\n                                              obs_freq,\n                                              climate_data_type,\n                                              crs_project)\n\n# Combine extract climate tibbles.\nhist_obs_rsm <- hist_obs_tas %>% add_column(hist_obs_pr %>% \n                                              dplyr::select(-Station),.name_repair = 'unique')\n\n# Add Discharge Data (monthly)\nq_dec <- riversCentralAsia::loadTabularData(q_path,\n                                              q_name,\n                                              gauge_code,\n                                              gauge_name,\n                                              river_name,\n                                              basin_name,\n                                              data_type_Q,\n                                              units_Q)\nfunc_type_lib <- list(mean = \"Q\")\nq_mon <- aggregate_to_monthly(q_dec,func_type_lib)\n\nq_mon <- q_mon %>% \n  mutate(date = floor_date(as.POSIXct(.$date,tz = tz), unit = \"day\")) \n# Note, the function above outputs dates as date class. \n# This causes problems down the road. \n# We address these by converting the date values to dttm class.\n\nq_mon <- q_mon %>% dplyr::select(date, data) %>% \n  dplyr::filter(date >= ymd(paste0(hist_obs_start, \"-01-01\"))) %>% \n  dplyr::filter(date <= ymd(paste0(hist_obs_end, \"-12-31\")))\n\ndates_char_Q <- \n  riversCentralAsia::posixct2rsminerveChar(q_mon$date, tz = \"UTC\") %>% \n  rename(Station = value) %>% \n  tibble::add_column(Q = (q_mon$data %>% as.character))\n\n# Get gauge location and elevation\ngauge_shp <- sf::st_read(paste0(gis_path,gauge_code,\"_gauge.shp\"))\ndem <- raster::raster(paste0(gis_path,dem_file))\ngauge_coord <- gauge_shp %>% sf::st_coordinates()\ngauge_Z <- exactextractr::exact_extract(dem,gauge_shp,'mean')\n\n# Combine everything\nhist_obs_rsm <- dplyr::full_join(hist_obs_rsm,dates_char_Q, by = 'Station') \n\n# now finish off by giving the required attributes in the table for the discharge station\nhist_obs_rsm$Q[1] = data_type_Q\nhist_obs_rsm$Q[2] = gauge_coord[1]\nhist_obs_rsm$Q[3] = gauge_coord[2]\nhist_obs_rsm$Q[4] = 550\nhist_obs_rsm$Q[5] = data_type_Q\nhist_obs_rsm$Q[6] = 'Flow'\nhist_obs_rsm$Q[7] = units_Q\nhist_obs_rsm$Q[8] = 'Constant after'\n\n# Write final file to disk\nreadr::write_csv(hist_obs_rsm,paste0(model_dir,climate_data_type,\"_rsm.csv\"),na = \"NA\",col_names = FALSE)\n\nThe last line of code writes the result back to the disk. Please check in your Case Study Pack the folder /RS_MINERVE/ where the corresponding hist_obs_rsm.csv is stored. The file format corresponds to import requirements from the side of RSMinerve. You can open the text file either with a Spreadsheet program such as Excel or with a text editor. The daily time series of temperature, precipitation for each HRU and the discharge for the gauging station are stored in the columns with a header section. Since observed discharge data is only available on a monthly basis, the time series is filled with NA where there are no obervations available.\nFor the monthly mean discharge values, the following convention is adhered to. Monthly mean discharge values are written/stored in the first day of the month.\nNext, we can process the historical GCM runs in a similar fashion.\n\n# Parameters\noutput_file_dir <- \"../../RS_MINERVE/\"\nclimate_data_type <- \"hist_sim\"\nclimate_files_tas <- list.files(hist_sim_dir,pattern = \"tas_\",full.names = TRUE)\nclimate_files_pr <- list.files(hist_sim_dir,pattern = \"pr_\",full.names = TRUE)\n\n# Extract GCM Model-Specfic Data\nhist_sim_rsm <- vector(mode = \"list\", length = length(gcm_Models))\nnames(hist_sim_rsm) <- gcm_Models\n\nfor (idxGCM in seq(length(gcm_Models))) {\n  \n  temp_or_precip <- \"Temperature\"\n  gcm_model <- gcm_Models[idxGCM]\n  hist_sim_T <- riversCentralAsia::gen_HRU_Climate_CSV_RSMinerve(climate_files_tas[idxGCM],\n                                              river_name,\n                                              temp_or_precip,\n                                              subbasins_hru_shp,\n                                              hist_sim_start,\n                                              hist_sim_end,\n                                              obs_freq,\n                                              climate_data_type,\n                                              crs_project)\n \n  temp_or_precip <- \"Precipitation\"\n  hist_sim_P <- riversCentralAsia::gen_HRU_Climate_CSV_RSMinerve(climate_files_pr[idxGCM],\n                                              river_name,\n                                              temp_or_precip,\n                                              subbasins_hru_shp,\n                                              hist_sim_start,\n                                              hist_sim_end,\n                                              obs_freq,\n                                              climate_data_type,\n                                              crs_project)\n  \n  hist_sim_rsm[[idxGCM]] <- hist_sim_T %>% \n    tibble::add_column(hist_sim_P %>% dplyr::select(-Station),.name_repair = 'unique')\n  write_csv(hist_sim_rsm[[idxGCM]],\n            paste0(model_dir,climate_data_type,\"_\",gcm_model,\"_\",\n                   gauge_code,\"_\",hist_sim_start,\"_\",hist_sim_end,\".csv\"),\n            col_names = FALSE)\n  \n}\n\nThe same applies to the future climate scenario runs.\n\nclimate_data_type <- \"fut_sim\"\n\n# Load HRU shapefile\nsubbasins_hru_shp <- sf::st_read(paste0(gis_path,gauge_code,'_HRU','.shp'))\n\n# Process and Extract GCM Model-Specific Data\nfut_sim_rsm <- base::vector(mode = \"list\", length = length(gcm_Models_Scenarios))\nnames(fut_sim_rsm) <- gcm_Models_Scenarios # now we have a named list\n\ndebug_T <- fut_sim_rsm\n\nfor (idxGCM in seq(length(gcm_Models_Scenarios))) {\n\n  # GCM Model and Scenario\n  str2proc <- gcm_Models_Scenarios[idxGCM]\n  gcm_model <- substr(str2proc, 1, nchar(str2proc) - 7)\n  gcm_scenario <- substr(str2proc, nchar(str2proc) - 6 + 1, nchar(str2proc))\n\n  # Process files - tas\n  temp_or_precip <- \"Temperature\"\n  climate_file_tas <- \n    list.files(fut_sim_dir,pattern = paste0(\"tas_day_\",gcm_Models_Scenarios[idxGCM]),full.names = TRUE)\n  fut_sim_T <- gen_HRU_Climate_CSV_RSMinerve(climate_file_tas,\n                                             river_name,\n                                             temp_or_precip,\n                                             subbasins_hru_shp,\n                                             fut_sim_start,\n                                             fut_sim_end,\n                                             obs_freq,\n                                             climate_data_type,\n                                             crs_project)\n  \n  # Process files - pr\n  temp_or_precip <- \"Precipitation\"\n  climate_file_pr <- \n    list.files(fut_sim_dir,pattern = paste0(\"pr_day_\",gcm_Models_Scenarios[idxGCM]),full.names = TRUE)\n  fut_sim_P <- gen_HRU_Climate_CSV_RSMinerve(climate_file_pr,\n                                             river_name,\n                                             temp_or_precip,\n                                             subbasins_hru_shp,\n                                             fut_sim_start,\n                                             fut_sim_end,\n                                             obs_freq,\n                                             climate_data_type,\n                                             crs_project)\n  \n  # Final dataframe\n  fut_sim_rsm[[idxGCM]] <- fut_sim_T %>% \n    tibble::add_column(fut_sim_P %>% dplyr::select(-Station),.name_repair = 'unique')\n  # Write result to disk\n  readr::write_csv(fut_sim_rsm[[idxGCM]],\n                   paste0(model_dir,climate_data_type,\"_\",gcm_model,\"_\",\n                          gcm_scenario,\"_\",river_name,\"_\",fut_sim_start,\"_\",\n                          fut_sim_end,\".csv\"),\n                   col_names = FALSE)\n}\n\nIn a final step, we can produce the bias corrected future climate scenarios with the following code.\n\n# Preparations\n## HRUs\nsubbasins_hru_shp <- \n  sf::st_read(paste0(gis_path,gauge_code,'_hru','.shp'))\nn_hru <- subbasins_hru_shp %>% nrow()\nhru_names <- subbasins_hru_shp$name\n\n# ================\n# Prepare hist_obs\n# ================\nclimate_data_type <- \"hist_obs\"\nhist_obs_path <- paste0(model_dir,climate_data_type,\"_rsm.csv\")\nhist_obs_orig <- hist_obs_path %>% \n  readr::read_csv(col_types = cols(.default = col_character())) %>%\n  dplyr::select(-Station,-Q)\n\n# Extract data by groups and convert T to deg. K\nhist_obs_T <- hist_obs_orig[,1:n_hru] %>% slice(-1:-7) %>% \n  type_convert() %>% \n  mutate(across(.cols = everything(), ~ . + 273.15))\nhist_obs_P <- \n  hist_obs_orig[, (n_hru + 1):(2 * n_hru)] %>% \n  slice(-1:-7) %>%  \n  type_convert()\n\n# Fix row names\nnames(hist_obs_T) <- hru_names\nnames(hist_obs_P) <- hru_names\n\nhist_obs_T_df <- hist_obs_T %>% as.data.frame()\nrow.names(hist_obs_T_df) <- hist_obs_dates$Date %>% as.character()\nhist_obs_P_df <- hist_obs_P %>% as.data.frame()\nrow.names(hist_obs_P_df) <- hist_obs_dates$Date %>% as.character()\n\n# ================\n# Prepare hist_sim\n# ================\nhist_sim_T_list <- \n  base::vector(mode = \"list\", length = length(gcm_Models))\nhist_sim_P_list <- \n  base::vector(mode = \"list\", length = length(gcm_Models))\nnames(hist_sim_T_list) <- \n  gcm_Models # now we have a named list\nnames(hist_sim_P_list) <- \n  gcm_Models # now we have a named list\n\nfor (idxGCM in 1:length(gcm_Models)) {\n  hist_sim_path <- \n    list.files(model_dir,\n               pattern = paste0(\"hist_sim_\",gcm_Models[idxGCM]),full.names = TRUE)\n  hist_sim_orig <- hist_sim_path %>% \n    readr::read_csv(col_types = cols(.default = col_character())) %>% \n    dplyr::select(-Station)\n  \n  hist_sim_T <- hist_sim_orig[,1:n_hru] %>% slice(-1:-7) %>% \n    type_convert() %>% \n    mutate(across(.cols = everything(), ~ . + 273.15))\n  hist_sim_P <- hist_sim_orig[,(n_hru+1):(2*n_hru)] %>% \n    slice(-1:-7) %>% \n    type.convert()\n  \n  # Fix row names\n  names(hist_sim_T) <- hru_names\n  names(hist_sim_P) <- hru_names\n  \n  hist_sim_T_df <- hist_sim_T %>% as.data.frame()\n  row.names(hist_sim_T_df) <- hist_sim_dates$Date %>% as.character()\n  hist_sim_P_df <- hist_sim_P %>% as.data.frame()\n  row.names(hist_sim_P_df) <- hist_sim_dates$Date %>% as.character()\n  \n  hist_sim_T_list[[idxGCM]] <- hist_sim_T_df\n  hist_sim_P_list[[idxGCM]] <- hist_sim_P_df\n}\n\n# ===============\n# Prepare fut_sim\n# ===============\nfut_sim_T_list <- base::vector(mode = \"list\", length = length(gcm_Models_Scenarios))\nfut_sim_P_list <- base::vector(mode = \"list\", length = length(gcm_Models_Scenarios))\nnames(fut_sim_T_list) <- gcm_Models_Scenarios # now we have a named list\nnames(fut_sim_P_list) <- gcm_Models_Scenarios\nfut_sim_T_list_bcsd <- fut_sim_T_list\nfut_sim_P_list_bcsc <- fut_sim_P_list\n\nfor (idxGCM in 1:length(gcm_Models_Scenarios)) {\n  fut_sim_orig_path <- \n    list.files(model_dir,pattern = \n                 paste0(\"fut_sim_\",gcm_Models_Scenarios[idxGCM]),full.names = TRUE)\n  fut_sim_orig <- fut_sim_orig_path %>% \n    readr::read_csv(col_types = cols(.default = col_character())) %>% \n    dplyr::select(-Station)\n  \n  fut_sim_T <- fut_sim_orig[,1:n_hru] %>% \n    slice(-1:-7) %>% \n    type_convert() %>% \n    mutate(across(.cols = everything(), ~ . + 273.15))\n  fut_sim_P <- fut_sim_orig[,(n_hru+1):(2*n_hru)] %>% \n    slice(-1:-7) %>% \n    type.convert()\n  \n  # Fix row names\n  names(fut_sim_T) <- hru_names\n  names(fut_sim_P) <- hru_names\n  \n  fut_sim_T_df <- fut_sim_T %>% as.data.frame()\n  row.names(fut_sim_T_df) <- fut_sim_dates$Date %>% as.character()\n  fut_sim_P_df <- fut_sim_P %>% as.data.frame()\n  row.names(fut_sim_P_df) <- fut_sim_dates$Date %>% as.character()\n  \n  fut_sim_T_list[[idxGCM]] <- fut_sim_T_df\n  fut_sim_P_list[[idxGCM]] <- fut_sim_P_df\n}\n\n# ===================\n# Do quantile mapping\n# ===================\n\n# --- Debugging\nfut_sim_rsm_qmapped <- \n  base::vector(mode = \"list\", length = length(gcm_Models_Scenarios))\nnames(fut_sim_rsm_qmapped) <- gcm_Models_Scenarios # now we have a named list\n# -----\n\nfor (idxGCM in 1:length(gcm_Models_Scenarios)) {\n  \n  # Preparation\n  str2proc <- gcm_Models_Scenarios[idxGCM]\n  gcm_model <- substr(str2proc, 1, nchar(str2proc) - 7)\n  gcm_scenario <- substr(str2proc, nchar(str2proc) - 6 + 1, nchar(str2proc))\n  \n  # Bias correction\n  hist_sim_T_df_gcmModel <- hist_sim_T_list[[gcm_model]]\n  hist_sim_P_df_gcmModel <- hist_sim_P_list[[gcm_model]]\n  \n  fut_sim_T_df_gcmModel <- fut_sim_T_list[[idxGCM]]\n  fut_sim_P_df_gcmModel <- fut_sim_P_list[[idxGCM]]\n  \n  qmap_param_T_gcm <- fitQmap(hist_obs_T_df, hist_sim_T_df_gcmModel, method = \"QUANT\")\n  qmap_param_P_gcm <- fitQmap(hist_obs_P_df, hist_sim_P_df_gcmModel, method = \"QUANT\")\n  \n  # T bias correction\n  fut_sim_T_df_gcmModel_qmapped <- \n    doQmap(fut_sim_T_df_gcmModel,qmap_param_T_gcm)\n  # Fill 0s where present. Occasionally, there are 0s resulting from the quantile mapping these \n  # are ironed out here by filling the missing values using the ones from the preceeding observation. \n  fut_sim_T_df_gcmModel_qmapped[fut_sim_T_df_gcmModel_qmapped == 0] <- NA\n  fut_sim_T_df_gcmModel_qmapped <- \n    zoo::na.locf(fut_sim_T_df_gcmModel_qmapped)\n  # P bias correction\n  fut_sim_P_df_gcmModel_qmapped <- \n    doQmap(fut_sim_P_df_gcmModel,qmap_param_P_gcm)\n\n  # go back to tibble and convert back to deg. C\n  fut_sim_T_gcmModel_qmapped <- \n    fut_sim_T_df_gcmModel_qmapped %>% as_tibble() %>% \n    add_column(Date = fut_sim_dates$Date,.before = 1)\n  fut_sim_T_gcmModel_qmapped <- \n    fut_sim_T_gcmModel_qmapped %>% mutate(across(-Date, ~ . - 273.15))\n  fut_sim_P_gcmModel_qmapped <- \n    fut_sim_P_df_gcmModel_qmapped %>% as_tibble() %>% \n    add_column(Date = fut_sim_dates$Date,.before = 1)  \n  \n  fut_sim_T_list[[idxGCM]] <- fut_sim_T_gcmModel_qmapped\n  fut_sim_P_list[[idxGCM]] <- fut_sim_P_gcmModel_qmapped\n  \n  # Export to .csv-file\n  fut_sim_qmapped <- \n    fut_sim_T_gcmModel_qmapped %>% #dplyr::select(-Date) %>% \n    tibble::add_column(fut_sim_P_gcmModel_qmapped %>% \n                         dplyr::select(-Date),.name_repair = \"universal\") %>% \n    mutate(across(.cols = everything(),~ as.character(.))) %>% \n    dplyr::select(-Date)\n  \n  # --- Debugging\n  fut_sim_rsm_qmapped[[idxGCM]] <- fut_sim_qmapped\n  # ---\n  \n  fut_sim_orig_path <- \n    list.files(model_dir,pattern = paste0(\"fut_sim_\",gcm_Models_Scenarios[idxGCM]),\n               full.names = TRUE)\n  fut_sim_orig <- \n    fut_sim_orig_path %>% readr::read_csv(col_types = cols(.default = col_character())) \n  \n  fut_sim_orig_header <- fut_sim_orig %>% dplyr::select(-Station) %>% \n    dplyr::slice(1:7,) \n  fut_sim_qmapped <- fut_sim_orig_header  %>% bind_rows(fut_sim_qmapped) %>% \n    add_column(Station = fut_sim_orig$Station,.before = 1)\n\n  # Write result to disk\n  climate_data_type <- \"fut_sim_bcsd\"\n  readr::write_csv(fut_sim_qmapped,\n                   paste0(model_dir,climate_data_type,\"_\",gcm_model,\"_\",\n                          gcm_scenario,\"_\",river_name,\"_\",fut_sim_start,\"_\",\n                          fut_sim_end,\".csv\"),\n                   col_names = FALSE)\n}"
  },
  {
    "objectID": "climate_data.html#sec-climate-data-references",
    "href": "climate_data.html#sec-climate-data-references",
    "title": "",
    "section": "References",
    "text": "References\n\n\n\n\nBuisán, Samuel T., Michael E. Earle, José Luı́s Collado, John Kochendorfer, Javier Alastrué, Mareile Wolff, Craig D. Smith, and Juan I. López-Moreno. 2017. “Assessment of Snowfall Accumulation Underestimation by Tipping Bucket Gauges in the Spanish Operational Network.” Atmospheric Measurement Techniques 10 (3): 1079–91.\n\n\n“CHELSA-W5e5 V1.0: W5e5 V1.0 Downscaled with CHELSA V2.0.” 2022. ISIMIP Repository. https://doi.org/10.48364/ISIMIP.836809.2.\n\n\nFeigenwinter, I., S. Kotlarski, A. Casanueva, A. M. Fischer, C. Schwierz, and M. A. Liniger. 2018. “Exploring Quantile Mapping as a Tool to Produce User-Tailored Climate Scenarios for Switzerland.” Technical Report 270. Federal Office of Meteorology; Climatology MeteoSwiss.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nKarger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-resolution monthly precipitation and temperature time series from 2006 to 2100.” Scientific Data 7 (1): 248. https://doi.org/10.1038/s41597-020-00587-y.\n\n\nKarger, Dirk Nikolaus, Adam M. Wilson, Colin Mahony, Niklaus E. Zimmermann, and Walter Jetz. 2021. “Global daily 1 km land surface precipitation based on cloud cover-informed downscaling.” Scientific Data 8 (1): 307. https://doi.org/10.1038/s41597-021-01084-6.\n\n\nKotlarski, S., K. Keuler, O. B. Christensen, A. Colette, M. Déqué, A. Gobiet, K. Goergen, et al. 2014. “Regional climate modeling on European scales: a joint standard evaluation of the EURO-CORDEX RCM ensemble.” Geoscientific Model Development 7 (4): 1297–1333. https://doi.org/10.5194/gmd-7-1297-2014.\n\n\nO’Neill, Brian C., Claudia Tebaldi, Detlef P. van Vuuren, Veronika Eyring, Pierre Friedlingstein, George Hurtt, Reto Knutti, et al. 2016. “The Scenario Model Intercomparison Project (ScenarioMIP) for CMIP6.” Geoscientific Model Development 9 (9): 3461–82. https://doi.org/10.5194/gmd-9-3461-2016.\n\n\nRiahi, Keywan, Detlef P. van Vuuren, Elmar Kriegler, Jae Edmonds, Brian C. O’Neill, Shinichiro Fujimori, Nico Bauer, et al. 2017. “The Shared Socioeconomic Pathways and their energy, land use, and greenhouse gas emissions implications: An overview.” Global Environmental Change 42: 153–68. https://doi.org/10.1016/j.gloenvcha.2016.05.009."
  },
  {
    "objectID": "appendix_b_riverscentralasia_r_toolbox.html",
    "href": "appendix_b_riverscentralasia_r_toolbox.html",
    "title": "",
    "section": "",
    "text": "R Toolbox riversCentralAsia\nMore information can be found on Github, where the package is maintained."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-register-with-earth-explorer",
    "href": "appendix_c_quick_guides.html#sec-register-with-earth-explorer",
    "title": "",
    "section": "Earth Explorer: Register for an Account",
    "text": "Earth Explorer: Register for an Account\nIn your internet browser, navigate to https://earthexplorer.usgs.gov/. The window will look similar as in Figure 1.\n\n\n\nFigure 1: Start window of the Earth Explorer interface.\n\n\nClick on Login in the top right corner. This will bring you to a new window where you can click on Create New Account which will open a form where you enter your information. After verifying your account by clicking on the appropriate link that you will be sent, you can download data from the Earth Explorer interface.\n\nEE: Download SRTM Data for a Selected Region\n\nLogin to the Earth Explorer (Register if you haven’t done so before How to).\nNavigate to your area of interest in the map panel of the Earth Explorer interface.\nDraw a polygon around your area of interest by clicking on the map (see Figure 2).\nIn the Data Set tab, look for the SRTM 1 arc-second global DEM (see Figure 3) and select it by ticking the box next to the product name in the list.\n\n\n\n\n\nFigure 2: Define a polygon around your area of interest by clicking on the map.\n\n\n\n\n\nFigure 3: Search for the SRTM 1 arc-second global DEM product.\n\n\n\nVerify that the result layer(s) cover your area of interest by pressing the foot icon in the Results tab and download if you are satisfied (Figure 4).\n\n\n\n\nFigure 4: Verify the results of your search and download the data products you need.\n\n\nBack to the Load DEM section."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-sofware-qgis-installation-guide",
    "href": "appendix_c_quick_guides.html#sec-sofware-qgis-installation-guide",
    "title": "",
    "section": "QGIS Installation Guide",
    "text": "QGIS Installation Guide\nIn your web browser go to https://qgis.org/en/site/forusers/download.html (see Figure 5).\n\n\n\nFigure 5: The QGIS download site.\n\n\nChoose the long-term support version of QGIS for your operating system (e.g. if you use a laptop with a 64-bit Windows operating system, open the Download for Windows tab and click on QGIS Standalone Installer Version 3.16 (64 bit), see Figure 6)). Clicking on the installer will start the download.\n\n\n\nFigure 6: The QGIS installer if you work on a 64-bit Windows operating system.\n\n\nDouble-click on the downloaded file to start the installation. Typically, SAGA GIS and GRASS GIS are installed alongside QGIS if you choose this installing option."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-qgis-window-overview",
    "href": "appendix_c_quick_guides.html#sec-qgis-window-overview",
    "title": "",
    "section": "The QGIS Window - Overview",
    "text": "The QGIS Window - Overview\nThe main parts of the QGIS window which are referenced in this tutorial, are highlighted in Figure 7.\n\n\n\nFigure 7: The QGIS window."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-saving-a-new-qgis-project",
    "href": "appendix_c_quick_guides.html#sec-saving-a-new-qgis-project",
    "title": "",
    "section": "QGIS: Saving a New QGIS Project",
    "text": "QGIS: Saving a New QGIS Project\nOpen your QGIS and open a new QGIS project by moving your cursor on the white sheet symbol in the upper-left corner of your QGIS window (see Figure 8).\n\n\n\nFigure 8: Tutorial project open in QGIS LTS.\n\n\nSave the QGIS project by pressing on the disk icon and selecting a location for the file on your computer and a name for the project. You can add freely available on-line maps as background.\nBack to setting up of QGIS."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-change-project-projection-qgis",
    "href": "appendix_c_quick_guides.html#sec-change-project-projection-qgis",
    "title": "",
    "section": "QGIS: Change the Projection of the QGIS Project",
    "text": "QGIS: Change the Projection of the QGIS Project\nFor this tutorial, the projection of the QGIS project should be in EPSG:32642 (i.e., UTM 42 N). Change it by clicking on the projects projection in the lower right corner of the QGIS window (see Figure 7). In the coordinate reference system (CRS) tab of the Project Properties, select WGS84 / UTM zone 42N with ID EPSG:32642 and click OK.\nFor modeling your own sample catchment, a different UTM zone may be applicable. The map in the lower right corner of the Project Properties window shows the area for the projection in red and the area where your data is located in violet so you can visually verify that you choose an appropriate projection.\nGenerally, you want to choose the CRS so that you have minimal distortion through the projection in your area of interest. The CRS with ID EPSG:32462 suits well for all of the student exercise catchments.\nBack to the setting up of QGIS."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-install-and-activate-plugins-in-QGIS3",
    "href": "appendix_c_quick_guides.html#sec-install-and-activate-plugins-in-QGIS3",
    "title": "",
    "section": "QGIS: Install and activate plugins in QGIS3",
    "text": "QGIS: Install and activate plugins in QGIS3\nNavigate to Plugins in the header toolbar and go to Manage and Install Plugins …. Search for the plugin name and go to Install Plugin to install a plugin or tick the box to the left of the plugin name in the list of plugins to activate it (see Figure 9).\n\n\n\nFigure 9: Plugin management window in QGIS 3.16. Install plugin with the Install Plugin button. Activate installed plugins by ticking the box in the list.\n\n\nBack to the setting up of QGIS."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-managing-panels",
    "href": "appendix_c_quick_guides.html#sec-managing-panels",
    "title": "",
    "section": "GQIS: Managing Panels Visibility in QGIS",
    "text": "GQIS: Managing Panels Visibility in QGIS\nShould one of the panels described here not be visible in your QGIS window navigate to View in your header toolbar and then to Panels. The visible panels are marked with a tick. Click on a panel name in the list to activate or deactivate it.\nBack to setting up of QGIS."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-loading-public-background-layers",
    "href": "appendix_c_quick_guides.html#sec-loading-public-background-layers",
    "title": "",
    "section": "QGIS: Loading Public Background Maps",
    "text": "QGIS: Loading Public Background Maps\nYou can add freely available on-line maps as backgrounds. Note that they will only be available as long as your computer is connected to the internet. In the Browser panel (see What to do if you don’t see the Browser panel), move the cursor to XYZ Tiles, do a right-click and select New Connection. Enter a descriptive name for the map layer and one of the links below and click OK. The new map layer will appear under XYZ Tiles. By double-clicking on the layer you can add it to your Layers panel.\nBack to setting up of QGIS."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-zoom-to-layer",
    "href": "appendix_c_quick_guides.html#sec-zoom-to-layer",
    "title": "",
    "section": "QGIS: Zoom to Layer",
    "text": "QGIS: Zoom to Layer\nYou can tell QGIS to zoom to the selected layer by selecting a layer in the Layers window and then on the white sheet and magnifying glass icon in the toolbar (see Figure 10).\n\n\n\nFigure 10: Zoom to layer.\n\n\nYou can also perform a right-click on the layer name in the Layers panel and select Zoom to layer."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-srtm-plugin",
    "href": "appendix_c_quick_guides.html#sec-srtm-plugin",
    "title": "",
    "section": "QGIS: Import SRTM layers using the SRTM plugin",
    "text": "QGIS: Import SRTM layers using the SRTM plugin\nMake sure the SRTM plugin is installed (see How to). Navigate to Plugins in the header toolbar and there to SRTM Downloader. Select the SRTM Downloader. Set the boundaries of the SRTM tiles to download and press the Download button. Close the window when done. The SRTM tiles are loaded to the Layers pane.\nBack to Load DEM in QGIS section"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-merge-srtm-tiles",
    "href": "appendix_c_quick_guides.html#sec-merge-srtm-tiles",
    "title": "",
    "section": "QGIS: Merge SRTM Tiles to a Single Layer",
    "text": "QGIS: Merge SRTM Tiles to a Single Layer\nNavigate to Raster in the header toolbar and then to Miscellaneous and Merge…. In the Merge window that opens, select the input layers by clicking on the … button. Tick the layers that need to be merged and press Run. When the algorithm is done, close the window. You can zoom to the extent of the new layer (see How to). You can change the color of the DEM file.\nBack to Load DEM in QGIS section"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-load-dem",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-load-dem",
    "title": "",
    "section": "QGIS: Add Raster Layer",
    "text": "QGIS: Add Raster Layer\nIn your QGIS window, navigate to Layer in your header toolbar, there to Add Layer and left-click on Add Raster Layer (see Figure 11). The Data Source Manager will open in a pop-up window (see Figure 12).\n\n\n\nFigure 11: Add raster layer to QGIS project, step 1.: Navigate to the Data Source Manager.\n\n\n\n\n\nFigure 12: Add raster layer to QGIS project, step 2.: Browse for the raster file to add to the QGIS project by pressing on the box with the three dots (…) to the right of the raster source input field.\n\n\nIn the example above, a DEM for the example of the Nauvalisoy river catchment is loaded. For loading the DEM of your sample catchments, browse for the DEM in the corresponding folder you downloaded from the provided Dropbox directory.\nPress the Add button at the bottom right of the Data Source Manager window to load the raster layer to your QGIS project and close the window by pressing the Close button (to the left of the Add button).\nYour QGIS project now shows a grey-scale version of the raster layer you loaded. If you are satisfied with the change, save your project by pressing the disk icon at the upper left corner of your QGIS window.\nYou can change the color of your raster file (how to). Here is a quick-guide of how to apply a topography-style color band to your DEM."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-change-color-of-raster-layer",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-change-color-of-raster-layer",
    "title": "",
    "section": "QGIS: Change Color of Raster Layer",
    "text": "QGIS: Change Color of Raster Layer\nYou can change the colors of your cut DEM by double-clicking on the layer name in the Layers window. Go to tab Symbology (with the paint and brush icon) and choose Render type Singleband-pseudocolor (see Figure 13) and select a Color ramp.\n\n\n\nFigure 13: Nicely color your DEM, step 1.\n\n\nQGIS comes with a large library of color ramps but you can also create your own. See below a description of how to get your DEM in a topography style color palette."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-topograpy-color-ramp",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-topograpy-color-ramp",
    "title": "",
    "section": "QGIS: Topography-style color palettes",
    "text": "QGIS: Topography-style color palettes\nFor our DEM we choose an existing topography-style color ramp. Open the Layer Properties window with a double-click on the raster layer in the Layers pane of the QGIS window. In the Symbology tab click the triangle to the right of the color ramp and select Create New Color Ramp… (see Figure 14).\n\n\n\nFigure 14: Nicely color your DEM, step 2.\n\n\nIn the drop down menu of the pop-up window choose Catalog: cpt-city and press OK (see Figure 15).\n\n\n\nFigure 15: Nicely color your DEM, step 3.\n\n\nA this will open another window containing the catalog of existing color ramps. Under Topography, we choose sd-a and press OK (see Figure 16).\n\n\n\nFigure 16: Nicely color your DEM, step 4.\n\n\nGo to tab Transparency, set Global Opacity to 30% and specify the No Data Value 0 (see Figure 17).\n\n\n\nFigure 17: Nicely color your DEM, step 5.\n\n\nThen go back to the Symbology tab. The minimum value of the color ramp should now not be 0 but 272. Adapt manually if need be. Then, press classify to get a discrete color ramp for your map and Apply to the map. If you are happy with the colors, quit by pressing OK (see Figure 18).\n\n\n\nFigure 18: Nicely color your DEM, step 6.\n\n\nThe result will look like Figure 19.\n\n\n\nFigure 19: Nicely color your DEM, step 7.\n\n\nYou can add decorations (e.g. scale and north arrow) to your map (how to)."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-add-map-decorations",
    "href": "appendix_c_quick_guides.html#sec-add-map-decorations",
    "title": "",
    "section": "QGIS: Add Map Decorations",
    "text": "QGIS: Add Map Decorations\nNavigate to View -> Decorations and choose among the decorations to add. Many options for configuring the decorations are available."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-verify-projection-reproject",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-verify-projection-reproject",
    "title": "",
    "section": "QGIS: Verify Projection of Layer and Re-project Layer",
    "text": "QGIS: Verify Projection of Layer and Re-project Layer\nOpen the Layer Properties window with a double-click on the layer in the Layers pane and navigate to the Information tab. Under CRS (coordinate reference system) you see the projection of the layer. For the Chirchiq river basin, the CRS should say “EPSG:32642 - WGS 84 / UTM zone 42N - Projected”. Other river catchments in Central Asia may require a different UTM zone. For the student exercises, it is good to choose this projection for all basins.\nA raster layer can be reprojected to a different CRS: Go to Raster in the header toolbar. From there move the cursor over Projections and click on Warp (Reproject)…. The Warp window will pop up (in fact, it is just an interface where the user can specify the parameters of the wrap algorithm in a convient way). Select the raster layer you wish to re-project in the Input layer and select the Target CRS.\nIf the target CRS you wish to re-project to is not available in the drop-down menu, you can browse for it by clicking on the globe icon to thr right of the Target CRS section. You may re-sample the raster to a coarser resolution by specifying the Output file resolution. You may also specify to save the reprojected raster layer: Scroll to the bottom of the Warp (Reproject) window where you see Reprojected and a white box where you can browse for a location to store the new layer.\n\n\n\n\n\n\nWARNING\n\n\n\nIf you do not specify a target location, only a temporary layer will be loaded to QGIS which will not be available anymore after you close the QGIS project (even if you save the project).\n\n\nYou may decide to load the temporary file and save it later (how to). Click the Run button at the bottom right to start the re-projection algorithm and press Close when the process is done. The reprojected layer will be available in the Layers pane.\nA vector layer can be reprojected by selecting Vector in the header toolbar, moving the cursor to Data Management Tools and clicking on Reproject Layer…. This opens the Reproject Layer window where you can specify a Target CRS and optionally a storage location. As for the reprojection of the raster layer, a temporary layer is loaded to your QGIS project if you do not specify a storage location. However, you can always store temporary layers later (how to).\nBack to the load DEM section."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guide-save-temp-layer",
    "href": "appendix_c_quick_guides.html#sec-quick-guide-save-temp-layer",
    "title": "",
    "section": "QGIS: Save a Temporary Layer",
    "text": "QGIS: Save a Temporary Layer\nRight-click on a temporary layer in the Layers pane. Temporary layers are indicated by a box to the right of the layer name. From the menu that opens upon right-click, select Export and Save As… (for raster layers) or Save Feature As… (for vector layers). An explorer window will open where you can specify a file name and a location to store the file to."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-add-vector-layer-to-qgis",
    "href": "appendix_c_quick_guides.html#sec-add-vector-layer-to-qgis",
    "title": "",
    "section": "QGIS: Add Vector Layer",
    "text": "QGIS: Add Vector Layer\nLeft-clicking on Layer, moving your cursor over Add Layer and left-clicking Add Vector Layer (see Figure 20).\n\n\n\nFigure 20: Add vector layer to QGIS project, step 1.: Navigate to the Data Source Manager.\n\n\nA window will pop up, asking you to specify the properties of the vector layer to add (see Figure 21).\n\n\n\nFigure 21: Add vector layer to QGIS project, step 2: Select vector layers to add. Note that you select the shp file but cpg, dbf and shx need to be present in the same location.\n\n\nPress the box with the three dots on the right of the source field to specify the location of the shape file to be added to your project (see Figure 22). Click Open and the window will close. The address of your shape files should now stand in the source field as in Figure 21.\n\n\n\nFigure 22: Add vector layer to QGIS project, step 3.\n\n\nNote that you load the .shp file but that all the files in the list in Figure 21 need to be present. In the Add Vector Layer window, click Add and then close the window. QGIS has attributed a random color to your shape file which can be changed manually How to."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-change-color-of-vector-layer",
    "href": "appendix_c_quick_guides.html#sec-change-color-of-vector-layer",
    "title": "",
    "section": "QGIS: Change Color of a Vector Layer",
    "text": "QGIS: Change Color of a Vector Layer\nYou can change the color of your layer by double-clicking on the layer name in the Layers Window to the left of the map. This will open the properties window. The third tab from the top shows paint and brush (see Figure 23).\n\n\n\nFigure 23: dd vector layer to QGIS project, step 5. Change the color of the layer.\n\n\nYou can activate Simple fill by clicking on it and select No brush in the drop-down menu in order to only show the outline of your vector layer (see Figure 24).\n\n\n\nFigure 24: Add vector layer to QGIS project, step 6. Only show the outline of the vector layer."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-fill-sinks",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-fill-sinks",
    "title": "",
    "section": "QGIS: Fill Sinks",
    "text": "QGIS: Fill Sinks\nBrowse for the Fill sinks algorithm in the Processing Toolbox panel (see Figure 25). If the Processing Toolbox panel is not visible go to Processing in the header toolbar and click on Toolbox to activate it. Alternatively, here is how to manage the visibility of panels in QGIS.\n\n\n\nFigure 25: Search for the Fill sinks algorithm in the Processing Toolbar panel.\n\n\nOpen the Fill sinks window with a double-click on the name of the algorithm in the Processing Toolbar panel. There, select the DEM you want to process and browse for a location to store the output file (see Figure 26). You can leave the default minimum slope.\n\n\n\nFigure 26: Select the raster file to process.\n\n\nWhen the algorithm is done it will load the new layer into your QGIS project. Close the Fill sinks window and save your project.\nBack to catchment delineation."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guide-upslope-area",
    "href": "appendix_c_quick_guides.html#sec-quick-guide-upslope-area",
    "title": "",
    "section": "QGIS: Calculate the area upslope of a point",
    "text": "QGIS: Calculate the area upslope of a point\nSearch for the SAGA algorithm Upslope Area in the Processing Toolbox panel and open the function window with a double click on the name. Enter the Longitude of your discharge station for the Target X coordinate and the Latitude for the Target Y coordinate for which the upslope area should be calculated. For the Elevation select the sink-filled DEM (see how to fill sinks in a DEM and why we need to fill in sinks).\nAll GIS layers and the coordinates of the discharge gauge need to be in the same UTM projection. Choose a method in the drop-down menue in the Method section and optionally specify a location for the output file. For the example of the Nauvalisoy basin, the Upslope Area window filled in correctly is shown in Figure 27.\n\n\n\nFigure 27: The Upslope Area window for the determination of the catchment area of the Nauvalisoy river basin.\n\n\nClick Run and Close after the algorithm is done. A new raster file with the values 0 for outside the catchment area and 100 for inside the catchment area is now loaded into your QGIS project.\nBack to catchment delineation."
  },
  {
    "objectID": "appendix_c_quick_guides.html#appendix-quick-guide-polygonize",
    "href": "appendix_c_quick_guides.html#appendix-quick-guide-polygonize",
    "title": "",
    "section": "QGIS: Polygonize a Raster",
    "text": "QGIS: Polygonize a Raster\nGot to Raster in the header toolbar, move your cursor over Conversion and click on Polygonize (Raster to Vector)… (see Figure 28).\n\n\n\nFigure 28: Open the Polygonize window.\n\n\nSelect the raster layer you wish to polygonize (for example the upslope area raster layer) and run the process. Close the polygonize window after the algorithm is done. A new shape file will be loaded to your GIS project (see Figure 29).\n\n\n\nFigure 29: The vector layer generated by a raster to vector conversion.\n\n\nThe new vector layer has 2 features: the watershed boundary and a box around the watershed the same size of the DEM we used. To get rid of the outer shape, open the attribute table with a right-click on the vector layer and selecting Open Attribute Table. In the attribute table, elect the outer shape by clicking on the second row of the attribute table and toggle the edit mode by clicking on the pen icon (see Figure 30).\n\n\n\nFigure 30: Select the outer shape to discard by clicking on the second row in the attribute table and toggle the edit mode by clicking on the pen icon.\n\n\nDelete the outer shape by pressing the red bin icon in the attribute table (see Figure 31).\n\n\n\nFigure 31: Delete the selected outer shape.\n\n\nSave the edits in the attribute table (see Figure 32) and press the pen icon again to un-toggle the edit mode.\n\n\n\nFigure 32: Save your edits in the attribute table.\n\n\nClose the attribute table window and save the boundary of your watershed and your QGIS project.\nBack to catchment delineation."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-graphical-edit-junctions",
    "href": "appendix_c_quick_guides.html#sec-graphical-edit-junctions",
    "title": "",
    "section": "QGIS: Edit Junctions Layer",
    "text": "QGIS: Edit Junctions Layer\nSelect the Junctions layer and toggle manual editing by clicking on the yellow pen (see Figure 33).\n\n\n\nFigure 33: Manually edit the layer with the river junctions, step 1: Toggle layer editing.\n\n\nWhen in editing mode, the yellow pen will appear in the Layers window next to the name of the layer being edited. The edit mode will also activate a button for adding points (i.e. junctions, we don’t need that now) and the vertex tool. Click on the vertex tool icon. It is active when a a boundary appears around the icon and the Vertex Editor windows opens (see Figure 34).\n\n\n\nFigure 34: Manually edit the layer with the river junctions, step 1: Toggle layer editing.\n\n\nRight-click on a junction point you would like to delete to activate it (see Figure 35).\n\n\n\nFigure 35: Manually edit the layer with the river junctions, step 3: Activate a junction node for editing.\n\n\nSelect the activated point by drawing a rectangle over the point with your mouse. The point will appear blue (see Figure 36).\n\n\n\nFigure 36: Manually edit the layer with the river junctions, step 3: Select the activated junction node.\n\n\nDelete the point with the delete key on your keyboard. You can save your edits by pressing the blue-white Save Layer Edits button that is decorated with an orange pen (see Figure 37). This saves your changes without exiting the edit mode.\n\n\n\nFigure 37: Manually edit the layer with the river junctions, step 3: Save edits.\n\n\nIf you have many points to remove, as in our case, it may be faster to identify the IDs of the features you want to keep, select these and delete all others. To start, you activate the Identify Features mode by clicking on the icon with the white i on the blue circle (see Figure 38).\n\n\n\nFigure 38: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 1.\n\n\nA black i will appear next to your cursor. You then click on the first of your nodes that you want to keep. This will highlight it in red and a list with information on the selected feature appears on the right in the Identify Results window. You will see the attribute NODE_ID with value 1 for the outflow node (see Figure 39). Note down the ID of the feature.\n\n\n\nFigure 39: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 2.\n\n\nYou then press on the node at the confluence of the two tributaries in the center of the catchment. The Identify Results window shows 2 results, that means, that two junction nodes are close to each other (see Figure 40).\n\n\n\nFigure 40: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 3.\n\n\nZoom in in your map window with your mouse to see the two nodes (see Figure 41).\n\n\n\nFigure 41: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 4.\n\n\nSelect the node that should be kept and not the ID of the node (NODE_ID 11) (see Figure 42).\n\n\n\nFigure 42: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 5.\n\n\nZoom back to the entire Junctions layer (see Figure 10) and go to Select Features by Values… in the toolbar (see Figure 43).\n\n\n\nFigure 43: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 1.\n\n\nAdd NODE_ID 1 to your selection as demonstrated in Figure 44.\n\n\n\nFigure 44: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 2.\n\n\nThe outflow node with ID 1 will change color in your map as shown in Figure 45.\n\n\n\nFigure 45: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 3.\n\n\nAdd node 11 to your selection in the same way and close the Select Node by Value window. Invert the feature selection as shown in Figure 46.\n\n\n\nFigure 46: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 4.\n\n\nAll other nodes will now be yellow and the ones to keep will appear in the layer color (see Figure 47).\n\n\n\nFigure 47: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 5.\n\n\nDelete the features (nodes) by pressing the Delete Selected button in the edit features toolbar as shown in Figure 48.\n\n\n\nFigure 48: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 6.\n\n\nSave your edits (see Figure 37). To verify that, indeed, all superfluous nodes are deleted from the Junctions file, open the Attribute table shown in Figure 49.\n\n\n\nFigure 49: Edit Attribute table. Step 1: Open the attribute table with a click on the Attribute Table icon in the QGIS toolbar.\n\n\nOnly 2 features should be listed under each attribute. We will now edit the attribute table to prepare it for the RSMinerve model. RSMinerve needs an identifier to differentiate between the junctions. We can use the attribute TYPE to uniquely identify the two junctions needed. RSMinerve further needs the ID of the downstream river. Add a column to the attribute table by pressing the Add Field button (see Figure 50).\n\n\n\nFigure 50: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 1.\n\n\nDefine a name for the attribute, a type and admissible length of each entry in the Add Field window. In our case, we choose a string consisting of letters as ID and allow it to be 20 characters long as shown in Figure 51.\n\n\n\nFigure 51: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 2.\n\n\nClose the window by pressing OK. By clicking in the newly created NULL fields, you can now type names for the downstream rivers and save your edits by pressing the save edits icon (3rd from the left in the toolbar of the attribute table window). As the outlet of the catchment goes directly into Charvak reservoir, we can type Charvak as the downstream river ID for this example. The river stretch between junction and outlet is called Pskem (see Figure 52).\n\n\n\nFigure 52: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 3.\n\n\nWe are done editing the Junctions layer. Deactivate the edit mode by clicking on the yellow pen in the attribute table window as demonstrated in Figure 53 and close the window.\n\n\n\nFigure 53: Save your edits.\n\n\nNow save the Junctions layer in an appropriate place on your drive. Time to save your QGIS project.\nIn the same way you can also edit the river channels layer."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-create-hbv-model",
    "href": "appendix_c_quick_guides.html#sec-quickguides-create-hbv-model",
    "title": "",
    "section": "RSM: Create HBV model and edit parameters",
    "text": "RSM: Create HBV model and edit parameters\nClick on the HBV model icon in the model selection panel (see Figure 54).\n\n\n\nFigure 54: To generate an HBV model, click on the HBV model icon in the model selection panel.\n\n\nMove your cursor to the white area in the center and create a HBV model with a click. The HBV model icon will now appear in the white model panel (see Figure 55).\n\n\n\nFigure 55: A HBV model has been generated.\n\n\nEdit the parameters of the model by double-clicking on the model icon and changing parameter values in the table that appears to the right of the RSMinerve window (see Figure 56).\n\n\n\nFigure 56: Double-click on the HBV model to open and edit the parameter table.\n\n\nAlternatively (especially if you have to change parameters for several models), activate the Parameters panel in the Model Properties toolbar (see Figure 57).\n\n\n\nFigure 57: Activate the Parameters panel in the Model Properties toolbar.\n\n\nSelect the HBV model in the Parameters panel as shown in Figure 58.\n\n\n\nFigure 58: Select HBV in the Parameters panel.\n\n\nIf you have several HBV models, you can select models by zone and apply edits in the parameter table in the left of the Parameters panel to all selected models (marked with tick). You can also edit paramers for individual models in the right-hand table of the Parameters panel as is visible in Figure 59.\n\n\n\nFigure 59: Edit parameters for groups of models (left parameter table) or for individual models (right parameter table).\n\n\nSave your model by clicking the floppy disk icon in the toolbar.\nYou can also export parameters to a text file via the button Export P in the Model Properties toolbar, edit the text file and import the edited parameter file through Import P.\nNote that for the Nauvalisoy demonstration case study, the area of the HBV model should be 99’000’000 m2.\nBack to the Nauvalisoy model guide."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-add-climate-station",
    "href": "appendix_c_quick_guides.html#sec-quickguides-add-climate-station",
    "title": "",
    "section": "RSM: Add and link climate station",
    "text": "RSM: Add and link climate station\nMove your cursor to the V-Station icon in the models panel (the first icon in Figure 54). Click on the icon to activate it and click next to the HBV model in your model pane to place the V-Station. With a double-click on the newly created V-Station, you can visualize the parameters of the virtual weather station.\nThen, connect the station to the HBV model by activating the Connections Mode in the Editing tools toolbar as shown in Figure 60.\n\n\n\nFigure 60: Create a virtual weather station.\n\n\nClick on the station, hold down the finger move your cursor to the HBV model, then release the hold. A grey line appears between the station and the model and a pop-up window asks you to verify the data links between the two components (see Figure 61).\n\n\n\nFigure 61: Link the virtual weather station to the HBV model.\n\n\nClick Ok to accept the suggested data links and a blue arrow will appear between the weather station and the model. Click on the black arrow in the Editing tools toolbar to leave the Connections Mode.\nBack to the Nauvalisoy model guide."
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-import-climate-data",
    "href": "appendix_c_quick_guides.html#sec-quickguides-import-climate-data",
    "title": "",
    "section": "RSM: Import climate data",
    "text": "RSM: Import climate data\nMake sure that you have downloaded storede the following climate data file ERA5_Nauvalisoy_1981_2013.csv for Nauvalisoy River on your computer.\nGo to the database tab and click on Open in the File database toolbar (see Figure 62).\n\n\n\nFigure 62: The database tab.\n\n\nSelect the file that you downloaded before. If you don’t see the file in your file browser, make sure the file ending .csv is selected in the file browser (see Figure 63).\n\n\n\nFigure 63: Make sure the file ending is .csv in the file browser.\n\n\nPress open to load the data into RSMinerve. Depending on your computer this may take a few seconds. Once the data is loaded, click through the database browser in the left pane to explore the data as shown in Figure 64.\n\n\n\nFigure 64: Explore the data base.\n\n\nTo connect the data base to the model you will need to make the data consistent with the model. To do this change the name “New group” to “Measurements” and select Inputs for the Category selection (see Figure 65).\n\n\n\nFigure 65: Change \"New group\" to \"Measurements\" and select Input as category.\n\n\nThen, browse the name of the station as done in Figure 66.\n\n\n\nFigure 66: The name of the station is \"Nauvalisoy\".\n\n\nAs our weather data is representative for the entire Nauvalisoy catchment, the station name in the data base needs to be consistent with the station name of the V-Station in the model pane. Change the name of the station in the model pane from V-Station to Nauvalisoy by clicking on the name and then editing it.\nChoose the nauvalisoy data set as source and adapt the simulation period as shown in Figure 67.\n\n\n\nFigure 67: Choose the nauvalisoy data set to link the weather station data to the virtual weather station. The simulation times should not extend the period of the input data. Simulation time step is 1 hour and the recording time step is 1 month.\n\n\nClick on Validation to verify that the model has been set up correctly. Once you have adapted the model settings to calculate evaporation based on temperature (how to), no errors should be reported. You can now run the model. A warning tells you that the number of meteo stations is not sufficient. Ignore the warning for now.\nBack to the Nauvalisoy model guide"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-atapt-model-settings",
    "href": "appendix_c_quick_guides.html#sec-quickguides-atapt-model-settings",
    "title": "",
    "section": "RSM: Adapt Model Settings",
    "text": "RSM: Adapt Model Settings\nNavigate to Edit in the Settings toolbar (see Figure 68).\n\n\n\nFigure 68: Open the models settings tab.\n\n\nOpen the Settings tab and choose an ET model. Adapt the coordinates of the project (choose the station coordinates mentioned in the Introduction Section) (see Figure 69).\n\n\n\nFigure 69: Edit the evaporation calculation method and the coordinates of the project.\n\n\nBack to the model guide"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-export-sim-results-to-db",
    "href": "appendix_c_quick_guides.html#sec-export-sim-results-to-db",
    "title": "",
    "section": "RSM: Export Simulation Results to Data Base",
    "text": "RSM: Export Simulation Results to Data Base\nGo to Export in the Database toolbar (see Figure 70). Define a name for the simulation results and choose a data base group to save the data to. Create a new group if you haven’t already done so.\n\n\n\nFigure 70: Export simulation results to the data base.\n\n\nThe data sets are now available in the data base tab and can be visualized in the Selection and Plots tab.\nBack to the model guide"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-add-comparator-and-discharge-data",
    "href": "appendix_c_quick_guides.html#sec-quickguides-add-comparator-and-discharge-data",
    "title": "",
    "section": "RSM: Add Comparator and Load Discharge Measurements",
    "text": "RSM: Add Comparator and Load Discharge Measurements\nThe comparator object allows the user to compare a simulated variable to a reference. In our case, we want to compare the simulated discharge of the Nauvalisoy river to the one actually measured at the only gauge in the catchment. The measured discharge can be imported to RSMinerve via the database tab.\nMove your cursor to the comparator icon as shown in Figure 71 in the model components panel.\n\n\n\nFigure 71: Comparator icon in RS Minerve.\n\n\nActivate it with a left-click and move your cursor next to the HBV model in the model pane. Place the comparator object with another click.\nAdd a source object to your model following the same procedure as for the comparator object. Figure 72 shows the icon of the source object.\n\n\n\nFigure 72: Source icon in RS Minerve.\n\n\nYou can now optionally rename your model objects.\nConnect the source and the HBV model to the comparator by activating the Connections mode in the Editing Tools toolbar. Right-click on the HBV model, hold the click and drag the cursor to the comparator object where you release the cursor. You will be asked in a pop-up window to specify the flow you wish to compare and if the data is to be viewed as simulation results or reference (measured) data. Connect the total discharge computed by the HBV model component as simulation result to the comparator (see Figure 73) and close the pop-up window by pressing OK.\n\n\n\nFigure 73: Connecting a Source to Comparater Object in RSMinerve.\n\n\nConnect the source object to the comparator in the same way as the HBV object. The source will be connected as reference (see Figure 74).\n\n\n\nFigure 74: Final, connected objects.\n\n\nNext, we have to load the discharge data into the database. Navigate to the database tab. In the database, go to Measurements -> Nauvalisoy and click Add and enter a name for the discharge station (see Figure 75). For this example, we do not need to bother with the coordinates of the station (later, in more complex models, we will have to though!).\n\n\n\nFigure 75: Connect the outflow of the HBV model as simulation result to the comparator.\n\n\nUnder the new discharge station, add a sensor and rename it to the source object in your model as shown in Figure 76.\n\n\n\nFigure 76: Connect the discharge measurements (source) as reference to the comparator.\n\n\nOpen the tab with the values. Here we need to import the discharge data. You can do that by opening the discharge data in a spreadsheet (e.g. Excel) and copy-pasting the dates and discharge values into the Values table in RSMinerve (here is how to do this step-by-step.\nNow we need to link the discharge data in the database to the source object in the model. Navigate to the model do the following.\n\n\n\nFigure 77: Select the data source for the source object (under Data Source in the left window pane) and select the sensor for discharge data for the source (under Source, Series identifier in the right window pane).\n\n\nValidate the model to see if the model setup went correctly. Run the model if the validation did not throw an error.\nBack to the practical model calibration and validation Section"
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-copy-paste-database-values",
    "href": "appendix_c_quick_guides.html#sec-quickguides-copy-paste-database-values",
    "title": "",
    "section": "RSM: Copy-paste Data to Database",
    "text": "RSM: Copy-paste Data to Database\nMake sure that the following file synthetic_discharge_Nauvalisoy_river_for_calibration_exercise.csv is available on your computer.\nNow, open the file in a spreadsheet software, e.g. Excel, Numbers, Google Sheets or OpenOffice Calc and select the rows containing dates and values (Figure 78). Press Control+C or perform a right-click with the mouse and choose copy.\n\n\n\nFigure 78: Select the rows and columns containing the dates and data to be copied. Press Control+C to copy the selected cells.\n\n\nThen navigate to the discharge sensor on the database tab in RSMinerve where you want to add the data to. Click in the small square to the left of the first row in the Values tab and click the keyboard keys Control+P. Alternatively perform a right-click in the small square and select Paste. The dates and values from the excel sheet will now appear in the table (Figure 79). Save the database.\n\n\n\nFigure 79: Paste dates and values to the database table.\n\n\nBack to the practical model calibration and validation Section"
  },
  {
    "objectID": "hydrological_modeling_applications.html",
    "href": "hydrological_modeling_applications.html",
    "title": "",
    "section": "",
    "text": "Hydrological Modeling Applications\nDiscuss application of hydrological modeling."
  },
  {
    "objectID": "modeling_using_predictive_inference.html#sec-em-prerequisites",
    "href": "modeling_using_predictive_inference.html#sec-em-prerequisites",
    "title": "",
    "section": "Prerequisites",
    "text": "Prerequisites\nTime series based hydrological models use past observations of discharge and auxilliary variables such as snow cover in the catchment and/or precipitation to predict the future from learned past patterns. Currently, there are no ready-made hydrological modeling packages that can be used for this purpose. Such types of models rather draw on available time series-based modeling algorithms that are implemented and available in dedicated packages for programming environments like R.\nWe will work with R and RStudio and the following packages that need to be installed to replicate and run the models shown here. Note that such a package installation step only has to be performed once.\n\n# Core Libraries\ninstall.packages('tidyverse')    # Meta - dplyr, ggplot2, purrr, tidyr, stringr, forcats\ninstall.packages('lubridate')    # date and time\ninstall.packages('timetk')       # Time series data wrangling, visualization and preprocessing\ninstall.packages('modeltime')    # latest and greatest time series modeling package for R\ninstall.packages('tidymodels')   # The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles.\n\n# Sample data from Central Asia river basins and helper functions\nlibrary('devtools')\ndevtools::install_github(\"hydrosolutions/riversCentralAsia\")\n\nThe packages can then be loaded and made available in your R session.\n\nlibrary(devtools)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(timetk)\nlibrary(tidymodels)\nlibrary(riversCentralAsia)\n\nWhen other, additional packages are needed, they will be loaded in the corresponding Sections below.\nPlease also remember the following rules when working with R dataframes in the tidyverse:\n\nEvery column is variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nA final note. In all of the following, we mostly use the powerful data manipulation and visualization techniques for time series data as provided by the timetk package. This package greatly facilitates any work with time series data as it, among other things, nicely integrates with the R ‘tidyverse’."
  },
  {
    "objectID": "modeling_using_predictive_inference.html#sec-em-forecasting-using-predictive-inference",
    "href": "modeling_using_predictive_inference.html#sec-em-forecasting-using-predictive-inference",
    "title": "",
    "section": "Forecasting Using Predictive Inference",
    "text": "Forecasting Using Predictive Inference\nIn this Section, we are concerned with predictive inference using observed data to predict future data that is not known yet but that is important to forecast with high confidence and low uncertainty. In other words, it is assumed that we can encapsulate historic patterns in a model for learning about the future with such model.\nIn hydrology, we are dealing with time series, i.e. ordered observations in time. A generic model structure thus can be specified in the following way\n\\[\ny(t+\\Delta t) = f(y(t),x(t)) + \\epsilon(t)\n\\]\nwhere \\(y(t+\\Delta t)\\) is called the forecast target (discharge at a particular gauge in our case) and is the variable that we want to forecast in the future, i.e. \\(\\Delta t\\) time away from now. \\(y(t)\\) denotes past known observations of discharge up and including time \\(t\\). Similarly, \\(x(t)\\) denotes other variables of interest, called external regressors, that might be relevant to obtain good quality forecasts such as meteorological data from local stations, including precipitation and temperature. Finally, \\(f()\\) denotes the type of model that is being used for forecasting and \\(\\epsilon\\) are the time-dependent error terms. If, for example, one would use a linear modeling approach without external regressors, such type of model could simply be written as\n\\[\ny(t+\\Delta t) = \\beta_{0} + \\beta_{1} \\cdot y(t) + \\epsilon(t)\n\\]\nIn the model specification above, the aim is to predict into the future with a lead time of \\(\\Delta t\\), i.e. for example one month ahead. The lead-time model can be written in equivalent form using lags in the following way\n\\[\ny(t) = f(y(t-lag),x(t-lag)) + \\epsilon(t)\n\\]\nwhere \\(lag = \\Delta t\\). This just means that we use all the available observations until and including \\(t-lag\\) for predicting the target at time \\(t\\). We will use these specifications throughout the Chapter when working with and developing new forecasting models."
  },
  {
    "objectID": "modeling_using_predictive_inference.html#sec-em-forecasting-in-central-asia",
    "href": "modeling_using_predictive_inference.html#sec-em-forecasting-in-central-asia",
    "title": "",
    "section": "Forecasting Discharge in Central Asian",
    "text": "Forecasting Discharge in Central Asian\nHydrometeorological Organizations\nThe key agencies that that are charged in predicting river discharge regularly in Central Asia are the Hydrometeorological Agencies. For predicting mean discharge over a certain future period, they use different types of statistical models.\nThe particular type of model they use depends on the available hydrological and meteorological data for a particular river whose mean discharge is to be forecasted and on the type of the forecast. Types of forecasts include\n\ndaily forecasts, i.e. \\(\\Delta t = 1 \\text{ day}\\),\npentadal forecasts, i.e. \\(\\Delta t = 5 \\text{ days}\\),\ndecadal forecasts, i.e. \\(\\Delta t = 10 \\text{ days}\\),\nmonthly forecasts, i.e. \\(\\Delta t = 1 \\text{ month}\\), and\nseasonal forecasts, i.e. \\(\\Delta t = 6 \\text{ months}\\).\n\nTo this date, these types of forecasts are performed at regular intervals by the operational hydrologists. Normally, this requires normally a lot of manual work. Recently, selected Hydrometeorological Agencies use automated software to automatize this type of work. The Chapter on Real-World Hydrological Modeling Applications discusses in greater detail such a software application, called iEasyHydro.\nIn Uzbekistan, for example, the following list of forecast objects exists in the Hydromet.\n\nList of Uzbek forecast target objects. Forecast type abbreviations are dl: daily forecast, m: monthly forecast, s: seasonal forecast. Country abbreviations are UZ: Uzbekistan, KG: Kyrgyzstan and TJ: Tajikistan. Source: Uzbek Hydrometeorological Service.\n\nRiver\nGauge (Target Object)\nGauge Code\nCountry\nTypes of Forecasts\n\n\n\nChirchik\nInflow to Charvak Res.\n16924\nUZ\ndl, m, s\n\n\nChirchik\nInflow to Charvak Res. + Ugam River\n16924 + 16300\nUZ\nm, s\n\n\nAkhangaran\nIrtash\n16230\nUZ\nm, s\n\n\nChadak\nDzhulaysay\n16202\nUZ\nm, s\n\n\nGavasay\nGava\n16193\nUZ\nm, s\n\n\nPadsha-Ata\nTostu\n16176\nKG\nm, s\n\n\nKara Darya\nInflow to Andizhan water reservoir\n16938\nUZ/KG\ndl, m, s\n\n\nIsfayramsoy\nUch-Kurgan\n16169\nKG\nm, s\n\n\nSokh\nSarykanda\n16198\nUZ\ndl (May - Sep.), m, s\n\n\nSanzar\nKyrk\n16223\nUZ\nm, s\n\n\nNaryn\nInflow to Toktogul water reservoir\n16936\nKG\nm, s\n\n\nVaksh\nInflow to Nurek water reservoir\n17084 (Vakh river - Darband gauge)\nTJ\nm, s\n\n\nKafirnigan\nTartki\n17137\nTJ\nm, s\n\n\nTupalang\nInflow to Tupalang water reservoir\n17194\nUZ\nm, s\n\n\nSangardak\nKeng-Guzar\n17211\nUZ\nm, s\n\n\nAkdarya\nInflow to Gissarak water reservoir\n17464 (Akfariya river - Hissarak gauge)\nUZ\nm, s\n\n\nYakkabagdarya\nTatar\n17260\nUZ\nm, s\n\n\nUryadariya + Kichik Uryadariya\nInflow to Pachkamar water reservoir\n17279 (Uryadariya river - Bazartepe gauge) + 17275 (Kichik Uryadariya - Gumbulak gauge)\nUZ\nm, s\n\n\nZeravshan\nInflow to Rovatkhodzha hydro work\n17461\nUZ\nm, s\n\n\n\nIt should be noted that all of the Hydromets have such type of lists with different forecast target and types. As can be seen from the above list, Uzbekistan does neither issue pentade nor decadal forecasts, i.e. types of forecasts which are widely used in the Kyrgyz Hydromet in contrast.\nFinally, seasonal forecasts in the Uzbek Hydromet are issued twice prior to the irrigation season with 3. - 5. March being the first issues data range and 3. - 5. April being the second one. Converse to this, monthly forecasts are issues between the 25. - 27. day each month. Finally, decadal and pentade forecasts are issued each morning at the day of the end of the corresponding pentade or decade.\nIt is important to emphasize again that there is currently no standardized way in the region to produce these forecasts. While in some instances, a particular approach and method works very well, it fails to produce acceptable forecasts in other basins. However, as we shall see, certain techniques work very well for particular forecast horizons which then explains why such type of technique has become widely used in the region.\nForecasting for What and Whom?\nWhy is all this work is required? What is the purpose of predicting mean discharge into the future at regular intervals? Important recipients of the forecast products include the Water Authorities which are in charge of delivering adequate amounts of water for irrigation at the right time and location.\nIt all starts with pre-season irrigation planning. The main irrigation season in most of Central Asia is from April 1. through the end of September. Previous to the start of the irrigation season, irrigation plans are drafted based on computed irrigation water demand of all the water users that are connected to a particular irrigation system. These irrigation system are defined in terms of canal topology where demand gets aggregated from the bottom up to the Rayvodkhozes and the Oblvodkhozes. The later then starts with the pre-season irrigation planning given the water irrigation system-level demand. These plans specify decadal water discharge for each irrigation system and the corresponding canals.\nDemand is one thing, but expected supply from the Central Asian rivers another. In order to be able to match the irrigation water demand, the water authorities do what is needed and receive from the Hydromets first the seasonal discharge forecasts. Given this forecast of irrigation-season water availability, the water authorities then adjust their plans given the particular expected circumstances. If, for example, an exceptionally dry year is expected, they activate limit plans and reduce planned water distributions according to forecasted quantities. If a wet year is expected, they do not perform these adjustments and maybe even release water from reservoirs previous to the irrigation season to ensure enough storage capacity in the reservoirs.\nWith the beginning of the irrigation season, another seasonal forecast is carried out by the Hydromets and communicated to the Central Asian water authorities. Given the updated forecast, planning is revised and adjusted accordingly. Then, finally, throughout the irrigation season pentadal, decadal and monthly forecasts are produced constantly to carefully balance water demand with supplies while the season is under way.\nNeedless to say that other important customers for hydrometeorological forecasts exist, including for example airports, road departments that need to ensure road safety, agricultural clusters that are interested in frost and hail warnings, local authorities that need to be alerted in the case of extreme local weather conditions, etc."
  },
  {
    "objectID": "modeling_using_predictive_inference.html#sec-em-data-and-preparation",
    "href": "modeling_using_predictive_inference.html#sec-em-data-and-preparation",
    "title": "",
    "section": "Data and Preparation",
    "text": "Data and Preparation\nAvailable Data\nThe riversCentralAsia Package provides available data of the gauging and meteorological stations in the Chirchik River Basin.\nBefore starting any type of modeling, it is important to get a good understanding of the data that we are dealing with and whether there exist problems with the raw data that need to be addressed prior to modeling. Problems usually include data gaps and outliers as data record that one obtains are usually ever complete nor clean of errors.\nThe steps performed here are thus required steps for any type of successful modeling and should be performed with great care. We concentrate our efforts here on discharge records and data from meteorological stations in the Chirchik River Basin. The techniques shown here for decadal (10-days) data naturally extend to monthly data and other basins.\nGap Filling Discharge Data\nIn the following, we will work with decadal discharge data from the two main tributaries, i.e. the Chatkal and (Gauge 16279) Pskem rivers (Gauge 16290) and the data of the inflow to the Charvak reservoir (Gauge 16924). The goal is to analyze the data and prepare for modeling. First, let us load the relevant discharge data.\n\ndata <- ChirchikRiverBasin # load data\nq_dec_tbl <- data %>% filter(code == '16279' | code == '16290' | code == '16924') # Note for the new name of the object, we choose to add periodicity (_dec_) and data type (_tbl for tibble/dataframe) to the data name. This just helps to stay organized and is good practice in R programming.\nq_dec_tbl\n\n# A tibble: 9,072 × 14\n   date        data  norm units type  code  station   river   basin   resolution\n   <date>     <dbl> <dbl> <chr> <fct> <chr> <chr>     <chr>   <chr>   <fct>     \n 1 1932-01-10  48.8  38.8 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 2 1932-01-20  48.4  37.5 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 3 1932-01-31  42.4  36.6 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 4 1932-02-10  43.7  36.4 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 5 1932-02-20  44.2  36.3 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 6 1932-02-29  47.7  36.9 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 7 1932-03-10  54.1  39.4 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 8 1932-03-20  63.2  47.6 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 9 1932-03-31 103    60.5 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n10 1932-04-10 103    86.4 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n# … with 9,062 more rows, and 4 more variables: lon_UTM42 <dbl>,\n#   lat_UTM42 <dbl>, altitude_masl <dbl>, basinSize_sqkm <dbl>\n\n\nYou can get more information about the available data by typing ?ChirchikRiverBasin.\nIt is advisable to check at this stage for missing data in time series and to fill gaps where present. As can be seen in Figure 1, close inspection of the time series indeed reveals some missing data in the 1940ies.\n\nq_dec_tbl %>% plot_time_series(date,data,\n                               .facet_vars  = code,\n                               .smooth      = FALSE,\n                               .interactive = TRUE,\n                               .x_lab       = \"year\",\n                               .y_lab       = \"m^3/s\",\n                               .title       = \"\"\n                               )\n\n\nFigure 1: Discharge data of selected gauges in the upstream zone of runoff formation in the Chirchik River Basin. Data Source: Uzbek Hydrometeorological Service.\n\n\n\nNote, Figure 1 and the following Figures are interactive, so you can zoom in to regions of interest.\nMissing data are also confirmed by the warning that the function timetk::plot_time_series() throws (suppressed here). Statistics of the missing data can be easily obtained. As the Table below shows, we can do this analysis for each discharge station separately.\n\nq_dec_tbl %>% group_by(code) %>% \n  summarize(n.na = sum(is.na(data)), na.perc = n.na/n()*100)\n\n# A tibble: 3 × 3\n  code   n.na na.perc\n  <chr> <int>   <dbl>\n1 16279    15   0.496\n2 16290    39   1.29 \n3 16924    42   1.39 \n\n\nSummarizing the number of observation with missing data reveals 15 data points for station 16279 (0.5 % of total record length) and 39 for station 16290 (1.3 % of total record length). As there are only very few gaps in the existing time series, we use a simple method to fill these. Wherever there is a gap, we fill in the corresponding decadal norm as stored in the norm column in the object q_dec_tbl. The visualization of the results confirms that our simple gap filling approach is indeed satisfactory as shown in Figure 2.\n\nq_dec_filled_tbl <- q_dec_tbl\n\nq_dec_filled_tbl$data[is.na(q_dec_filled_tbl$data)] = \n  q_dec_filled_tbl$norm[is.na(q_dec_filled_tbl$data)] # Gap filling step\n\nq_dec_filled_tbl %>% plot_time_series(date, data, \n                                      .facet_vars  = code, \n                                      .smooth      = FALSE,\n                                      .interactive = TRUE,\n                                      .x_lab       = \"year\",\n                                      .y_lab       = \"m^3/s\",\n                                      .title       = \"\"\n                                      )\n\n\nFigure 2: Gap filled Pskem and Chatkal river discharges.\n\n\n\nA note of caution here. This simple gap filling technique reduces variance in the time series. It should only be used when the percentage of missing data is low. As will be discussed in the next Section 1.4.3 below, better techniques have to be utilized when there exist substantial gaps and in the case of less regular data.\nFinally, we discard the norm data which we used for gap filling of the missing discharge data and convert the data to wide format (see the Table below) to add to it meteorological data in the next Section.\n\nq_dec_filled_wide_tbl <- q_dec_filled_tbl %>% # again we use the name convention of objects as introduced above\n  mutate(code = paste0('Q',code %>% as.character())) %>% # Since we convert everything to long form, we want to keep information as compact as possible. Hence, we paste the type identifier (Q for discharge here) in from of the 5 digit station code.\n  dplyr::select(date,data,code) %>% # ... and then ditch all the remainder information\n  pivot_wider(names_from = \"code\",values_from = \"data\") # in order to pivot to the long format, we need to make a small detour via the wide format.\n\nq_dec_filled_long_tbl <- q_dec_filled_wide_tbl %>% pivot_longer(-date) # and then pivot back\nq_dec_filled_wide_tbl\n\n# A tibble: 3,024 × 4\n   date       Q16279 Q16290 Q16924\n   <date>      <dbl>  <dbl>  <dbl>\n 1 1932-01-10   48.8   38.3   87.1\n 2 1932-01-20   48.4   37.7   86.1\n 3 1932-01-31   42.4   36.2   78.6\n 4 1932-02-10   43.7   35.6   79.3\n 5 1932-02-20   44.2   35     79.2\n 6 1932-02-29   47.7   37.1   84.8\n 7 1932-03-10   54.1   43.1   97.2\n 8 1932-03-20   63.2   47    110  \n 9 1932-03-31  103     72.1  175  \n10 1932-04-10  103     73.2  176  \n# … with 3,014 more rows\n\n\nAs a result, we now have a complete record of decadal discharge data for the two main tributaries of the Chirchik river and the inflow time series to Charvak Reservoir from the beginning of 1932 until and including 2015, i.e. 84 years. The same type of preparatory analysis will now be carried out for the meteorological data.\nGap Filling of Meteorological Data\nHere, we use precipitation and temperature data from Pskem (38462), Chatkal (38471) and Charvak Reservoir (38464) Meteorological Stations (see the Example River Basins Chapter for more information on these stations). We also have data from Oygaing station (Station Code 38339) but the record only starts in 1962 and the time resolution is monthly. Therefore, we do not take this station into account here for the time being.\nWe start with precipitation and plot the available data.\n\np_dec_tbl <- data %>% filter(type==\"P\" & code!=\"38339\") \np_dec_tbl %>% plot_time_series(date,data,\n                               .facet_vars  = code,\n                               .interactive = TRUE,\n                               .smooth      = FALSE,\n                               .title       = \"\",\n                               .y_lab       = \"mm/decade\",\n                               .x_lab       = \"year\"\n                               )\n\n\nFigure 3: Raw decadal precipitation data from Pskem (38462), Charvak Reservoir (38471) and Chatkal Meteo Station (38471).\n\n\n\nThe precipitation data from these 3 stations shows some significant data gaps. The Chatkal Meteorological Station that is located in Kyrgyzstan apparently did not work in the post-transition years and continuous measurements were only resumed there in 1998.\nLet us see what happens if we were to use the same simple gap filling technique that we introduced above for discharge.\n\np_dec_filled_tbl <- p_dec_tbl\np_dec_filled_tbl$data[is.na(p_dec_filled_tbl$data)] = p_dec_filled_tbl$norm[is.na(p_dec_filled_tbl$data)]\np_dec_filled_tbl %>% plot_time_series(date,data,\n                                      .facet_vars  = code,\n                                      .interactive = TRUE,\n                                      .smooth      = FALSE,\n                                      .title       = \"\",\n                                      .y_lab       = \"mm/decade\",\n                                      .x_lab       = \"year\"\n                                      )\n\n\nFigure 4: Precipitation Data gap-filled with norms. The filled values from 1990 - 2000 in the case of the Station 38471 indicate that the norm-filling technique is not good.\n\n\n\nClosely inspect the significant data gap in the 1990ies at Station 38741 (tip: play around and zoom into the time series in the 1990ies in Figure 3 and comparing it with the resulting gap-filled timeseries in Figure 4. We see that our technique of gap filling with long-term norms is not suitable for this type of data and the significant gap size. The effect of variance reduction is also clearly visible.\nHence, we resort to a more powerful gap filling technique that uses a (regression) model to impute the missing values from existing ones at the neighboring stations, i.e. Stations 38462 and 38464. To do so, we utilize an R package that is tightly integrated in the tidyverse. Please note that if you do not have the required package installed locally, you should install it prior to its use with the following command install.packages('simputation')\n\nlibrary(simputation)\n# First, we bring the data into the suitable format. \np_dec_wide_tbl <- p_dec_tbl %>% \n  mutate(code = paste0('P',code %>% as.character())) %>% \n  dplyr::select(date,data,code) %>% \n  pivot_wider(names_from = \"code\",values_from = \"data\")\n\n# Second, we impute missing values.\np_dec_filled_wide_tbl <- p_dec_wide_tbl  %>% \n  impute_rlm(P38471 ~ P38462 + P38464) %>% # Imputing precipitation at station 38471 using a robust linear regression model\n  impute_rlm(P38462 ~ P38471 + P38464) %>% # Imputing precipitation at station 38462 using a robust linear regression model\n  impute_rlm(P38464 ~ P38462 + P38471) # Imputing precipitation at station 38464 using a robust linear regression model\n\np_dec_filled_long_tbl <- p_dec_filled_wide_tbl %>% pivot_longer(c('P38462','P38464','P38471')) \n\np_dec_filled_long_tbl%>% plot_time_series(date,value,\n                                          .facet_vars  = name,\n                                          .interactive = TRUE,\n                                          .smooth      = FALSE,\n                                          .title       = '',\n                                          .y_lab       = \"mm/decade\",\n                                          .x_lab       = \"year\"\n                                          )\n\n\nFigure 5: Precipitation Data gap filled with a robust linear regression modeling approach.\n\n\n\nAs you can see, we use simple linear regression models to impute missing value in the target time series using observations from the neighboring stations.\nThrough simple visual inspection, it becomes clear that this type of regression model for gap filling is better suited than the previous approach chosen. Let us check whether we could successfully fill all gaps with this robust linear regression approach.\n\np_dec_filled_long_tbl %>% group_by(name) %>% summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100)\n\n# A tibble: 3 × 3\n  name    n.na n.na.perc\n  <chr>  <int>     <dbl>\n1 P38462    12     0.402\n2 P38464    12     0.402\n3 P38471     3     0.100\n\n\nIt turns out that we still have very few gaps to deal with. We can see them by simply visualizing the wide tibble. The problem persisted at times when two or more values were missing across the available stations at the same time and where thus the linear regression could not be carried out.\n\np_dec_filled_wide_tbl %>% head(10)\n\n# A tibble: 10 × 4\n   date       P38462 P38464 P38471\n   <date>      <dbl>  <dbl>  <dbl>\n 1 1933-01-10     NA   NA        2\n 2 1933-01-20     NA   NA       10\n 3 1933-01-31     NA   NA        5\n 4 1933-02-10     NA   NA       33\n 5 1933-02-20     NA   NA        8\n 6 1933-02-28     NA   NA       10\n 7 1933-03-10     NA   NA       31\n 8 1933-03-20     NA   NA       50\n 9 1933-03-31     NA   NA        6\n10 1933-04-10     23   21.3     13\n\n\n\np_dec_filled_wide_tbl %>% tail()\n\n# A tibble: 6 × 4\n  date       P38462 P38464 P38471\n  <date>      <dbl>  <dbl>  <dbl>\n1 2015-11-10     72     81     19\n2 2015-11-20    122     76     43\n3 2015-11-30      7      2      3\n4 2015-12-10     NA     NA     NA\n5 2015-12-20     NA     NA     NA\n6 2015-12-31     NA     NA     NA\n\n\nWe can solve the issues related to the missing values at the start of the observation record by using the same technique as above and by only regressing P38462 and P38464 on P38471.\n\np_dec_filled_wide_tbl <- p_dec_filled_wide_tbl  %>% \n  impute_rlm(P38462 ~ P38471) %>% # Imputing precipitation at station 38462 using a robust linear regression model\n  impute_rlm(P38464 ~ P38471) # Imputing precipitation at station 38464 using a robust linear regression model\np_dec_filled_wide_tbl %>% head(10)\n\n# A tibble: 10 × 4\n   date       P38462 P38464 P38471\n   <date>      <dbl>  <dbl>  <dbl>\n 1 1933-01-10   5.60   5.08      2\n 2 1933-01-20  18.3   16.7      10\n 3 1933-01-31  10.4    9.46      5\n 4 1933-02-10  54.9   50.3      33\n 5 1933-02-20  15.2   13.8       8\n 6 1933-02-28  18.3   16.7      10\n 7 1933-03-10  51.8   47.3      31\n 8 1933-03-20  82.0   75.0      50\n 9 1933-03-31  12.0   10.9       6\n10 1933-04-10  23     21.3      13\n\n\nConverse to this, the complete set of observations is missing for December 2015. We will thus remove these non-observations from our tibble.\n\np_dec_filled_wide_tbl <- p_dec_filled_wide_tbl %>% na.omit()\np_dec_filled_wide_tbl %>% tail()\n\n# A tibble: 6 × 4\n  date       P38462 P38464 P38471\n  <date>      <dbl>  <dbl>  <dbl>\n1 2015-10-10      5      1      0\n2 2015-10-20     89    108     58\n3 2015-10-31     34     40     12\n4 2015-11-10     72     81     19\n5 2015-11-20    122     76     43\n6 2015-11-30      7      2      3\n\np_dec_filled_long_tbl <-  p_dec_filled_wide_tbl %>% pivot_longer(-date)\n\nInspecting the temperature data, we see similar data issues as in the precipitation data set.\n\nt_dec_tbl <- data %>% filter(type==\"T\") \nt_dec_tbl %>% plot_time_series(date,data,\n                               .facet_vars  = code,\n                               .interactive = TRUE,\n                               .smooth      = FALSE,\n                               .title       = '',\n                               .y_lab       = \"deg. Celsius\",\n                               .x_lab       = \"year\"\n                               )\n\n\nFigure 6: Raw temperature data from the meteorological stations Pskem (38462) and Chatkal (38471).\n\n\n\n\n# First, we bring the data into the suitable format. \nt_dec_wide_tbl <- t_dec_tbl %>% \n  mutate(code = paste0('T',code %>% as.character())) %>% \n  dplyr::select(date,data,code) %>% \n  pivot_wider(names_from = \"code\",values_from = \"data\")\n\n# Second, we impute missing values.\nt_dec_filled_wide_tbl <- t_dec_wide_tbl  %>% \n  impute_rlm(T38471 ~ T38462) %>% # Imputing precipitation at station 38471 using a robust linear regression model\n  impute_rlm(T38462 ~ T38471) # Imputing precipitation at station 38462 using a robust linear regression model\n\nt_dec_filled_long_tbl <- t_dec_filled_wide_tbl %>% \n  pivot_longer(c('T38462','T38471')) \n\nt_dec_filled_long_tbl%>% \n  plot_time_series(date,value,\n                   .facet_vars  = name,\n                   .interactive = TRUE,\n                   .smooth      = FALSE,\n                   .title       = '',\n                   .y_lab       = \"deg. Celsius\",\n                   .x_lab       = \"year\"\n                   )\n\n\nFigure 7: Temperature data gap filled with robust linear regression modeling.\n\n\n\nThere are some irregularities in the temperature time series of Chatkal Meteorological Station in the first decade of the 20th century (tip: zoom in to see these more clearly). Note that these were not introduced by the gap filling technique that we used but are most likely wrong temperature readings. We will return to these in the outlier analysis below in Section 1.4.4.\n\nt_dec_filled_long_tbl %>% \n  group_by(name) %>% \n  summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100)\n\n# A tibble: 2 × 3\n  name    n.na n.na.perc\n  <chr>  <int>     <dbl>\n1 T38462     3     0.100\n2 T38471     3     0.100\n\n\nTo see where the missing value are, we find them easily again by looking at the head and tail of the tibble.\n\nt_dec_filled_wide_tbl %>% head()\n\n# A tibble: 6 × 3\n  date       T38462 T38471\n  <date>      <dbl>  <dbl>\n1 1933-01-10   -6.9  -16.6\n2 1933-01-20   -6.1  -15.5\n3 1933-01-31   -6.3  -15.6\n4 1933-02-10   -2     -8.6\n5 1933-02-20   -3.3  -12.5\n6 1933-02-28   -0.1   -8.5\n\n\n\nt_dec_filled_wide_tbl %>% tail()\n\n# A tibble: 6 × 3\n  date       T38462 T38471\n  <date>      <dbl>  <dbl>\n1 2015-11-10    2.4   -2.5\n2 2015-11-20    2     -2.2\n3 2015-11-30    4.6   -3.7\n4 2015-12-10   NA     NA  \n5 2015-12-20   NA     NA  \n6 2015-12-31   NA     NA  \n\n\nFinally, we remove the non observations again as above with the function na.omit.\n\nt_dec_filled_wide_tbl <- t_dec_filled_wide_tbl %>% na.omit()\nt_dec_filled_long_tbl <- t_dec_filled_wide_tbl %>% pivot_longer(-date)\n\nTo deal with the missing values at the end of the observational record, we could also have used any other technique. Using the norm values however would have artificially reduced the variance in both cases as explained above. Furthermore and at least in the case of temperature, it is also questionable to what extent a norm calculated over the last 84 years is still representative given global warming. We will look in this important and interesting topic in the next section.\nAnomalies and Outliers\nWe use the function timetk::plot_anomaly_diagnostics to investigate anomalies in the time series. For discharge, we first log-transform the raw data with the following transformation to reduce the variance of the original data.\n\\[\n\\hat{q}(t) = log(q(t) + 1)\n\\]\nwhere \\(\\hat{q}(t)\\) denotes the transformed discharge. Prior to the log transformation, 1 is added so as to avoid cases where discharge would be 0 and the logarithmic transform thus undefined. The transformation can easily be done with the log1p() function in R. Backtransformation via the function expm1() simply involves taking the exponent and subtracting 1 from the result.\nThe exceptionally wet year 19169 shows up as anomalous in the Chatkal River Basin and at the downstream Charvak Reservoir inflow gauge. Figure 8 and Figure 9 show anomalies diagnostics of the available data.\n\nq_dec_filled_long_tbl %>% \n  plot_anomaly_diagnostics(date,\n                           value %>% log1p(),\n                           .facet_vars  = name,\n                           .frequency = 36,\n                           .interactive = TRUE,\n                           .title = \"\")\n\n\nFigure 8: Anomaly diagnostics of discharge data. The transparent grey band shows the width of the normal range. The highly anomalous wet year of 1969 is clearly visible in the discharge record of the Chatkal river basin (Station 16279).\n\n\n\nThe investigation of precipitation anomalies shows a succession of regular anomalous wet events over time. It is interesting to see that the winter 1968/69 regularly anomalous at all three stations (Figure @ref(fig:EManomaliesP), zoom in to investigate).\n\np_dec_filled_long_tbl %>% \n  plot_anomaly_diagnostics(date,\n                           value,\n                           .facet_vars  = name,\n                           .interactive = TRUE,\n                           .title = \"\")\n\n\nFigure 9: Anomaly diagnostics of precipitation data.\n\n\n\nWhile intuitively, we would have expected an eceptionally mild winter in 1968/69 due to the precipitation excess, the corresponding anomaly does not show up in the temperature record as shown in Figure 10.\n\nt_dec_filled_long_tbl %>%  \n  plot_anomaly_diagnostics(date,value,\n                           .facet_vars  = name,\n                           .interactive = TRUE,\n                           .title = \"\")\n\n\nFigure 10: Anomaly diagnostics of temperature data.\n\n\n\nApart from the identification of extremal periods since as the 1969 discharge year in the Chatkal river basin, the diagnostics of anomalies also helps to identify likely erroneous data records. In Figure 10, for example, when we zoom into the data of the series T38471 in the first decade of the 21st century, problems in relation to positive anomalies during the winter are visible in 4 instances. One explanation would be that in at least some instances, the data are erroneously recorded as positive values when in fact they were negative (see dates ‘2002-01-31’, ‘2005-01-10’ and ‘2007-02-28’, Chatkal Station 38471).\nPutting it all Together\nFinally, we are now in the position to assemble all data that we will use for empirical modeling. The data is stored in long and wide form and used accordingly where required. For example, in Section @ref{TimeSeriesReg}, we are working with the wide data format to investigate model features in linear regression. Note that we also add a column with a decade identifier. Its use will become apparent in the Section @ref(Chap9FeatureEngineering) below.\n\n# Final concatenation\ndata_wide_tbl <- right_join(q_dec_filled_wide_tbl,p_dec_filled_wide_tbl,by='date')\ndata_wide_tbl <- right_join(data_wide_tbl,t_dec_filled_wide_tbl,by='date')\n# Add period identifiers (decades in this case)\ns <- data_wide_tbl$date %>% first()\ne <- data_wide_tbl$date %>% last()\ndecs <- decadeMaker(s,e,'end')\ndecs <- decs %>% rename(per=dec)\ndata_wide_tbl <- data_wide_tbl %>% left_join(decs,by='date')\n# Creating long form\ndata_long_tbl <- data_wide_tbl %>% \n  pivot_longer(-date)\n# Cross checking completeness of record\ndata_long_tbl %>% \n  group_by(name) %>% \n  summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100)\n\n# A tibble: 9 × 3\n  name    n.na n.na.perc\n  <chr>  <int>     <dbl>\n1 P38462     0         0\n2 P38464     0         0\n3 P38471     0         0\n4 per        0         0\n5 Q16279     0         0\n6 Q16290     0         0\n7 Q16924     0         0\n8 T38462     0         0\n9 T38471     0         0\n\n\nA consistent data record from 1933 until and including November 2015 is now prepared^Please note that by using left_join above, we have cut off discharge data from the year 1932 since we do not have meteorological data there.^. Let us analyze these data now."
  },
  {
    "objectID": "modeling_using_predictive_inference.html#sec-em-data-analysis",
    "href": "modeling_using_predictive_inference.html#sec-em-data-analysis",
    "title": "",
    "section": "Data Anlysis",
    "text": "Data Anlysis\nIn this Section, the goal is to explore and understand the available time series data and their relationships and to take the necessary steps towards feature engineering. Features are predictors that we want to include in our forecasting models that are powerful in the sense that they help to improve the quality of forecasts in a significant manner. Sometimes, the modeler also wants to include synthetic features, i.e. predictors that are not observed but for example derived from observations.\nDifferent techniques are demonstrated that allow us to get familiar with the data that we are using. While we are interested to model discharge of Chatkal and Pskem rivers, it should be emphasized that all the techniques utilized for forecasting easily carry over to other rivers and settings.\nLet us start with a visualisation of the complete data record. Using timetk::plot_time_series and groups, we can plot all data into separate, individual facets as shown in Figure @ref(fig:completeDataRecord).\n\ndata_long_tbl %>% \n  group_by(name) %>% \n  plot_time_series(date, value,\n                   .smooth      = FALSE,\n                   .interactive = FALSE,\n                   .facet_ncol  = 2,\n                   .title       = \"\"\n                   )\n\n\n\nFigure 11: Complete Data hydro-meteorological record for the zone of runoff formation in the Chirchik river basin.\n\n\n\n\nData Transformation\nIt is interesting to observe that discharge values range over 2 - 3 orders of magnitude between minimum and maximum flow regimes. As can be seen in Figure 12, discharge and precipitation data are heavily skewed. When this is the case, it is generally advisable to consider data transformations as they help to improve predictive modeling accuracy of regression models.\n\ndata_long_tbl %>% \n  group_by(name)  %>%\n  ggplot(aes(x=value,colour = name)) +\n  geom_histogram(bins=50) +\n  facet_wrap(~name, scales = \"free\") + \n  theme(legend.position = \"none\")\n\n\n\nFigure 12: Histograms of available raw data.\n\n\n\n\nLet us for example look at a very simple uniform non-parametric transformation, i.e. a logarithmic transformation (see Figure 13). As compared to parametric transformation, the logarithmic transformation is simple to apply for data greater than zero and does not require us to keep track of transformation parameters as, for example, is the case when we center and scale the data.\n\ndata_wide_tbl %>% \n  mutate(across(Q16279:P38471,.fns = log1p)) %>% # transforms  discharge and precipitation time series\n  pivot_longer(-date) %>% \n  ggplot(aes(x=value,colour = name)) +\n  geom_histogram(bins=50) +\n  facet_wrap(~name, scales = \"free\") + \n  theme(legend.position = \"none\")\n\n\n\nFigure 13: Histograms of transformed discharge and precipitation data together with the raw temperature data.\n\n\n\n\nPlease note that with the base-R command log1p, 1 is added prior to the logarithmic transformation to avoid cases where the transformed values would not be defined, i.e. where discharge or precipitation is 0. More information about the log1p() function can be obtained by simply typing ?log1p. Recovering the original data after the log1p transformation is simply achieved by taking the exponent of the transformed data and subtracting 1 from the result. The corresponding R function is expm1().\nClearly, the log-transformed discharge values are no longer skewed (Figure @ref(fig:histogramsData_transformed)). We now see interesting bimodal distributions. At the same time, the variance of the transformed variables is greatly reduced. These are two properties that will help us construct a good model as we shall see below. Finally, the transformed discharge time series are shown in Figure @ref().\n\ndata_long_tbl %>% \n  filter(name=='Q16279' | name=='Q16290') %>% \n  plot_time_series(date, log(value+1),\n                   .facet_vars  = name,\n                   .smooth      = FALSE,\n                   .interactive = FALSE,\n                   .title       = \"\",\n                   .y_lab       = \"[-]\",\n                   .x_lab       = \"year\"\n                   )\n\n\n\nFigure 14: log1p() transformed discharge data.\n\n\n\n\nDetecting Trends\nLower frequency variability in time series, including trends, can be visualized by using the .smooth = TRUE option in the plot_time_series() function. To demonstrate this here, we have a closer look at the temperature data in our data record as shown in Figure 15.\n\ndata_long_tbl %>% \n  filter(name == 'T38462' | name == 'T38471') %>% \n  plot_time_series(date, value, \n                   .smooth     = TRUE,\n                   .facet_vars = name,\n                   .title      = \"\",\n                   .y_lab      = \"deg. C.\",\n                   .x_lab      = \"year\"\n                   )\n\n\nFigure 15: Temperature time series and trends.\n\n\n\nIn both time series, a slight upward trend is visible that picks up over the most recent decades. We can look at these trends in greater detail, for example at monthly levels as shown in Figure 16.\n\ndata_long_tbl %>% \n  filter(name == 'T38462') %>% \n  summarise_by_time(.date_var = date, .by=\"month\",value=mean(value)) %>% \n  tk_ts(frequency = 12) %>% \n  forecast::ggsubseriesplot(year.labels = FALSE) + \n              geom_smooth(method = \"lm\",color=\"red\") +\n              #ggtitle('Development of Monthly Mean Temperatures from 1933 - 2015 at Station 38462') +\n              xlab('month') +\n              ylab('Degrees Celsius')\n\n\n\nFigure 16: Sample development of Monthly Mean Temperatures from 1933 - 2015 at Station 38462.\n\n\n\n\nIn the Figure above, a significant winter warming over the period of data availability is confirmed at Pskem meteorological station. As discussed in the background Chapters on the Central Asia Hydrological Systems and the Example River Basins, these trends are observed throughout the Central Asian region and are an indication of the changing climate there. We will have to take into account such type of trends in our modeling approach.\nAuto- and Cross-Correlations\nA time series may have relationships to previous versions of itself - these are the ‘lags’. The autocorrelation is a measure of the strength of this relationship of a series to its lags. The autocorrelation function ACF looks at all possible correlations between observation at different times and how they emerge. Contrary to that, the partial autocorrelation function PACF only looks at the correlation between a particular past observation and the current one. So in other words, ACF includes direct and indirect effects whereas PACF only includes the direct effects between observations at time t and the lag. As we shall see below, PACF is super powerful to identify relevant lagged time series predictors in autoregressive models (AR Models).\nFigure 17 shows the ACF and PACF over the interval of 72 lags (2 years). The AC function shows the highly seasonal characteristics of the underlying time series. It also shows the pronounced short-term memory in the basin, i.e. the tendency to observe subsequent values of high flows and subsequent values of low flow - hence the smoothness of the curve. This time of autocorrelation behavior is typical for basins with large surface storage in the form of lakes, swamps, snow and glaciers, permafrost and groundwater reserves (A. 2019). The Chatkal river basin certainly belongs to that category.\n\ndata_long_tbl %>% filter(name == 'Q16279') %>% \n  plot_acf_diagnostics(date, value,\n                      .show_white_noise_bars = TRUE,\n                      .lags = 72,\n                      .title = \"\"\n                      )\n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nPlease use `gather()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\n\nFigure 17: Autocorrelation function (ACF) and partial autocorrelation function (PACF) are shown for the discharge time series at station 16279.\n\n\n\nBut is there also autocorrelation of the annual time series? Let us test.\n\nQ16279_annual <- data_long_tbl %>% filter(name == 'Q16279') %>% dplyr::select(-name) %>% \n  summarize_by_time(.date_var = date,\n                    .by=\"year\",\n                    sum=sum(value)*3600*24*10/10^9) \n\nQ16279_annual %>% plot_time_series(date,sum,\n                                   .smooth = FALSE,\n                                   .title = \"\",\n                                   .x_lab = \"year\",\n                                   .y_lab = \"Discharge [cubic km per year]\")\n\n\nFigure 18: Testing autocorrelation at annual scales for discharge at station 16279.\n\n\nQ16279_annual %>% \n  plot_acf_diagnostics(.date_var = date,\n                       .value = sum,\n                       .lags = 50,\n                       .show_white_noise_bars = TRUE,\n                       .title = \"\",\n                       .x_lab = \"year\")\n\n\nFigure 19: Testing autocorrelation at annual scales for discharge at station 16279.\n\n\n\nThe ?@fig-annual-auto-corr shows a fast decaying autocorrelation function for the annualized time series where even lag 1 values are no longer correlated in a significant manner.\nThe PAC function, on the other hand, demonstrates that lag 1 is really critical in terms of direct effects (Figure 17). After that, the PACF tapers off quickly. To utilize these findings in our modeling approach that uses lagged regression is important, as we shall see below.\nWe can also study cross-correlations between two different time series. In other words, in cross-correlation analysis between two different time series, we estimate the correlation one variable and another, time-shifted variable. For example, we can cross-correlate discharge at Gauge 16279 (Chatkal river) to discharge at Gauge 16290 (Pskem River) as shown in (fig_cross_corr_q?). As is easily visible, the discharge behavior of the two rivers is highly correlated.\n\ndata_wide_tbl %>% plot_acf_diagnostics(date,Q16279,\n                                       .ccf_vars = Q16290,\n                                       .show_ccf_vars_only = TRUE,\n                                       .show_white_noise_bars = TRUE,\n                                       .lags = 72,\n                                       .title = \"\"\n                                       )\n\n\nCross-correlation analysis of the two discharge time series Q16279 and Q16290.\n\n\nConverse to this, discharge shows a lagged response to temperature which is clearly visible in the cross-correlation function.\n\ndata_wide_tbl %>% plot_acf_diagnostics(date,T38471,\n                                       .ccf_vars = Q16279,\n                                       .show_ccf_vars_only = TRUE,\n                                       .show_white_noise_bars = TRUE,\n                                       .lags = 72,\n                                       .title = \"\"\n                                       )\n\n\nFigure 20: Cross-correlation between temperature at station 38471 and the discharge at station 16279.\n\n\n\nA less pronounced cross-correlation exists between precipitation and discharge when measured at the same stations (Figure @ref(ccf_PQ)).\n\ndata_wide_tbl %>% plot_acf_diagnostics(date,P38471,\n                                       .ccf_vars = Q16279,\n                                       .show_ccf_vars_only = TRUE,\n                                       .show_white_noise_bars = TRUE,\n                                       .lags = 72,\n                                       .title = \"\"\n                                       )\n\n\nFigure 21: Cross-correlation between temperature at station 38471 and the discharge at station 16279.\n\n\n\nTime Series Seasonality\nThere is a pronounced seasonality in the discharge characteristics of Central Asian rivers. One of the key reason of this is the annual melt process of the winter snow pack. Figure 22 shows the seasonality of the log-transformed discharge. These observations can help in investigating and detecting time-based (calendar) features that have cyclic or trend effects.\n\ndata_long_tbl %>% \n  filter(name==\"Q16279\" | name==\"Q16290\") %>% \n  plot_seasonal_diagnostics(date,\n                            log(value+1),\n                            .facet_vars = name, \n                            .feature_set = c(\"week\",\"month.lbl\",\"year\"),\n                            .interactive = FALSE,\n                            .title = \"\"\n                            )\n\n\n\nFigure 22: Seasonal diagnostics of log1p discharge. Weekly (top row), monthly (middle row) and yearly diagnostics (bottom row) are shown for the two discharge time series.\n\n\n\n\nFigure 23 shows the seasonal diagnostic for the log-transformed precipitation time series. The significant interannual variability is visible. In the annualized time series, no trend is available.\n\ndata_long_tbl %>% \n  filter(name==\"P38462\" | name==\"P38464\" | name==\"P38471\") %>% \n  plot_seasonal_diagnostics(date,\n                            log1p(value),\n                            .facet_vars = name, \n                            .feature_set = c(\"week\",\"month.lbl\",\"year\"),\n                            .interactive = FALSE,\n                            .title = \"\"\n                            )\n\n\n\nFigure 23: Seasonal Diagnostics of precipitation Weekly (top row), monthly (middle row) and yearly diagnostics (bottom row) are shown for the available precipitation data in the zone of runoff formation of the two tributary rivers.\n\n\n\n\nFinally, Figure 24 displays the seasonal diagnostics of the temperature time series. Notice that we use untransformed, raw values here for plotting.\n\ndata_long_tbl %>% \n  filter(name==\"T38462\" | name==\"T38471\") %>% \n  plot_seasonal_diagnostics(date,\n                            value,\n                            .facet_vars = name, \n                            .feature_set = c(\"week\",\"month.lbl\",\"year\"),\n                            .interactive = FALSE,\n                            .title = \"\",\n                            )\n\n\n\nFigure 24: Seasonal Diagnostics of temperature Weekly (top row), monthly (middle row) and yearly diagnostics (bottom row) are shown for the available temperature data in the zone of runoff formation of the two tributary rivers."
  },
  {
    "objectID": "modeling_using_predictive_inference.html#sec-em-feature-engineering",
    "href": "modeling_using_predictive_inference.html#sec-em-feature-engineering",
    "title": "",
    "section": "Investigating and Engineering Predictors",
    "text": "Investigating and Engineering Predictors\nAll the data that we have available have been analyzed by now and we can now move to generating a good and solid understanding of the relevance of predictors for statistical modeling. To start with, we will keep things deliberately simple. Our approach is tailored to the particular local circumstances and the needs and wants of the hydrometeorological agencies that are using such types of model to issue high quality forecasts.\nFirst, the plan here to start with the introduction and discussion of the current forecasting techniques that are used operationally inside the Kyrgyz Hydrometeorological agency. These models and their forecast quality will serve as benchmark to beat any of the other models introduced here. At the same time, we will introduce a measure with which to judge forecast quality.\nSecondly, we evaluate the simplest linear models using time series regression. This will also help to introduce and explain key concepts that will be discussed in the third and final section below.\nFinally, we show the application of more advanced forecasting modeling techniques that use state-of-the-art regression type algorithms.\nThe forecasting techniques will be demonstrated by focussing on the Pskem river. The techniques extend to other rivers in the region and beyond in a straight forward manner.\nBenchmark: Current Operational Forecasting Models in the Hydrometeorological Agencies\nMore information to come. Check back soon here.\nTime Series Regression Models\nThe simplest linear regression model can be written as\n\\[\ny_{t} = \\beta_{0} + \\beta_{1} x_{t} + \\epsilon_{t}\n\\]\nwhere the coefficient \\(\\beta_{0}\\) is the intercept term, \\(\\beta_{1}\\) is the slope and \\(\\epsilon_{t}\\) is the error term. The subscripts \\(t\\) denote the time dependency of the target and the explanatory variables and the error. \\(y_{t}\\) is our target variable, i.e. discharge in our case, that we want to forecast. At the same time, \\(x_{t}\\) is an explanatory variable that is already observed at time \\(t\\) and that we can use for prediction.\nAs we shall see below, we are not limited to the inclusion of only one explanatory variable but can think of adding multiple variables that we suspect to help improve forecast modeling performance.\nTo demonstrate the effects of different explanatory variables on our forcasting target and the quality of our model for forecasting discharge at stations 16290, the function plot_time_series_regression from the timetk package is used. First, we we only want to specify a model with a trend over the time \\(t\\). Hence, we fit the model\n\\[\ny_{t} = \\beta_{0} + \\beta_{1} t + \\epsilon_{t}\n\\]\n\nmodel_formula <- as.formula(log1p(Q16290) ~ \n                              as.numeric(date)\n                            )\n\nmodel_data <- data_wide_tbl %>% dplyr::select(date,Q16290)\n\nmodel_data %>% \n  plot_time_series_regression(\n    .date_var = date,\n    .formula = model_formula,\n    .show_summary = TRUE,\n    .title = \"\"\n  )\n\n\nCall:\nstats::lm(formula = .formula, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3940 -0.7068 -0.2114  0.7193  2.0274 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       4.056e+00  1.489e-02  272.39   <2e-16 ***\nas.numeric(date) -2.997e-06  1.675e-06   -1.79   0.0736 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7998 on 2983 degrees of freedom\nMultiple R-squared:  0.001073,  Adjusted R-squared:  0.0007378 \nF-statistic: 3.203 on 1 and 2983 DF,  p-value: 0.07359\n\n\n\nFigure 25: Linear regression trend model.\n\n\n\nNote that the timetk::plot_time_series function is a convenience wrapper to make our lives easy in terms of modeling and immediately getting a resulting plot. The same model could be specified in the traditional R-way, i.e. as follows\n\nmodel_data %>% \n  lm(formula = model_formula) %>% \n  summary()\n\n\nCall:\nlm(formula = model_formula, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3940 -0.7068 -0.2114  0.7193  2.0274 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       4.056e+00  1.489e-02  272.39   <2e-16 ***\nas.numeric(date) -2.997e-06  1.675e-06   -1.79   0.0736 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7998 on 2983 degrees of freedom\nMultiple R-squared:  0.001073,  Adjusted R-squared:  0.0007378 \nF-statistic: 3.203 on 1 and 2983 DF,  p-value: 0.07359\n\n\nThe adjusted R-squared shows the mediocre performance of our simple model as it cannot capture any of the seasonal variability. Furthermore we see that the trend coefficient is negative which indicates a decrease in mean discharge. However, as the p-value confirms, the trend is only significant at the 0.1 level.\nThe first step in improving our model is to account for seasonality. In the case of decadal time series, we can add categorical variables (as factor variables) decoding the corresponding decades. Similarly, in the case of monthly data, we could use month names or factors 1..12 to achieve the same. The same reasoning extends to other periods (quarters, weekdays, etc.). We will use a quarterly model to explain the concept since the inclusion of 4 indicator variables for the individual quarters is easier to grasp than to work with 36 decadal indicators.\n\n# Computing quarterly mean discharge values\nq16290_quarter_tbl <- model_data %>% \n  summarize_by_time(date,value=mean(log1p(Q16290)),.by = \"quarter\")\n# adding quarters identifier\nq16290_quarter_tbl <- q16290_quarter_tbl %>% \n  mutate(per = quarter(date) %>% as.factor())\n\nmodel_formula <- as.formula(value ~ \n                              as.numeric(date) + \n                              per\n                            )\nq16290_quarter_tbl %>% \n  plot_time_series_regression(date,\n                              .formula = model_formula,\n                              .show_summary = TRUE,\n                              .title = \"\"\n                              )\n\n\nCall:\nstats::lm(formula = .formula, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.51477 -0.14936  0.00228  0.14410  0.66161 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       3.255e+00  2.204e-02 147.691  < 2e-16 ***\nas.numeric(date) -3.158e-06  1.255e-06  -2.516   0.0123 *  \nper2              1.551e+00  3.106e-02  49.919  < 2e-16 ***\nper3              1.396e+00  3.107e-02  44.938  < 2e-16 ***\nper4              2.562e-01  3.107e-02   8.248 3.98e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2001 on 327 degrees of freedom\nMultiple R-squared:  0.9217,    Adjusted R-squared:  0.9207 \nF-statistic: 962.4 on 4 and 327 DF,  p-value: < 2.2e-16\n\n\n\nFigure 26: Example quarterly linear model with trend and seasonality.\n\n\n\nWhat we did here is to compare a continuous variable, i.e. the discharge, across 4 categories. Hence, we can write down the model in the following way:\n\\[\ny_{t} = \\beta_{0} + \\beta_{1} \\delta_{t}^{Qtr2} + \\beta_{2} \\delta_{t}^{Qtr3} + \\beta_{3} \\delta_{t}^{Qtr4} + \\epsilon_{t}\n\\]\nUsing ‘one hot encoding’, we include only N-1 (here, 3) variables out of the N (here,4) in the regression because we can safely assume that if we are in Quarter 4, all the other indicator variables are simply 0. If we are in quarter 1 (Qtr1), the model would just be\n\\[\ny_{t} = \\beta_{0} + \\epsilon_{t}\n\\]\nIf we are in Quarter 2 (Qtr2), the model would be\n\\[\ny_{t} = \\beta_{0} + \\beta_{1} + \\epsilon_{t}\n\\]\nsince \\(\\delta_{t}^{Qtr2} = 1\\). Hence, whereas \\(\\beta_{0}\\) is to be interpreted as the estimated mean discharge in Quarter 1 (called (Intercept) in the results table below), \\(\\beta_{1}\\) (called qtr2 in the results table below) is the estimated difference of mean discharge between the two categories/quarters We can get the values and confidence intervals of the estimates easily in the following way\n\nlm_quarterlyModel <- q16290_quarter_tbl %>% \n  lm(formula = model_formula)\n\nmeanQtrEstimates <- lm_quarterlyModel %>% coefficients()\nmeanQtrEstimates %>% expm1()\n\n     (Intercept) as.numeric(date)             per2             per3 \n    2.493125e+01    -3.158103e-06     3.714894e+00     3.039112e+00 \n            per4 \n    2.920533e-01 \n\nlm_quarterlyModel %>% confint() %>% expm1()\n\n                         2.5 %        97.5 %\n(Intercept)       2.383084e+01  2.608043e+01\nas.numeric(date) -5.627148e-06 -6.890525e-07\nper2              3.435387e+00  4.012016e+00\nper3              2.799662e+00  3.293653e+00\nper4              2.154539e-01  3.734800e-01\n\n\nThe same reasoning holds true for the model with decadal observations to which we return now again. First, we add decades as factors to our data_wide_tbl.\n\n\n\n\n\n\n\n\n\nNow, we can specify and calculate the new model.\n\nmodel_formula <- as.formula(log1p(Q16290) ~ \n                              as.numeric(date) + # trend components\n                              per # seasonality (as.factor)\n                            )\n\nmodel_data <- data_wide_tbl %>% dplyr::select(date,Q16290,per)\n\nmodel_data %>% \n  plot_time_series_regression(\n    .date_var = date,\n    .formula = model_formula,\n    .show_summary = TRUE,\n    .title = \"\"\n  )\n\n\nCall:\nstats::lm(formula = .formula, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3225 -0.6972 -0.2318  0.7268  2.0364 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       3.943e+00  2.991e-02 131.827  < 2e-16 ***\nas.numeric(date) -3.065e-06  1.670e-06  -1.836   0.0665 .  \nper               6.150e-03  1.406e-03   4.374 1.26e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7974 on 2982 degrees of freedom\nMultiple R-squared:  0.00744,   Adjusted R-squared:  0.006774 \nF-statistic: 11.18 on 2 and 2982 DF,  p-value: 1.46e-05\n\n\n\nFigure 27: Decadal linear regression model with trend and seasonality.\n\n\n\nWhat we see is that through the inclusion of the categorical decade variables, we have greatly improved our modeling results since we can now capture the seasonality very well (Tip: zoom into the time series to compare highs and lows and their timing for the target variable and its forecast). However, despite the excellent adjusted R-squared value of 0.9117, our model is far from perfect as it is not able to account for inter-annual variability in any way.\nLet us quickly glance at the errors.\n\n#} fig-cap: \"Scatterplot of observed versus calculated values.\"\nlm_decadalModel <- model_data %>% \n  lm(formula = model_formula)\n\nobs_pred_wide_tbl <- model_data%>% \n  mutate(pred_Q16290 = predict(lm_decadalModel) %>% expm1()) %>% \n  mutate(error = Q16290 - pred_Q16290)\n\nggplot(obs_pred_wide_tbl, aes(x      = Q16290,\n                           y         = pred_Q16290,\n                           colour    = per )) +\n  geom_point() + \n  geom_abline(intercept = 0, slope = 1)\n\n\n\nFigure 28: ?(caption)\n\n\n\n\nWe do not seem to make a systematic error as also confirmed by inspecting the histogram or errors (they are nicely centered around 0).\n\nggplot(obs_pred_wide_tbl,aes(x=error)) +\n  geom_histogram(bins=100)\n\n\n\n\nIn Section 1.5.3 above, we saw that the PAC function is very high at lag 1. We exploit this fact be incorporating in the regression equation the observed previous discharge, i.e. \\(y_{t-1}\\) at time \\(t-1\\) to predict discharge at time \\(t\\). Hence, our regression can be written as\n\\[\ny_{t} = \\beta_{0} + \\beta_{1} t + \\beta_{2} y_{t-1} + \\sum_{j=2}^{36} \\beta_{j} \\delta_{t}^{j} + \\epsilon_{t}\n\\]\nwhere the \\(\\delta_t^{j}\\) correspondingly are the 35 indicator variables as discussed above in the case of quarterly time series where we had 3 of these variables included. Before we can estimate this model, we prepare a tibble with the relevant data as shown in the table below (note that we simply renamed the discharge column to Q out of convenience).\n\nmodel_data <- data_wide_tbl %>% \n  dplyr::select(date,Q16290,per) %>% \n  rename(Q=Q16290) %>% \n  mutate(Q = log1p(Q)) %>% \n  mutate(Q_lag1 = lag(Q,1))\nmodel_data\n\n# A tibble: 2,985 × 4\n   date           Q   per Q_lag1\n   <date>     <dbl> <int>  <dbl>\n 1 1933-01-10  3.37     1  NA   \n 2 1933-01-20  3.23     2   3.37\n 3 1933-01-31  3.17     3   3.23\n 4 1933-02-10  3.21     4   3.17\n 5 1933-02-20  3.26     5   3.21\n 6 1933-02-28  3.28     6   3.26\n 7 1933-03-10  3.30     7   3.28\n 8 1933-03-20  3.53     8   3.30\n 9 1933-03-31  3.57     9   3.53\n10 1933-04-10  3.92    10   3.57\n# … with 2,975 more rows\n\n\nNotice that to accommodate the \\(y_{t-1}\\) in the data, we simply add a column that contains a lagged version of the discharge time series itself (see column Q_lag1). Now, for example, for our regression we have a first complete set of data points on \\(t = '1933-01-20'\\), with \\(Q=3.226844\\), \\(dec=2\\) and \\(Q_{lag1}=3.374169\\). Notice how the last value corresponds to the previously observed and now known \\(y_{t-1}\\).\n\n# Specification of the model formula\nmodel_formula <- as.formula(Q ~ as.numeric(date) + per + Q_lag1)\n# Note that we use na.omit() to delete incomplete data records, ie. the first observation where we lack the lagged value of the discharge. \nmodel_data %>%  na.omit() %>% lm(formula = model_formula) %>% summary()\n\n\nCall:\nlm(formula = model_formula, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09294 -0.12965 -0.03140  0.07656  1.17980 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       1.955e-01  1.801e-02  10.850   <2e-16 ***\nas.numeric(date) -6.402e-08  3.925e-07  -0.163     0.87    \nper              -7.010e-03  3.355e-04 -20.897   <2e-16 ***\nQ_lag1            9.838e-01  4.353e-03 225.992   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1873 on 2980 degrees of freedom\nMultiple R-squared:  0.9453,    Adjusted R-squared:  0.9452 \nF-statistic: 1.716e+04 on 3 and 2980 DF,  p-value: < 2.2e-16\n\n\nIt looks like we have made a decisive step in the right direction by incorporating the previously observed discharge value. Also, notice that some of the decade factors have lost their statistical significance meaning that the seasonality can now be captured in part also by the lagged version of the time series.\nLet us visualize the results quickly (tip: also zoom in to explore the fit).\n\nmodel_data %>% \n  na.omit() %>% \n  plot_time_series_regression(\n  .date_var         = date,\n  .formula          = model_formula,\n  .show_summary     = FALSE, # We do show the summary since we have plotted the summary output already above.\n  .title            = \"\"\n)\n\n\nFigure 29: Linear regression model results with trend, seasonality and lag 1 predictors.\n\n\n\nThis is clearly an astonishing result. Nevertheless, we should keep a couple of things in mind:\n\nWhat about the rate of change of the discharge and the acceleration of discharge? Would the incorporation of these features help to improve the model?\nWe have not assess the quality of the forecasts using the stringent quality criteria as they exist in the Central Asian Hydrometeorological Services. How does our forecast perform under this criteria?\nDoes the incorporation of precipitation and temperature data help to improve our forecast skills?\nWe did not test our model on out-of-sample data. Maybe our model does not generalize well? We will discuss these and related issues soon when using more advanced models but for the time being declare this a benchmark model due to its simplicity and predictive power.\n\nWe will work on these questions now and focus first on the incorporation of the rate of change in discharge and the acceleration of discharge over time. First, we add \\(Q_{lag2}\\) to our model data and then compute change and acceleration accordingly.\n\nmodel_data <- model_data %>% \n  mutate(Q_lag2 = lag(Q,2)) %>% \n  mutate(change = Q_lag1 -Q_lag2) %>% # that is the speed of change in discharge\n  mutate(change_lag1 = lag(change,1)) %>% \n  mutate(acc = change - change_lag1) %>% na.omit() # and that is the acceleration of discharge\nmodel_data\n\n# A tibble: 2,982 × 8\n   date           Q   per Q_lag1 Q_lag2  change change_lag1      acc\n   <date>     <dbl> <int>  <dbl>  <dbl>   <dbl>       <dbl>    <dbl>\n 1 1933-02-10  3.21     4   3.17   3.23 -0.0614     -0.147   0.0860 \n 2 1933-02-20  3.26     5   3.21   3.17  0.0413     -0.0614  0.103  \n 3 1933-02-28  3.28     6   3.26   3.21  0.0551      0.0413  0.0138 \n 4 1933-03-10  3.30     7   3.28   3.26  0.0152      0.0551 -0.0399 \n 5 1933-03-20  3.53     8   3.30   3.28  0.0187      0.0152  0.00348\n 6 1933-03-31  3.57     9   3.53   3.30  0.231       0.0187  0.212  \n 7 1933-04-10  3.92    10   3.57   3.53  0.0432      0.231  -0.187  \n 8 1933-04-20  4.05    11   3.92   3.57  0.348       0.0432  0.305  \n 9 1933-04-30  4.47    12   4.05   3.92  0.132       0.348  -0.216  \n10 1933-05-10  4.88    13   4.47   4.05  0.416       0.132   0.284  \n# … with 2,972 more rows\n\n# Specification of the model formula\nmodel_formula <- as.formula(Q ~ as.numeric(date) + per + Q_lag1 + change + acc)\nmodel_data %>% na.omit() %>% \n  plot_time_series_regression(\n  .date_var = date,\n  .formula = model_formula,\n  .show_summary = TRUE, # We do show the summary since we have plotted the summary output already above.\n  .title = \"\"\n)\n\n\nCall:\nstats::lm(formula = .formula, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.01892 -0.09223 -0.02294  0.05501  1.16531 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       2.574e-01  1.626e-02  15.830   <2e-16 ***\nas.numeric(date) -1.834e-07  3.480e-07  -0.527    0.598    \nper              -2.859e-03  3.318e-04  -8.616   <2e-16 ***\nQ_lag1            9.496e-01  4.087e-03 232.331   <2e-16 ***\nchange            5.650e-01  2.002e-02  28.221   <2e-16 ***\nacc              -2.215e-01  1.806e-02 -12.265   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1658 on 2976 degrees of freedom\nMultiple R-squared:  0.9571,    Adjusted R-squared:  0.957 \nF-statistic: 1.328e+04 on 5 and 2976 DF,  p-value: < 2.2e-16\n\n\n\n\n\nmodel <- model_data %>% lm(formula=model_formula)\n\nmodel %>% summary()\n\n\nCall:\nlm(formula = model_formula, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.01892 -0.09223 -0.02294  0.05501  1.16531 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       2.574e-01  1.626e-02  15.830   <2e-16 ***\nas.numeric(date) -1.834e-07  3.480e-07  -0.527    0.598    \nper              -2.859e-03  3.318e-04  -8.616   <2e-16 ***\nQ_lag1            9.496e-01  4.087e-03 232.331   <2e-16 ***\nchange            5.650e-01  2.002e-02  28.221   <2e-16 ***\nacc              -2.215e-01  1.806e-02 -12.265   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1658 on 2976 degrees of freedom\nMultiple R-squared:  0.9571,    Adjusted R-squared:  0.957 \nF-statistic: 1.328e+04 on 5 and 2976 DF,  p-value: < 2.2e-16\n\n# Here, we add the the prediction to our tibble so that we can assess model predictive quality later.\nmodel_fc_wide_tbl <- model_data %>% \n  mutate(pred = predict(model)) %>% \n  mutate(obs = expm1(Q), pred = expm1(pred)) %>% \n  dplyr::select(date,obs,pred,per)\nmodel_fc_wide_tbl\n\n# A tibble: 2,982 × 4\n   date         obs  pred   per\n   <date>     <dbl> <dbl> <int>\n 1 1933-02-10  23.7  23.6     4\n 2 1933-02-20  25.1  25.9     5\n 3 1933-02-28  25.5  28.0     6\n 4 1933-03-10  26    28.1     7\n 5 1933-03-20  33    28.3     8\n 6 1933-03-31  34.5  38.1     9\n 7 1933-04-10  49.3  38.9    10\n 8 1933-04-20  56.4  58.0    11\n 9 1933-04-30  86    65.3    12\n10 1933-05-10 130   102.     13\n# … with 2,972 more rows\n\n\nAnother, albeit small improvement in the forecast of predicting discharge 1-step ahead! It is now time to properly gauge the quality of this seemingly excellent model. Does it conform to local quality standards that apply to decadal forecasts? The Figure 30 shows the un-transformed data. We see that we are not doing so well during the summer peak flows. As we shall see further below, these are the notoriously hard to predict values, even just for 1-step ahead decadal predictions.\n\nmodel_fc_wide_tbl %>% \n  dplyr::select(-per) %>% \n  pivot_longer(-date) %>% \n  plot_time_series(date,\n                   value,\n                   name,\n                   .smooth = F,\n                   .title = \"\")\n\n\nFigure 30: Forecast model quality assessment.\n\n\n\nAssessing the Quality of Forecasts\nHow well are we doing with our simple linear model? Let us assess the model quality using the local practices. For the Central Asian Hydromets, a forecast at a particular decade \\(d\\) is considered to be excellent if the following holds true\n\\[\n|Q_{obs}(d,y) - Q_{pred}(d,y)| \\le 0.674 \\cdot \\sigma[\\Delta Q(d)]\n\\]\nwhere \\(Q_{obs}(d,y)\\) is the observed discharge at decade \\(d\\) and year \\(d\\), \\(Q_{pred}(d,y)\\) is the predicted discharge at decade \\(d\\) and year \\(y\\), \\(|Q_{obs}(d) - Q_{pred}(d)|\\) thus the absolute error and \\(\\sigma[\\Delta Q(d)] = \\sigma[Q(d) - Q(d-1)]\\) is the standard deviation of the difference of decadal observations at decade \\(d\\) and \\(d-1\\) over the entire observation record (hence, the year indicator \\(y\\) is omitted there). The equation above can be reformulated to\n\\[\n\\frac{|Q_{obs}(d,y) - Q_{pred}(d,y)|}{\\sigma[\\Delta Q(d)]} \\le 0.674\n\\]\nSo let us assess the forecast performance over the entire record using the `riversCentralAsia::assess_fc_qual`` function. Note that the function returns a list of three objects. First, it returns a tibble of the number of forecasts that are of acceptable quality for the corresponding period (i.e. decade or month) as a percentage of the total number of observations that are available for that particular period. Second, it returns the period-averaged mean and third a figure that shows forecast quality in two panels.\nSo, for our model which we consiedered to be performing well above, we get the following performance specs\n\nplot01 <- TRUE \nte <- assess_fc_qual(model_fc_wide_tbl,plot01)\nte[[3]]\n\n\n\nFigure 31: Benchamrk model performance assessment.\n\n\n\n\nIn other words, roughly two thirds of our in-sample forecasts comply will be considered good enough when measured according to the quality criterion. Furthermore, the model performs worse than average during the second quarter (Q2) decades, i.e. from decade 10 through 17. This is an indication that providing good forecasts in Q2 might be hard.\nIt is, however, generally not considered to be good practice to assess model quality on in-sample data. Rather, model performance should be assessed on out-of-sample data that was not used for model training and is thus data that is entirely unseen.\nGenerating and Assessing Out-of-Sample Forecasts\n\n\n\n\n\n\nWarning\n\n\n\nNote, this Section is work in progress. Please check back later!\n\n\nWe start off with our model_data tibble and want to divide it into two sets, one for model training and one for model testing. We refer to these sets as training set and test set. For the creation of these sets, we can use the timetk::time_series_split() function.\nMachine Learning Models\n\n\n\n\n\n\nWarning\n\n\n\nNote, this Section is work in progress. Please check back later!"
  },
  {
    "objectID": "modeling_using_predictive_inference.html#sec-modeling-using-predictive-inference-references",
    "href": "modeling_using_predictive_inference.html#sec-modeling-using-predictive-inference-references",
    "title": "",
    "section": "References",
    "text": "References\n\n\n\n\nA., Cancelliere. 2019. “Statistical Analysis of Hydrologic Variables.” In, edited by Stedinger Teegavarapu R. S. V. Salas J. D., 203–29. ASCE."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "",
    "section": "Welcome",
    "text": "Welcome\nThis handbook on hydrological modeling of Central Asian river basins is geared towards young water professionals in Central Asia. They inherit fascinatingly complex natural and man-made hydrological systems. They face work where opportunities for modernization abound after decades of limited investments in the water sectors of the countries and where continuous population growth and a changing climate pose emerging challenges. At the same time, they face work in a field that has enabled the region to prosper and flourish over hundreds if not thousands of years.\nThe authors hope that this easily online translatable textbook provides a source of inspiration for these students and that the text and the methods presented will also be used by teachers and integrated in university curricula locally.\nThe book is dedicated to colleagues at the Central Asian Hydrometeorological Agencies whose tireless work in collecting and analyzing hydro-meteorological data in Central Asia has helped to significantly improve our understanding of the complex runoff generation processes at work in the region.\nThe authors are grateful for the support by the Global Water Programme of the Swiss Agency for Development and Cooperation who greatly helped to push the envelop further with regard to modern water education in the Central Asia region. Finally, Mr. Andrey Yakovlev and his tremendous knowledge of the region is acknowledged."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "",
    "section": "License",
    "text": "License\n\n\nDOI\n\n\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\n\nThe development of this book was supported by the Swiss Agency for Development and Cooperation."
  },
  {
    "objectID": "study_guide_materials.html#sec-study-guide",
    "href": "study_guide_materials.html#sec-study-guide",
    "title": "",
    "section": "Study Guide",
    "text": "Study Guide\nOver the duration of the course and as part of the Applied Modeling track, students are guided through implementing their own conceptual hydrological rainfall-runoff model of one of the Central Asian sample catchments that they can choose from the Case Studies Pack.\nStudents are required to work through the Chapters, including the occasional tasks that serve to deepen reflection on the course material and to do their daily homework assignments. As the final exam, the homework results are presented in a final student conference for which the students have to submit a conference abstract prior to the conference.\nThis Chapter explains how to use this course book.\nDifferent callout blocks appear throughout the text. These include Exercise, Tasks and Take Home Messages. Caution and Warning callouts highlight possibly problematic issues.\n\n\n\n\n\n\nEXERCISE\n\n\n\nExercise boxes are highlighted in blue color. With the description of the exercise, hints and a link to the solution are provided. Wherever they appear in the text, exercises should be completed before starting the next course chapter.\n\n\n\n\n\n\n\n\nTASK\n\n\n\n\n\n\n\n\n\n\n\n\nTAKE HOME MESSAGE\n\n\n\n\n\n\n\n\n\n\n\n\nCAUTION\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING\n\n\n\n\n\n\nCode blocks of R code with corresponding output are regularly shown throughout the text and look like this. Note that in grayed-out code cell, the code can be copied and the pasted into RStudio locally. Note that code blocks in Chapters are executed sequentially.\n\na <- 1 + 1\nprint(paste(\"a is set to\", a))\n\n[1] \"a is set to 2\""
  },
  {
    "objectID": "study_guide_materials.html#sec-materials",
    "href": "study_guide_materials.html#sec-materials",
    "title": "",
    "section": "Materials",
    "text": "Materials\nIn the highly intensive hydrological modeling course at GKU, students have to pass 4 GRADED EXERCISEs to be admitted to the final presentation in addidion to preparatory home work. The following section describes the daily course content as well as the homework and the GRADED EXERCISEs with links to the relevant supporting chapters in the course book. The descriptions of the GRADED EXERCISEs are highlighted with exercise boxes.\nDay 1: Introduction & Installation of Software\nRead Chapter 1: A short history of Water in Central Asia and Chapter 2: Hydrological Systems in Semi-Arid Central Asia in the course book. Then make sure the required software for this course is installed on your computer. Section Open-source resources of the Appendix includes installation instructions and the on-line learning material that can get you started with the software. Below is a quick summary:\n\n\nQGIS\n\nR\n\nRStudio\n\nRS Minerve\n\nIf you have not used the software above before we recommend the following resources to get your started (remember, more detailed instructions for most tasks are available in the Appendix):\n\n\nQGIS training manual\n\n\nModern Dive for getting started with R and RStudio\n\nRS Minerve User Manual\n\nInevitably, you will also perform a lot of geocomputations with R in the future. After all, a GIS system like QGIS is nothing more than a nicely packed bunch of geocomputation algorithms and a window for visualizing geospatial assets. Well, rest assured, all of this can be done inside R. It is recommended therefore that you also consult the following excellent online resource Geocomputation with R.\n\n\n\n\n\n\nHOMEWORK\n\n\n\nDay 1 involves a lot of preparatory homework:\n- Reading the introductory chapters linked above and\n- Downloading and installing the required software linked above.\nThe homework is not graded but completion is a requirement for being able to work through the course.\n\n\nDay 2: Hydrological modelling and processes\nDay 2 involves a continued introduction to the hydrological modelling process and a deepending of the understanding what hydrological models are used for as well as a first part on hydrological processes (the partitioning of rainfall, transfer of water through the hydrological compartments).\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\nRoleplay on model uses. Read the role play exercise. You will be assigned a role. With your study colleagues, discuss the questions and take notes (about 15 min). One person per group will briefly (1 min) present the answers to the questions.\nIn preparation of tomorrows lecture and GRADED EXERCISE: Read the chapter on the case studies of Central Asian river basins and on [Hydraulic-Hydrological Modeling] (#sec-hydraulic-hydrological-modeling).\n\nThe homework is not graded but supports reflection about the use of hydrological models and how to judge the quality of hydrological models on day 3.\n\n\nDay 3: Hydrological modelling concepts and Catchment Characterization\nWatch the [tutorial video] explaining one method for the catchment characterization. Familiarize yourself with the Geospatial Data. Do the catchment characterization of the basin that you selected to work on by filling in the Table Table 1 below. If you have downloaded the entire folder on your local drive, you already have all the data available for the analysis.\n\n\nTable 1: As an example, key relevant basin statistics for Gunt river basin are shown with individual data sources indicated. Using the data available in the data pack, you should characterize your case study basin in a similar way.\n\n\n\n\n\nATTRIBUTE\nVALUE\n\n\n\n\nGeography (srtmgl12020?)\n\n\n\n\nBasin Area \\(A\\)\n\n13’693 km2\n\n\n\nMinimum Elevation \\(h_{min}\\)\n\n2’068 masl\n\n\nMaximum Elevation \\(h_{max}\\)\n\n6’652 masl\n\n\nMean Elevation \\(h_{mean}\\)\n\n4’267 masl\n\n\nHydrology [Source: Tajik Hydromet Service]\n\n\n\nNorm hydrological year discharge \\(Q_{norm}\\)\n\n103.8 m3/s\n\n\nNorm cold season discharge (Oct. - Mar., Q4/Q1)\n19.8 m3/s\n\n\nNorm warm season discharge (Apr. - Sept., Q2/Q3)\n84.2 m3/s\n\n\nAnnual norm discharge volume\n3.28 km3\n\n\n\nAnnual norm specific discharge\n239 mm\n\n\nClimate\n\n\n\nMean basin temperature \\(T\\) (Karger et al. 2017)\n\n-5.96 deg. Celsius\n\n\nMean basin precipitation \\(P\\) (Beck et al. 2020)\n\n351 mm\n\n\nPotential Evaporation \\(E_{pot}\\) (Trabucco and Zomer 2019)\n\n929 mm\n\n\nAridity Index \\(\\phi = E_{pot} / P\\)\n\n2.7\n\n\nAridity Index (Trabucco and Zomer 2019)\n\n3.6\n\n\nLand Cover (Buchhorn et al. 2019)\n\n\n\nShrubland\n8 km2\n\n\n\nHerbaceous Vegetation\n4’241 km2\n\n\n\nCrop Land\n0.5 km2\n\n\n\nBuilt up\n4 km2\n\n\n\nBare / Sparse Vegetation\n8’410 km2\n\n\n\nSnow and Ice\n969 km2\n\n\n\nPermanent Water Bodies\n80 km2\n\n\n\nLand Ice\n\n\n\nTotal glacier area (RGI Consortium 2017)\n\n875 km2\n\n\n\nTotal glacier volume (calculated with (Erasov 1968))\n699 km3\n\n\n\n\n\n\n\n\n\n\n\nGRADED EXERCISE : Catchment characterization\n\n\n\nFollowing the video tutorial, fill in the table above with the characteristic numbers of your catchment together with your colleague. Compare your numbers to the ones of the Gunt catchment (table above). Submit your table via email to the lecturer before the start of tomorrows lecture.\n\n\nDay 4: Discharge and climate data\nYet more data preparation is required before you can start modelling: The review of the basins discharge and climate forcing.\n\n\n\n\n\n\nGRADED EXERCISE : Discharge characterization\n\n\n\nRead the chapters on discharge station data and climate data and, together with your colleague, perform a discharge characterization of your basin following the video tutorial.\nSubmit your discharge characterization via email to your lecturer before the start of tomorrows lecture.\n\n\nDay 5: Discussion of Types of Hydrological Models\nHydrological models in general are discussed. Consult the introductory Section of Part III: Hydrological Modeling and Applications. All three types of modeling approaches will be presented but with a focus on hydraulic-hydrological rainfall-runoff modeling.\n\n\n\n\n\n\nHOMEWORK : RS Minerve tutorial\n\n\n\n\nRead the modelling chapter\n\nGo through the RS Minerve tutorial (TODO LINK)\n\nThis homework is not graded but basic knowledge of RS Minerve is required for the second part of the course.\n\n\nDay 6 & 7: Model Calibration and Validation\nRead the chapter on Model calibration and validation and go through the example of the Nauvalisoy catchment which illustrates the iterative model refinement process.\nStudents will implement a hydrological model of their study catchment and calibrate it.\n\n\n\n\n\n\nGRADED EXERCISE : Model implementation and calibration in RS Minerve\n\n\n\n\nRead the modelling chapter\n\nImplementing a hydrological model of your study basin in RS Minerve.\n\nShow your working model and calibration results to your lecturer during group work session on day 7.\n\n\n\n\n\n\n\n\nGRADED EXERCISE : Abstract submission for student conference\n\n\n\n\nCarefully read the abstract submission guidelines and write an abstract for your model.\n\nSubmitt your abstract by Saturday, 10 a.m. Almaty time to your lecturer by email.\n\n\nDay 8: Student Conference & Course Wrap Up\nThe last day of the course is organized as a student conference where students present their modeling work on their respective case study catchment. The groups need to prepare a presentation of 12 minutes duration. Each presentation will be followed by a 3 minutes Q&A session. After all the groups have presented, impressions and feedback will be shared by the teachers followed by a larger group discussion.\nOnly students that have passed the GRADED EXERCISEs will be admitted to the student conference which consists the final exam.\nAt the end, students are invited to provide feedback with regard to their impression of the course. A key question will be hoe the course can be further improved to reach future students even more effectively.\n\n\n\n\n\n\nFINAL EXAM : Model presentation\n\n\n\n\nPresent an overview of your catchment, discharge characterization and your model implementation and results at the students conference."
  },
  {
    "objectID": "study_guide_materials.html#references",
    "href": "study_guide_materials.html#references",
    "title": "",
    "section": "References",
    "text": "References\n\n\n\n\nBeck, Hylke E., Eric F. Wood, Tim R. McVicar, Mauricio Zambrano-Bigiarini, Camila Alvarez-Garreton, Oscar M. Baez-Villanueva, Justin Sheffield, and Dirk N. Karger. 2020. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1.\n\n\nBuchhorn, M., B. Smets, L. Bertels, B. De Roo, M. Lesiv, N. E. Tsendbazar, M. Herold, and S. Fritz. 2019. “Copernicus Global Land Service: Land Cover 100m: Collection 3: Epoch 2019: Globe.”\n\n\nErasov, N. V. 1968. “Method for Determining of Volume of Mountain Glaciers.” MGI, no. 14: 307–8.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nRGI Consortium. 2017. “Randolph Glacier Inventory – a Dataset of Global Glacier Outlines: Version 6.0: Technical Report.” Global Land Ice Measurements from Space, Colorado, USA. Digital Media. https://doi.org/https://doi.org/10.7265/N5-RGI-60.\n\n\nTrabucco, Antonio, and Robert Zomer. 2019. “Global Aridity Index and Potential Evapotranspiration (ET0) Climate Database v2,” January. https://doi.org/10.6084/m9.figshare.7504448.v3."
  },
  {
    "objectID": "appendix_a_free_software.html#literature",
    "href": "appendix_a_free_software.html#literature",
    "title": "",
    "section": "Literature",
    "text": "Literature\nMany authors of scientific literature are on the web platform researchgate where they can privately share their work with students (users need to register for an account)."
  },
  {
    "objectID": "appendix_a_free_software.html#sec-open-resouces-software-QGIS",
    "href": "appendix_a_free_software.html#sec-open-resouces-software-QGIS",
    "title": "",
    "section": "QGIS",
    "text": "QGIS\nQGIS is a free and open source Geographical Information System that offers very similar tools as their commercial counterparts. The latest version of QGIS can be downloaded from the QGIS website. We recommend to install the stable long-term support version (installation guide).\n\nResources for learning QGIS\nA general tutorial for beginners is the QGIS training manual. It includes a short chapter on the use of QGIS for hydrological analysis (Chapter 17.16). For this course you should be familiar with the QGIS window and know the difference between raster and vector data. If you have used QGIS or a similar GIS software before you will not need to do a tutorial prior to this course."
  },
  {
    "objectID": "appendix_a_free_software.html#sec-open-resouces-software-R",
    "href": "appendix_a_free_software.html#sec-open-resouces-software-R",
    "title": "",
    "section": "R and RStudio",
    "text": "R and RStudio\nR is a free and open source statistical programming language. It’s large user community ensure active development and up-to-date help resources available on the internet. RStudio is a free user interface for R. To install R and RStudio follow the installation guide on ModernDive - Statistical Inference via Data Science.\nFor the bare beginners, also with regard to programming, the book Hands-On Programming with R is an excellent start\n\nResources for learning R and R studio\n\n“Help! I’m new to R and RStudio and I need to learn them! What do I do?” If you’re asking yourself this, this book is for you: ModernDive - Statistical Inference via Data Science.\nA thorough guide for data science in R: R for Data Science\n\n\n\nRS Minerve\n\nHow to download and install RS Minerve\nGo to the software download page of CREALP’s website https://www.crealp.ch/fr/accueil/outils-services/logiciels/rs-minerve/telechargement-rsm.html (last accessed March 18, 2021) and click on Version actuelle to download the latest installer for Windows as shown in Figure 1. This will start the download process for the installer RSMinerve-install.exe.\n\n\n\nFigure 1: Download RS Minerve from the CREALP website https://www.crealp.ch/fr/accueil/outils-services/logiciels/rs-minerve/telechargement-rsm.html (last accessed March 18, 2021).\n\n\nYou should also download the user manual (RS MINERVE user manual, written in English) and the example files used for the tutorials in the user manual (Exemple de fichiers, a zip file with data) as well as the technical manual (RS MINERVE technical manual, written in English).\nOnce the installer is downloaded, install RSMinerve with a double-click on the installer and follow the Setup guide. Open RSMinerve once you have it installed.\nBack to the prerequisites for RS Minerve modelling"
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "",
    "section": "",
    "text": "Preface\nThis is a book and study guide about the hydrology of semi-arid Central Asia and applied hydrological modeling in the region. It is geared towards students and young professionals in Central Asia who are interested in learning modern hydrological modeling approaches. The book teaches by example and focuses on two example catchments in the Syr Darya and Amu Darya river basins as case studies. The methods demonstrated here can be applied elsewhere.\n\n\n\n\n\n\nThe What and Why of Hydrological Modeling\n\n\n\nHydrological models come in different incarnations and flavors. Hydrological water balance models, in this text also sometimes referred to as rainfall-runoff models, were developed to help us gain an understanding of the partitioning of available water into different fluxes and storage compartments over time in the natural system under consideration. This natural system normally consists of different interlinked compartments, including surface water and the unsaturated (soil moisture) and unsaturated (groundwater) zones. These models simulate the flow of water through these compartments. They are most often used in the context of water management and planning applications, i.e., for basin planning under climate change and population growth scenarios to allocate water between different uses and users on the one hand. On the other, these models are also used operationally to close supply-demand gaps in real-time management tasks and for short-term forecasting.\nWhere large amounts of data are available, empirical models can be implemented. These models strictly speaking do rely on the explicit simulation of the water balance of individual compartments. Rather, they learn patterns from measured time series of discharge and other relevant variables, such as temperature, precipitation, snow cover and then use these pattern between the time-ordered data for forecasting variable of interest (i.e., discharge, water levels, etc.).\n\n\nIn Part I of the book, key hydro-climatological characteristics of the region are presented. This Section draws inspiration from Victor Shults’ “Rivers of Middle Asia” (Shults 1965). Through the collection of a large number of in-situ hydrological data from all over the region and in combination with a plethora of newly available data, a grand modern regional perspective on Central Asian hydrology becomes possible in the tradition of Shults.\nTwo important basins are highlighted as in-depth case studies, i.e. the Gunt River Basins in the Amu Darya catchment and the Chirchik River basin in the Syr Darya. The analyses of these catchments draws on available data from the Central Asian Hydrometeorological Services and on global and entirely public hydro-climatological as well as land cover datasets. The goal of these introductory chapters is to familiarize the student with the first steps prior to any hydrological modeling, i.e. to obtain a robust understand of the system of interest through a thorough hydro-climatological characterization of the study area.\nPart II is a rather large section of the book which is then devoted to data where different open data sources are presented. Retrieval and data preparation in the context of hydrological modeling are discussed. These data include data on topography, land cover, climate reanalysis data and parameters on biophysical climate and climate projections data. The preparation of these data often requires significant work. Hence, a focus lies on demonstrating workflows to facilitate the handling and preparation of these type of data for hydrological modeling.\nIn Part III, three different modeling approach are presented and discussed. First, long-term water balance modeling using the Budyko framework is presented and discussed in depth with an application to a large dataset from the region. This approach can yield powerful insights for example on regional hydrological changes due to climatic changes and where the detailed hydrological-hydraulic modeling of a large number of rivers is impracticable. Second, detailed hydrological-hydraulic modeling of individual river basins is presented. These types of models are normally developed for tradeoff analysis between different water uses and users in a basin, i.e. in the planning context and also under different climate scenarios. They can also be run in operational mode to respond to real-time management challenges. Finally, the last modeling chapter introduces modeling through predictive inference where empirical data-driven models are setup for forecasting discharge at particular locations in a basin. These models rely on large amounts of measured discharge data and hence, their application is limited to places where such data are available.\nPart III also includes two Chapters on hydrological model applications where real-world model deployment is presented and discussed. A special emphasis here is aspects of operation and maintenance of deployed models in local agencies and options for this, also in relation to staff education and learning.\nThe book / study guide is accompanied by a Study Exercise Pack that encompasses data from 7 Central Asian catchments which can be used by students for learning and applying skills acquired to real-world examples in the region. The Exercise Pack can accessed and downloaded here. Furthermore, a dedicated R Package has been developed which implements many of the data analyses and processing steps shown in this book (see also Section for more information).\nWith everything that is presented, the focus is on the use of open source and free software. For data preparation and analysis as well as for water balance and empirical modeling, R and RStudio are utilized (r-base?). For the processing of geographic data, workflows in QGIS are demonstrated (QGIS Development Team 2021). For hydrological-hydraulic modeling, the free RS MINERVE is utilized which is a environment for the modeling of free surface runoff flow formation and propagation (Foehn et al. 2020; Garcia Hernandez et al. 2020). The reader is expected to have a basic understanding of R and QGIS and how to use these software for data analysis and processing.\nThe outlook having to learn hydrology together with quantitative geospatial analysis and programming may sound overwhelming at the beginning. Really the best way is just to dive into the book and learn through the many examples provided. All code with which the analysis and modeling is carried out is provided and can be thus adapted to any other local context or relevant task. So, this handbook on applied hydrological modeling is hopefully inviting students to learn through experimentation and not to get scared.\nBefore we get going, a small note on how to translate this text in any other language of interest. Should the reader struggle with the English language, there is a very easy way to translate this book into any of the local languages spoken in Central Asia, including Russian. The picture below shows a screenshot from the online book translated into Russian language via Google Chrome’s translation service. The screenshot shows how to activiate the translation panel (1). The translated book text then appears (2). Alternatively right-click anywhere on the page. Then, click Translate to [Language].\n\n\n\n\n\nFoehn, A., J. Garcia Hernandez, B. Roquier, J. Fluixa-Sanmartin, T. Brauchli, J. Paredes Arquiola, and G. De Cesare. 2020. “RS MINERVE - User Manual, V2.15.” ISSN 2673-2653. Switzerland: Ed. CREALP.\n\n\nGarcia Hernandez, J., A. Foehn, J. Fluixa-Sanmartin, B. Roquier, T. Brauchli, J. Paredes Arquiola, and De Cesare G. 2020. “RS MINERVE - Technical Manual, V2.25.” ISSN 2673-2661. Switzerland: Ed. CREALP.\n\n\nQGIS Development Team. 2021. QGIS Geographic Information System. QGIS Association.\n\n\nShults, Victor. 1965. Rivers of Middle Asia. 2nd Edition. Gidrometeoizdat, Leningrad."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "discharge_station_data.html#sec-available-data",
    "href": "discharge_station_data.html#sec-available-data",
    "title": "",
    "section": "Available Data",
    "text": "Available Data\nThe riversCentralAsia Package provides available data of the gauging and meteorological stations in the Chirchik River Basin (where other data are used, their source and access options are indicated). This is the time then to install and load the package with\n\ndevtools::install_github(\"hydrosolutions/riversCentralAsia\") # download package from github\nlibrary('riversCentralAsia') # load package\n\nBefore starting any type of modeling, it is important to get a good understanding of the data that we are dealing with and whether there exist problems with the raw data that need to be addressed prior to modeling. This is actually also one of the more hidden agendas when doing a basin characterization.\nProblems in real-world data usually include data gaps and outliers as data records that one obtains are usually neither complete nor cleaned (of errors).\nThe steps performed here are thus required steps for any type of successful modeling and should be performed with great care prior to starting hydrological modeling.\n\n\n\n\n\n\nGarbage in - Garbage out\n\n\n\nThe importance of good quality data for modeling cannot be overstated. It can very easily be summarized in the following way\n\nData \\(\\rightarrow\\) Model \\(\\rightarrow\\) Results\n\nIf the underlying data is erroneous, then this translated into\n\nGarbage in \\(\\rightarrow\\) Model \\(\\rightarrow\\) Garbage out\n\n\n\nWe concentrate our efforts here on discharge records and data from meteorological stations in the Chirchik River Basin for demonstration purposes. The techniques shown here for decadal (10-days) data naturally extend to monthly data and also, to data from other basins and other sources."
  },
  {
    "objectID": "discharge_station_data.html#sec-gap-filling-discharge-data",
    "href": "discharge_station_data.html#sec-gap-filling-discharge-data",
    "title": "",
    "section": "Gap Filling Discharge Data",
    "text": "Gap Filling Discharge Data\nIn the following, we will work with decadal discharge data from the two main tributaries of the Chirchik River, i.e. the Chatkal River (Gauge 16279) and the Pskem River (Gauge 16290) as well as on the data of the inflow to the Charvak reservoir (Gauge 16924). The goal is to analyze the data and prepare for modeling. First, let us load the relevant discharge data.\n\ndata <- ChirchikRiverBasin # load data\nq_dec_tbl <- data %>% filter(code == '16279' | code == '16290' | code == '16924') # Note for the new name of the data object, we use snake notation. We choose to add periodicity (_dec_) and data type (_tbl for tibble/dataframe) to the data name. This just helps to stay organized and is good practice in R programming.\nq_dec_tbl\n\n# A tibble: 9,072 × 14\n   date        data  norm units type  code  station   river   basin   resolution\n   <date>     <dbl> <dbl> <chr> <fct> <chr> <chr>     <chr>   <chr>   <fct>     \n 1 1932-01-10  48.8  38.8 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 2 1932-01-20  48.4  37.5 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 3 1932-01-31  42.4  36.6 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 4 1932-02-10  43.7  36.4 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 5 1932-02-20  44.2  36.3 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 6 1932-02-29  47.7  36.9 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 7 1932-03-10  54.1  39.4 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 8 1932-03-20  63.2  47.6 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 9 1932-03-31 103    60.5 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n10 1932-04-10 103    86.4 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n# … with 9,062 more rows, and 4 more variables: lon_UTM42 <dbl>,\n#   lat_UTM42 <dbl>, altitude_masl <dbl>, basinSize_sqkm <dbl>\n\n\nYou can get more information about the available data by typing ?ChirchikRiverBasin. Note that the original time series data has been packaged in this format by the riversCentralAsia::loadTabularData() function which takes a simple .csv file as input.\nIt is advisable to check at this stage for missing data in time series and to fill gaps where present. Are there missing data? How can these be filled so as to arrive at complete time series that are required for hydrological modeling?\nAs can be seen in Figure ?@fig-discharge-data-chirchik-river-basin, close inspection of the time series indeed reveals some missing data in the 1940ies.\n\n\n\n\n\n\nTip\n\n\n\nNote, ?@fig-discharge-data-chirchik-river-basin is an interactive figure where you can zoom in. Try it and zoom into the 1940ies to visualize the missing data fore clearly. You can zoom out again by clicking the Autoscale hover over. For the viualization of time series, we normally use the excellent timetk R Package. Check it out and try yourself!\n\n\n\nq_dec_tbl %>% plot_time_series(date,data,\n                               .facet_vars  = code,\n                               .smooth      = FALSE,\n                               .interactive = TRUE,\n                               .x_lab       = \"year\",\n                               .y_lab       = \"m^3/s\",\n                               .title       = \"\"\n                               )\n\n\nFigure 1: Discharge data of selected gauges in the upstream zone of runoff formation in the Chirchik River Basin. Data Source: Uzbek Hydrometeorological Service.\n\n\n\nMissing data are also confirmed by the warning that the function timetk::plot_time_series() throws (suppressed here). Statistics of the missing data can be easily obtained. As the Table below shows, we can do this analysis for each discharge station separately.\n\nq_dec_tbl %>% group_by(code) %>% \n  summarize(n.na = sum(is.na(data)), na.perc = n.na/n()*100)\n\n# A tibble: 3 × 3\n  code   n.na na.perc\n  <chr> <int>   <dbl>\n1 16279    15   0.496\n2 16290    39   1.29 \n3 16924    42   1.39 \n\n\nSummarizing the number of observation with missing data reveals that 15 data points for station 16279 (0.5 % of total record length) and 39 for station 16290 (1.3 % of total record length) are missing. As there are only very few gaps in the existing time series, we use a simple method to fill these. Wherever there is a gap, we fill in the corresponding decadal norm as stored in the norm column in the object q_dec_tbl at the timestamp of the missing data. The visualization of the results confirms that our simple gap filling approach is indeed satisfactory (see Figure 2).\n\n# Make a copy of the original data\nq_dec_filled_tbl <- q_dec_tbl\n\n# Actual gap filling step\nq_dec_filled_tbl$data[is.na(q_dec_filled_tbl$data)] = \n  q_dec_filled_tbl$norm[is.na(q_dec_filled_tbl$data)] \n\n# Inspect results\nq_dec_filled_tbl %>% plot_time_series(date, data, \n                                      .facet_vars  = code, \n                                      .smooth      = FALSE,\n                                      .interactive = TRUE,\n                                      .x_lab       = \"year\",\n                                      .y_lab       = \"m^3/s\",\n                                      .title       = \"\"\n                                      )\n\n\nFigure 2: Gap filled Pskem and Chatkal river discharges.\n\n\n\nAll missing data are gone now as can easily be validated.\n\nq_dec_filled_tbl %>% group_by(code) %>% \n  summarize(n.na = sum(is.na(data)), na.perc = n.na/n()*100)\n\n# A tibble: 3 × 3\n  code   n.na na.perc\n  <chr> <int>   <dbl>\n1 16279     0       0\n2 16290     0       0\n3 16924     0       0\n\n\nA note of caution here. This simple gap filling technique reduces variance in the time series. It should only be used when the percentage of missing data is low. As will be discussed in the next Section Section 1.3 below, more sophisticated techniques should be utilized when there exist substantial gaps and in the case of less regular data.\nFinally, we discard the data that we no longer need, including the norm data, which we used for gap filling of the missing discharge data and convert the data to wide format (see ?@tbl-gap-filled-discharge-result-tibble below) to add to it meteorological data in the next Section.\n\nq_dec_filled_wide_tbl <- q_dec_filled_tbl %>% # again we use the name convention of objects as introduced above\n  mutate(code = paste0('Q',code %>% as.character())) %>% # Since we convert everything to long form, we want to keep information as compact as possible. Hence, we paste the type identifier (Q for discharge here) in from of the 5 digit station code.\n  dplyr::select(date,data,code) %>% # ... and then ditch all the remainder information\n  pivot_wider(names_from = \"code\",values_from = \"data\") # in order to pivot to the long format, we need to make a small detour via the wide format.\n\nq_dec_filled_long_tbl <- q_dec_filled_wide_tbl %>% pivot_longer(-date) # and then pivot back\nq_dec_filled_wide_tbl\n\n\n?(caption)\n\n\n\n# A tibble: 3,024 × 4\n   date       Q16279 Q16290 Q16924\n   <date>      <dbl>  <dbl>  <dbl>\n 1 1932-01-10   48.8   38.3   87.1\n 2 1932-01-20   48.4   37.7   86.1\n 3 1932-01-31   42.4   36.2   78.6\n 4 1932-02-10   43.7   35.6   79.3\n 5 1932-02-20   44.2   35     79.2\n 6 1932-02-29   47.7   37.1   84.8\n 7 1932-03-10   54.1   43.1   97.2\n 8 1932-03-20   63.2   47    110  \n 9 1932-03-31  103     72.1  175  \n10 1932-04-10  103     73.2  176  \n# … with 3,014 more rows\n\n\n\n\nAs a result, we now have a complete record of decadal discharge data for the two main tributaries of the Chirchik river and the inflow time series to Charvak Reservoir from the beginning of 1932 until and including 2015, i.e. 84 years. The same type of preparatory analysis will now be carried out for the meteorological data but in a slightly more sophisticated way."
  },
  {
    "objectID": "discharge_station_data.html#sec-gap-filling-meteorological-data",
    "href": "discharge_station_data.html#sec-gap-filling-meteorological-data",
    "title": "",
    "section": "Gap Filling Meteorological Data",
    "text": "Gap Filling Meteorological Data\nHere, we use precipitation and temperature data from Pskem (38462), Chatkal (38471) and Charvak Reservoir (38464) Meteorological Stations (see ?@sec-example-chirchik-river-basin for more information on these stations). We also have data from Oygaing station (Station Code 38339) but the record only starts in 1962 and the time resolution is monthly. Therefore, we do not take this station into account here for the time being.\nWe start with precipitation and plot the available data.\n\np_dec_tbl <- data %>% filter(type == \"P\" & code != \"38339\") \np_dec_tbl %>% plot_time_series(date,data,\n                               .facet_vars  = code,\n                               .interactive = TRUE,\n                               .smooth      = FALSE,\n                               .title       = \"\",\n                               .y_lab       = \"mm/decade\",\n                               .x_lab       = \"year\"\n                               )\n\n\nFigure 3: Raw decadal precipitation data from Pskem (38462), Charvak Reservoir (38471) and Chatkal Meteo Station (38471).\n\n\n\nThe precipitation data from these 3 stations shows some significant data gaps. The Chatkal Meteorological Station that is located in Kyrgyzstan apparently did not work in the post-transition years as continuous measurements were only resumed there in 1998.\nLet us see what happens if we were to use the same simple gap filling technique that we introduced above for discharge.\n\np_dec_filled_tbl <- p_dec_tbl\np_dec_filled_tbl$data[is.na(p_dec_filled_tbl$data)] = p_dec_filled_tbl$norm[is.na(p_dec_filled_tbl$data)]\np_dec_filled_tbl %>% plot_time_series(date,data,\n                                      .facet_vars  = code,\n                                      .interactive = TRUE,\n                                      .smooth      = FALSE,\n                                      .title       = \"\",\n                                      .y_lab       = \"mm/decade\",\n                                      .x_lab       = \"year\"\n                                      )\n\n\nFigure 4: Precipitation Data gap-filled with norms. The filled values from 1990 - 2000 in the case of the Station 38471 indicate that the norm-filling technique is not adequate for this type of data.\n\n\n\nClosely inspect the significant data gap in the 1990ies at Station 38741. Play around and zoom into the time series in the 1990ies in Figure 3 and compare it with the resulting gap-filled time series in Figure 4. We see that our technique of gap filling with long-term norms is not suitable for this type of data and the significant gap size. The effect of variance reduction is clearly visible.\nHence, we resort to a more powerful gap filling technique that uses a (regression) model to impute the missing values from existing ones at the neighboring stations, i.e. Stations 38462 and 38464. To do so, we utilize the simputation R package. Please note that if you do not have the required package installed locally, you should install it prior to its use with the following command install.packages('simputation')\n\nlibrary(simputation)\n# First, we bring the data into the suitable format. \np_dec_wide_tbl <- p_dec_tbl %>% \n  mutate(code = paste0('P',code %>% as.character())) %>% \n  dplyr::select(date,data,code) %>% \n  pivot_wider(names_from = \"code\",values_from = \"data\")\n\n# Second, we impute missing values.\np_dec_filled_wide_tbl <- p_dec_wide_tbl  %>% \n  impute_rlm(P38471 ~ P38462 + P38464) %>% # Imputing precipitation at station 38471 using a robust linear regression model\n  impute_rlm(P38462 ~ P38471 + P38464) %>% # Imputing precipitation at station 38462 using a robust linear regression model\n  impute_rlm(P38464 ~ P38462 + P38471) # Imputing precipitation at station 38464 using a robust linear regression model\n\np_dec_filled_long_tbl <- p_dec_filled_wide_tbl %>% pivot_longer(c('P38462','P38464','P38471')) \n\np_dec_filled_long_tbl %>% plot_time_series(date,value,\n                                          .facet_vars  = name,\n                                          .interactive = TRUE,\n                                          .smooth      = FALSE,\n                                          .title       = '',\n                                          .y_lab       = \"mm/decade\",\n                                          .x_lab       = \"year\"\n                                          )\n\n\nFigure 5: Precipitation Data gap filled with a robust linear regression modeling approach\n\n\n\nAs you can see, we use simple linear regression models to impute missing value in the target time series using observations from the neighboring stations. This is of course only possible where data is not missing across the time series, as we will discuss below.\nThrough simple visual inspection, it becomes clear that this type of regression model for gap filling is better suited than the previous approach chosen. Let us check whether we could successfully fill all gaps with this robust linear regression approach.\n\np_dec_filled_long_tbl %>% \n  group_by(name) %>% \n  summarize(n.na = sum(is.na(value)), n.na.perc = n.na / n() * 100)\n\n# A tibble: 3 × 3\n  name    n.na n.na.perc\n  <chr>  <int>     <dbl>\n1 P38462    12     0.402\n2 P38464    12     0.402\n3 P38471     3     0.100\n\n\nIt turns out that we still have very few gaps to deal with. We can see them by simply visualizing the wide tibble. The problem persisted at times when two or more values were missing across the available stations at the same time and where thus the linear regression could not be carried out. Let us look at the start of the record…\n\np_dec_filled_wide_tbl %>% \n  head(10)\n\n# A tibble: 10 × 4\n   date       P38462 P38464 P38471\n   <date>      <dbl>  <dbl>  <dbl>\n 1 1933-01-10     NA   NA        2\n 2 1933-01-20     NA   NA       10\n 3 1933-01-31     NA   NA        5\n 4 1933-02-10     NA   NA       33\n 5 1933-02-20     NA   NA        8\n 6 1933-02-28     NA   NA       10\n 7 1933-03-10     NA   NA       31\n 8 1933-03-20     NA   NA       50\n 9 1933-03-31     NA   NA        6\n10 1933-04-10     23   21.3     13\n\n\n… and the end of the record. The missing values are easily spotted.\n\np_dec_filled_wide_tbl %>% \n  tail()\n\n# A tibble: 6 × 4\n  date       P38462 P38464 P38471\n  <date>      <dbl>  <dbl>  <dbl>\n1 2015-11-10     72     81     19\n2 2015-11-20    122     76     43\n3 2015-11-30      7      2      3\n4 2015-12-10     NA     NA     NA\n5 2015-12-20     NA     NA     NA\n6 2015-12-31     NA     NA     NA\n\n\nWe can solve the issues related to the missing values at the start of the observation record by using the same technique as above and by only regressing P38462 and P38464 on P38471.\n\np_dec_filled_wide_tbl <- \n  p_dec_filled_wide_tbl  %>% \n  impute_rlm(P38462 ~ P38471) %>% # Imputing precipitation at station 38462 using a robust linear regression model\n  impute_rlm(P38464 ~ P38471) # Imputing precipitation at station 38464 using a robust linear regression model\np_dec_filled_wide_tbl %>% head(10)\n\n# A tibble: 10 × 4\n   date       P38462 P38464 P38471\n   <date>      <dbl>  <dbl>  <dbl>\n 1 1933-01-10   5.60   5.08      2\n 2 1933-01-20  18.3   16.7      10\n 3 1933-01-31  10.4    9.46      5\n 4 1933-02-10  54.9   50.3      33\n 5 1933-02-20  15.2   13.8       8\n 6 1933-02-28  18.3   16.7      10\n 7 1933-03-10  51.8   47.3      31\n 8 1933-03-20  82.0   75.0      50\n 9 1933-03-31  12.0   10.9       6\n10 1933-04-10  23     21.3      13\n\n\nConverse to this, the complete set of observations is missing for December 2015. We will thus remove these non-observations from our tibble. This can be done once and for all with na.omit() as shown in the code block below.\n\np_dec_filled_wide_tbl <- p_dec_filled_wide_tbl %>% na.omit()\np_dec_filled_wide_tbl %>% tail()\n\n# A tibble: 6 × 4\n  date       P38462 P38464 P38471\n  <date>      <dbl>  <dbl>  <dbl>\n1 2015-10-10      5      1      0\n2 2015-10-20     89    108     58\n3 2015-10-31     34     40     12\n4 2015-11-10     72     81     19\n5 2015-11-20    122     76     43\n6 2015-11-30      7      2      3\n\np_dec_filled_long_tbl <-  p_dec_filled_wide_tbl %>% pivot_longer(-date)\n\nInspecting the temperature data, we see similar data issues as in the precipitation data set and can proceed accordingly for gap filling.\n\nt_dec_tbl <- data %>% filter(type == \"T\") \nt_dec_tbl %>% plot_time_series(date,data,\n                               .facet_vars  = code,\n                               .interactive = TRUE,\n                               .smooth      = FALSE,\n                               .title       = '',\n                               .y_lab       = \"deg. Celsius\",\n                               .x_lab       = \"year\"\n                               )\n\n\nFigure 6: Raw temperature data from the meteorological stations Pskem (38462) and Chatkal (38471)\n\n\n\n\n# First, we bring the data into the suitable format. \nt_dec_wide_tbl <- t_dec_tbl %>% \n  mutate(code = paste0('T',code %>% as.character())) %>% \n  dplyr::select(date,data,code) %>% \n  pivot_wider(names_from = \"code\",values_from = \"data\")\n\n# Second, we impute missing values.\nt_dec_filled_wide_tbl <- t_dec_wide_tbl  %>% \n  impute_rlm(T38471 ~ T38462) %>% # Imputing precipitation at station 38471 using a robust linear regression model\n  impute_rlm(T38462 ~ T38471) # Imputing precipitation at station 38462 using a robust linear regression model\n\nt_dec_filled_long_tbl <- t_dec_filled_wide_tbl %>% \n  pivot_longer(c('T38462','T38471')) \n\nt_dec_filled_long_tbl %>% \n  plot_time_series(date,value,\n                   .facet_vars  = name,\n                   .interactive = TRUE,\n                   .smooth      = FALSE,\n                   .title       = '',\n                   .y_lab       = \"deg. Celsius\",\n                   .x_lab       = \"year\"\n                   )\n\n\nFigure 7: Temperature data gap filled with robust linear regression modeling.\n\n\n\nThere are some irregularities in the temperature time series of Chatkal Meteorological Station in the first decade of the 20th century (tip: zoom in to see these more clearly). Note that these were not introduced by the gap filling technique that we used but are most likely wrong temperature readings or recordings. We will return to these in the outlier analysis below in Section 1.4.\nAny missing values left in the temperature time series? Let’s check!\n\nt_dec_filled_long_tbl %>% \n  group_by(name) %>% \n  summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100)\n\n# A tibble: 2 × 3\n  name    n.na n.na.perc\n  <chr>  <int>     <dbl>\n1 T38462     3     0.100\n2 T38471     3     0.100\n\n\nTo see where the missing value are, we find them easily again by looking at the head and tail of the tibble.\n\nt_dec_filled_wide_tbl %>% head()\n\n# A tibble: 6 × 3\n  date       T38462 T38471\n  <date>      <dbl>  <dbl>\n1 1933-01-10   -6.9  -16.6\n2 1933-01-20   -6.1  -15.5\n3 1933-01-31   -6.3  -15.6\n4 1933-02-10   -2     -8.6\n5 1933-02-20   -3.3  -12.5\n6 1933-02-28   -0.1   -8.5\n\n\n\nt_dec_filled_wide_tbl %>% tail()\n\n# A tibble: 6 × 3\n  date       T38462 T38471\n  <date>      <dbl>  <dbl>\n1 2015-11-10    2.4   -2.5\n2 2015-11-20    2     -2.2\n3 2015-11-30    4.6   -3.7\n4 2015-12-10   NA     NA  \n5 2015-12-20   NA     NA  \n6 2015-12-31   NA     NA  \n\n\nFinally, we remove these non observations again as above with the function na.omit().\n\nt_dec_filled_wide_tbl <- t_dec_filled_wide_tbl %>% na.omit()\nt_dec_filled_long_tbl <- t_dec_filled_wide_tbl %>% pivot_longer(-date)\n\nTo deal with the missing values at the end of the observational record, we could also have used any other technique. Using the norm values however would have artificially reduced the variance in both cases as explained above. Furthermore and at least in the case of temperature, it is also questionable to what extent a norm calculated over the last 84 years is still representative given global warming. We will look in this important and interesting topic in the next section."
  },
  {
    "objectID": "discharge_station_data.html#sec-anomalies-and-outliers",
    "href": "discharge_station_data.html#sec-anomalies-and-outliers",
    "title": "",
    "section": "Anomalies and Outliers",
    "text": "Anomalies and Outliers\nWe use the function timetk::plot_anomaly_diagnostics() to investigate these anomalies in the time series. For discharge, we first log-transform the raw data with the following transformation to reduce the variance of the original data.\n\\[\n\\hat{q}(t) = log(q(t) + 1)\n\\] where \\(\\hat{q}(t)\\) denotes the transformed discharge. Prior to the log transformation, 1 is added so as to avoid cases where discharge would be 0 and the logarithmic transform thus undefined. The transformation can easily be done with the log1p() function in R. Back-transformation is then via the function expm1() simply involves taking the exponent and subtracting 1 from the result. Figure 8 shows the result.\nResults are shown in Figure 8, Figure 9 and Figure 10 below.\nThe exceptionally wet year 19169 shows up as anomalous in the Chatkal River Basin and at the downstream Charvak Reservoir inflow gauge.\n\nq_dec_filled_long_tbl %>% \n  plot_anomaly_diagnostics(date,\n                           value %>% log1p(),\n                           .facet_vars  = name,\n                           .frequency = 36,\n                           .interactive = TRUE,\n                           .title = \"\")\n\n\nFigure 8: Anomaly diagnostics of discharge data. The transparent grey band shows the width of the normal range. The highly anomalous wet year of 1969 is clearly visible in the discharge record of the Chatkal river basin (Station 16279).\n\n\n\nThe investigation of precipitation anomalies shows a succession of regular anomalous wet events over time. It is interesting to see that the winter 1968/69 regularly anomalous at all three stations (Figure 9, zoom in to investigate).\n\np_dec_filled_long_tbl %>% \n  plot_anomaly_diagnostics(date,\n                           value,\n                           .facet_vars  = name,\n                           .interactive = TRUE,\n                           .title = \"\")\n\n\nFigure 9: Anomaly diagnostics of precipitation data.\n\n\n\nWhile intuitively, we would have expected an exceptionally mild winter in 1968/69 due to the precipitation excess, the corresponding anomaly does not show up in the temperature record as shown in Figure 10.\n\nt_dec_filled_long_tbl %>%  \n  plot_anomaly_diagnostics(date,value,\n                           .facet_vars  = name,\n                           .interactive = TRUE,\n                           .title = \"\")\n\n\nFigure 10: Anomaly diagnostics of temperature data.\n\n\n\nApart from the identification of extremal periods since as the 1969 discharge year in the Chatkal river basin, the diagnostics of anomalies also helps to identify likely erroneous data records. In Figure 10 for example, when we zoom into the data of the series T38471 in the first decade of the 21st century, problems in relation to positive anomalies during the winter are visible in 4 instances. One explanation would be that in at least some instances, the data are erroneously recorded as positive values when in fact they were negative (see dates ‘2002-01-31’, ‘2005-01-10’ and ‘2007-02-28’, Chatkal Station 38471).\nObvious errors can be spotted like this and corrected. However, non-obvious data errors should be communicated with the data producing agency and replacement strategy jointly defined. If this is not possible, the values could be set to NA and then imputed as shown above.\nThe discharge data is now ready to be used for modelling and we can move on to the next Chapter on Geospatial Data."
  },
  {
    "objectID": "discharge_station_data.html#sec-discharge-station-data-references",
    "href": "discharge_station_data.html#sec-discharge-station-data-references",
    "title": "",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "climate_projections_data.html#sec-climate-projections-data",
    "href": "climate_projections_data.html#sec-climate-projections-data",
    "title": "",
    "section": "Climate Projections Data",
    "text": "Climate Projections Data\nChapter on climate projections data with a focus on CMIP6. Also Discussion on quantile mapping for downscaling."
  },
  {
    "objectID": "real_world_examples.html#sec-real-world-examples-references",
    "href": "real_world_examples.html#sec-real-world-examples-references",
    "title": "",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-budyko-introduction",
    "href": "long_term_water_balance_modeling.html#sec-budyko-introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nThe general water balance of a catchment can be written as\n\\[\n\\Delta S = P - E - Q\n\\qquad(1)\\]\nwhere \\(\\Delta S\\) is net storage change in millimeter [mm], \\(P\\) is precipitation in mm, \\(E\\) is evaporation in mm, and \\(Q\\) is specific discharge in mm. Evaporation is the phenomenon by which a substance is converted from its liquid into its vapor phase, independently of where it lies in nature (Miralles et al. 2020). This definition of evaporation encompasses evaporation from inside leaves (transpiration), evaporation from bare soils, evaporation from intercepted precipitation (interception loss), evaporation from open water surfaces, and finally, evaporation over ice- and snow-covered surfaces (often referred to as sublimation).\nOver the period of a hydrological year and longer time scales, we expect \\(\\Delta S\\) to be 0 since neither water storage nor destorage happen over longer periods. This would of course not be true for catchments where for example man-made storage infrastructure was built over the period under consideration or for catchments with ongoing glacier melt over a prolonged time. If \\(\\Delta S = 0\\), the above Equation @ref(eq:WB1) can be rewritten as\n\\[\nQ = P - E\n\\qquad(2)\\]\nDividing by \\(P\\), we get\n\\[\n\\frac{Q}{P} = 1 - \\frac{E}{P}\n\\qquad(3)\\]\nwhere \\(Q/P\\) can be called the runoff index and \\(E/P\\) is the evaporation index or evaporative fraction.\nFor a catchment, annual mean \\(E\\) and \\(Q\\) are governed by total water supply \\(P\\) and the total available energy which is normally expressed as potential evaporation \\(E_{pot}\\) and which denotes the (atmospheric) water demand. If \\(E_{pot}\\) is small, the discharge \\(Q\\) is normally bigger than evaporation \\(E\\). Similarly, if the available radiative energy is very high, the water demand \\(E_{pot}\\) is very large and \\(Q<<E\\) (Arora 2002). \\(E_{pot}\\) and \\(P\\) are thus the key determinants of annual or longer timescale runoff and evaporation rates. Michael Budyko has termed the ratio \\(E_{pot} / P\\) aridity index (Budyko 1974).\nAs explained above, water demand is determined by energy. Solar radiation is the primary energy source for the earth-atmosphere system and the key driver of the hydrological cycle. At the earth’s surface, the net radiative flux \\(R_N\\) is the energy that is available for a) heating and cooling of the soil (ground heat flux), b) changing the phase of water (latent heat flux), and c) heating or cooling air in the boundary layer thus causing atmospheric dynamics (sensible heat flux).\nThis can be formalized with the following relationship\n\\[\nR_{N} = H_{S} + H_{L} + \\Delta H_{G}\n\\qquad(4)\\]\nwhere \\(R_{N}\\) is the net radiation [in W/m2 = kg/s3], \\(H_{S}\\) is the upward sensible heat flux, \\(H_{L}\\) is the latent heat flux and \\(\\Delta H_{G}\\) the net ground heat flux. The latent heat flux is directly proportional to evaporation \\(E\\). Thus, \\(H_{L} = L \\cdot E\\) where \\(L = 2.5 \\cdot 10^{6}\\) J/kg [= m2/s2] is the latent heat of vaporization and \\(E\\) is the actual evaporation in [m/s]. As in the case of the water balance, at the annual or longer time scales, we can neglect the heat storage effect in the ground and get\n\\[\nR_{N} = H_{S} + L \\cdot E\n\\qquad(5)\\]\nWith the Bowen ratio defined as the fraction of the sensible heat flux divided by the latent heat flux, i.e.\n\\[\n\\gamma = \\frac{H_{S}}{H_{L}} = \\frac{H_{S}}{L \\cdot E }\n\\qquad(6)\\]\nand by rearranging the terms, the long-term energy balance in Equation Equation 5 can simply be rewritten as\n\\[\nR_{N} = (1 + \\gamma)L E\n\\qquad(7)\\]\nUsing the fact that \\(R_{N} = L E_{pot}\\), where \\(E_{pot}\\) is the potential evaporation, and dividing by precipitation, we can rewrite the above Equation 7 as\n\\[\n\\frac{E_{pot}}{P} = (1 + \\gamma) \\frac{E}{P}\n\\qquad(8)\\]\nwhere the left-hand side is called the aridity index, i.e. \\(\\phi = E_{pot}/P\\) and \\(E/P\\) is called the evaporative fraction or evaporation index as mentioned above. With this, Equation 8 from above can be written as a function of the Bowen ratio and the aridity index, i.e.\n\\[\n\\frac{E}{P} = 1 - \\frac{Q}{P} = \\frac{\\phi}{(1 + \\gamma)}\n\\qquad(9)\\]\n\\(Q/P\\) is again the runoff index. Since the Bowen ratio is also water supply and energy demand limited, it too is a function of the aridity index and we can thus rewrite Equation 9 as\n\\[\n\\frac{E}{P} = \\frac{\\phi}{1 + f(\\phi)} = F[\\phi]\n\\qquad(10)\\]\nThe Budyko relationship thus allows for a simple parameterization of how the aridity index \\(\\phi\\) controls the long-term mean partitioning of precipitation into stream-flow and evaporation and it is capable of capturing the behavior of thousands of catchments around the world. This explains its growing popularity over recent years (Berghuijs, Gnann, and Woods 2020).\n\n\n\nFigure @ref(fig:budykoSpace) shows a plot of data from catchments in the US for which consistent long-term hydro-climatological data records are available. Individual catchments’ aridity indices are plotted against evaporative fractions, averaged over many years. The catchment data plots along the Budyko curve in the two-dimensional Budyko space as indicated in the Figure where the Budyko curve is defined as\n\\[\\begin{equation}\n  \\frac{E}{P} = \\left[ \\frac{E_{pot}}{P} \\text{tanh} \\left( \\frac{P}{E_{pot}} \\right) \\left( 1 - \\text{exp} \\left( - \\frac{E_{pot}}{P} \\right) \\right) \\right]^{1/2}\n  (\\#eq:OriginalBudykoCurveEquation)\n\\end{equation}\\]\nThis non-parametric relationship between the aridity index and the evaporative fraction was developed by M. Budyko (Budyko 1951).\nThe Budyko space is delineated by the demand and supply limits. Catchments within the space should theoretically fall below the supply limit (\\(E/P = 1\\)) and the demand limit (\\(E/E_{pot} = 1\\)), but tend to approach these limits under very arid or very wet conditions (Berghuijs, Gnann, and Woods 2020). The data from the US shows that a large percentage of in-between catchment variability can be explained by the Budyko curve. After the seminal work Budyko in the last century, the evidence for a strong universal relationship between aridity and evaporative fraction via the Budyko curve has since grown. As catchment hydrology still lacks a comprehensive theory that could explain this simple behavior across diverse catchments Gentine et al. (2012), the ongoing debate about the the underlying reasons for this relationship continues (see e.g. (Padron et al. 2017; Berghuijs, Gnann, and Woods 2020)).\nWhile almost all catchments plot within a small envelope of the original Budyko curve, systematic deviations are nevertheless observed from the original Budyko curve. Several new expressions for \\(F[\\phi]\\) were therefore developed to describe the long-term catchment water balance with one parameter (see e.g. Budyko (1974); Sposito (2017); Choudhury (1999)). One popular equation using only 1 parameter is the Choudhury equation which relates the aridity index \\(\\phi\\) to the evaporative fraction \\(E/P\\) in the following way\n\\[\\begin{equation}\n  \\frac{E}{P} = \\left[ 1 + \\left( \\frac{E_{pot}}{P} \\right) ^{-n} \\right]^{1/n}\n  (\\#eq:Choudhury1)\n\\end{equation}\\]\nwhere \\(n\\) is a catchment-specific parameter which accounts for factors such as vegetation type and coverage, soil type and topography, etc. (see e.g. Zhang et al. (2015) for more information). In other words, \\(n\\) integrates the net effects of all controls of of the evaporative fraction other than aridity. The Figure @ref(fig:ChoudhuryEquationStateSpace) shows the control of \\(n\\) over the shape of the Budyko Curve."
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-budyko-data-and-methods",
    "href": "long_term_water_balance_modeling.html#sec-budyko-data-and-methods",
    "title": "",
    "section": "Data and Methods",
    "text": "Data and Methods\n\nData\nA large number of geospatial data were collected for the Central Asia region. The domain of interest was defined as 55 deg. E - 85. deg. E and 30 deg. N - 50 deg. N.. Shapefiles from the large river basin were retrieved from the Global Runoff Data Center and extracted for the following basins: Amu Darya, Chu, Issy Kul, Murghab-Harirud, Syr Darya and Talas. Where necessary, the polygons of the downstream flat areas were corrected to account for man-made water transfers via large canal systems and corresponding flow alterations across basins there. These large river basins define the area of interest (AOI).\nFor the selected basins, the WMOBB River Network data was extracted from the layers wmobb_rivnets_Q09_10 (containing line sections representing an upland area above 4’504 km2), wmobb_rivnets_Q08_09 (containing line sections representing an upland area between 1’150 km2 and 4’504 km2) and wmobb_rivnets_Q07_08 (containing line sections representing an upland area above between 487 and 1’150 km2) (GRDC, Koblenz, Germany: Federal Institute of Hydrology (BfG). 2020). Permanent water bodies and courses were taken from the global HydroLakes Database (Messager et al. 2016). Information on land cover were taken from the Copernicus Global Land Service: Land Cover 100m: collection 3: epoch 2019: Globe data (Buchhorn et al. 2019). The NASA SRTM digital elevation model 1 Arc-second (30 m) global product was used as a DEM (“NASA Shuttle Radar Topography Mission (SRTM)(2013)” 2013).\nIn total, data from 277 gauging stations from Afghanistan, Kyrgyzstan, Kazakhstan, Uzbekistan and Tajikistan could be obtained from the local Hydrometeorological Organization, public reports and the Soviet compendia Surface Water Resources, Vol 14 Issues 1 and 3 . Except for the Afghan stations, all stations were manually located in a Geographic Information System (GIS) using the relevant Soviet Military Topographic maps (1:200’000) from the corresponding region. The maps were downloaded from https://maps.vlasenko.net and subsequently geo-referenced in QGIS (QGIS Development Team 2021). Data from northern Afghan rivers’ stream flow characteristics and the location of these gauging stations was taken from (Olson and Williams-Sether 2010).\nFor each gauge, the contributing area was delineated in R with the WhiteboxTools v2.0.0 and long-term norm mean discharge was obtained over variable observation periods between 1900 and 2018 was acquired. For a few selected stations, monthly and decadal time series data are available over the entire observational record. The FLO1K, global maps of mean, maximum and minimum annual stream flow at 1 km resolution from 1960 through 2015 were retrieved (Barbarossa et al. 2018). The goodness of the FLO1K product in the Central Asia domain was validated at the locations of the 277 gauges through linear regression.\nGeospatial information on glaciers was taken from the Randolph Glacier Inventory (RGI) 6.0. Information from 16’617 glaciers was retrieved, together with glacier length, thickness and glacier thinning rates Hugonnet et al. (2021).\nThe CHELSA V21 global daily high-resolution climatology, available from 01-01-1979 until 31-12-2011 was processed over the Central Asia domain to map climate trends, including on temperature, precipitation, snow fraction. The data is available upon request from this site: https://chelsa-climate.org Karger et al. (2021). The CHELSA V21 product is corrected for snow undercatch in the high elevation ranges and thus is able to better represent actual high mountain precipitation than other available global climatologies (Beck et al. 2020). The aridity index (AI) fields were taken from the bio-climate CHELSA V21 data set and compared with the CGIAR AI product (Trabucco and Zomer 2019). Data on an additional 70 bio-climatic indicators were downloaded from the CHELSA V21 1980 - 2010 climatology and statistics extracted for each of the 277 gauged catchments, together with the AI.\nHigh-resolution crop disaggregated irrigated areas were mapped over the entire Central Asia domain (Ragettli, Herberz, and Siegfried 2018). Like this 30 m crop maps were produced with Google Earth Engine using unsupervised classification for the years 2016 - 2020. Vector information on the irrigation systems in the Chu and Talas River basins as well as from the Uzbek Fergana Oblast, including the land cadaster there, are available.\nFinally, data from the GOODD data set was used to retrieve information from 88 dams in the region of interest (Mulligan, Soesbergen, and Sáenz 2020).\n\n\nMethods\nA strategy for hydrological modeling of the regional Central Asian hydrology using the Budyko framework was devised. The Budyko principle posits that, over the long-run, runoff at a particular location is governed by the long-term availability of water (supply) and energy (demand) there (Budyko M., 1974). Under this assumption, the evaporative fraction of a basin, i.e. the long-term mean actual evaporation divided by long-term mean precipitation, can be expressed as a function of the aridity index (long-term mean potential evaporation divided by long-term mean precipitation)."
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-budyko-results",
    "href": "long_term_water_balance_modeling.html#sec-budyko-results",
    "title": "",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-discussion-and-conclusions",
    "href": "long_term_water_balance_modeling.html#sec-discussion-and-conclusions",
    "title": "",
    "section": "Discussion and Conclusions",
    "text": "Discussion and Conclusions"
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-long-term-water-balance-modeling-references",
    "href": "long_term_water_balance_modeling.html#sec-long-term-water-balance-modeling-references",
    "title": "",
    "section": "References",
    "text": "References\n\n\n\n\nArora, Vivek K. 2002. “The Use of the Aridity Index to Assess Climate Change Effect on Annual Runoff.” Journal of Hydrology 265 (1): 164–77. https://doi.org/https://doi.org/10.1016/S0022-1694(02)00101-4.\n\n\nBarbarossa, Valerio, Mark A. J. Huijbregts, Arthur H. W. Beusen, Hylke E. Beck, Henry King, and Aafke M. Schipper. 2018. “Flo1k, Global Maps of Mean, Maximum and Minimum Annual Streamflow at 1 Km Resolution from 1960 Through 2015.” Scientific Data 5 (1): 180052. https://doi.org/10.1038/sdata.2018.52.\n\n\nBeck, Hylke E., Eric F. Wood, Tim R. McVicar, Mauricio Zambrano-Bigiarini, Camila Alvarez-Garreton, Oscar M. Baez-Villanueva, Justin Sheffield, and Dirk N. Karger. 2020. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1.\n\n\nBerghuijs, W. R., S. J. Gnann, and R. A. Woods. 2020. “Unanswered Questions on the Budyko Framework.” Hydrological Processes.\n\n\nBuchhorn, M., B. Smets, L. Bertels, B. De Roo, M. Lesiv, N. E. Tsendbazar, M. Herold, and S. Fritz. 2019. “Copernicus Global Land Service: Land Cover 100m: Collection 3: Epoch 2019: Globe.”\n\n\nBudyko, M. I. 1951. “On Climatic Factors of Runof (in Russian).” Problemy Fiz Geeografii 16: 41–48.\n\n\n———. 1974. Climate and Life. Academic Press.\n\n\nChoudhury, B. J. 1999. “Evaluation of an Empirical Equation for Annual Evaporation Using Field Observations and Results from a Biophysical Model.” Journal of Hydrology 216: 99–110.\n\n\nFarinotti, Daniel, Matthias Huss, Johannes J. Fürst, Johannes Landmann, Horst Machguth, Fabien Maussion, and Ankur Pandit. 2019. “A Consensus Estimate for the Ice Thickness Distribution of All Glaciers on Earth.” Nature Geoscience 12 (3): 168–73. https://doi.org/10.1038/s41561-019-0300-3.\n\n\nGentine, P., P. D’Odorico B. R. Lintner, G. Sivandran, and G. Salvucci. 2012. “Interdependence of Climate, Soil, and Vegetation as Constrained by the Budyko Curve.” Geophysical Research Letters 39 (19).\n\n\nGLIMS, and NSIDC. 2005, updated 2018. Global Land Ice Measurements from Space Glacier Database. Compiled and made available by the international GLIMS community and the National Snow and Ice Data Center, Boulder CO, U.S.A. DOI:10.7265/N5V98602.\n\n\nGRDC, Koblenz, Germany: Federal Institute of Hydrology (BfG). 2020. “Major River Basins of the World / Global Runoff Data Centre, GRDC. 2nd, Rev. Ext. Ed.” Shape.\n\n\nHugonnet, Romain, Robert McNabb, Etienne Berthier, Brian Menounos, Christopher Nuth, Luc Girod, Daniel Farinotti, et al. 2021. “Accelerated Global Glacier Mass Loss in the Early Twenty-First Century.” Nature 592 (7856): 726–31. https://doi.org/10.1038/s41586-021-03436-z.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nKarger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-resolution monthly precipitation and temperature time series from 2006 to 2100.” Scientific Data 7 (1): 248. https://doi.org/10.1038/s41597-020-00587-y.\n\n\nKarger, Dirk Nikolaus, Adam M. Wilson, Colin Mahony, Niklaus E. Zimmermann, and Walter Jetz. 2021. “Global daily 1 km land surface precipitation based on cloud cover-informed downscaling.” Scientific Data 8 (1): 307. https://doi.org/10.1038/s41597-021-01084-6.\n\n\nMessager, M. L., B. Lehner, Grill G., I. Nedeva, and O. Schmitt. 2016. “Estimating the Volume and Age of Water Stored in Global Lakes Using a Geo-Statistical Approach.” Nature Communications 13603.\n\n\nMiralles, D. G., W. Brutsaert, A. J. Dolman, and J. H. Gash. 2020. “On the Use of the Term \"Evapotranspiration\".” Water Resources Research 56 (https://doi.org/10.1029/2020WR028055).\n\n\nMulligan, Mark, Arnout van Soesbergen, and Leonardo Sáenz. 2020. “GOODD, a Global Dataset of More Than 38,000 Georeferenced Dams.” Scientific Data 7 (1): 31. https://doi.org/10.1038/s41597-020-0362-5.\n\n\n“NASA Shuttle Radar Topography Mission (SRTM)(2013).” 2013. NASA. https://earthdata.nasa.gov/learn/articles/nasa-shuttle-radar-topography-mission-srtm-version-3-0-global-1-arc-second-data-released-over-asia-and-australia.\n\n\nOlson, S. A., and T. Williams-Sether. 2010. “Streamflow Characteristics at Streamgages in Northern Afghanistan and Selected Locations.” U.S. Geological Survey Data Series 529. USGS.\n\n\nPadron, R. S., L. Gudmundsson, P. Greve, and S. Seneviratne. 2017. “Largescale Controls of the Surface Water Balance over Land: Insights from a Systematic Review and Meta-Analysis.” Water Resources Research.\n\n\nQGIS Development Team. 2021. QGIS Geographic Information System. QGIS Association.\n\n\nRagettli, Silvan, Timo Herberz, and Tobias Siegfried. 2018. “An Unsupervised Classification Algorithm for Multi- Temporal Irrigated Area Mapping in Central Asia.” Remote Sensing 10 (11): 1823. https://doi.org/10.3390/rs10111823.\n\n\nSposito, Garrison. 2017. “Understanding the Budyko Equation.” Water 9 (4): 236. https://doi.org/10.3390/w9040236.\n\n\nTrabucco, Antonio, and Robert Zomer. 2019. “Global Aridity Index and Potential Evapotranspiration (ET0) Climate Database v2,” January. https://doi.org/10.6084/m9.figshare.7504448.v3.\n\n\nZhang, D., Z. Cong, G. Ni, D. Yang, and S. Hu. 2015. “Effects of snow ratio on annual runoff within the Budyko framework.” Hydrology and Earth System Sciences 19 (4): 1977–92. https://doi.org/10.5194/hess-19-1977-2015."
  },
  {
    "objectID": "glacier_modeling.html#sec-glacier-modeling",
    "href": "glacier_modeling.html#sec-glacier-modeling",
    "title": "",
    "section": "Modeling of Discharge from Glacier Melt",
    "text": "Modeling of Discharge from Glacier Melt\nThe glacier melt modeling in RSMinerve (Garcia Hernandez et al. 2020) is done (at the time of writing, March 2022) with the GSM model which features a constant area and an unlimited glacier reservoir. It is suitable for short-term simulations of glacier melt where effects of glacier volume change can be neglected but it is not well suited for climate change impact studies where substantial changes in glacier volume are to be expected. However, by assuming that discharge from snow melt can be treated independently from discharge from glacier melt, the discharge contribution from glacier melt can be simulated in R and included as source into RSMinerve models. The following chapter relies on the basic understanding of the glacier mass balance as presented and discussed in Chapter Snow and Glacier Data, describes the glacier modelling tools in the package riversCentralAsia and demonstrates how to use them for joint applied hydrological modelling with RSMinerve."
  },
  {
    "objectID": "glacier_modeling.html#temperature-index-model",
    "href": "glacier_modeling.html#temperature-index-model",
    "title": "11  Modeling of Discharge from Glacier Melt",
    "section": "\n11.1 Temperature Index Model",
    "text": "11.1 Temperature Index Model\nHock (2003) describes several variants of the temperature index model for simulating glacier melt. The riversCentralAsia package implements the temperature index melt model described in (Hock 2003) in the function glacierMelt_TI (Equation 11.1).\n\\[\nM = \\biggl\\{ \\begin{array}{l, l}\n0, & T < T_{threshold} \\\\\nf_{M} \\cdot \\left( T - T_{threshold} \\right), & T >= T_{threshold}\n\\end{array}\n\\qquad(11.1)\\]\nwhere \\(M\\) is the glacier melt in \\(mm/d\\), \\(T\\) is the daily average temperature in \\(^{\\circ} C\\). The two parameters \\(f_{M}\\) and \\(T_{\\text{threshold}}\\) refer to the melt factor and the threshold temperature above which glacier melt occurs and need to be calibrated. They have the units \\(\\frac{mm}{^{\\circ} C \\cdot d}\\) and \\(^{\\circ} C\\) respectively. Glacier melt is calculated in daily time steps."
  },
  {
    "objectID": "glacier_modeling.html#glacier-mass-balance",
    "href": "glacier_modeling.html#glacier-mass-balance",
    "title": "11  Modeling of Discharge from Glacier Melt",
    "section": "\n11.2 Glacier Mass Balance",
    "text": "11.2 Glacier Mass Balance\nThe glacier mass balance is simplified to Equation 11.2:\n\\[\n\\Delta S = P-M = A_{\\text{imbal}}\n\\qquad(11.2)\\]\nwhere the change of water storage (\\(\\Delta S\\)) is equal to the precipitation (\\(P\\)) minus the glacier melt (\\(M\\)). Typically melt exceeds precipitation and we have negative \\(\\Delta S\\), that is imbalance ablation, indicating glacier storage loss. The glacier mass balance is calculated in annual time steps. We thereby refer to the hydrological year starting on October 1st of the previous year to take a full accumulation and ablation season into account."
  },
  {
    "objectID": "glacier_modeling.html#glacier-volume-development",
    "href": "glacier_modeling.html#glacier-volume-development",
    "title": "11  Modeling of Discharge from Glacier Melt",
    "section": "\n11.3 Glacier Volume Development",
    "text": "11.3 Glacier Volume Development\nAs glaciers melt, their volume changes. This has to be taken into account for the long-term simulation of glacier discharge. To determine the initial glacier volume, the area of the geometry of the Randolph Glacier Inventory (RGI) v6.0 data set is multiplied with the average thickness of the glacier (the Farinotti data set). Please note that the data and the retrieval of the data are described in Part II of this book. Glaciers larger than 1km2 are sub-divided into elevation bands of 100 m altitude to account for elevation dependent temperature forcing. The large glaciers melt from the lowest elevation band to the highest elevation band whereby the glacier melt is subtracted from the glacier volume of lowest elevation band that is still glacierized. The small glaciers are not spatially discretized and thus they are melted homogeneously.\nFor annual time step \\(t\\), the evolution of the glacier volume is calculated as follows:\n\\[\nA(t) = \\text{glacierArea RGIF}\\bigl(V(t)\\bigr)\n\\qquad(11.3)\\]\n\\[\nQ_{glacier}(t) = M(t) \\cdot A(t)\n\\qquad(11.4)\\]\n\\[\nV(t+1) = V(t) + \\Delta S = V(t) + \\text{glacierImbalAbl}\\bigl(M(t)\\bigr)\n\\qquad(11.5)\\]\n\\(Q_{\\text{glacier}}\\) can be calibrated against glacier discharge derived from the Miles & Hugonnet data sets. The automated calibration is currently not included in the riversCentralAsia package.\nThe function glacierArea_RGIF() is an empirical scaling function analogue the inverse of the scaling function derived by Erasov (1968) but based on the modern RGI v6.0 glacier geometries and the glacier thickness data set by Farinotti et al. (2019). The package riversCentralAsia implements volume-area and area-volume scaling functions based on both, Erashov and RGI-Farinotti data, allowing the estimation of glacier areas based on glacier volumes (glacierArea_Erasov and glacierArea_RGIF) and estimations of glacier volumes based on glacier areas (glacierVolume_Erasov and glacierVolume_RGIF).\nThe function glacierImbalAbl is an empirical scaling function relating glacier imbalance ablation, i.e. the glacier storage loss glacier melt. It is derived from the glacier discharge data set by Miles et al. (2021) and the glacier thinning data set by Hugonnet et al. (2021)."
  },
  {
    "objectID": "glacier_modeling.html#summary-of-workflow",
    "href": "glacier_modeling.html#summary-of-workflow",
    "title": "11  Modeling of Discharge from Glacier Melt",
    "section": "\n11.4 Summary of Workflow",
    "text": "11.4 Summary of Workflow\nTo model the contribution to discharge from glacier melt in a basin, the following steps are required:\n\nPre-processing of GIS layers\n\nPre-processing of climate forcing data\n\nCalculation of daily glacier melt\n\nCalculation of annual glacier masss balance\n\nScaling of annual glacier contribution to daily values\n\nAggregation of per glacier contribution to sub-basins (optional)\nWriting of RSMinerve source intput files\n\nIntegration of glacier discharge sources in RSMinerve"
  },
  {
    "objectID": "glacier_modeling.html#propagation-of-uncertainty",
    "href": "glacier_modeling.html#propagation-of-uncertainty",
    "title": "11  Modeling of Discharge from Glacier Melt",
    "section": "\n11.5 Propagation of Uncertainty",
    "text": "11.5 Propagation of Uncertainty\nThe initial glacier volume of each glacier is attributed an uncertainty of p/m 26%. This number is based on the average uncertainty of the glacier volume per RGI region reported in Farinotti et al. (2019). The uncertainty of the area of the RGI v6.0 glacier outlines is not known. It is therefore assumed that the error of the glacier volume stems to 50% from the estimation of the glacier thickness and to 50% from the glacier area. We further assume that the errors of the glacier area and glacier thickness are un-correlated and can thus estimate the uncertainties of the glacier area data and the glacier thickness data to be p/m 13% each.\nFor the non-linear relationships in glacierVolume_RGIF and glacierArea_RGIF, the standard deviation of the residuals of the fit was computed. The estimated error of the fit is assumed to be equal plus/minus twice the standard deviation of the residuals and yields 31% and 53% respectively. The residuals are not normally distributed and their actual distribution is unknown. Further, uncorrelated errors and the applicability of linear error propagation are assumed. Therefore, the error of the function outputs is simply computed by adding the error of the function input to the error of the fit.\n\\[\n\\varepsilon_{V} = \\varepsilon_{A} + \\varepsilon_{\\text{glacierVolume RGIF}} = 0.26 + 0.31 = 0.57\n\\qquad(11.6)\\]\n\\[\n\\varepsilon_{A} = \\varepsilon_{V} + \\varepsilon_{\\text{glacierArea RGIF}} = 0.26 + 0.53 = 0.79\n\\qquad(11.7)\\]\nError estimates for the temperature index model are not available. A conservative relative error of 2 is therefore assumed, indicating that the estimated glacier melt is within a range of plus/minus 2 times it’s value.\nThe error of the imbalance ablation amounts to 73%. Assuming independent errors from the temperature index model for glacier melt and the scaling function between imbalance ablation and glacier melt, the error of the imbalance ablation amounts to approximately:\n\\[\n\\varepsilon_{Q_{\\text{imb,melt}}} = \\varepsilon_{Q_{\\text{M}}} + \\varepsilon_{\\text{glacierImbalAbl}} = 2 + 0.73 = 2.73\n\\qquad(11.8)\\]\nThe errors stated above relate to initial estimates of the glacier areas, volumes, total ablation and imbalance ablation. The errors of the glacier model variables of each subsequent time step depend on the errors of the previous time steps, i.e. they are not uncorrelated over time. For simplicity reason, this non-linear propagation of errors is neglected. In any case, the uncertainty margins for any glacier melt modelling done with the presented method are large. The following table summarises the estimated errors for each variable\n\n# Relative error estimates for the initial state in %\nerror_stats = tibble::tibble(\n  sV = 0.26,  # Glacier volume\n  sA = 0.13,  # Glacier area\n  sTh = 0.13,  # Glacier thickness\n  sMelt = 2)  # Glacier melt"
  },
  {
    "objectID": "glacier_modeling.html#demonstration",
    "href": "glacier_modeling.html#demonstration",
    "title": "11  Modeling of Discharge from Glacier Melt",
    "section": "\n11.6 Demonstration",
    "text": "11.6 Demonstration\nWe demonstrate the above described method with the data from the Atbashy basin.\n\n\n\n\n\n\nWarning\n\n\n\nAll required data is available from the downloadable data package. Note however that this folder required 12 GB of local storage space!\n\n\n\nlibrary(tmap)\nlibrary(sf)\nlibrary(raster)\nlibrary(exactextractr)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(riversCentralAsia)\n# Path to the data directory downloaded from the download link provided above. \n# Here the data is extracted to a folder called atbashy_glacier_demo_data\ndata_path <- \"../caham_data/SyrDarya/Atbashy/\"\n\n\n11.6.1 Forcing\nThere will be a separate Section to demonstrate how to prepare the forcing. For now, we provide you with the pre-processed forcing data and give you just a brief overview over the data source.\nHistorical Temperatures\nAs meteorological data for high elevations in Central Asia is very scarce we use the CHELSA v2.1 data set (Karger et al. 2017). This is a global data set of forcing data for hydrolgical models based on ERA5 but corrected for biases and especially suited for high elevations. The daily CHELSA forcing has been cut to the Central Region by the originator of the data set, D. Karger, WSL and extracted to the hydrological response units of the glaciers in the Atbashy basin by T. Siegfried.\n\nhist_obs <- readRDS(file = paste0(data_path, \"CLIMATE/hist_obs_glacier_tas.rds\"))\n# Plot the temperature time series for a given glacier/elevation band\nglacier <- \"RGI60-13.08930_1\"\nggplot(hist_obs) + \n  geom_line(aes(date, get(glacier))) + \n  labs(x = \"Date\", y = \"T [deg C]\") +\n  theme_bw()\n\n\n\nFigure 11.1: Daily temperature time series of a small glacier (RGI60-13.08930) in the Atbashy basin. Data source: CHELSA v2.1.\n\n\n\n\nFuture Temperatures\nFuture temperature development per glacier or elevation band is extracted from the 3 CMIP6 GCM models with highest priorities for the region downloaded from COPERNICUS. We take 4 socioeconomic scenarios into account, covering 4 different emission scenarios. The temperatures of the climate models are bias corrected using the CHELSA data and a quantile mapping method. More details in the climate data preparation section.\n\nfut_sim <- readRDS(file = paste0(data_path, \"CLIMATE/fut_sim_glacier_tas_qmapped.RDS\"))\n# Plot the temperature time series for a given glacier/elevation band\nglacier <- \"RGI60-13.08930_1\"\n# Extract the temperature for the selected glaciers for all GCMs and SSPs\nscenarios <- names(fut_sim)\nfut_temp <- NULL\nfor (scenario in scenarios) {\n  fut_temp <- rbind(fut_temp, \n                    fut_sim[[scenario]] |> \n                      dplyr::select(Date, all_of(glacier)) |> \n                      mutate(Scenario = scenario))\n}\nfut_temp <- fut_temp |> \n  mutate(Hyear = hyear(Date)) |> \n  group_by(Hyear, Scenario) |> \n  summarise(Date = first(Date), \n            Temp = mean(get(glacier))) |> \n  separate(Scenario, into = c(\"GCM\", \"SSP\"), sep = \"_\") |> \n  ungroup() |>\n  dplyr::filter(Hyear > min(Hyear) & Hyear < max(Hyear), \n                GCM != \"IPSL-CM6A-LR\")\n# Plot annual data for better readability\nggplot() + \n  geom_line(data = hist_obs |> \n              mutate(Hyear = hyear(date)) |> \n              group_by(Hyear) |> \n              summarise(date = first(date), \n                        Temp = mean(get(glacier))) |> \n              ungroup() |> \n              dplyr::filter(Hyear > min(Hyear) & Hyear < max(Hyear)), \n            aes(date, Temp)) + \n  geom_line(data = fut_temp, aes(Date, Temp, colour = GCM, \n            linetype = SSP)) + \n  scale_colour_viridis_d() + \n  labs(x = \"Date\", y = \"T [deg C]\") +\n  theme_bw()\n\n\n\nFigure 11.2: Annual historical (CHELSA) and future (CMIP6, Copernicus) temperature for glacier RGI60-13.08930.\n\n\n\n\n\n11.6.2 Glacier Geometry\n\n# Load data\n## Digital elevation model (DEM)\ndem <- raster(paste0(data_path, \"GIS/16076_DEM.tif\"))\n\n## Glacier outlines from the Randolph Glacier Inventory (RGI) v6.0 \nrgi <- st_read(paste0(data_path, \"GIS/16076_Glaciers_per_subbasin.shp\"), \n               quiet = TRUE) |> \n  st_transform(crs = crs(dem))\n\n## Outlines of hydrological response units for the modelling of glacier discharge.  \nrgi_elbands <- st_read(paste0(data_path, \n                              \"GIS/rgi_glaciers_atbaschy_el_bands.shp\"), \n               quiet = TRUE) |> \n  st_transform(crs = crs(dem))\n\n# Load a pre-processed raster file with glacier thickness in the Atbashy basin\n# (see vignette [glaciers 01]{glaciers-01-intro.html} for details on the pre\n# -processing)\nglacier_thickness <- raster(\n  paste0(data_path, \n  \"GLACIERS/Farinotti/pre-processed_glacier_thickness.tif\"))\n\n# Load the glacier thickness data set and filter it to the glaciers in the \n# catchment of interest.  \nhugonnet <- read_csv(paste0(\n  data_path, \"/GLACIERS/Hugonnet/dh_13_rgi60_pergla_rates.csv\"))\nhugonnet <- hugonnet |> \n  dplyr::filter(rgiid %in% rgi$RGIId) |> \n  tidyr::separate(period, c(\"start\", \"end\"), sep = \"_\") |> \n  mutate(start = as_date(start, format = \"%Y-%m-%d\"), \n         end = as_date(end, format = \"%Y-%m-%d\"), \n         period = round(as.numeric(end - start, units = \"days\")/366))\nglaciers_hugonnet <- rgi |> \n  left_join(hugonnet |> dplyr::select(rgiid, area, start, end, dhdt, err_dhdt, \n                                      dvoldt, err_dvoldt, dmdt, err_dmdt, \n                                      dmdtda, err_dmdtda, period),  \n            by = c(\"RGIId\" = \"rgiid\")) \n# Only keep the variables we need for this analysis\nrgi_elbands <- rgi_elbands |> \n  dplyr::select(RGIId, Area, elvtn_b) |> \n  rename(Area_tot_glacier_km2 = Area) |> \n  mutate(ID = paste0(RGIId, \"_\", elvtn_b))\n# Get mean elevation of each glacier/elevation band from DEM\nrgi_elbands$z_masl <- exact_extract(dem, rgi_elbands, \"mean\", progress = FALSE)\n# Update the glacier area within the basin boundaries\nrgi_elbands$A_km2 <- as.numeric(st_area(rgi_elbands))*10^(-6)\nglaciers <- unique(rgi_elbands$RGIId)\n# Calculate the average glacier thickness for each elevation band. \nrgi_elbands$thickness_m = exact_extract(glacier_thickness, \n                                        rgi_elbands, \"mean\", progress = FALSE)\n\n\n11.6.3 Processing of Forcing\nTemperature time series for each glacier/elevation band are available. Figure 11.3} shows temperature time series extracted from the CHELSA data set on the example of one of the larger glaciers in the Atbashy basin. The data is aggregated to the hydrological year to better illustrate the temperature gradient over the elevation. The highest temperatures are measured at the lowest elevations and vice versa.\n\nggplot(hist_obs[, 1:9] |> \n         pivot_longer(-date, names_to = \"ID\", values_to = \"Temp\") |> \n         mutate(Hyear = hyear(date)) |> \n         group_by(Hyear, ID) |> \n         summarise(date = first(date), \n                   Temp = mean(Temp))|> \n         ungroup() |> \n         dplyr::filter(Hyear>min(Hyear) & Hyear<max(Hyear)) |> \n         left_join(rgi_elbands |> \n                     st_drop_geometry() |> \n                     dplyr::select(ID, z_masl), by = \"ID\") |> \n         mutate(\"Elevation [masl]\" = as.factor(round(z_masl)))) + \n  geom_line(aes(date, Temp, colour = `Elevation [masl]`)) + \n  scale_colour_viridis_d() + \n  labs(x = \"Date\", y = \"T [deg C]\") + \n  theme_bw()\n\n\n\nFigure 11.3: Temperature forcing for the elevation bands of a glacier.\n\n\n\n\n\n11.6.4 Calculating Glacier Melt\nThe code snippet below illustrates how to calculate the annual glacier melt of the catchment. Figure 11.4 shows daily melt rates for the different elevation bands of one of the larger glaciers (same as in the figure above). The highest melt occurs at low elevation bands.\n\nMF_small = 1\nMF_large = 0.5\nthreshold_temperature = 0\nArea <- rgi_elbands |> \n  st_drop_geometry() |> \n  dplyr::select(ID, A_km2) |> \n  pivot_wider(names_from = ID, values_from = A_km2)\n\n# Assign different melt factors to large and small glaciers. \nMF <- Area |> \n  mutate(across(everything(), ~MF_large), \n         across(ends_with(\"_1\"), ~MF_small))\nmelt <- glacierMelt_TI(temperature = hist_obs |> dplyr::select(-date),\n                       MF = MF,\n                       threshold_temperature = threshold_temperature)\nmelt <- as_tibble(melt) |> \n  mutate(date = hist_obs$date) |> \n  relocate(date, .before =  where(is.numeric))\nggplot(melt[, 1:9] |> \n         pivot_longer(-date, names_to = \"ID\", values_to = \"Melt\") |> \n         left_join(rgi_elbands |> \n                     st_drop_geometry() |> \n                     dplyr::select(ID, z_masl), by = \"ID\") |> \n         mutate(\"Elevation [masl]\" = as.factor(round(z_masl)))) + \n  geom_line(aes(date, Melt, colour = `Elevation [masl]`)) + \n  scale_colour_viridis_d() + \n  labs(x = \"Date\", y = \"Melt [mm/d]\") + \n  theme_bw()\n\n\n\nFigure 11.4: Daily glacier melt per elevation band.\n\n\n\n\n\n11.6.5 Compare to Measured Glacier Melt\nAggregate the daily glacier melt to annual data and compare it to the observed glacier melt. Note that the glacier melt simulated with the temperature index model (sim) is never negative whereas the glacier mass change derived from Hugonnet et al. (2021) and Miles et al. (2021) can be negative, indicating glacier growth.\n\nmelt_a_eb <- melt |> \n  pivot_longer(-date, names_to = \"ID\", values_to = \"Melt\") |> \n  mutate(Hyear = hyear(date)) |> \n  group_by(Hyear, ID) |> \n  summarise(date = first(date), \n            Melt = sum(Melt), \n            .lb_Melt = ifelse(Melt*(1-error_stats$sMelt)<0, 0, \n                              Melt*(1-error_stats$sMelt)), \n            .ub_Melt = Melt*(1+error_stats$sMelt))|> \n  ungroup() \nmelt_a <- melt_a_eb |> \n  separate(ID, into = c(\"RGIId\", \"elB\"), sep = \"_\") |> \n  group_by(Hyear, RGIId) |> \n  summarise(date = as_date(first(date)), \n            Melt = sum(Melt), \n            .lb_Melt = ifelse(Melt*(1-error_stats$sMelt)<0, 0, \n                              Melt*(1-error_stats$sMelt)), \n            .ub_Melt = Melt*(1+error_stats$sMelt)) |> \n  ungroup() |> \n  dplyr::filter(Hyear > min(Hyear) & Hyear < max(Hyear))\nglaciers_hugonnet <- glaciers_hugonnet |> \n  mutate(Qgl_m3a = glacierDischarge_HM(dhdt), \n         .lb_Qgl_m3a = ifelse(\n           dhdt > 0, \n           ifelse(Qgl_m3a*(1-error_stats$sQglgrowth)<0, 0, \n                  Qgl_m3a*(1-error_stats$sQglgrowth)), \n           ifelse(Qgl_m3a*(1-error_stats$sQglmelt)<0, 0, \n                  Qgl_m3a*(1-error_stats$sQglmelt))),\n         .ub_Qgl_m3a = ifelse(dhdt > 0, \n                              Qgl_m3a*(1+error_stats$sQglgrowth), \n                              Qgl_m3a*(1+error_stats$sQglmelt)))\nmelt_obs_a <- glaciers_hugonnet |> \n  dplyr::filter(RGIId %in% glaciers[6:9], \n                period == 1)\n  \nggplot() + \n  geom_ribbon(data = melt_a |> \n                dplyr::filter(RGIId %in% glaciers[6:9]), \n              aes(date, Melt/1000, ymin = .lb_Melt/1000, ymax = .ub_Melt/1000, \n                  fill = RGIId), colour = NA, alpha = 0.2) + \n  geom_ribbon(data = melt_obs_a |> \n                dplyr::filter(RGIId %in% glaciers[6:9]), \n              aes(start, -dmdtda, \n                  ymin = -dmdtda-err_dmdtda, \n                  ymax = -dmdtda+err_dmdtda, \n                  colour = RGIId, linetype = \"obs\", fill = RGIId), \n              size = 0.2, alpha = 0.2) + \n  geom_line(data = melt_a |> \n              dplyr::filter(RGIId %in% glaciers[6:9]), \n            aes(date, Melt/1000, colour = RGIId, linetype = \"sim\")) + \n  geom_line(data = melt_obs_a, aes(start, -dmdtda, colour = RGIId, \n                                   linetype = \"obs\")) + \n  labs(x = \"Date\", y = \"Melt [m weq/a]\") + \n  scale_linetype_manual(name = \"Source\", \n                        values = c(\"sim\" = 1, \"obs\" = 2)) + \n  scale_colour_viridis_d() + \n  scale_fill_viridis_d() + \n  ggtitle(paste0(\"MF: \", MF, \"Tth: \", threshold_temperature)) + \n  theme_bw()\n\n\n\nFigure 11.5: Daily glacier melt per elevation band.\n\n\n\n\nThe temperature index model can be calibrated with the specific glacier volume change provided by Hugonnet et al. (2021) (see Figure 11.5}). For the moment, manual calibration of the parameters is required.\n\n11.6.6 Glacier Mass Balance\nThe following figure shows the components of the glacier mass balance for a few glaciers in the Atbashy basin.\n\nglacier_balance <- glacierBalance(melt_a_eb = melt_a_eb, \n                                  rgi_elbands = rgi_elbands)\nglacier_balance <- glacier_balance |> \n  mutate(.lb = ifelse(Variable == \"A_km2\", \n                      ifelse(Value*(1-error_stats$sA)>0, \n                             Value*(1-error_stats$sA), 0), \n                      ifelse(Variable == \"V_km3\", \n                             ifelse(Value*(1-error_stats$sV)>0, \n                                    Value*(1-error_stats$sV), 0), \n                             ifelse(Variable == \"Q_m3a\", \n                                    ifelse(Value*(1-error_stats$sQglmelt)>0,\n                                           Value*(1-error_stats$sQglmelt), 0), \n                                    ifelse(Variable == \"Qimb_m3a\", \n                                           Value*(1-error_stats$sImbal), NA)))), \n         .ub = ifelse(Variable == \"A_km2\", \n                      Value*(1+error_stats$sA), \n                      ifelse(Variable == \"V_km3\", \n                             Value*(1+error_stats$sV), \n                             ifelse(Variable == \"Q_m3a\", \n                                    Value*(1+error_stats$sQglmelt), \n                                    ifelse(Variable == \"Qimb_m3a\", \n                                           Value*(1+error_stats$sImbal), NA)))))\nggplot(glacier_balance |> \n         dplyr::filter(RGIId %in% glaciers[6:9], \n                       Hyear > min(Hyear) & Hyear < max(Hyear), \n                       Variable %in% c(\"A_km2\", \"V_km3\", \"Q_m3a\", \"Qimb_m3a\"))) + \n  geom_ribbon(aes(Hyear, ymin = .lb, ymax = .ub, fill = RGIId), \n              alpha = 0.2, colour = NA) + \n  geom_line(aes(Hyear, Value, colour = RGIId)) + \n  facet_wrap(\"Variable\", scales = \"free_y\") + \n  scale_colour_viridis_d() + \n  scale_fill_viridis_d() + \n  theme_bw()\n\n\n\nFigure 11.6: Glacier area and volume development and total and imbalance discharge.\n\n\n\n\nWe now have glacier discharge (Q_m3s) and the unsustainable contribution to glacier discharge, the imbalance ablation (Qimb_m3a) which is negative for glacier loss and positive for growing glaciers. We are only interested in the contribution of imbalance ablation to river discharge, that is, only the negative part of Qimb_m3a is relevant to us."
  },
  {
    "objectID": "glacier_modeling.html#from-annual-to-daily-melt",
    "href": "glacier_modeling.html#from-annual-to-daily-melt",
    "title": "11  Modeling of Discharge from Glacier Melt",
    "section": "\n11.7 From Annual to Daily Melt",
    "text": "11.7 From Annual to Daily Melt\nThe glacier mass balance is done on a yearly basis (if not at lower frequency). Hydrological models, however, typically run at higher frequency, for example monthly or daily time steps. One simple method to distribute glacier discharge on a year is to scale it according to the daily melt computed.\n\n# Aggregate the daily melt per glacier\nmelt_mmd <- melt |> \n  pivot_longer(-date, names_to = \"ID\", values_to = \"melt\") |> \n  separate(ID, into = c(\"RGIId\", \"elB\"), sep = \"_\") |> \n  group_by(date, RGIId) |> \n  summarize(melt = sum(melt)) |> \n  ungroup() |> \n  rename(M_mmd = melt)\n\n# Compute the annual melt per glacier \nmelt_mma <- melt_mmd |> \n  mutate(Hyear = hyear(date)) |> \n  group_by(Hyear, RGIId) |> \n  summarise(M_mma = sum(M_mmd)) |> \n  ungroup()\n\n# Rescale the annual imbalance glacier ablation with the daily melt rates\nimbalAbl_m3s <- melt_mmd |> \n  mutate(Hyear = hyear(date)) |> \n  left_join(melt_mma, by = c(\"RGIId\", \"Hyear\")) |> \n  left_join(glacier_balance |> \n              dplyr::filter(Variable == \"Qimb_m3a\") |> \n              transmute(Hyear = Hyear, \n                        RGIId = RGIId, \n                        Qimba_m3s = -1* Value/(60*60*24*365)) |> \n              mutate(Qimba_m3s = ifelse(Qimba_m3s < 0, 0, Qimba_m3s)), \n            by = c(\"RGIId\", \"Hyear\")) |> \n  mutate(Qimb_m3s = Qimba_m3s * (M_mmd/M_mma), \n         Qimb_m3s = ifelse(is.na(Qimb_m3s), 0, Qimb_m3s))\n\n# Visualise daily imbalance ablation\nggplot(imbalAbl_m3s |> \n         dplyr::filter(RGIId %in% unique(glacier_balance$RGIId)[1:7])) + \n  geom_line(aes(date, Qimb_m3s, colour = RGIId)) + \n  scale_colour_viridis_d() + \n  labs(x = \"Date\", y = \"Glacier discharge from imbalance ablation [m3/s]\") + \n  theme_bw()\n\n\n\nFigure 11.7: Daily glacier discharge from imbalance ablation."
  },
  {
    "objectID": "glacier_modeling.html#writing-input-file-for-rs-minerve",
    "href": "glacier_modeling.html#writing-input-file-for-rs-minerve",
    "title": "",
    "section": "Writing input file for RS Minerve",
    "text": "Writing input file for RS Minerve\nThe input file format for RS Minerve is described in Garcia Hernandez et al. (2020), page 136.\n\nQ <- Qimb_m3s_sub |>\n  mutate(Qimb_m3s = round(Qimb_m3s, digits = 7))\ntemp_wide <- Q |>\n  pivot_wider(names_from = name_2, values_from = Qimb_m3s) |> \n  rename(Station = date) \ndatechar <- posixct2rsminerveChar(temp_wide$Station)$value\ndatechar <- gsub(\" 01:00:00\", \" 00:00:00\", datechar)\ndatechar <- gsub(\" 02:00:00\", \" 00:00:00\", datechar)\noutput <- rbind(colnames(temp_wide),\n                c(\"X\", \"1\", \"1\", \"1\", \"1\"),  # Random coordinates, not relevant\n                c(\"Y\", \"2\", \"2\", \"2\", \"3\"), \n                c(\"Z\", \"3\", \"3\", \"3\", \"3\"), \n                c(\"Sensor\", \"Q\", \"Q\", \"Q\", \"Q\"), \n                c(\"Category\", \"Flow\", \"Flow\", \"Flow\", \"Flow\"), \n                c(\"Unit\", \"m3/s\", \"m3/s\", \"m3/s\", \"m3/s\"), \n                c(\"Interpolation\", \"Linear\", \"Linear\", \"Linear\", \n                  \"Linear\"), \n                cbind(datechar, \n                      as.character(temp_wide$Dzhaldzhur_Subbasin), \n                      as.character(temp_wide$Ulak_Subbasin), \n                      as.character(temp_wide$Atbaschy_Midstream_Subbasin), \n                      as.character(temp_wide$Atbaschy_Downstream_Subbasin)))\n\nFinally, the prepared data is written to a csv file which can be read into RSMinerve.\n\nwritefilename <- paste0(data_path, \"RSM_demo_glacier_source.csv\")\nwrite.table(output, file = writefilename, col.names = FALSE, \n            row.names = FALSE, append = FALSE, quote = FALSE, \n            sep = \",\", dec = \".\")"
  },
  {
    "objectID": "glacier_modeling.html#references",
    "href": "glacier_modeling.html#references",
    "title": "11  Modeling of Discharge from Glacier Melt",
    "section": "\n12.2 References",
    "text": "12.2 References\n\n\n\n\nErasov, N. V. 1968. “Method for Determining of Volume of Mountain Glaciers.” MGI, no. 14: 307–8.\n\n\nFarinotti, Daniel, Matthias Huss, Johannes J. Fürst, Johannes Landmann, Horst Machguth, Fabien Maussion, and Ankur Pandit. 2019. “A Consensus Estimate for the Ice Thickness Distribution of All Glaciers on Earth.” Nature Geoscience 12 (3): 168–73. https://doi.org/10.1038/s41561-019-0300-3.\n\n\nGarcia Hernandez, J., A. Foehn, J. Fluixa-Sanmartin, B. Roquier, T. Brauchli, J. Paredes Arquiola, and De Cesare G. 2020. “RS MINERVE - Technical Manual, V2.25.” ISSN 2673-2661. Switzerland: Ed. CREALP.\n\n\nHock, Regine. 2003. “Temperature index melt modelling in mountain areas.” Journal of Hydrology 282 (1-4): 104–15. https://doi.org/10.1016/s0022-1694(03)00257-9.\n\n\nHugonnet, Romain, Robert McNabb, Etienne Berthier, Brian Menounos, Christopher Nuth, Luc Girod, Daniel Farinotti, et al. 2021. “Accelerated Global Glacier Mass Loss in the Early Twenty-First Century.” Nature 592 (7856): 726–31. https://doi.org/10.1038/s41586-021-03436-z.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nMiles, Evan, Michael McCarthy, Amaury Dehecq, Marin Kneib, Stefan Fugger, and Francesca Pellicciotti. 2021. “Health and Sustainability of Glaciers in High Mountain Asia.” Nature Communications 12 (2868): 10. https://doi.org/https://doi.org/10.1038/s41467-021-23073-4."
  },
  {
    "objectID": "geospatial_data.html#sec-geospatial-data-prerequisites",
    "href": "geospatial_data.html#sec-geospatial-data-prerequisites",
    "title": "",
    "section": "Geospatial Data Prerequisites",
    "text": "Geospatial Data Prerequisites\nHere, a local installation of QGIS and a basic understanding of Geographic Information Systems are required. Please see ?@sec-study-guide-materials for more information how to install QGIS.\n\n\n\n\n\n\nTip\n\n\n\n?@sec-quick-guides in the Appendix walks you through in detailed manner of many of the required steps in the Chapter. Therefore, please also consult the resources there.\n\n\nThere are very many online resources that can be consulted for learning QGIS. You can google them or start with a video tutorial like the following one.\n\n\n\n\n\n\n\n\n\nTo process the data for your case study pack, make sure that you downloaded the corresponding files via this link. Depending on the catchment you have chosen, the files are either in the folder ./AMUDARYA, ./CHIRCHIK, ./CHU or ./SYRDAYA."
  },
  {
    "objectID": "geospatial_data.html#sec-catchment-delineation",
    "href": "geospatial_data.html#sec-catchment-delineation",
    "title": "",
    "section": "Catchment Delineation",
    "text": "Catchment Delineation\nAssuming that you have downloaded the catchment data folder of your choice, the corresponding DEM file should be loaded in a new, empty QGIS project. Prior to do this, make sure that the projects projection is in UTM (How to change the projection of a project). Check the projection of the DEM layer and reproject it if necessary (how to).\nIf you do not have a DEM available to start with, the process of downloading one is straight forward. Probably the easiest way is to install the QGIS SRTM3 plugin (how to). An alternative way is to download it via the USGS Earth Explorer (how to). Both solutions requires you to register an Earth Explorer Account (how to register). Finally and after downloading the DEM tiles for the region of interest, the tiles need to be merged (how to merge DEM tiles).\nTypically, geospatial data from open sources is stored in the standard projection WGS84 (EPSG:4326). The WGS84 is in degrees, minutes and seconds but for hydrological analysis it is more convenient to have spatial layers in the UTM projection. Look up in which UTM Zone your catchment lies and re-project to that zone. In the example of the Chirchiq basin, the preferred UTM zone is 42N, i.e. EPSG:32642.\nAs a next step, we are going to load the shapefile of the discharge gauge station location. This file contains the point location where discharge is measured and from which on we want to delineate the upstream contributing area. The data is stored in the ./GaugeData folder and is called XXXXX_Gauge.shp where the XXXXX are placeholders for the five digit code that uniquely identifies your station. Once the shapefile is loaded, check in Properties/Information that it is correctly projected. If this is not the case, reproject to the relevant UTM zone as described above.\nNow, we are ready to start with the catchment delineation.\nInstead of just loading the existing catchment shapefile from the corresponding ./Basin folder (how to add a vector layer to a QGIS project), it is better to actually go through the steps step-by-step to learning of to derive them. The steps are also described in this online tutorial\n\n\n\n\n\n\n\n\n\nTo derive the the catchment area upstream of the discharge gauge that we have loaded in the previous step, the DEM is traced upstream until elevation values do not increase any more, i.e. until the boundary of the watershed is reached. The SRTM DEM contains the rounded average elevation in each cell of about 25 - 30 meters [m] resolution. Due to the averaging and rounding it may happen, that in a SRTM DEM, an upstream river cell contains lower elevations than the downstream river cell. If the catchment delineation algorithm reaches such a situation, it stops, thinking it has reached the boundary of the watershed. To avoid this, cells that form local depressions should be filled to form a river bed which is continuously increasing in elevation in the upstream direction until it reaches the watershed boundary.\nIn QGIS, there are several methods that can fill terrain depressions. We are using the r.fill.dir algorithm to perform this task. Figure 2 shows the process in detail. First, we make sure that we have loaded the DEM in the correct projection. Second, we open the Processing Toolbox and type r.fill.dir in the search bar and then double click on the corresponding algorithm. Third, in the openend window, we ensure that the DEM is selected under the Elevation header. On top of that, the Flow Direction and Problem Areas output files are not needed and their checkboxes unchecked.\n\n\nFigure 2: Filling sinks in a DEM with the r.fill.dir algorithm.\n\n\nIf the algorithm has run, a new entry for the Depressionless DEM will appear in the layers panel with the correct DEM. This raster can now be used to delineate the basin area. The following steps need to be carried out for this;\n\nUse the r.watershed algorithm to obtain the flow accumulation and drainage direction rasters.\nEnsure that the gauge is correctly located on the DEM\nUse the r.water.outlet algorithm to delineate the upstream area\nConvert the resulting Basin raster into a polygon via the Conversion/Polygonize method.\n\nWe show the process in detail here.\n\n\nFigure 3: Running the r.watershed algorithm. Select the sink-filled DEM, set the minimum size of the exterior watershed to 200’000, check the box in Use positive flow accumulation even for likely underestimation.\n\n\nFigure 3 shows step-by-step how to run the r.watershed algorithm. First, make sure that the Depressionless DEM raster is selected in the Elevation field. Fill in 200’000 as a minimum size of the exterior watershed (this is a guessed number that normally yields good results). Also, check the the box to use positive flow accumulation even for likely underestimates. Finally, just select the top two output files, i.e., Number of cell that drain through each cell and Drainage direction. The remaining resulting rasters are not required during the next steps.\nOnce you click run, the resulting raster alyers will be computed. As is evident, the flow accumulation raster (called Number of cells that drain through each cell) contains a large range of values. The largest value is obviously in the one cell that drains the largest area of the DEM.\n\n\n\n\n\n\nTip\n\n\n\nUse the raster calculator to compute the logarithm of the flow accumulation raster. This helps to better visualize the raster. This raster can be used to ensure that the gauge from which the upstream area will be delineated are actually correctly located on the streams. If misplacement is evident, we can edit the gauge shapefile and relocate the gauge to the correct river stretch.\n\n\nOnce it is ensured that the gauge is correctly located on the stream, we are ready to delieate the upstream area by using the r.water.outlet algorithm. For this, zoom in to display a close up of the gauge and select and open the r.water.outlet window. The algorithm only needs two inputs, i.e. the Drainage direction raster and the coordinates of the outlet point. Instead of manually entering them, you can just click on the mapto select precise point of the gauge location which will transfer the coordinates to the algorithm’s interface. After pressing run, the Basin raster will be computed. Figure 4 shows the process.\n\n\nFigure 4: Basin delineation using the r.water.outlet algorithm. The resulting Basin raster can easily be polygonized.\n\n\nAs a final step, you should polygonize the Basin raster. We are now ready to run the process model which generates the required shapefiles for further processing in RSMinerve. The resulting vectorized basin can be stored in the corresponding folder on the local computer.\nThe process can be repeated for any other basin in a similar manner. Note also that in the case you have several gauges to map the upstream areas from, the Flow accumulation as well as the Drainage direction rasters can be computed once, stored, and reused again for all gauges.\nWe now have all the necessary files to process them in the Graphical Modeler model that is provided with the learning pack."
  },
  {
    "objectID": "geospatial_data.html#sec-preparation-of-rsminerve-input-files",
    "href": "geospatial_data.html#sec-preparation-of-rsminerve-input-files",
    "title": "",
    "section": "Preparation of RSMinerve Input files",
    "text": "Preparation of RSMinerve Input files\nYou can download the Graphical Modeler model from the Students_CaseStudyPacks online repository. The model file is called rsminerve_gis_files_preparation_2022_win_os.model3 and should be downloaded in your local working directory. The model can then be loaded via the Processing/Graphical Modeler menu in QGIS. Once you have loaded the model, you should see the model pop up on your screen as is shown in a similar fashion in Figure 5.\n\n\nFigure 5: The Graphical Modeler model rsminerve_gis_files_preparation_2022_win_os.model3 is shown in a graphical manner in the right window (1). When you click Run, the parameters specification window on the left will pop up (2). See text for further explanations\n\n\nFigure 5 shows nothing more than a graphical representation of a script. This script is like a recipie to execute algorithms in QGIS in a sequential manner where an output of one algorithm feeds as input into the next algorithm. The yellow highlighted elements in window (1) are input parameters that are specified in the window (2) prior to the execution of the script. The green elements in the window (1) are the results that are stored during and after the execution of the algorithm and available for further processing then.\nAs explained above, the rsminerve_gis_files_preparation_2022_win_os.model3 model, in a nutshell, prepares input files for RSMinerve using the DEM, the basin shapefile as well as the location of the gauge as input. The key output elements are called SUBBASINS, RIVERS and JUNCTIONS.\nPrior to the execution of the model (script), the parameters have to be set in a careful manner. For some of the parameter values, a first best guess might produce outcomes that are not a the required level of detail or, alternatively, too detailed. Depending on the basin under consideration, an iterative approach normally must be followed to arrive at the desired output as is explained in the following.\nIn Figure 6, we show a meaningful parameter selection for the Chatkal River basin in the Case Studies pack (Gauge 16279). The basin and gauge shapefiles as well as the DEM are the geospatial assets that need to be specified are shown in (1), (5) and (4). The following parameters are important to set correctly\n\n\nFigure 6: Parameter selection menu of the Graphical Modeler rsminerve_gis_files_preparation_2022_win_os.model3 model. The detailed description of the input elements as well as parameter values chosen is explained in the main text.\n\n\n\nBasinShapeBuffer_meters (2): This is buffer value to be specified in meters. The default value of 500 m is a value that can be used for a wide range of different catchment sizes and does not need to be changed.\nChannel Network Cutoff Value (3): This is a crucial parameter for the determination of the subbasin granularity. If many the main basin is to be subdivided into a larger number of subbtains, this value needs to be chosen to be around 107 - 108. For Chatkal, as our example shows, a perfect value for the subdivision into the main 4 subbasins is 109.\nElevation Bands Table (4): Figure 7 shows the table than can be customized according to user needs and wants. The table specifies that granuarity with which the basin domain will be subdivided into elevation bands (hydrological response units, i.e. HRUs). As a rule of thumb, in climate change impact studies and for snow melt dominated basins, a spacing of 100 m - 200 m is an optimal choice for use with HBV models in RSMinerve. Note however, the finer the domain is, the larger the number of HRUs will need to be modeled which proportionally increases the computational demand of the hydrological model.\nRiver Network Level (8): Here you set a number between 1 and 8. The lower the number, the higher the number of tributaries and subtributaries that will be delineated for the RIVERS output shapefile. If you choose 8, this means that normally only the river’s main stem will be delineated. This is relevant for the case where not further subdivisions into small subbasins is desired for modeling in RSMinerve. 7 is a robust choice if only the main tributaries should be delineated.\n\n\n\nFigure 7: The elevation bands table is shown. It shows the user customizable elevation band intervals. The example shows the specification of 9 elevation bands (HRUs) of 500 m spacing between each. For each HRU and for each subbasin, a corrsponding hydrological model will be configured in RSMinerve\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou will very likely have to iterate and run the model a couple of times to achieve the desired results. Try it yourself! Note that if you get an error message that POLYGONS.shp was not found, try increasing the Channel Network Cutoff Value by 50 % - 70 %.\n\n\nSample output results for our demonstration catchment are shown in Figure 8.\n\n\nFigure 8: Resulting output of the Graphical Modeler model for Chatkal River basin. The next step is to clean up manually the SUBBASINS, RIVERS and JUNCTIONS shapefiles."
  },
  {
    "objectID": "geospatial_data.html#sec-post-processing-results-shapefiles",
    "href": "geospatial_data.html#sec-post-processing-results-shapefiles",
    "title": "",
    "section": "Post-Processing Results Shapefiles",
    "text": "Post-Processing Results Shapefiles\nAs mentioned in the previous Section, we need to post-process the SUBBASINS, RIVERS and JUNCTIONS shapefiles so that they correctly contain the required fields as shown in Figure 1. A good way to post-process the files is to start with the RIVERS layer, the continue with the JUNCTIONS layer and to end with the SUBBASINS layer.\nJUNCTIONS and RIVER Layers\nIn editing the junctions layer, the basic ideas are to edit the junctions layer in a way to remove junctions that are not needed, given the desired subdivision of the larger basin into subbasins. For the demo catchment shown in Figure 9, the siutation is explained in greater detail.\n\n\nFigure 9: The Figure shows the output files of the Graphical Modeler model. For this demonstration catchment, the only junction that we want to keep (apart from the outflow junction located at the gauge) is the one highlighted with the red arrow which marks the confluence of the two major upstream tributaries.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you struggle with shapefile editing, please consult the detailed step-by-step guide as shown in the cooresponding Quick Guide in the Appendix.\n\n\nOnce we have removed the junctions that are not needed, we can very easily also remote the river lines that are not needed. Generally, we do not need river sections above the most upstream junctions for any given subbasin within the river basin or the river basin itself.\nIf we follow these guides, we end up with the results shown in Figure 10. After editing the attribute tables of these shapefiles, the files are ready for import in RSMinerve.\n\n\nFigure 10: Final edited shapefiles with the unnecessary junctions and river stretches removed. The two red arrows point to the two remaining junctions and the blue arrow shows the river stretch that is left.\n\n\nWhat is left now is to ensure that the attribute tables contain the correct fields and that these fields contain the correct labels. Figure 1 shows the required fields. These are\n\nJUNCTIONS: Junctions Name (junct_name), Junctions ID (junct_id), Rivers ID (riv_id)\nRIVERS: Rivers Name (riv_name), Rivers ID (riv_id), Junctions ID (junct_id)\n\nThe abbreviated field names are used in QGIS since field names are limited in shapefiles. The fields and values of the final junctions shapefile are shown in Figure 11. Compare this with the edited final attribute table of the rivers shapefile as shown in Figure 12.\n\n\nFigure 11: The final attribute table of the demo junctions shapefile is shown. (1) denotes the fields for the donwstream outlet junction and (2) shows the values for the upstream junction. The naming convention has to be understood in the context of the naming convention using in the RIVERS shapefile\n\n\n\n\nFigure 12: The final rivers shapefile attribute table is shown with the only river highlighted (1). Very importantly, the junt_id field referes to the downstream junction where the river drains into.\n\n\nSUBBASINS Layer\nNow, we are working on the subbasins. The major subbasins are highlighted in the following Figure 13. We call them Chatkal River downstream, Chatkal River upstream and Sandalash River. These big subbasins can be understood as hydrological response units (HRUs) for hydrological modeling.\n\n\nFigure 13: The major subbasins of Chatkal River basin above Gauge 16279 are shown. (1) is Chatkal downstream, (2) is Sandalash River and (3) is Chatkal upstream.\n\n\nHowever, do not forget that we have classified the entire basin into zones of elevation bands. These are shown in the Figure 13 as polygons with black outlines from the shapefile layer BasinElevationBands_poly_fixed.shp. Exactly 7 elevation bands (or, in other words, HRUs) were delineated with the Graphical Modeler like this. We need to somehow intersect the subbasin HRUs with the elevation bands to generate elevation band HRUs for each subbasin. We will show the steps required to achieve this in the following.\nFirst, we prepare the attribute table of the subbasins shapefile. The way this is done correctly for the demonstration catchment is shown in Figure 14. The attribute table contains two fields, i.e., basin_name and junct_id. We name the downstream and upstream sections of the Chatkal River with chatkal_ds and chatkal_us correspondingly. The cells of the field junct_id are populated with the name of the corresponding downstream junction where the subbasin drains into. Since both, the subbasin chatkal_us and sandalash drain into the junction upstream_junct, the later appears in both cells of the junct_id field for these rivers.\n\n\nFigure 14: Subbasins attribute table after adding the required fields and entires as per the requirements of RSMinerve. (1) is for the downstream part of Chatkal River, (2) is for the upstream part of Chatkal River and (3) is for Sandalash River.\n\n\nSecond, we clean up and prepare the attribute table for intersecting with the subbasins layer in a thrid step. The attribute table of the layer BasinElevationbands_Poly_fixed contains three fields, i.e., ID, VALUE and NAME from which you can safely delete the ID and VALUE fields. What remains is the NAME field that contains a unique identifier for the elevation bands in ascending order, starting from the lowest. Figure 15 shows the attribute table.\n\n\nFigure 15: The attribute table of the elevation bands shapefile is shown after the fields ID and VALUE have been removed (1). When an elevation band is selected in te edit mode, the corresponding geometry is highlighted on the map as is shown by the arrows (2).\n\n\nThird, we need to intersect the elevation bands shapefile with the subbasins shapefile. This can easily be accomplished using the Vector/Geoprocessing/Intersection tool in QGIS. When you select the Intersection Algorithm, make sure that you set the elevation bands shapefile as the Input layer and set subbasins shapefile as the Overlay Layer. When executed, you receive an attribute table that looks as the one shown in Figure 16.\n\n\nFigure 16: The attribute table resulting from the Intersection Algorithm. You now have to manually add the numbers of the Name field as suffix to the existing names of the basin_name field.\n\n\nAs a final step, you have to manually add the numbers stored in the NAME field to the basin names in the field basin_name. After this is done, you can safely delete the NAME field and have proudly completed the required steps to setup the GIS shapefiles that can be used as input to the RSMinerve hydrological-hydraulic modeling tool. Figure 17 shows the result.\n\n\nFigure 17: The final attribute table of the subbasins file is shown. The elevation band identifiers have been added as suffixes with “_x” to the subbasin names where x denotes the number of the subbasin-specific elevation band."
  },
  {
    "objectID": "geospatial_data.html#sec-geospatial-data-references",
    "href": "geospatial_data.html#sec-geospatial-data-references",
    "title": "",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "example_river_basins.html#sec-example-gunt-river-basin",
    "href": "example_river_basins.html#sec-example-gunt-river-basin",
    "title": "",
    "section": "Gunt River Basin",
    "text": "Gunt River Basin\nGunt Basin Characterization\nThe Gunt river basin is located in the Pamir mountains in the Gorno Badakhshan Autonomous Region in south-east Tajikistan. The basin covers approximately 14’000 km2. The Gunt river is a large right tributary of the upstream Pyandzh and joins the latter downstream of the town of Khorog. Mean elevation is approx 4’270 meters above mean sea level (masl) with an altitude range from 2’000 – 6’700 masl. The highest elevations in the catchment are Peak Karl Marx (6726 m) and Peak Engels (6510 m) at the southern border of the catchment.\nInformation on the available discharge and meteorological stations in the basin is provided in Table 1.\n\n\n\n\nTable 1: Details on meteorological and discharge measurement stations from which data is available.\n\n\n\n\n\n\n\n\n\n\n\nStationName\nStationCode\nlat\nlon\neasting\nnorthing\nmasl\ntype\n\n\n\nBulunkul\n38953\n37.70417\n72.94583\n847890.5\n4180325\n3746\nMeteo\n\n\nKhorog\n38954\n37.50361\n71.51500\n722309.0\n4153714\n2075\nMeteo\n\n\nKhorog\n17050\n37.50361\n71.51500\n722309.0\n4153714\n2075\nDischarge Gauge\n\n\nJavshangoz\n38956\n37.39083\n72.29583\n791785.1\n4143330\n3438\nMeteo\n\n\nNavobod\n38950\n37.59417\n71.86556\n752995.5\n4164650\n2566\nMeteo\n\n\n\n\n\n\nA map of the basin is provided in Figure 2.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to experiment with the R code yourself, the data for this Chapter can be downloaded here. You can store these data locally and then perform the analysis on your local computer by correspondingly adjusting the data path in the code block below before loading the data.\n\n\n\ndata_dir <- \"../caham_data/AmuDarya/gunt_data/geospatial\"\n\n# Load vector data (shapefiles)\ngunt_basin_shp <- st_read(file.path(data_dir, \"gunt_basin_shp.shp\"), quiet = TRUE)\ngunt_rivers_shp <- st_read(file.path(data_dir,\"gunt_rivers_shp.shp\"), quiet = TRUE)\ngunt_subbasins_shp <- st_read(file.path(data_dir, \"gunt_subbasins_shp.shp\"), quiet = TRUE)\n\n# Load raster data (.tif-files)\ngunt_dem <- raster::raster(file.path(data_dir,\"gunt_dem.tif\"))\ngunt_dem_hillshade <- raster::raster(file.path(data_dir,\"gunt_dem_hillshade.tif\"))\n\nAfter successfully loading the data, we can easily visualize it.\n\n# Visualize data\ntm_shape(gunt_dem_hillshade) +\n tm_raster(palette = gray(0:100 / 100), n = 100, legend.show = FALSE)  +\n  tm_shape(gunt_dem) +\n  tm_raster(alpha = 0.5, palette = terrain.colors(25), legend.show = TRUE, title = \"Elevation (masl)\") +\n  tm_shape(gunt_basin_shp) +\n  tm_polygons(\"area\", alpha = 0, legend.show = FALSE) +\n  tm_shape(gunt_subbasins_shp) +\n  tm_polygons(\"ID\", alpha = 0, legend.show = FALSE) +\n  tm_layout(legend.position = c(\"right\",\"bottom\")) +\n  tm_shape(gunt_rivers_shp) +\n  tm_lines(col = \"name\", scale = 3) + \n  tm_shape(meteo_stations_sf) +\n  tm_dots(col = \"black\", scale = 2) +\n  tm_shape(meteo_stations_sf) +\n  tm_text(\"StationName\", size = .75, auto.placement = TRUE, just = \"left\")\n\n\n\nFigure 2: The Gunt River basin, its topography and the main tributaries are shown (see corresponding legend). The discharge measurement station 17050 is located in Khorog on the Gunt Downstream, shortly after the confluence of Gunt Upstream and Shakara rivers.\n\n\n\n\nThe Table below summarizes key basin statistics that are relevant from the hydro-climatological perspective. Data from various sources are summarized here. These data are presented in Section II of the book in ?@sec-data and discussed in great detail there.\nThe basin area has been derived from the basin shapefile. Raster statistics of the SRTM digital elevation model (srtmgl12020?), the climate raster files as well as the land cover raster are calculated using the QGIS Raster Layer Statistics processing toolbox algorithm. The land ice total polygon area is computed with the Statistical Summary Option in QGIS.\nThe norm hydrological year discharge and the corresponding norm cold and warm season discharge values have been computed with data from the Tajik Hydrometeorological Service. The mean basin precipitation is computed using a state-of-the-art bias corrected high-resolution reanalysis product (beck2020?). Such data will also be used for hydro-climatological modeling in later Chapters. Potential evaporation is from (Trabucco2019?) using the Penman-Montieth equation.\n\n\nTable 2: Key relevant basin statistics for Gunt river basin. Individual data sources are indicated. Note that the hydrological year in Central Asia is defined as starting from October in year 1 and lasts until end of September in the subsequent year 2. More information on the relevance of hydrological year-based water accounting is given below.\n\n\n\n\n\nATTRIBUTE\nVALUE\n\n\n\n\nGeography (srtmgl12020?)\n\n\n\n\nBasin Area \\(A\\)\n\n13’693 km2\n\n\n\nMinimum Elevation \\(h_{min}\\)\n\n2’068 masl\n\n\nMaximum Elevation \\(h_{max}\\)\n\n6’652 masl\n\n\nMean Elevation \\(h_{mean}\\)\n\n4’267 masl\n\n\nHydrology [Source: Tajik Hydromet Service]\n\n\n\nNorm hydrological year discharge \\(Q_{norm}\\)\n\n103.8 m3/s\n\n\nNorm cold season discharge (Oct. - Mar., Q4/Q1)\n19.8 m3/s\n\n\nNorm warm season discharge (Apr. - Sept., Q2/Q3)\n84.2 m3/s\n\n\nAnnual norm discharge volume\n3.28 km3\n\n\n\nAnnual norm specific discharge\n239 mm\n\n\nClimate\n\n\n\nMean basin temperature \\(T\\) (Karger et al. 2017)\n\n-5.96 deg. Celsius\n\n\nMean basin precipitation \\(P\\) (beck2020?)\n\n351 mm\n\n\nPotential Evaporation \\(E_{pot}\\) (Trabucco2019?)\n\n929 mm\n\n\nAridity Index \\(\\phi = E_{pot} / P\\)\n\n2.7\n\n\nAridity Index (Trabucco2019?)\n\n3.6\n\n\nLand Cover (CopernicusLandCover?)\n\n\n\nShrubland\n8 km2\n\n\n\nHerbaceous Vegetation\n4’241 km2\n\n\n\nCrop Land\n0.5 km2\n\n\n\nBuilt up\n4 km2\n\n\n\nBare / Sparse Vegetation\n8’410 km2\n\n\n\nSnow and Ice\n969 km2\n\n\n\nPermanent Water Bodies\n80 km2\n\n\n\nLand Ice\n\n\n\nTotal glacier area (glims2005?)\n\n875 km2\n\n\n\nTotal glacier volume (calculated with (Erasov 1968))\n699 km3\n\n\n\n\n\nWith the values provided in the table above, the discharge index \\(Q/P\\) is 68.5 % and the evaporative index \\(E/P\\) is 31.5 %. In other words, the long-term water balance shows that 3 precipitation units gets partitioned into 2 discharge units and 1 evaporation unit, approximately. The aridity index \\(\\phi\\) , when calculated using \\(P\\) from (Karger et al. 2020) and \\(E_{pot}\\) from (Trabucco2019?) is 2.7. The aridity index from (Trabucco2019?) is 3.6. These values indicate some uncertainty in relation to the global climate products used. Despite this, they confirm the highly arid characteristics of the basin.\nGunt Basin Hydrology\nFor the analysis of the key hydro-climatological characteristics, we first load the available decadal and monthly station data1. The data used in this Chapter can be accessed downloaded from the following online repository.\nFirst, we load the available station data of the Gunt River basin into R. Note that the monthly data is available for one discharge station and 4 meteorological stations. See also Table 1 for more information on the stations.\n\ndata_dir <- \"../caham_data/AmuDarya/gunt_data/station_data/\"\nfile_name = 'gunt_data_cleaned.Rds'\ngunt_station_data <- read_rds(file.path(data_dir, file_name))\ngunt_station_data\n\n# A tibble: 23,352 × 10\n   date        data  norm units type  code  station     river basin  resolution\n   <date>     <dbl> <dbl> <chr> <chr> <chr> <chr>       <chr> <chr>  <fct>     \n 1 1940-01-31  30.5  32.9 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 2 1940-02-29  27.3  30.1 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 3 1940-03-31  24.9  28.4 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 4 1940-04-30  26.4  30.7 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 5 1940-05-31  59    68.5 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 6 1940-06-30 309   232.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 7 1940-07-31 224   319.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 8 1940-08-31 201   237.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 9 1940-09-30 121   117.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n10 1940-10-31  60.8  63.1 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n# … with 23,342 more rows\n\n\nThis dataframe now contains all available data hydro-meteorological data from the basin. Most data are available at monthly time scales. As an example, the monthly discharge data from Gauge 17050 can be accessed and extracted from the Gunt dataset in the following way.\n\nq_17050_mon <- gunt_station_data %>% filter(type == \"Q\" & code == '17050' & resolution == 'mon')\nq_17050_mon\n\n# A tibble: 972 × 10\n   date        data  norm units type  code  station     river basin  resolution\n   <date>     <dbl> <dbl> <chr> <chr> <chr> <chr>       <chr> <chr>  <fct>     \n 1 1940-01-31  30.5  32.9 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 2 1940-02-29  27.3  30.1 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 3 1940-03-31  24.9  28.4 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 4 1940-04-30  26.4  30.7 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 5 1940-05-31  59    68.5 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 6 1940-06-30 309   232.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 7 1940-07-31 224   319.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 8 1940-08-31 201   237.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 9 1940-09-30 121   117.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n10 1940-10-31  60.8  63.1 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n# … with 962 more rows\n\n\nWhen we plot the data, we see that we have a near complete monthly record from 1940 onward (see Figure 3). The data gap in the 1990ies was during the Tajik civil war.\n\nq_17050_mon %>% \n  plot_time_series(date,\n    data,\n    .smooth        = FALSE,\n    .interactive   = TRUE,\n    .title         = \"\",    \n    .x_lab         = 'Year',\n    .y_lab         = 'Mean monthly Q [m3/s]',\n    .plotly_slider = TRUE)\n\n\nFigure 3: Visualized monthly discharge data at Gunt gauging station (17050)\n\n\n\nThere are visible changes in the winter low flow regime from 2007 onward. This is because of hydropower production that started upstream at that time. Pamir Energy, the local generation company supplies hydropower electricity especially during the cold winter months to the communities in the valley. When hydropower is required, the water table of the Yashikul Lake in the Pamir plateau (Alishur catchment, see Figure 2) gets lowered to increase the discharge for energy production in the downstream.\n\n\nFigure 4: Since 2006, a run off the river hydropower plant operated by Pamir Energy produces hydropower to cover local electric energy demand. Lake Yashikul is used as a regulator. The increase in winter discharge from 2007 onwards is due to HPP operations.\n\n\nThe seasonal diagnostics of the monthly discharge time series is shown in @ref(fig-gunt-seasonal-diagnostics). As is easily visible, the peak discharge of Gunt river measured at Khorog station is in July.\n\nq_17050_mon %>% \n  plot_seasonal_diagnostics(.date_var      = date,\n                            .value         = data,\n                            .title         = \"\",\n                            .feature_set   = c(\"month.lbl\"),\n                            .interactive   = FALSE,\n                            .x_lab         = \"Year\",\n                            .y_lab         = \"Mean monthly Q [m3/s]\") +\n  scale_x_discrete(breaks = c(\"January\", \"February\", \"March\", \"April\", \"May\", \n                            \"June\", \"July\", \"August\", \"September\", \"October\", \n                            \"November\", \"December\", \"1\", \"2\", \"3\", \"4\"),\n                   labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \"J\", \"A\", \"S\", \"O\", \"N\", \"D\",\"1\", \"2\", \"3\", \"4\"))\n\n\n\nFigure 5: Seasonal diagnostics of the monthly discharge time series at the Gunt-Khorog gauging station (17050). The analysis can be easily carried out using the function plot_seasonal_diagnostics from the R package timetk.\n\n\n\n\nunlike in the lower lying Chirchik tributaries as shown in Figure 17 further below in Section 1.2.\n\n\n\n\n\n\nTip\n\n\n\nCompare the discharge seasonality of Gunt River with the seasonality of the large and small Chirchik River tributaries? Obtain the information of all the other rivers in the Case Study packs and their seasonality. What is the single most important determinant of peak discharge timing in Central Asia rivers?\n\n\nBelow in Figure 6, we are plotting changes to monthly flows over time by binning all available data in the corresponding monthly slots. The red lines are linear regression lines that indicate trends for the individual months. Over the observational record of approx. 80 years, changes in monthly discharge regimes are clearly visible. On the one hand, summer discharge of Gunt river during the third quarter (Q3), i.e., July, August and September, is decreasing whereas the cold season discharge in Q1 and Q4 is increasing. This is a clear indication that the basin hydrology is undergoing changes over the long run. These could either be climate-related or, as discussed above, also the result of man-made interventions such as the regulation of river discharge for winter hydropower energy production. However, the shift of discharge from the warm season (Q2 and Q3) towards the cold season (Q4 and Q1) has already happened before river regulation started and hence, it is likely that we see a compound effect here.\n\nq_17050_mon %>% \n  summarise_by_time(.date_var = date, \n                    .by       = \"month\",\n                    value     = mean(data)) %>% \n  tk_ts(frequency = 12) %>% \n  forecast::ggsubseriesplot(year.labels = FALSE) + \n              geom_smooth(method = \"lm\", color = \"red\") +\n              xlab('Month') +\n              ylab('Mean monthly Q [m3/s]')\n\n\n\nFigure 6: The Figure shows data from the entire record. All monthly data are binned in their corresponding month (black lines). A large interannual variability (year-to-year variations in discharge in the same months) is visible. The red lines show simple regression lines for each month separately.\n\n\n\n\nWhenever we analyze annual data and changes therein, we should work with data as observed during the hydrological year. The hydrological year in Central Asia is defined as:\n\nmonHY(Oct) = 1\nmonHY(Nov) = 2\n…\nmonHY(Sep) = 12\n\nThis also holds for meteorological data. Using this definition, we can further define cold and warm seasons easily where the cold season lasts from October through end of March (Q4 to Q1 the following year) and the warm season from April through end of September (Q2 and Q3). With this in mind, we can define the hydrological year discharge.\nGiven a time series of observations, the function convert2HYY() as part of the riversCentralAsia package provides a convenient way to compute hydrological year mean discharge, including for cold and warm seasons. For monthly mean temperatures mean(T), it computes hydrological year mean temperatures, including for cold and warm seasons. Finally, for precipitation, the function computes the hydrological year sum, including also for cold and warm season months. Figure 7 shows the discharge time series analysis for the Khorog gauging station.\n\nqHYY <- q_17050_mon %>% convert2HYY(.,'17050','Q')\nqHYY %>%   pivot_longer(-hyYear) %>% \n  plot_time_series(hyYear,value,name,\n                   .title = '',\n                   .x_lab = 'Year',\n                   .y_lab = 'Mean monthly Q [m3/s]',\n                   .interactive = TRUE,\n                   .smooth = FALSE)\n\n\nFigure 7: Hydrological year discharge time series, incl. cold and warm season values. If data are not complete for all 12 months, the hydrological year statistics are not computed. Q_mean_ann: entire hydrological year discharge, Q_mean_cs: cold season Q1/Q4 discharge and Q_mean_ws: warm season Q2/Q3 discharge.\n\n\n\nFigure 7 confirms the findings from the seasonal analysis. However, it also shows that the first two decades of the 21st century show a marked decline in total discharge as compared to the period between 1960 to 2000.\nA common way to plot changes over time in hydro-meteorological time series is to plot annual deviations from corresponding long-term norms (long-term mean values). For this, we can use the plotNormDevHYY() function from the riversCentralAsia package. Given the three hydrological year annual time series, it computes long-term norms over the entire data set and subtracts actual annual values from the norm value. Like this, temporal changes and trends become even better visible. Figure 8 shows the results for the hydrological year data.\n\nplotNormDevHYY(qHYY,'Q','Khorog-Gunt 17050')\n\n\n\nFigure 8: Deviations from the corresponding long-term norms for the discharge time series at gauging station 17050. It should be noted that the values shown are deviations from the corresponding norms which are shown in the subtitles above the Figure plates.\n\n\n\n\nFigure 8 shows that, in absolute terms, the discharge in the high-flow season is undergoing a much greater reduction than an increase in the low-flow season. Hence, we cannot simply explain the decline of discharge in one season with the increase in the other. In other words, the early melting of the winter snow pack cannot alone explain the summer decline in water availability. Some other mechanism much be at work which we still need to better understand. One hypothesis could be that an increase in summer temperatures leads to higher evaporation over the basin thus leading to reduced discharge (see also Section 1.1.3 below).\nAlso and as mentioned above, winter discharge is influence by human regulation after 2006. This needs to be carefully taken into account when carrying out climate change impact analysis over the period of the observational record. For example, the cold season discharge deviation from the norm in 2006 and 2007 is 10 m3/s (see Figure 8) indicating that this amount of additional water was used for hydropower energy production during the winter.\nIn order to gauge whether there is a robust trend in discharge over the observed time period, we compute decadal (10 year means) and plot the results.\n\nmean10yearQ <- qHYY %>% \n  filter(hyYear < '2020-01-01') %>% \n  pivot_longer(-hyYear) %>% \n  group_by(name) %>% \n  summarise_by_time(hyYear,value, .by = \"10 year\", mean10yearQ = mean(value, na.rm = TRUE)) %>%\n  dplyr::select(-value) %>% \n  distinct() %>% \n  ungroup()\nmean10yearQ %>% pivot_wider(names_from = name,values_from = mean10yearQ)\n\n# A tibble: 8 × 4\n  hyYear     Q_mean_ann Q_mean_cs Q_mean_ws\n  <date>          <dbl>     <dbl>     <dbl>\n1 1940-01-01      111.       40.3      182.\n2 1950-01-01      108.       39.0      177.\n3 1960-01-01       96.2      35.7      156.\n4 1970-01-01      105.       35.9      174.\n5 1980-01-01      105.       37.5      171.\n6 1990-01-01      114.       41.0      188.\n7 2000-01-01      104.       43.8      165.\n8 2010-01-01       94.2      44.7      143.\n\nmean10yearQ %>%  plot_time_series(hyYear,mean10yearQ,name,\n                                  .smooth = FALSE,\n                                  .x_lab  = \"Year\",\n                                  .y_lab  = \"Q [m^3/s]\",\n                                  .title  = \"\")\n\n\nFigure 9: 10-year mean hydrological year discharge of Gunt River, including the cold and warm season components. The decadal mean values are related in time to the beginning of the corresponding decade in the Figure. The strongly declining trend in warm season discharge causes the overall observed decline in hydrological year discharge.\n\n\n\nThis is informative. From the 1990ies onwards, a strong reduction in mean hydrological year warm season discharge is observed of about -16 % relative to the mean 1940 - 1989 values. At the same time, 10-year mean hydrological year cold season discharge remained almost stable. As already mentioned, such types of findings are the a key motivation to study possible climate impacts in such basins in greater detail with hydrological modeling.\nGunt Basin Climate\nA significant amount of meteorological station data are available. Some of these data are analyzed in this Section. While we mostly concentrate on mean monthly data for temperature, we should note that the available data record also contains data on absolute and mean minimum and maximum temperatures.\n\n# Extracting mean station data from the four stations.\nTmean_38954 <- gunt_station_data %>% \n  filter(code == \"38954\" & type == 'mean(T)') %>% \n  filter(date >= '1939-01-01') %>% \n  dplyr::select(date, data) %>% \n  rename(Tmean_38954 = data)\nTmean_38950 <- gunt_station_data %>% \n  filter(code == \"38950\" & type == 'mean(T)') %>% \n  filter(date >= '1939-01-01') %>% \n  dplyr::select(date, data) %>% \n  rename(Tmean_38950 = data)\nTmean_38953 <- gunt_station_data %>% \n  filter(code == \"38953\" & type == 'mean(T)') %>% \n  filter(date >= '1939-01-01') %>% \n  dplyr::select(date, data) %>% \n  rename(Tmean_38953 = data)\nTmean_38956 <- gunt_station_data %>% \n  filter(code == \"38956\" & type == 'mean(T)') %>% \n  filter(date >= '1939-01-01') %>% \n  dplyr::select(date, data) %>% \n  rename(Tmean_38956 = data)\n# Assembling the data. \nT <- full_join(Tmean_38950, Tmean_38953, by = \"date\")\nT <- full_join(T, Tmean_38954, by = \"date\")\nT <- full_join(T, Tmean_38956, by = \"date\")\n# Plotting the dataframe \nT %>% pivot_longer(-date) %>% \n  filter(date >= '1940-01-01') %>% \n  plot_time_series(date,\n                   value,\n                   name,\n                   .smooth = FALSE,\n                   .x_lab = 'Year',\n                   .y_lab = 'Mean monthly T [deg. C]',\n                   .title = \"\",\n                   .interactive = TRUE)\n\n\nFigure 10: Mean Monthly Temperature Climatology in the Gunt River Basin from 1940 - 2020. While first observations are available from the very beginning of the 20th century, data are only shown from 1940 onward which marks the start of a coherent record.\n\n\n# add a month identifier\nT <- T %>% \n  mutate(mon = month(date))\n\nBecause of the high quality and the consistency of the long-term record of the data at Khorog station 39854, we focus the further climatological analysis there. Figure 11 shows deviations from norm mean temperatures over the last 120 years. The recent two decades stand out because of the pronounced warming observed at the station, especially during the cold season where norm deviations on average range between 1 - 2 degrees Celsius (deg. C.).\n\n# Station Khorog 38954\nmeanTHYY_38954 <- gunt_station_data %>% convert2HYY(38954,'mean(T)') %>% filter(hyYear >= \"1900-10-01\")\nmeanTHYY_38954 %>% plotNormDevHYY(.,'mean(T)','Khorog 38954')\n\n\n\nFigure 11: Annual devations from the norm of the mean temperature for the Khorog station 38954 record are shown for the entire hydrological year and for the corresponding cold and warm seasons. Note that the entire data record is taken into account here from the start of the 20th century."
  },
  {
    "objectID": "example_river_basins.html#sec-example-chirchik-river-basin",
    "href": "example_river_basins.html#sec-example-chirchik-river-basin",
    "title": "",
    "section": "Chirchik River Basin",
    "text": "Chirchik River Basin\nThe Chirchik is a river in the Tashkent region of Uzbekistan. Its natural basin covers 13’112 km2, not accounting for the modern-time interbasin water transfers to the neighboring Akhangaran basin in the south (the outline of the basin is shown in Figure 12) and to the north. In terms of total runoff contribution, it is the biggest right tributary of the Syr Darya (see also further below in Section Section 1.2.1).\nThe river is formed by the confluence of the Chatkal and the Pskem rivers. They emerge at the south-western end of the Tien Shan mountains, i.e. the Talas Alatau, in the border region of Kyrgyzstan, Kazakhstan and Uzbekistan. The main tributaries are in clock-wise direction starting from north: Ugam, Pskem, Kosku and Chatkal. The Charvak reservoir receives water from these rivers. Ugam is the largest right tributary downstream of the reservoir and Aksak Ata the largest left-side tributary.\nBelow the Charvak hydroelectric power station, the river water gets diverted in numerous canals for irrigation in and around the Tashkent oasis and for interbasin water transfer to the Akhangaran basin in the south. As part of the Chirchik-Bozsuu cascade, several smaller dams along the river serve hydropower production and irrigation purposes.\n\n\nFigure 12: Overview over the Chirchik river basin with tributaries and the location of the main gauging stations in the zone of runoff formation and near the confluence with the Syr Darya.\n\n\nFigure 12 shows a comprehensive overview of the Chirchik river basin and its tributaries as well as relevant modern gauging stations. Gauges are indicated with the semi-round shapes and the corresponding five digit official code as utilized by the Uzbek Hydrometeorological Service (HMS) indicated. The gauge 16924 is not a real gauge in the sense that reservoir inflow is not measured at one point but rather is calculated from all contributing tributary flow components, i.e. the Chatkal river, the Pskem river, Nauvalisoy and the Koksu River.\nKoksu however, with a basin area of 392 km\\(^2\\), is ungauged. Its discharge contribution is calculated using an established empirical relationship between discharge in Chatkal River and discharge in Koksu. The empirical relationship is derived in Section Section 1.2.3. First, we now turn our attention to the description of key hydrological basin features.\nChirchik Basin Characterization\nThis Section available data to characterize the Chirchik River Basin from the hydro-climatological perspective. Data access and modeling is further described in Chapter ?@sec-data in Part II and Part III ?@sec-hydrological-modeling of this Book.\nThe available discharge data is shown in Figure 13. These are near complete historic records. See above Figure 12 for the station locations.\n\nchirchik_river_data <- ChirchikRiverBasin\nchirchik_river_data %>% \n  filter(type == 'Q') %>% \n  group_by(type,code,station,resolution) %>% \n  plot_time_series(date,\n                   data,\n                   .facet_ncol      = 2,\n                   .interactive     = FALSE, \n                   .smooth          = FALSE,\n                   .title           = '')\n\n\n\nFigure 13: Available discharge data of Chirchik River basin.\n\n\n\n\nThe discharge measurements at Gazalkent gauge (number 16262) started already in 1900. It is one of the longest complete records available in Central Asia. The monthly record of the station is shown in Figure 14. You can zoom into the time series and investigate it in detail.\n\nchirchik_river_data %>% \n  filter(code == '16262') %>% \n  plot_time_series(date,data,\n    .interactive = TRUE,\n    .smooth = FALSE,\n    .title = \"\",\n    .x_lab = 'date',\n    .y_lab = 'Discharge in cubic meters per second',\n    .plotly_slider = TRUE)\n\n\nFigure 14: Monthly discharge at Gauge 16262, Gazalkent.\n\n\n\nAs is easily visible, the June 1969 discharge was the historic monthly mean maximum with 1’220 m3/s. The time series features the typical snow-melt-driven runoff pattern with pronounced seasonality and interannual variability.\nAt Chinaz near the confluence of the Chirchik River with the Syr Darya (Gauge 16275), however, a changing discharge regime can be identified over time (see Figure 15). The drastic decrease in discharge there is due to two effects. First, water diversions and interbasin water transfers for irrigation purposes have greatly increased over the course of the 20th century. Second, the closure of the Charvak dam in 1974 and the subsequent filling of the dam decreased discharge during the filling period. Furthermore, the interannual variability of flows decreased from there onward due to the now regulated flow regime. This latter effect is also visible at the Gazalkent gauge (Figure 14). The non-stationarity in the discharge time series at these stations is thus explained by anthropogenic effects.\n\nchirchik_river_data %>% \n  filter(code == '16275') %>% \n  plot_time_series(date,data,\n    .interactive = TRUE,\n    .smooth = FALSE,\n    .title = \"\",\n    .x_lab = 'date',\n    .y_lab = 'Discharge in cubic meters per second',\n    .plotly_slider = TRUE)\n\n\nFigure 15: Monthly discharge at Gauge 16275, Chinaz.\n\n\n\nThe effect of water diversion becomes even more apparent when the annual discharge at Gazalkent gauging station upstream of any major water diversion and at Chinaz gauge, which is in the very downstream of Chirchik River right before its confluence with the Syr Darya, are compared. The corresponding annual time series are shown in Figure 16 together with the difference of the two time series.\n\n\n\n\nFigure 16: Annual discharge at Gauge 16262, Gazalkent and Gauge 16275, Chinaz and the difference of the two timeseries. The difference of the two time series is from the allocation of water for human purposes, mostly for irrigation.\n\n\n\n\nFigure 16 shows the growing water allocation in the catchment from the 1930ies up to the end of the 20th century. Allocation grew almost 3-fold over this period. Interestingly, in the first decade of the 21st century, trends in allocation completely reversed and in 2009, roughly one third of the total flow at Gazalkent was allocated consumptively. The trend reversal might be due to a change in irrigation policy, problems with intake infrastructure between the two gauges, or both.\n\n\n# A tibble: 7 × 5\n# Groups:   code [7]\n  code   mean   min    max    sd\n  <chr> <dbl> <dbl>  <dbl> <dbl>\n1 16262 229.   48.7 1220   186. \n2 16275 105.    1.2  912   121  \n3 16279 116.   21.1  729   110. \n4 16290  79.4  12.7  438    69.3\n5 16298   3.8   0.9   21.1   2.8\n6 16300  22.4   3.9  114    19.3\n7 16924 205.   40.7 1231   183. \n\n\n\n\nTable 3: Key statistics of Chirchik basin rivers.\n\ncode\nmean\nmin\nmax\nsd\n\n\n\n16262\n228.6\n48.7\n1220.0\n186.4\n\n\n16275\n104.9\n1.2\n912.0\n121.0\n\n\n16279\n115.7\n21.1\n729.0\n109.9\n\n\n16290\n79.4\n12.7\n438.0\n69.3\n\n\n16298\n3.8\n0.9\n21.1\n2.8\n\n\n16300\n22.4\n3.9\n114.0\n19.3\n\n\n16924\n205.3\n40.7\n1231.0\n183.2\n\n\n\n\n\n\nThe largest left tributary to Chirchik below the Charvak reservoir Aksak Ata. The gauging station on the river got dismantled a long time ago. An average long-term mean discharge of 2.35 m\\(^{3}\\)/s is a solid estimated of its contribution to the overall discharge of Chirchik. Thus, if we add up long-term average discharge at Gazalkent and the one from Aksak Ata, we obtain an annual norm discharge (total average water availability) of 231 m\\(^{3}\\)/s.\nChirchik river is thus the biggest right-tributary of the Syr Darya. Chatkal river contributes exactly half to it (115.7 m\\(^{3}\\)/s) and Pskem river approximately one third (34.4 % or 79.4 m\\(^{3}\\)/s). Nauvalisoy is only a very small river with 1.6 % runoff contribution (3.8 m\\(^{3}\\)/s). From the available data, the long-term average runoff contribution by the ungauged Koksu river can be estimated to be 6.4 m\\(^{3}\\)/s or 2.8 %. Downstream of the reservoir, Ugam river contributes an additional 9.7 % (22.4 m\\(^{3}\\)/s) to the total flow.\nLet us now turn our attention to the seasonality of the tributaries. We exclude both, the Chinaz Gauge and Gazalkent Gauge data in our analysis for the above-mentioned reason that flows there are no longer representing a natural runoff regimes but are influenced by human interference. For the analysis, we plot seasonalities of the key gauged and unregulated tributaries, i.e. Chatkal, Pskem, Nauvalisoy and Ugam rivers in Figure 17 and Figure 18 below.\n\nchirchik_river_data %>% \n  filter(type == 'Q', \n         code != \"16275\",\n         code != \"16262\",\n         code != \"16924\",\n         code != \"16298\",\n         code != \"16300\") %>% \n  dplyr::select(date,data,code,river) %>% \n  group_by(code, river) %>% \n  plot_seasonal_diagnostics(.date_var = date,\n                            .value = data,\n                            .interactive = FALSE,\n                            .feature_set = c(\"week\",\"month.lbl\"),\n                            .title = \"\")\n\nWarning: Removed 108 rows containing non-finite values (stat_boxplot).\n\n\n\n\nFigure 17: Seasonality diagnostics of the two large tributaries, i.e. the Chatkal and Pskem rivers. Analyses of weekly (top row) and monthly data (bottom row) are shown. Year to year variability is pronounced during the peak runoff season in Q2 and Q3 where the amount of water in the rivers is critically determined by the amount of snow deposited in the mountains during the previous winter season.\n\n\n\n\nDischarge seasonality of the gauging stations downstream of Charvak reservoir is shown below. Note that we only have monthly values for Ugam station which explains the appearance of the weekly plot in the upper right panel of @ref(fig:seasonalitySmallTribs).\n\nchirchik_river_data %>% \n  filter(type == 'Q', \n         code != \"16275\",\n         code != \"16262\",\n         code != \"16924\",\n         code != \"16279\",\n         code != \"16290\") %>% \n  dplyr::select(date,data,code,river) %>% \n  group_by(code, river) %>% \n  plot_seasonal_diagnostics(.date_var = date,\n                            .value = data,\n                            .interactive = FALSE,\n                            .feature_set = c(\"week\",\"month.lbl\"),\n                            .title = \"\")\n\n\n\nFigure 18: Seasonality diagnostics of the two minor tributaries to the Chirchik River that are gauged.\n\n\n\n\nThe seasonality with the spring (small rivers) and summer (large tributaries) runoff peaks is striking in all the rivers. Nauvalisoy discharge peaks, on average, during or around week 20. Chatkal river discharge peaks around week 23 and Pskem river around week 26. These differences can be explained with the difference in mean catchment elevations which are as follows (ordered according to descending mean catchment elevation:\n\nPskem Catchment: 2’795 masl,\nChatkal catchment: 2’692 masl,\nNauvalisoy catchment: 2’160 masl,\nUgam catchment: 803 masl,\n\nwhere Ugam is the lowest lying and Pskem catchment the highest catchment when measured according to mean catchment elevation. Figure 19 shows the hypsometric curves of the main tributaries to the Chirchik River.\n\n\nFigure 19: Hypsometric Curves of the tributaries to the Chirchik River Basin.\n\n\nUsing a LOESS smoother, we can remove discharge time series seasonality and catch a glimpse of the underlying long-term trends. This is shown for gauging station 16294, i.e. the inflow to the Charvak Reservoir, in Figure 20. If anything, a slightly increasing trend in mean discharge can be observed over the last 40 years. We will further discuss this finding also in the context of the analysis of the meteorological data record in the next Section.\n\nchirchik_river_data %>% \n  filter(code == \"16924\") %>% \n  dplyr::select(date, data, code, river)  %>% \n  summarise_by_time(.date_var = date, .by = \"month\", value = mean(data)) %>% \n  plot_time_series(date,value)\n\n\nFigure 20: Changes in mean monthly discharges are plotted with black lines over the entire observational record for Charvak Reservoir gauge (16924).The blue line shows a smoothed trend using a LOESS smoother.\n\n\n\nBut what about changes for particular seasons and months? To understand these changes, we plot monthly average data grouped together individually for all months. Figure 21 shows the resulting graphs together with their best fit regression lines for each month. Several interesting observations can be done.\n\nchirchik_river_data %>% \n  filter(code == \"16924\") %>% \n  dplyr::select(date, data, code, river)  %>% \n  summarise_by_time(.date_var = date, .by = \"month\", value = mean(data)) %>% \n  tk_ts(frequency = 12) %>% \n  forecast::ggsubseriesplot(year.labels = FALSE) + \n              geom_smooth(method = \"lm\", color = \"red\") +\n              xlab('month') +\n              ylab('m^3/month')\n\n\n\nFigure 21: Changes in mean monthly discharges are plotted with black lines over the entire observational record for Charvak Reservoir gauge (16924).The red lines are the per month best fit regression lines.\n\n\n\n\nFirst, cold season discharge in quarter 1 (Q1) and Q4 have a slightly increasing trend. Converse to this, the warm season quarterly trends are not uniform where Q2 trends are strongly increasing and Q3 trends are markedly decreasing. This is in line with what one would expect from a warming climate, i.e. that the snow-melt driven hydrograph peak flows shift in their timing towards earlier towards spring. At the same time, Q3 warm season discharge diminishes because of the earlier snow melt, assuming no changes in the precipitated water (compare also with the findings in ?@sec-gunt-river-basin where a high elevation basin from the Pamir mountains is discussed). We will investigate the available climate and precipitation record of Chirchik River basin in the following section.\n\n\n\n\nFigure 22: The plates show mean, minimum and maximum quarterly discharges for Q1 (upper left plate), Q2 (upper right plate), Q3 (lower left plate) and Q4 (lower right plate). All values are in mean quarterly discharge per second.\n\n\n\n\nThe development of the quarterly minimum, maximum and mean discharge Q over the years for Gauge 16924 (Charvak reservoir inflow) is shown in Figure 22. The increasing trends in cold season discharge (Q1 and Q4) is confirmed. In these quarters, minimum, mean and maximum discharges appear to increase with a probably link to temperature increases during these quarters (see Section 1.2.2 for a discussion). In Q2, minimum and mean discharges have an increasing trend. In Q3, maximum discharge appears to decrease over time.\nChirchik Basin Climate\nLong-term climate data from three different stations located in the vicinity and upstream of Charvak Reservoir is available. The stations are meteorological stations 38642 and 38339, both in Pskem River Basin, station 38471, Chatkal River Basin and station 38464 in the vicinity of the Charvak Reservoir (see also Figure 12 above for the locations of these stations.\nThe raw temperature data is shown in Figure 23 whereas the per month temperature trends are shown in Figure 24 and Figure 25. At both stations, a significant cold season warning trend is visible.\n\nchirchik_river_data %>% \n  filter(type == 'T') %>% \n  group_by(type,code,station,resolution) %>% \n  plot_time_series(date,\n                   data,\n                   .facet_ncol      = 1,\n                   .interactive     = FALSE, \n                   .smooth          = TRUE,\n                   .title           = '')\n\n\n\nFigure 23: Available decadal temperature records at Pskem and Chatkal meteorological stations. The record at the Kyrgyz Chatkal Meteo Station shows a large data gap in the post-transition years. The blue trend lines (LOESS smoother) indicate an increasing temperature trend at both mountain stations.\n\n\n\n\n\nchirchik_river_data %>% \n  filter(type == \"T\" & code == \"38462\") %>% \n  dplyr::select(date,data,code,river)  %>% \n  summarise_by_time(.date_var = date, .by = \"month\", value = mean(data)) %>% \n  tk_ts(frequency = 12) %>% \n  forecast::ggsubseriesplot(year.labels = FALSE) + \n              geom_smooth(method = \"lm\", color = \"red\") +\n              xlab('month') +\n              ylab('deg. C.')\n\n\n\nFigure 24: Changes in mean monthly temperatures are plotted with black lines over the entire observational record for Pskem meteorological station.The red lines are the per month best fit regression lines.\n\n\n\n\n\nchirchik_river_data %>% \n  filter(type == \"T\" & code == \"38471\") %>% \n  dplyr::select(date, data, code, river)  %>% \n  summarise_by_time(.date_var = date, .by = \"month\", value = mean(data)) %>% \n  tk_ts(frequency = 12) %>% \n  forecast::ggsubseriesplot(year.labels = FALSE) + \n              geom_smooth(method = \"lm\", color = \"red\") +\n              xlab('month') +\n              ylab('deg. C.')\n\n\n\nFigure 25: Changes in mean monthly temperatures are plotted with black lines over the entire observational record for Chatkal meteorological station.The red lines are the per month best fit regression lines. The one spike in the September bin is an erroneous time series entry.\n\n\n\n\nThe increasing cold season temperatures have an impact on the snow fractions in the basins, i.e., on the fraction falling as solid precipitation versus total precipitation. Generally speaking, warming winter temperatures will lead to an increase in elevation of the snowline and thus reduce the area and volume of the winter snow deposits.\nSimilarly to the analysis carried out above for the development of quarterly flows, we can analyze the development of quarterly temperature statistics. Figure 26 shows the results.\n\nCodequarterT_mean <- chirchik_river_data %>% \n  filter(type == \"T\" & code == \"38471\") %>% \n  dplyr::select(date, data, code, river)  %>% \n  summarise_by_time(.date_var = date, .by = \"quarter\",value = mean(data, na.rm = TRUE)) %>% \n  rename(mean = value) %>% \n  na.omit()\n\nquarterT_max <- chirchik_river_data %>% \n  filter(type == \"T\" & code == \"38471\") %>% \n  dplyr::select(date,data,code,river)  %>% \n  summarise_by_time(.date_var = date, .by = \"quarter\",value = max(data, na.rm = TRUE)) %>% \n  rename(max = value) %>% \n  na.omit()\n\nquarterT_min <- chirchik_river_data %>% \n  filter(type == \"T\" & code == \"38471\") %>% \n  dplyr::select(date, data, code, river)  %>% \n  summarise_by_time(.date_var = date,.by = \"quarter\",value = min(data, na.rm = TRUE)) %>% \n  rename(min = value) %>% na.omit()\n\nquarterT <- left_join(quarterT_mean, quarterT_max, by = 'date')\nquarterT <- left_join(quarterT, quarterT_min, by = 'date')\n\nquarterT <- \n  bind_cols(quarterT, quarterT$date %>% \n              tsibble::yearquarter()) %>% \n  rename(quarter = '...5')\nquarterT$quarter <- quarterT$quarter %>% \n  format(., format = \"Q%q\")\n\nq1T <- quarterT %>% filter(quarter == 'Q1') %>% dplyr::select(-quarter) %>% pivot_longer(-date)\nq2T <- quarterT %>% filter(quarter == 'Q2') %>% dplyr::select(-quarter) %>% pivot_longer(-date)\nq3T <- quarterT %>% filter(quarter == 'Q3') %>% dplyr::select(-quarter) %>% pivot_longer(-date)\nq4T <- quarterT %>% filter(quarter == 'Q4') %>% dplyr::select(-quarter) %>% pivot_longer(-date)\n\npQ1 <- q1T  %>% \n  plot_time_series(.date_var = date,.value = value,.color_var = name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',.title = \"Q1\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"year\", .y_lab = \"deg. C.\",.legend_show = TRUE)\npQ2 <- q2T  %>% \n  plot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',.title = \"Q2\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"year\",.y_lab = \"deg. C.\",.legend_show = FALSE)\npQ3 <- q3T  %>%\nplot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',.title = \"Q3\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"year\",.y_lab = \"deg. C.\", .legend_show = FALSE)\npQ4 <- q4T  %>% na.omit() %>%\nplot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',.title = \"Q4\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"year\",.y_lab = \"deg. C.\",.legend_show = FALSE)\n\npQ1 + pQ2 + pQ3 + pQ4 + plot_layout(guides = 'collect') & theme(legend.position = 'bottom')\n\n\n\nFigure 26: Development of mean, minimum and maximum quarterly temperatures for Q1 at Station 38462.\n\n\n\n\nApart from a generally increasing trend in temperature, especially in the cold seasons, there are also significant positive anomalies since the year 2000 at Station 38462 in the maximum winter (Q1) temperatures. Changes in the fraction of precipitation falling as snow are expected under such developments.\nWe can in fact compute these changes with high resolution daily climate fields that are nowadays available (see Karger et al. (2017), Karger et al. (2020) and Karger et al. (2021) and also more on these in ?@sec-data). Figure 27 shows the corresponding results.\n\n\nFigure 27: Changes in fractional snow cover in the in the Western Tien Shan mountains, including the Chirchik River basin. Shown are percentage changes over the period 1980 - 2011. Changes in snow cover fraction was computed using CHELSA V21 daily precipitation and temperature with a liquid-solid temperature threshold value of 1 deg. C.. (@karger_2017, @karger_2020, @karger_2021).\n\n\nThe atmospheric warming between 1980 and 2011 (this is period for which daily precipitation and temperature fields are available at 1 km2 resolution) has lead to a reduction of the fraction of the total precipitation falling as snow between 5 - 24 % within the elevation range from 800 masl to 1’500 masl, approx. Changes for pixels where the trend is not significant at the 95 % confidence level are not shown, i.e., they are transparent. In the region shown, no significant positive trends could be identified over the period of consideration. Finally, at higher elevations in the Chirchik River basin, no significant changes can be seen in most places.\nNow we look into the development of precipitation over the available record of station data.\n\nchirchik_river_data %>% \n  filter(type == 'P') %>% \n  group_by(type,code,station,resolution) %>% \n  plot_time_series(date,\n                   data,\n                   .facet_ncol      = 1,\n                   .interactive     = FALSE, \n                   .smooth          = FALSE,\n                   .title           = '')\n\n\n\nFigure 28: Available decadal and monthly data records from different meteorological stations that are located in the zone of runoff formation. As in the case of temperature, the precipitation record at the Kyrgyz Chatkal Meteo Station shows a large data gap in the post-transition years.\n\n\n\n\n\nCodequarterP_mean <- chirchik_river_data %>% \n  filter(type == \"P\" & code == \"38464\") %>% \n  dplyr::select(date,data,code,river)  %>% \n  summarise_by_time(.date_var = date, .by = \"quarter\",value = mean(data,na.rm = TRUE)) %>% \n  rename(mean = value) %>% na.omit()\n\nquarterP_max <- chirchik_river_data %>% \n  filter(type == \"P\" & code == \"38464\") %>% \n  dplyr::select(date,data,code,river)  %>% \n  summarise_by_time(.date_var = date, .by = \"quarter\",value = max(data,na.rm = TRUE)) %>% \n  rename(max = value) %>% na.omit()\n\nquarterP_min <- chirchik_river_data %>% \n  filter(type == \"P\" & code == \"38464\") %>% \n  dplyr::select(date,data,code,river)  %>% \n  summarise_by_time(.date_var = date,.by = \"quarter\",value = min(data,na.rm = TRUE)) %>% \n  rename(min = value) %>% na.omit()\n\nquarterP <- left_join(quarterP_mean, quarterP_max, by = 'date')\nquarterP <- left_join(quarterP, quarterP_min, by = 'date')\n\nquarterP <- \n  bind_cols(quarterP,quarterP$date %>% \n              tsibble::yearquarter()) %>% \n  rename(quarter = '...5')\nquarterP$quarter <- quarterP$quarter %>% \n  format(., format = \"Q%q\")\n\nq1P <- quarterP %>% filter(quarter == 'Q1') %>% dplyr::select(-quarter) %>% pivot_longer(-date)\nq2P <- quarterP %>% filter(quarter == 'Q2') %>% dplyr::select(-quarter) %>% pivot_longer(-date)\nq3P <- quarterP %>% filter(quarter == 'Q3') %>% dplyr::select(-quarter) %>% pivot_longer(-date)\nq4P <- quarterP %>% filter(quarter == 'Q4') %>% dplyr::select(-quarter) %>% pivot_longer(-date)\n\npQ1 <- q1P  %>% #group_by(name) %>% \n  plot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',\n      .title = \"Q1\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"year\", .y_lab = \"mm / 10 days\",.legend_show = TRUE\n      )\npQ2 <- q2P  %>% #group_by(name) %>% \n  plot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',\n      .title = \"Q2\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"year\",.y_lab = \"mm / 10 days\",.legend_show = FALSE\n      )\npQ3 <- q3P  %>% #group_by(name) %>% \nplot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',\n      .title = \"Q3\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"year\",.y_lab = \"mm / 10 days\", .legend_show = FALSE\n      )\npQ4 <- q4P  %>% na.omit() %>% #group_by(name) %>% \nplot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',\n      .title = \"Q4\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"year\",.y_lab = \"mm / 10 days\",.legend_show = FALSE\n      )\n\npQ1 + pQ2 + pQ3 + pQ4 + plot_layout(guides = 'collect') & theme(legend.position = 'bottom')\n\n\n\nFigure 29: Development of mean, minimum and maximum quarterly precipitation at Station 38462 in the Chirchik River basin.\n\n\n\n\nFigure 29 shows that at the meteorological station 38462, precipitation levels have been increasing until the 1980ies for Q3 and Q4. Contrary to that, there is a slightly increasing trend in Q1 since the year 2000.\nIn summary, the general global warming trend is clearly visible in in-situ station records as well as in high-resolution climatologies. As our quick analyis of the changes in snow cover fractions show, the warming translates into less snow cover in intermediate elevation ranges but not in the high-mountain zones where the critical snow storage for warm season runoff is found. This might be one of the possible explanations that we do not see any reduction in observed discharge in the basin.\nDischarge Estimation from the Ungauged Kosku Tributary\nThe construction of the Charvak reservoir and the subsequent water impoundment of the rivers water in the reservoir led to a destruction of the previous gauge that measured Chatkal discharge, including the contribution from the Koksu right-tributary. To be able to estimate the Koksu discharge contribution to the Charvak reservoir, even after the impoundment, an empirical relationship between Chatkal river discharge and Koksu was established. The procedure allows for the more or less precise estimation of Koksu discharge, even in the absence of direct measurements, as described below.\n\n\n\n\n\n\nNote\n\n\n\nThe model shown here to estimate Koksu discharge using measured quantities at Khudaydod is a simple type of an empirical model. You will learn more about such types of models in ?@sec-modeling-using-predictive-inference.\n\n\nBefore the closure of the Charvak dam and the subsequent filling of the reservoir in and after 1974, the Uzbek experts from the Hydrometeorological Agency started a detailed 3-years measurement comparison campaign at Charvak gauge and at gauge 16279 in Khudaydod (see Figure 12). Both are located on Chatkal river\nThe confluence of Koksu river with Charvak river is just upstream of the former Charvak gauge and hence between gauge 16279 and the former Charvak gauge. Using daily data from the measurement comparisons campaign, Charvak gauge discharge was related to Khudaydod discharge using a linear relationship. At the same time, they were now able to relate Koksu discharge to the discharge at Chatkal River in Khudaydod as shown in Equation 1.\n\\[\nQ_{Koksu} \\propto Q_{Charvak} - Q_{16279}\n\\qquad(1)\\]\nWe show the procedure here. After loading the riversCentralAsia Package as shown above, the relevant daily data from 01/01/1965 - 31/12/1967 can be loaded as follows (the data was kindly provided by Mr. Andrey Yakovlev).\n\ndata_koksu_discharge_derivation <- riversCentralAsia:::KoksuDischargeDerivation # load Data\ndata_koksu_discharge_derivation\n\n# A tibble: 1,976 × 4\n   date        data code    units\n   <date>     <dbl> <chr>   <chr>\n 1 1965-01-01  35.6 Charvak m3/s \n 2 1965-01-02  35.6 Charvak m3/s \n 3 1965-01-03  33   Charvak m3/s \n 4 1965-01-04  33   Charvak m3/s \n 5 1965-01-05  33   Charvak m3/s \n 6 1965-01-06  33   Charvak m3/s \n 7 1965-01-07  34.3 Charvak m3/s \n 8 1965-01-08  35.6 Charvak m3/s \n 9 1965-01-09  38.4 Charvak m3/s \n10 1965-01-10  35.6 Charvak m3/s \n# … with 1,966 more rows\n\n\nThe data is stored in long format, meaning that measurements in time and for the two gauging stations are just stacked on top of each other in one long table with a total of 1’976 measurement points. For the purpose here, we prefer the wide format where we have one date column with unique dates and then the data listed for each station in corresponding columns.\n\ndata_koksu_discharge_derivation_wide <- data_koksu_discharge_derivation %>% pivot_wider(id_cols = 'date',values_from = 'data',names_from = \"code\")\ndata_koksu_discharge_derivation_wide\n\n# A tibble: 988 × 3\n   date       Charvak `16279`\n   <date>       <dbl>   <dbl>\n 1 1965-01-01    35.6    33.4\n 2 1965-01-02    35.6    32.4\n 3 1965-01-03    33      30.4\n 4 1965-01-04    33      29.4\n 5 1965-01-05    33      30.4\n 6 1965-01-06    33      30.4\n 7 1965-01-07    34.3    31.4\n 8 1965-01-08    35.6    32.4\n 9 1965-01-09    38.4    34.4\n10 1965-01-10    35.6    32.4\n# … with 978 more rows\n\n\nThe runoff contribution of Koksu can be calculated in a simple manner.\n\n# Adding Koksu discharge to the dataframe\ndata_koksu_discharge_derivation_wide <- data_koksu_discharge_derivation_wide %>% mutate(Koksu = Charvak - `16279`)\ndata_koksu_discharge_derivation_wide\n\n# A tibble: 988 × 4\n   date       Charvak `16279` Koksu\n   <date>       <dbl>   <dbl> <dbl>\n 1 1965-01-01    35.6    33.4  2.20\n 2 1965-01-02    35.6    32.4  3.20\n 3 1965-01-03    33      30.4  2.6 \n 4 1965-01-04    33      29.4  3.6 \n 5 1965-01-05    33      30.4  2.6 \n 6 1965-01-06    33      30.4  2.6 \n 7 1965-01-07    34.3    31.4  2.9 \n 8 1965-01-08    35.6    32.4  3.20\n 9 1965-01-09    38.4    34.4  4   \n10 1965-01-10    35.6    32.4  3.20\n# … with 978 more rows\n\n\nThe relationship can now be visualized as shown in Figure 30.\n\nggplot(data_koksu_discharge_derivation_wide, aes(`16279`, Koksu)) +\n  geom_point() + \n  xlab(bquote('Discharge at Gauge 16279 Khudaydod in '~m^3/s)) +\n  ylab (bquote('Koksu river discharge in '~m^3/s))\n\n\n\nFigure 30: Scatterplot showing the relation between daily mean discharge as measured at gauge 16279 and Koksu river. The data was acquired during a compaign between 1965 and 1967.\n\n\n\n\n::: {callout icon=false} The relation between Chatkal River discharge and Koksu River discharge is remarkable, despite the complex mountain terrain from which these neighboring rivers emerge. Can you think of reasons why this is so? How do you explain outlier data? Hint: To answer these questions, you might want to consult @#sec-hydrological-systems. :::\nWe can perform a linear regression to related discharge at Khudaydod to the one at Koksu. The coefficients of the linear regression can be obtained in the following way:\n\nlm_koksu <- lm(Koksu ~ 0 + `16279`,data_koksu_discharge_derivation_wide)\nsummary(lm_koksu)\n\n\nCall:\nlm(formula = Koksu ~ 0 + `16279`, data = data_koksu_discharge_derivation_wide)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.295  -5.867  -1.918   1.416 120.419 \n\nCoefficients:\n        Estimate Std. Error t value Pr(>|t|)    \n`16279`  0.14469    0.00298   48.55   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.4 on 987 degrees of freedom\nMultiple R-squared:  0.7049,    Adjusted R-squared:  0.7046 \nF-statistic:  2357 on 1 and 987 DF,  p-value: < 2.2e-16\n\n\nPlease note, in the specification of the linear model we add the 0 term to force the regression through the origin. Hence, the discharge contribution of Koksu River is estimated to be\n\\[\nQ_{Koksu} = 0.145 * Q_{Khudaydod}\n\\qquad(2)\\]\nThe resulting model can easily be visualized as is shown in Figure 31.\n\nggplot(data_koksu_discharge_derivation_wide, aes(`16279`, Koksu)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE, formula = y~0+x) +\n  xlab(bquote('Discharge at Gauge 16279 Khudaydod in '~m^3/s)) +\n  ylab(bquote('Koksu river discharge in '~m^3/s))\n\n\n\nFigure 31: The linear model in $eq-koksu-estimated-model is shown.\n\n\n\n\n\n\n\n\nErasov, N. V. 1968. “Method for Determining of Volume of Mountain Glaciers.” MGI, no. 14: 307–8.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nKarger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-resolution monthly precipitation and temperature time series from 2006 to 2100.” Scientific Data 7 (1): 248. https://doi.org/10.1038/s41597-020-00587-y.\n\n\nKarger, Dirk Nikolaus, Adam M. Wilson, Colin Mahony, Niklaus E. Zimmermann, and Walter Jetz. 2021. “Global daily 1 km land surface precipitation based on cloud cover-informed downscaling.” Scientific Data 8 (1): 307. https://doi.org/10.1038/s41597-021-01084-6."
  },
  {
    "objectID": "snow_and_glacier_data.html#introduction",
    "href": "snow_and_glacier_data.html#introduction",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.1 Introduction",
    "text": "6.1 Introduction\nThe runoff of the rivers Syr Darya and Amu Darya consists of 65%-75% snow melt, 23% precipitation and 2-8% glacier melt approximately (Armstrong et al. 2019). In smaller, highly glaciated catchments, the glacier contribution to discharge can be more important (Khanal et al. 2021). Generally, the cryosphere is a major contributor to the water balance in Central Asia (Barandun et al. 2020). While glacier runoff is a small contributor to the annual runoff, it is seasonally important as it covers the irrigation demand in summer, when snow melt is over (Kaser, Grosshauser, and Marzeion 2010).\nAmong other things, climate impacts translate into long-term changes of runoff formation fractions and the distribution of runoff formation within the hydrological year. Typical rainfall-runoff models such as the HBV Model simulate the fractionation of precipitation into snow and rain with a temperature threshold method. Snow and liquid water reservoirs and corresponding fluxes are then accounted for. However, these models have only a limited understanding of glacier processes which are normally inadequate at best to estimate glacier contributions to discharge.\nThe following section gives a brief overview over the available regional open source data regarding Central Asias cryosphere. A later chapter [TODO LINK TO CHAPTER] will then focus on the modelling of the cryosphere.\nPlease note that new (highly relevant and public) glacier data are released ever more frequently. The summary provided here refers to the latest data sets at the time of writing in February 2022.\nWe use the catchment of the gauging station on the Atabshy river, a tributary to the Naryn river in Central Asia as a demo site. If you’d like to reproduce the examples presented in this chapter you can download the zipped data in the example data set available here. You can extract the the downloaded data into a location of your choice and adapt the reference path below. The rest of the code will run as it is, provided you have the required r packages installed. The size of the data package is 14.1 GB.\n\nlibrary(tmap)\nlibrary(sf)\nlibrary(raster)\nlibrary(tidyverse)\nlibrary(lubridate)\n\ndevtools::install_github(\"hydrosolutions/riversCentralAsia\")\nlibrary(riversCentralAsia)\n\n# Path to the data directory downloaded from the download link provided above. \n# Here the data is extracted to a folder called atbashy_glacier_demo_data\ndata_path <- \"../caham_data/SyrDarya/Atbashy/\""
  },
  {
    "objectID": "snow_and_glacier_data.html#high-mountain-asia-snow-reanalysis-product",
    "href": "snow_and_glacier_data.html#high-mountain-asia-snow-reanalysis-product",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.2 High Mountain Asia Snow Reanalysis Product",
    "text": "6.2 High Mountain Asia Snow Reanalysis Product\nYufei Liu, Fang, and Margulis (2021) provide a reanalysis product for snow covered area and snow water equivalent (SWE) in High Mountain Asia. The data is available via NSIDC (Y. Liu, Fang, and Margulis 2021). Their SWE can directly be compared to the SWE computed in hydrological models like HBV.\nFrom the downloaded data, only the SWE and the validity mask (showing the pixels where the snow water equivalent product is valid) is required.\n\ndem <- raster(paste0(data_path, \"GIS/16076_DEM.tif\"))\nbasin <- st_read(paste0(data_path, \"GIS/16076_Basin_outline.shp\"), quiet = TRUE)\n\n# Load one example file and display SWE for a random date in the cold season. \nfilespath <- paste0(data_path, \"SNOW/\")\nyear <- 1999\n\n# Load non-seasonal snow mask\nfilepart <- \"_MASK.nc\"\nindex = sprintf(\"%02d\", (year - 1999))\n\n# The Atbashy basin is covered by two raster stacks\nmask_w <- raster::brick(paste0(filespath, \n                               \"HMA_SR_D_v01_N41_0E76_0_agg_16_WY\", \n                               year, \"_\", index, filepart), \n                     varname = \"Non_seasonal_snow_mask\")\nraster::crs(mask_w) = raster::crs(\"+proj=longlat +datum=WGS84 +no_defs\")\nmask_e <- raster::brick(paste0(filespath,\n                               \"HMA_SR_D_v01_N41_0E77_0_agg_16_WY\", \n                               year, \"_\", index, filepart), \n                     varname = \"Non_seasonal_snow_mask\")\nraster::crs(mask_e) = raster::crs(\"+proj=longlat +datum=WGS84 +no_defs\")\n\n# The rasters need to be rotated\ntemplate <- raster::projectRaster(from = mask_e, to= mask_w, alignOnly = TRUE)\n\n# template is an empty raster that has the projected extent of r2 but is \n# aligned with r1 (i.e. same resolution, origin, and crs of r1)\nmask_e_aligned <- raster::projectRaster(from = mask_e, to = template)\nmask_w <- flip(t(mask_w), direction = 'x')\nmask_e_aligned <- flip(t(mask_e_aligned), direction = 'x')\nmask <- merge(mask_w, mask_e_aligned, tolerance = 0.1) \nmask = raster::projectRaster(from = mask, \n                             crs = crs(\"+proj=utm +zone=42 +datum=WGS84 +units=m +no_defs\"))\n\n# Load snow data\nvarname = \"SWE_Post\"\nfilepart <- \"_SWE_SCA_POST.nc\"\nsca_w <- raster::brick(paste0(filespath, \n                              \"HMA_SR_D_v01_N41_0E76_0_agg_16_WY\", \n                              year, \"_\", index, filepart), \n                       varname = varname)\n\n[1] \"vobjtovarid4: **** WARNING **** I was asked to get a varid for dimension named Day BUT this dimension HAS NO DIMVAR! Code will probably fail at this point\"\n\nraster::crs(sca_w) = raster::crs(\"+proj=longlat +datum=WGS84 +no_defs\")\nsca_e <- raster::brick(paste0(filespath,\n                              \"HMA_SR_D_v01_N41_0E77_0_agg_16_WY\", \n                              year, \"_\", index, filepart), \n                       varname = varname)\n\n[1] \"vobjtovarid4: **** WARNING **** I was asked to get a varid for dimension named Day BUT this dimension HAS NO DIMVAR! Code will probably fail at this point\"\n\nraster::crs(sca_e) = raster::crs(\"+proj=longlat +datum=WGS84 +no_defs\")\ntemplate <- raster::projectRaster(from = sca_e, to = sca_w, alignOnly = TRUE)\n# template is an empty raster that has the projected extent of r2 but is \n# aligned with r1 (i.e. same resolution, origin, and crs of r1)\nsca_e_aligned<- raster::projectRaster(from = sca_e, to = template)\nsca_w <- flip(t(sca_w), direction = 'x')\nsca_e_aligned <- flip(t(sca_e_aligned), direction = 'x')\nsca <- raster::merge(sca_w, sca_e_aligned, tolerance = 0.1)\nsca <- projectRaster(from = sca, \n                     crs = crs(\"+proj=utm +zone=42 +datum=WGS84 +units=m +no_defs\"))\n\nsca_masked <- mask(sca, mask, maskvalue = 1)\nsca_masked <- mask(sca_masked, basin)\n\n# Visualize snow water equivalent\ntmap_mode(\"view\")\ntm_shape(sca_masked$layer.1) + \n  tm_raster(n = 6,\n            palette = \"Blues\",\n            alpha = 0.8,\n            legend.show = TRUE, \n            title = \"SWE (-)\") + \n  tm_shape(basin) + \n  tm_borders(col = \"black\", lwd = 0.6)\n\n\nFigure 6.1: Snow water equivalent map of October 1, 1998. (Source: HMASR)."
  },
  {
    "objectID": "snow_and_glacier_data.html#randolph-glacier-inventory-rgi",
    "href": "snow_and_glacier_data.html#randolph-glacier-inventory-rgi",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.3 Randolph Glacier Inventory (RGI)",
    "text": "6.3 Randolph Glacier Inventory (RGI)\nThe Randolph Glacier Inventory (RGI) v6.0 (RGI Consortium 2017) makes a consistent global glacier data base publicly available. It includes geo-located glacier geometry and some additional parameters like elevation, length, slope and aspect. A new version (v7) is under review at the time of writing beginning of 2022. For Central Asian water resources modelling, RGI regions 13 (Central Asia) and 14 (South Asia West) are relevant. You can download the glacier geometries for all RGI regions from the GLIMS RGI v6.0 web site. For this demo, the data for the Atbashy basin is available from the data download link given above.\n\n# Loading the data\nrgi <- st_read(paste0(data_path, \"GIS/16076_Glaciers_per_subbasin.shp\"), \n               quiet = TRUE) |> \n  st_transform(crs = crs(dem))\n\n# Generation of figure\ntmap_mode(\"view\")\ntm_shape(dem) +\n  tm_raster(n = 6, \n            palette = terrain.colors(6),\n            alpha = 0.8,\n            legend.show = TRUE, \n            title = \"Elevation (masl)\") + \n  tm_shape(rgi) + \n  tm_polygons(col = \"lightgray\", lwd = 0.2)\n\n\nFigure 6.2: DEM & Glaciers (light gray) of the demo basin."
  },
  {
    "objectID": "snow_and_glacier_data.html#glacier-thickness",
    "href": "snow_and_glacier_data.html#glacier-thickness",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.4 Glacier Thickness",
    "text": "6.4 Glacier Thickness\nFarinotti et al. (2019) make distributed glacier thickness maps available for each glacier in the RGI v6 data set. We have downloaded the required maps of glacier thickness for you and made them available in the download link above. We will refer to this data set as the glacier thickness data set or the Farinotti data set.\nThe original glacier thickness data set is available from the data collection of Farinotti et al. (2019) which is available from the data section of their online article.\nThe following code chunk demonstrates how to extract glacier thickness data from the Farinotti data set.\n\n6.4.1 How to Extract Glacier Thickness\n\n# Get a list of all files in the glacier thickness data set. The files are named \n# after the glacier ID in the RGI v6.0 data set (variable RGIId).  \nglacier_thickness_dir <- paste0(data_path, \"GLACIERS/Farinotti/\") \nfilelist <- list.files(path = glacier_thickness_dir, pattern = \".tif$\", \n                       full.names = TRUE)\n\n# Filter the glacier thickness file list for the glacier ids in the catchment of \n# interest. \nfilelist <- filelist[sapply(rgi$RGIId, grep, filelist)]\n\n# Get the maximum glacier thickness for each of the glaciers in filelist. \n# Note: this works only for small catchments as the origin of the rasters to be \n# mosaiced needs to be consistent. For a larger data set you will need to implement \n# a loop over all glaciers to extract the thickness per glacier or per elevation \n# band. This operation can take a while. \nglacier_thickness <- Reduce(function(x, y) raster::mosaic(x, y, fun = max),\n                            lapply(filelist, raster::raster)) \n\n# For plotting, clip the glacier thickness raster of the basin to the basin boundary\nglacier_thickness <- mask(glacier_thickness |> \n                            projectRaster(crs = crs(dem)), basin)\n\n\ntmap_mode(\"view\")\ntm_shape(dem) + \n  tm_raster(n = 6, \n            palette = terrain.colors(6),\n            alpha = 0.8,\n            legend.show = TRUE, \n            title = \"Elevation (masl)\") + \n  tm_shape(glacier_thickness) +\n  tm_raster(n = 6, \n            palette = \"Blues\",\n            legend.show = TRUE, \n            title = \"Glacier thickness\\n(m)\") + \n  tm_shape(rgi) + \n  tm_borders(col = \"gray\", lwd = 0.4) + \n  tm_shape(basin) + \n  tm_borders(col = \"black\", lwd = 0.6)\n\n\nFigure 6.3: Glacier thickness by Farinotti et al., 2019\n\n\n\nA more recent glacier thickness data set by Millan et al. (2022) estimates much larger ice reservoirs in the Himalayan region but similar goodness of fit for the glaciers in the Central Asian region as the Farinotti data set. The Millan et al. (2022) data set is not included in the present workflow yet but could be an alternative for the Farinotti data set."
  },
  {
    "objectID": "snow_and_glacier_data.html#glacier-thinning-rates",
    "href": "snow_and_glacier_data.html#glacier-thinning-rates",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.5 Glacier Thinning Rates",
    "text": "6.5 Glacier Thinning Rates\nHugonnet et al. (2021) provide annual estimates of glacier thinning rates for each glacier in the RGI v6.0 data set. It is advised to not to rely on the annual data but rather on an average over at least 5 years to get reliable thinning rates for individual glaciers. We compare trends in glacier thinning rates to trends in computed glacier balance components. We will refer to this data set as the thinning rates data set or the Hugonnet data set. A copy of the Hugonnet thinning rates is included in the download link above.\nThe original per-glacier time series of thinning rates can be downloaded from the data repository as described in the github site linked under the code availability section of the online paper of Hugonnet et al. (2021).\n\nhugonnet <- read_csv(paste0(data_path, \"/GLACIERS/Hugonnet/dh_13_rgi60_pergla_rates.csv\"))\n# Explanation of variables:\n# - dhdt is the elevation change rate in meters per year,\n# - dvoldt is the volume change rate in meters cube per year,\n# - dmdt is the mass change rate in gigatons per year,\n# - dmdtda is the specific-mass change rate in meters water-equivalent per year.\n\n# Filter the basin glaciers from the Hugonnet data set. \nhugonnet <- hugonnet |> \n  dplyr::filter(rgiid %in% rgi$RGIId) |> \n  tidyr::separate(period, c(\"start\", \"end\"), sep = \"_\") |> \n  mutate(start = as_date(start, format = \"%Y-%m-%d\"), \n         end = as_date(end, format = \"%Y-%m-%d\"), \n         period = round(as.numeric(end - start, units = \"days\")/366))\n\n# Join the Hugonnet data set to the RGI data set to be able to plot the thinning \n# rates on the glacier geometry. \nglaciers_hugonnet <- rgi |> \n  left_join(hugonnet |> dplyr::select(rgiid, area, start, end, dhdt, err_dhdt, \n                                      dvoldt, err_dvoldt, dmdt, err_dmdt, \n                                      dmdtda, err_dmdtda, period),  \n            by = c(\"RGIId\" = \"rgiid\")) \n\n# Visualization of data\ntmap_mode(\"view\")\ntm_shape(dem) + \n  tm_raster(n = 6, \n            palette = terrain.colors(6),\n            alpha = 0.8, \n            legend.show = TRUE, \n            title = \"Elevation (masl)\") + \n  tm_shape(glaciers_hugonnet |> dplyr::filter(period == 20)) +\n  tm_fill(col = \"dmdtda\", \n          n = 6, \n          palette = \"RdBu\",\n          midpoint = 0, \n          legend.show = TRUE, \n          title = \"Glacier thinning\\n(m weq/a)\") + \n  tm_shape(rgi) + \n  tm_borders(col = \"gray\", lwd = 0.4) + \n  tm_shape(basin) + \n  tm_borders(col = \"black\", lwd = 0.6)\n\n\nFigure 6.4: Average glacier mass change by Hugonnet et al., 2021."
  },
  {
    "objectID": "snow_and_glacier_data.html#glacier-discharge",
    "href": "snow_and_glacier_data.html#glacier-discharge",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.6 Glacier Discharge",
    "text": "6.6 Glacier Discharge\nMiles et al. (2021) ran specific mass balance calculations over many glaciers larger than 2 km2 of High Mountain Asia. They provide the average glacier discharge between 2000 and 2016. The package riversCentralAsia includes an empirical relationship based on a regression between glacier thinning rates and glacier discharge which allows the estimation of glacier discharge. We will refer to this data set as the glacier discharge data set or the Miles data set. A copy of the glacier discharge data is available from the data download link provided above.\nThe original data is available from the data repository linked in the online version of the paper.\n\n# Calculate glacier discharge using the glacierDischarge_HM function of the \n# riversCentralAsia package. An empirical relationship between glacier thinning \n# rates by Hugonnet et al., 2021 and glacier discharge by Miles et al., 2021.  \nglaciers_hugonnet <- glaciers_hugonnet |> \n  mutate(Qgl_m3a = glacierDischarge_HM(dhdt))\n\n# Data visualization\ntmap_mode(\"view\")\ntm_shape(dem) + \n  tm_raster(n = 6, \n            palette = terrain.colors(6),\n            alpha = 0.8, \n            legend.show = TRUE, \n            title = \"Elevation (masl)\") + \n  tm_shape(glaciers_hugonnet |> dplyr::filter(period == 20)) +\n  tm_fill(col = \"Qgl_m3a\", \n          n = 6, \n          palette = \"RdBu\",\n          midpoint = 0, \n          legend.show = TRUE, \n          title = \"Glacier discharge\\n(m3/a)\") + \n  tm_shape(rgi) + \n  tm_borders(col = \"gray\", lwd = 0.4) + \n  tm_shape(basin) + \n  tm_borders(col = \"black\", lwd = 0.6)\n\n\nFigure 6.5: Glacier discharge derived from Miles et al., 2021."
  },
  {
    "objectID": "snow_and_glacier_data.html#a-note-on-the-uncertainties-of-glacier-data-sets",
    "href": "snow_and_glacier_data.html#a-note-on-the-uncertainties-of-glacier-data-sets",
    "title": "",
    "section": "A note on the uncertainties of glacier data sets",
    "text": "A note on the uncertainties of glacier data sets\nThe geometries of the RGI v6.0 data set are generally very good. If you simulate glacier discharge in a small catchment with few glaciers it is advisable to visually check the glacier geometries and make sure, all relevant glaciers in the basin are included in the RGI data set. You may have to manually add missing glaciers or correct the geometry.\nFor some regions in Central Asia, OpenStreetMap is an excellent reference for glacier locations and names in Central Asia. You can import the map layer in QGIS or also download individual GIS layers.\nThe glacier thickness data set is validated only at few locations as measurements of glacier thickness are typically not available. Farinotti et al. (2019) list an uncertainty range for the volume estimate in regions RGI 13 and 14 of 26% each.\nHugonnet et al. (2021) & Miles et al. (2021) provide the uncertainties of their estimates for per-glacier glacier thinning & discharge rates in the data set itself. They typically lie around p/m 150%.\n## References {#sec-snow-and-glacier-data-references}\n\n\n\n\nArmstrong, Richard L., Karl Rittger, Mary J. Brodzik, Adina Racoviteanu, Andrew P. Barrett, Siri-Jodha Singh Khalsa, Bruce Raup, et al. 2019. “Runoff from Glacier Ice and Seasonal Snow in High Asia: Separating Melt Water Sources in River Flow.” Regional Environmental Change 19 (5): 1249–61. https://doi.org/10.1007/s10113-018-1429-0.\n\n\nBarandun, Martina, Joel Fiddes, Martin Scherler, Tamara Mathys, Tomas Saks, Dimitry Petrakov, and Martin Hoelzle. 2020. “The State and Future of the Cryosphere in Central Asia.” Water Security 11. https://doi.org/10.1016/j.wasec.2020.100072.\n\n\nFarinotti, Daniel, Matthias Huss, Johannes J. Fürst, Johannes Landmann, Horst Machguth, Fabien Maussion, and Ankur Pandit. 2019. “A Consensus Estimate for the Ice Thickness Distribution of All Glaciers on Earth.” Nature Geoscience 12 (3): 168–73. https://doi.org/10.1038/s41561-019-0300-3.\n\n\nHugonnet, Romain, Robert McNabb, Etienne Berthier, Brian Menounos, Christopher Nuth, Luc Girod, Daniel Farinotti, et al. 2021. “Accelerated Global Glacier Mass Loss in the Early Twenty-First Century.” Nature 592 (7856): 726–31. https://doi.org/10.1038/s41586-021-03436-z.\n\n\nKaser, G., M. Grosshauser, and B. Marzeion. 2010. “Contribution Potential of Glaciers to Water Availability in Different Climate Regimes.” Proceedings of the National Academy of Sciences 107 (47): 20223–27. https://doi.org/10.1073/pnas.1008162107.\n\n\nKhanal, S., A. F. Lutz, P. D. A. Kraaijenbrink, B. van den Hurk, T. Yao, and W. W. Immerzeel. 2021. “Variable 21st Century Climate Change Response for Rivers in High Mountain Asia at Seasonal to Decadal Time Scales.” Water Resources Research 57 (5). https://doi.org/10.1029/2020WR029266.\n\n\nLiu, Y., Y. Fang, and S. A. Margulis. 2021. High Mountain Asia UCLA Daily Snow Reanalysis (version Version 1). Boulder, Colorado USA: NASA National Snow; Ice Data Center Distributed Active Archive Center. doi: https://doi.org/10.5067/HNAUGJQXSCVU.\n\n\nLiu, Yufei, Yiwen Fang, and Steven A. Margulis. 2021. “Spatiotemporal Distribution of Seasonal Snow Water Equivalent in High-Mountain Asia from an 18-Year Landsat-MODIS Era Snow Reanalysis Dataset.” The Cryosphere 15: 5261–80. https://doi.org/10.5194/tc-2021-139.\n\n\nMiles, Evan, Michael McCarthy, Amaury Dehecq, Marin Kneib, Stefan Fugger, and Francesca Pellicciotti. 2021. “Health and Sustainability of Glaciers in High Mountain Asia.” Nature Communications 12 (2868): 10. https://doi.org/https://doi.org/10.1038/s41467-021-23073-4.\n\n\nMillan, Romain, Jérémie Mouginot, Antoine Rabatel, and Mathieu Morlighem. 2022. “Ice Velocity and Thickness of the World’s Glaciers.” Nature Geoscience 15 (2): 124–29. https://doi.org/10.1038/s41561-021-00885-z.\n\n\nRGI Consortium. 2017. “Randolph Glacier Inventory – a Dataset of Global Glacier Outlines: Version 6.0: Technical Report.” Global Land Ice Measurements from Space, Colorado, USA. Digital Media. https://doi.org/https://doi.org/10.7265/N5-RGI-60."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "hydrological_systems.html#sec-regional-hydroclimatological-features",
    "href": "hydrological_systems.html#sec-regional-hydroclimatological-features",
    "title": "",
    "section": "Regional Hydroclimatological Features",
    "text": "Regional Hydroclimatological Features"
  },
  {
    "objectID": "hydrological_systems.html#sec-zone-of-runoff-formation",
    "href": "hydrological_systems.html#sec-zone-of-runoff-formation",
    "title": "",
    "section": "Zone of Runoff Formation",
    "text": "Zone of Runoff Formation"
  },
  {
    "objectID": "hydrological_systems.html#sec-zone-of-water-distribution-and-use",
    "href": "hydrological_systems.html#sec-zone-of-water-distribution-and-use",
    "title": "",
    "section": "Zone of Water Distribution and Use",
    "text": "Zone of Water Distribution and Use"
  },
  {
    "objectID": "hydrological_systems.html#sec-hydrological-systems-references",
    "href": "hydrological_systems.html#sec-hydrological-systems-references",
    "title": "",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "climate_forcing_data.html#sec-climate-forcing-data",
    "href": "climate_forcing_data.html#sec-climate-forcing-data",
    "title": "",
    "section": "Climate Forcing Data",
    "text": "Climate Forcing Data\nChapter on climate forcing data with a focus on CHELSA V21 climatology."
  },
  {
    "objectID": "appendix_d_exercise_solutions.html#sec-appendix-solutions-exercise1",
    "href": "appendix_d_exercise_solutions.html#sec-appendix-solutions-exercise1",
    "title": "",
    "section": "Exercise on Linear Reservoir modelling",
    "text": "Exercise on Linear Reservoir modelling\nTask 1\nWhat will determine the flow through your bucket?\nThe flow through the bucket will be influenced by the volume to bottom area fraction of the bucket, the amount and speed of water added to the bucket and the size of the outlet hole.\nWhat do you need to measure?\nYou will need to measure:\n\nthe discharge from your bucket over time,\n\nthe recharge volume (how much water you put into the bucket over a given time),\n\nthe time when you start pouring water and when you stop pouring water into the bucket, and\nthe approximate volume of your bucket.\nHow can you measure it?\nThis depends on what you have available. You can draw a water level line outside of your outflow receptacle every 10 seconds and then determine the volume change over time. Maybe you have a scale and a smart phone so you can put your outflow receptacle on the scale and make a movie of the weight change over time (note, 1 kg of water is approximately 1 liter of water).\nFor the inflow pour a well defined volume over a well defined time interval. You can do this manually unless of course you have pipes and valves lying around that you can use.\nYou will need a watch for measuring time and a receptacle with known volume to measure volumes (or a scale).\nA couple of notes on measurement accuracy:\n\nGenerally, the larger the volumes, the smaller the relative measurement error. Say you measure a discharge of 50 ml in 1 s (i.e. 50 ml/s) and you can read your volume with an accuracy of 5 ml and the time with an accuracy of 0.2 s. Your measurement uncertainty becomes 11 ml/s which is more than 20 % of your discharge. If on the other hand, you measure 500 ml over 10 s (which is the same discharge of 50 ml/s) with the same inaccuracies for volume and time your measurement uncertainty for discharge becomes 1 ml/s which is only 2 % of your discharge.\n\nHow do you estimate measurement uncertainties: Measure several times, compute the average and the standard deviation of your measurements assuming a student-t distribution.\n\nHow do you combine uncertainties of volume and time to the uncertainty of discharge: By applying Gaussian error propagation.\nWhat materials you will need to set up the experiment?\n\nFor the bucket (the linear reservoir): A plastic bottle, a box or a can that is no longer used. It should have an opening at the top and the material should repel water and be thin enough that you can drill a hole into the wall.\nA pair of pointy scissors or a knife to drill a hole into the bucket.\n\nA water source (a tap, hose or a water container larger than the one above). This will be your rain machine.\nA watch to measure time.\n\nNote paper and pen.\n\nA receptacle for measuring the outflow.\n\nAdditional material to facilitate measurement according to availability.\n\n\n\nFigure 1: Example for material needed and example setup to perform the linear reservoir experiment. Add a minion with a stop watch or a smart phone to help you logging the discharge from the bucket.\n\n\nTask 2\nThe video was recorded with a smart phone. The weight of the outflow receptacle was noted down every second. The discharge is computed as the change of volume in the outflow receptacle over time.\n\n\nTask 3\nThe height of the measured discharge peak can be best reproduced with k = 0.42. However, the measured discharge peaks 1s later than the simulated discharge peak.\nReasons for the discrepancy can be the shape of the linear reservoir, non-linear pouring speed, and measurement uncertainties."
  },
  {
    "objectID": "appendix_d_exercise_solutions.html#sec-appendix-solutions-hbv-exercises",
    "href": "appendix_d_exercise_solutions.html#sec-appendix-solutions-hbv-exercises",
    "title": "",
    "section": "Exercises on the HBV Model",
    "text": "Exercises on the HBV Model\nExercise: Driving Forces of the HBV Model\nThe model drivers are precipitation, temperature and evaporation (P, T and ET in ?@fig-overview-hbv-model). You need to provide time series of the model drivers to the model. Evaporation is typically not measured at climate stations but many empirical functions are available in the literature to estimate evaporation. RSMinerve offers the possibility to calculate evaporation based on temperature measurements and catchment location (you will do that later in this tutorial).\nExercise - HBV Model States\nThe model states are the snow water equivalent height (SWE), the relative water content in the snow pack (WH), the humidity (Hum), the upper reservoir water level (SU) and the lower reservoir water level (SL). The model states are initialized using the initial conditions.\nExercise: Data Visualization in RSMinerve\nSimulate from the 01/01/1981 01:00:00 to 31/12/1983 23:00:00, then choose data from 31/12/1983 23:00:00 as the initial conditions and run the model from 01/01/1984 01:00:00 to 31/12/1984 23:00:00. Choose hourly output for the simulation results.\nOpen the Selection and plots tab by clicking on the Selection and plots button in the Modules toolbar and select simulated P and T from the Nauvalisoy station as shown in Figure 2.\n\n\nFigure 2: Hourly precipitation and temperature at the virtual Nauvalisoy weather station.\n\n\nNote: If you want to repeat a simulation with specific initial conditions, you can store them through Export IC in the Model Properties toolbar.\nThe approximate temperature range is -13 deg. C. in December to 34 deg. C. in August. The annual precipitation is about 1.4 m (visualize Pcum and click on the last value of the time series). No precipitation falls during the summer months.\nExercise: Compare Evaporation Methods\nFigure Figure 3 shows the evaporation computed with various methods and the resulting discharge. Uniform evaporation should not be used for sub-annual modeling time steps for obvious reasons that ET shows a strong seasonality. The difference between the different methods by Turc, McGuinness and Oudin are within 5 % of total discharge which is negligible for a regional model.\n\n\nFigure 3: Hourly precipitation and temperature at the virtual Nauvalisoy weather station.\n\n\nFor advanced modeling, the choice of the evaporation model may be relevant but only if a validation with measured data is possible.\nExercise: Common Difficulties in Model Calibration\n\nEspecially fully and semi-distributed hydrological models are typically over-parameterized, i.e. the number of model parameters is much larger than the number of observations for the model states. The true parameter values of the system cannot be uniquely identified based on a discharge time series alone.\n\nThe outcome of the calibration depends on the measure of similarity between the simulated and the measured discharge.\nThe water balance is often forgotten during model calibration. A nice fit of the discharge curve can for example be achieved by increasing the volume of water in the model over time.\nThe model is calibrated against historical data. Its ability to predict future discharge may be limited.\n\nThe model is not perfect, it remains an approximation of the real system and may not incorporate all relevant processes of the hydrological cycle (e.g. water storage and transport in glaciers for the case of the HBV model or significant sub-surface water fluxes that very difficult to capture as for example in Karst regions).\n\nDischarge measurements typically have uncertainties of 20 %. Particularly measurements at the lower and upper ends of the rating curve (i.e. the water table - discharge relationship) are typically prone to larger uncertainties (bonus question: think about why this is so!). Is the measurement location or the equipment not properly maintained, biases may grow over time. On the other hand, if the measurement method is updated and changed, the measured discharge may display a different pattern as was for example discussed in the data from Gunt River basin (?@sec-example-gunt-river-basin).\nExercise: Strategies to Overcome some of the Model Calibration Difficulties\n\nOver-parameterization:\n\n\n\nConsider simplifying the model, i.e. reducing the number of parameters. If the model complexity is required, try adding additional measured variables, e.g. snow cover from MODIS data (see Chapter on snow cover data) to validate individual components of the HBV model.\n\nCollect data to verify individual fluxes of the model components (e.g. soil parameters, snow water equivalent, etc.). As physical measurements in the field are not always possible you may have to become creative here, e.g. use MODIS snow cover data to validate the snow/no snow partitioning of the HBV model. Also consult the literature for parameterizations of similar catchments.\n\n\n\nUse a combination of similarity measures. This will be demonstrated later on in the model calibration section.\n\nDuring model calibration, look at the components of the model as well as the total discharge time series. Make sure that the storage of water changes within reasonable bounds and that the partitioning of the water in your system is physically reasonable (e.g. comparatively small storage compartments for rocky mountain catchments).\n\nExclude part of your data set from model calibration and use it for model validation. If the model does not perform well in the validation period, its parameters are too specific for the calibration period (you have over-fitted the model) and the model is said to not generalize well. If this happens you should try to reduce the number of parameters in your model. To understand better which parameters are responsible for the over-fit of the historical discharge, use different calibration and validation periods and compare the resulting parameters. Through sensitivity analysis, identify the model components that are most sensitive to predicted changes of model forcings, geometry or parameterization and perform scenario analysis.\n\nImplement and validate multiple possible conceptual models. All of the models must be calibrated and validated individually. It is further recommended to calculate at the highest possible temporal resolution and to try and compare the model outcome at different spatial resolution.\n\nSquare-root filters or data assimilation algorithms are able to account for non-correlated measurement errors (they are not implemented in RSMinerve and not topic of this course). Error bands for the measurements should be adapted when communicating model results.\n\nMost of the above points will be discussed in more detail during this course.\nExercise: Calibrate a Simple HBV Model {#sec-appendix-solutions-calibrated parameters .unnumbered}\nThe parameter set of the calibrated model are: ::: {.cell} ::: {.cell-output-stdout}\n\n\n|                |      |\n|:---------------|-----:|\n|CFMax (mm/°C/d) |  0.50|\n|CFR (-)         |  0.05|\n|CWH (-)         |  0.10|\n|TT (°C)         |  3.00|\n|TTInt (°C)      |  3.00|\n|TTSM (°C)       |  0.00|\n|Beta (-)        |  2.50|\n|FC (mm)         | 20.00|\n|PWP (-)         |  0.50|\n|SUMax (mm)      | 10.00|\n|Kr (1/d)        |  0.09|\n|Ku (1/d)        |  0.02|\n|Kl (1/d)        |  0.00|\n|Kperc (1/d)     |  0.00|\n::: :::\nYou can import the calibrated parameters via Import P in the Model Properties toolbar (note that the calibrated parameters are available for download here."
  },
  {
    "objectID": "hydraulic_hydrological_modeling.html#sec-hydrological-hydraulic-modeling-references",
    "href": "hydraulic_hydrological_modeling.html#sec-hydrological-hydraulic-modeling-references",
    "title": "",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "data_sources.html#sec-geospatial-data-sources",
    "href": "data_sources.html#sec-geospatial-data-sources",
    "title": "",
    "section": "Vector data",
    "text": "Vector data\nFor our purpose, we define the area of interest (AOI) as 55 deg. E - 85. deg. E and 30 deg. N - 50 deg. N..\nGlobal political boundaries can be obtained from the Global Administrative Divisions database at gadm.org. Except for Turkmenistan, data on first (Oblast) and second-level (Rayon) administrative divisions is available for all Central Asian states.\nShapefiles of large river basins can be retrieved from the Global Runoff Data Center. Note that for the Central Asian region, the flat downstream areas of these basins are delineating natural hydrological borders. They do not account for man-made inter-basin transfers and thus would need to be corrected where necessary.\nThe river network can be obtained from the Global Runoff Data Center WMOBB data that was released in 2021. It can be downloaded via this link. With 161 MB, approx., data covering the whole globe can be downloaded in a straight forward manner. The global data can be clipped easily with the bounding box as defined above.\nData for the large rivers can be extracted from the layer wmobb_rivnets_Q09_10 (containing line sections representing an upland area above 4’504 km2). The layer called wmobb_rivnets_Q08_09 contains line sections representing an upland area between 1’150 km2 and 4’504 km2 and, finally, the wmobb_rivnets_Q07_08 (containing line sections representing an upland area above between 487 and 1’150 km2) (GRDC, Koblenz, Germany: Federal Institute of Hydrology (BfG). 2020). Smaller rivers can be further added to a QGIS project on a case per case base using the additional datasets in the files obtained from GRDC.\nIn total, data from 277 gauging stations from Afghanistan, Kyrgyzstan, Kazakhstan, Uzbekistan and Tajikistan are available with hydrosolutions GmbH where requests for access to these data can be sent to. The locations were obtained from the Central Asian Hydrometeorological Organizations, public reports and the Soviet compendia Surface Water Resources, Vol 14 Issues 1 and 3. It should be emphasized that the location featured on the World Bank Group website Hydrometeorological Services in Central Asia are not correctly georeferenced in most cases.\nExcept for the Afghan stations, all stations were manually located in a Geographic Information System (GIS) using the relevant Soviet Military Topographic maps (1:200’000) from the corresponding region. The maps can be downloaded from https://maps.vlasenko.net and subsequently be georeferenced in QGIS with the Raster/Georeferencr tool there (QGIS Development Team 2021). Data from northern Afghan rivers’ stream flow characteristics and the location of gauging stations there can be obtained from (Olson and Williams-Sether 2010).\nPermanent water bodies and courses can be obtained from the global HydroLakes Database (Messager et al. 2016). It can be downloaded via this link.\nGlacier data can be taken from the Randolph Glacier Inventory (RGI) 6.0. The inventory contains a global archive of glacier outlines and can be obtained via this website. Information from 16’617 glaciers is available in the AOI. The corresponding data on glacier thickness and glacier thinning rates is available as complementary data Hugonnet et al. (2021). More information on these data can be found in ?@sec-snow-and-glacier-data whereas galciers models are dicussed in a separate Section in the Chapter on Hydrological Modeling.\nData on dams is available from the GOODD data set. Information from 88 dams in the region of interest is mapped (Mulligan, Soesbergen, and Sáenz 2020). The data is available from Global Dams Watch."
  },
  {
    "objectID": "data_sources.html#sec-raster-data-sources",
    "href": "data_sources.html#sec-raster-data-sources",
    "title": "",
    "section": "Raster Data",
    "text": "Raster Data\nThe NASA SRTM digital elevation model 1 Arc-second (30 m) global product is used as a DEM (“NASA Shuttle Radar Topography Mission (SRTM)(2013)” 2013). There are many ways to access these data, some more, some less convenient. An easy way to access these data is in QGIS by using the SRTM-Downloader Plugin. For a web-based access it is recommended to use the NASA EarthExplorer. Sample instructions on how to download DEM data from the EarthExplorer can for example be found by watching the following youtube tutorial.\n\n\n\n\n\n\n\n\n\nLand cover information can be obtained from the Copernicus Global Land Service: Land Cover 100m: collection 3: epoch 2019: Globe data (Buchhorn et al. 2019). The Global Land Cover Viewer allows to access, view and download annual land cover data from 2015 - 2019.\nHigh resolution climate data can be obtained from Climatologies at high resolution for the earth’s land surface areas (CHELSA) dataset via www.chelsa-climate.org. For the Version 2.1 product, climatologies for the periods 1981 - 2010, 2011 - 2040, 2041 - 2070 and 2071 - 2100 for a large number of variables are available for download as GeoTiff-files. With regard to the daily data, it is recommended to use the Global daily 1km land surface precipitation based on cloud cover-informed downscaling. This precipitation product reflects actual conditions in high mountain Asia in a much better way than the precipitation from the CHELSA-W5E5 V1.1 product. For daily temperature, the data from the CHELSA-W5E5 V1.1 product can be downloaded for a given domain of interest via this link.\n\n\n\n\n\n\nWarning\n\n\n\nNote that the daily high resolution climate fields for the entire Central Asia domain require a lot of storage space. Their processing for later analysis is computationally intensive.\n\n\nThe FLO1K, global maps of mean, maximum and minimum annual stream flow at 1 km resolution from 1960 through 2015 can be retrieved from this website. FLO1K delivers relevant data for water resources analyses at a global scale and yet high spatial resolution (Barbarossa et al. 2018). These data can be useful for long-term water balance assessments and for the study of the hydropower potential in the high mountain regions where flow measurements are sparse.\nThe CHELSA V21 global daily high-resolution climatology, available from 01-01-1979 until 31-12-2011 was processed over the Central Asia domain to map climate trends, including on temperature, precipitation, snow fraction. The data is available upon request from this site: https://chelsa-climate.org Karger et al. (2017), Karger et al. (2020), Karger et al. (2021). The CHELSA V21 product is corrected for snow undercatch in the high elevation ranges and thus is able to better represent actual high mountain precipitation than other available global climatologies (Beck et al. 2020). The aridity index (AI) fields were taken from the bio-climate CHELSA V21 data set and compared with the CGIAR AI product (Trabucco and Zomer 2019). Data on an additional 70 bio-climatic indicators were downloaded from the CHELSA V21 1980 - 2010 climatology and statistics extracted for each of the 277 gauged catchments, together with the AI.\nHigh-resolution crop disaggregated irrigated areas were mapped over the entire Central Asia domain by hydrosolutions GmbH (see also (Ragettli, Herberz, and Siegfried 2018) for more information). Like this 30 m crop maps were produced with Google Earth Engine using unsupervised classification for the years 2016 - 2020. These maps, in conjunction with estimates of irrigation water intake volumes and estimates of actual evapotranspiration help in irrigation scheme performance assessments. In hydrological modeling, these data can be used to introduce sectoral consumption estimates and help to come up with sound and effective basin planning."
  },
  {
    "objectID": "data_sources.html#sec-data-sources-references",
    "href": "data_sources.html#sec-data-sources-references",
    "title": "",
    "section": "References",
    "text": "References\n\n\n\n\nBarbarossa, Valerio, Mark A. J. Huijbregts, Arthur H. W. Beusen, Hylke E. Beck, Henry King, and Aafke M. Schipper. 2018. “Flo1k, Global Maps of Mean, Maximum and Minimum Annual Streamflow at 1 Km Resolution from 1960 Through 2015.” Scientific Data 5 (1): 180052. https://doi.org/10.1038/sdata.2018.52.\n\n\nBeck, Hylke E., Eric F. Wood, Tim R. McVicar, Mauricio Zambrano-Bigiarini, Camila Alvarez-Garreton, Oscar M. Baez-Villanueva, Justin Sheffield, and Dirk N. Karger. 2020. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1.\n\n\nBuchhorn, M., B. Smets, L. Bertels, B. De Roo, M. Lesiv, N. E. Tsendbazar, M. Herold, and S. Fritz. 2019. “Copernicus Global Land Service: Land Cover 100m: Collection 3: Epoch 2019: Globe.”\n\n\nFarinotti, Daniel, Matthias Huss, Johannes J. Fürst, Johannes Landmann, Horst Machguth, Fabien Maussion, and Ankur Pandit. 2019. “A Consensus Estimate for the Ice Thickness Distribution of All Glaciers on Earth.” Nature Geoscience 12 (3): 168–73. https://doi.org/10.1038/s41561-019-0300-3.\n\n\nGRDC, Koblenz, Germany: Federal Institute of Hydrology (BfG). 2020. “Major River Basins of the World / Global Runoff Data Centre, GRDC. 2nd, Rev. Ext. Ed.” Shape.\n\n\nHugonnet, Romain, Robert McNabb, Etienne Berthier, Brian Menounos, Christopher Nuth, Luc Girod, Daniel Farinotti, et al. 2021. “Accelerated Global Glacier Mass Loss in the Early Twenty-First Century.” Nature 592 (7856): 726–31. https://doi.org/10.1038/s41586-021-03436-z.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nKarger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-resolution monthly precipitation and temperature time series from 2006 to 2100.” Scientific Data 7 (1): 248. https://doi.org/10.1038/s41597-020-00587-y.\n\n\nKarger, Dirk Nikolaus, Adam M. Wilson, Colin Mahony, Niklaus E. Zimmermann, and Walter Jetz. 2021. “Global daily 1 km land surface precipitation based on cloud cover-informed downscaling.” Scientific Data 8 (1): 307. https://doi.org/10.1038/s41597-021-01084-6.\n\n\nMessager, M. L., B. Lehner, Grill G., I. Nedeva, and O. Schmitt. 2016. “Estimating the Volume and Age of Water Stored in Global Lakes Using a Geo-Statistical Approach.” Nature Communications 13603.\n\n\nMulligan, Mark, Arnout van Soesbergen, and Leonardo Sáenz. 2020. “GOODD, a Global Dataset of More Than 38,000 Georeferenced Dams.” Scientific Data 7 (1): 31. https://doi.org/10.1038/s41597-020-0362-5.\n\n\n“NASA Shuttle Radar Topography Mission (SRTM)(2013).” 2013. NASA. https://earthdata.nasa.gov/learn/articles/nasa-shuttle-radar-topography-mission-srtm-version-3-0-global-1-arc-second-data-released-over-asia-and-australia.\n\n\nOlson, S. A., and T. Williams-Sether. 2010. “Streamflow Characteristics at Streamgages in Northern Afghanistan and Selected Locations.” U.S. Geological Survey Data Series 529. USGS.\n\n\nQGIS Development Team. 2021. QGIS Geographic Information System. QGIS Association.\n\n\nRagettli, Silvan, Timo Herberz, and Tobias Siegfried. 2018. “An Unsupervised Classification Algorithm for Multi- Temporal Irrigated Area Mapping in Central Asia.” Remote Sensing 10 (11): 1823. https://doi.org/10.3390/rs10111823.\n\n\nTrabucco, Antonio, and Robert Zomer. 2019. “Global Aridity Index and Potential Evapotranspiration (ET0) Climate Database v2,” January. https://doi.org/10.6084/m9.figshare.7504448.v3."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Part II: Data Sources, Retrieval and Preparation",
    "section": "",
    "text": "In this Chapter and its Sections, we will discuss how to retrieve, prepare and process the data that is required for hydrological modeling.\nData include\n\nin-situ station data,\ngeospatial data,\nsnow and glacier data, and\nclimate reanalysis and projections data.\n\nAs will become clear, the preparation of these data requires a substantial amount of work, local storage space and, in some instances, computational power.\nData needs vary according to the modeling approach and the model chain. First, the preparation of the discharge data as described in Section 1 is a necessary step for data quality control independent of the type of hydrological modeling approach it is aimed for. The Section 1 shows the necessary geospatial analysis steps for basin delineation and the generation of the required input files for physically-based modeling using RSMinerve as described in Chapter Section 1 and the ?sec-hydrological-hydraulic-modeling.\nThe Section 6 and ?climate-data demonstrate the generation of time series data for individual hydrological response units with regard to glacier contributions and climate forcing.\nThe following diagram shows the entire modeling chain for hydrological modeling using RSMinerve. It shows that the data preparation step involves many interrelated components that partially depend on each other in a sequential way. This Chapter aims at carefully working through this modeling chain to carefully demonstrate the preparation of all the relevant data.\n\n\n\nFigure 1: Example hydrological modeling chain that include data preparation, the implementation of the hydrological model and the analysis of results. As is visbile, the data preparation steps are a very important part of hydrological modeling."
  },
  {
    "objectID": "glacier_modeling.html#writing-input-file-for-rsminerve",
    "href": "glacier_modeling.html#writing-input-file-for-rsminerve",
    "title": "11  Modeling of Discharge from Glacier Melt",
    "section": "\n12.1 Writing Input File for RSMinerve",
    "text": "12.1 Writing Input File for RSMinerve\nThe input file format for RSMinerve is described in Garcia Hernandez et al. (2020), page 136.\n\nQ <- Qimb_m3s_sub |>\n  mutate(Qimb_m3s = round(Qimb_m3s, digits = 7))\ntemp_wide <- Q |>\n  pivot_wider(names_from = name_2, values_from = Qimb_m3s) |> \n  rename(Station = date) \ndatechar <- posixct2rsminerveChar(temp_wide$Station)$value\ndatechar <- gsub(\" 01:00:00\", \" 00:00:00\", datechar)\ndatechar <- gsub(\" 02:00:00\", \" 00:00:00\", datechar)\noutput <- rbind(colnames(temp_wide),\n                c(\"X\", \"1\", \"1\", \"1\", \"1\"),  # Random coordinates, not relevant\n                c(\"Y\", \"2\", \"2\", \"2\", \"3\"), \n                c(\"Z\", \"3\", \"3\", \"3\", \"3\"), \n                c(\"Sensor\", \"Q\", \"Q\", \"Q\", \"Q\"), \n                c(\"Category\", \"Flow\", \"Flow\", \"Flow\", \"Flow\"), \n                c(\"Unit\", \"m3/s\", \"m3/s\", \"m3/s\", \"m3/s\"), \n                c(\"Interpolation\", \"Linear\", \"Linear\", \"Linear\", \n                  \"Linear\"), \n                cbind(datechar, \n                      as.character(temp_wide$Dzhaldzhur_Subbasin), \n                      as.character(temp_wide$Ulak_Subbasin), \n                      as.character(temp_wide$Atbaschy_Midstream_Subbasin), \n                      as.character(temp_wide$Atbaschy_Downstream_Subbasin)))\n\nFinally, the prepared data is written to a csv file which can be read into RSMinerve.\n\nwritefilename <- paste0(data_path, \"RSM_demo_glacier_source.csv\")\nwrite.table(output, file = writefilename, col.names = FALSE, \n            row.names = FALSE, append = FALSE, quote = FALSE, \n            sep = \",\", dec = \".\")"
  },
  {
    "objectID": "snow_and_glacier_data.html#a-note-on-the-uncertainties-of-the-glacier-data-sets",
    "href": "snow_and_glacier_data.html#a-note-on-the-uncertainties-of-the-glacier-data-sets",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.7 A Note on the Uncertainties of the Glacier Data Sets",
    "text": "6.7 A Note on the Uncertainties of the Glacier Data Sets\nThe geometries of the RGI v6.0 data set are generally very good. If you simulate glacier discharge in a small catchment with few glaciers it is advisable to visually check the glacier geometries and make sure, all relevant glaciers in the basin are included in the RGI data set. You may have to manually add missing glaciers or correct the geometry.\nFor some regions in Central Asia, OpenStreetMap is an excellent reference for glacier locations and names in Central Asia. You can import the map layer in QGIS or also download individual GIS layers.\nThe glacier thickness data set is validated only at few locations as measurements of glacier thickness are typically not available. Farinotti et al. (2019) list an uncertainty range for the volume estimate in regions RGI 13 and 14 of 26 % each.\nHugonnet et al. (2021) & Miles et al. (2021) provide the uncertainties of their estimates for per-glacier glacier thinning & discharge rates in the data set itself. They typically lie around p/m 150 %.\n## References {#sec-snow-and-glacier-data-references}\n\n\n\n\nArmstrong, Richard L., Karl Rittger, Mary J. Brodzik, Adina Racoviteanu, Andrew P. Barrett, Siri-Jodha Singh Khalsa, Bruce Raup, et al. 2019. “Runoff from Glacier Ice and Seasonal Snow in High Asia: Separating Melt Water Sources in River Flow.” Regional Environmental Change 19 (5): 1249–61. https://doi.org/10.1007/s10113-018-1429-0.\n\n\nBarandun, Martina, Joel Fiddes, Martin Scherler, Tamara Mathys, Tomas Saks, Dimitry Petrakov, and Martin Hoelzle. 2020. “The State and Future of the Cryosphere in Central Asia.” Water Security 11. https://doi.org/10.1016/j.wasec.2020.100072.\n\n\nFarinotti, Daniel, Matthias Huss, Johannes J. Fürst, Johannes Landmann, Horst Machguth, Fabien Maussion, and Ankur Pandit. 2019. “A Consensus Estimate for the Ice Thickness Distribution of All Glaciers on Earth.” Nature Geoscience 12 (3): 168–73. https://doi.org/10.1038/s41561-019-0300-3.\n\n\nHugonnet, Romain, Robert McNabb, Etienne Berthier, Brian Menounos, Christopher Nuth, Luc Girod, Daniel Farinotti, et al. 2021. “Accelerated Global Glacier Mass Loss in the Early Twenty-First Century.” Nature 592 (7856): 726–31. https://doi.org/10.1038/s41586-021-03436-z.\n\n\nKaser, G., M. Grosshauser, and B. Marzeion. 2010. “Contribution Potential of Glaciers to Water Availability in Different Climate Regimes.” Proceedings of the National Academy of Sciences 107 (47): 20223–27. https://doi.org/10.1073/pnas.1008162107.\n\n\nKhanal, S., A. F. Lutz, P. D. A. Kraaijenbrink, B. van den Hurk, T. Yao, and W. W. Immerzeel. 2021. “Variable 21st Century Climate Change Response for Rivers in High Mountain Asia at Seasonal to Decadal Time Scales.” Water Resources Research 57 (5). https://doi.org/10.1029/2020WR029266.\n\n\nLiu, Y., Y. Fang, and S. A. Margulis. 2021. High Mountain Asia UCLA Daily Snow Reanalysis (version Version 1). Boulder, Colorado USA: NASA National Snow; Ice Data Center Distributed Active Archive Center. doi: https://doi.org/10.5067/HNAUGJQXSCVU.\n\n\nLiu, Yufei, Yiwen Fang, and Steven A. Margulis. 2021. “Spatiotemporal Distribution of Seasonal Snow Water Equivalent in High-Mountain Asia from an 18-Year Landsat-MODIS Era Snow Reanalysis Dataset.” The Cryosphere 15: 5261–80. https://doi.org/10.5194/tc-2021-139.\n\n\nMiles, Evan, Michael McCarthy, Amaury Dehecq, Marin Kneib, Stefan Fugger, and Francesca Pellicciotti. 2021. “Health and Sustainability of Glaciers in High Mountain Asia.” Nature Communications 12 (2868): 10. https://doi.org/https://doi.org/10.1038/s41467-021-23073-4.\n\n\nMillan, Romain, Jérémie Mouginot, Antoine Rabatel, and Mathieu Morlighem. 2022. “Ice Velocity and Thickness of the World’s Glaciers.” Nature Geoscience 15 (2): 124–29. https://doi.org/10.1038/s41561-021-00885-z.\n\n\nRGI Consortium. 2017. “Randolph Glacier Inventory – a Dataset of Global Glacier Outlines: Version 6.0: Technical Report.” Global Land Ice Measurements from Space, Colorado, USA. Digital Media. https://doi.org/https://doi.org/10.7265/N5-RGI-60."
  }
]