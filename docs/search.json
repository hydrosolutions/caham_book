[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modeling of Hydrological Systems in Semi-Arid Central Asia",
    "section": "",
    "text": "About This Book\nThis handbook is designed to serve as a comprehensive guide for early- or in-career water professionals engaged in the hydrological modeling of river basins within Central Asia. It addresses the intricacies of the natural and anthropogenically influenced hydrological systems characteristic of this region. The text underscores the dual nature of the challenges faced by these professionals. On the one hand, considerable potential exists for modernization within the water sector, a field that has historically seen constrained investment across the Central Asian nations. On the other hand, these professionals must navigate the complexities introduced by continuous demographic expansion and the multifaceted impacts of climate change. Concurrently, the handbook acknowledges the enduring significance of hydrological work in fostering the region’s prosperity and development, a tradition that spans several centuries, if not millennia. This dual focus highlights the immediate challenges and situates the work within a broader historical and socio-economic context, emphasizing its critical role in the region’s sustained growth and stability.\nThis freely accessible online textbook is designed to be translated into any language through the utilization of browser-based translation functionalities. It is a valuable inspirational resource for students pursuing studies in hydrology or integrated water resources management. Additionally, the methodologies delineated within this text are intended for academic utilization, enabling educators to incorporate them into the existing curricula of higher education institutions at a local level. This feature enhances the textbook’s utility by facilitating its integration into diverse educational settings, thereby broadening the scope of its applicability and impact in the field of water resource management education.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#whats-new-in-the-2024.q1-edition",
    "href": "index.html#whats-new-in-the-2024.q1-edition",
    "title": "Modeling of Hydrological Systems in Semi-Arid Central Asia",
    "section": "What’s New in the 2024.Q1 Edition?",
    "text": "What’s New in the 2024.Q1 Edition?\nChanges include:\n\nIncreasing focus on providing YouTube learning materials in English and Russian languages to complement the existing text-based learning materials. The material is available on a dedicated YouTube Channel - subscribe here.\nWork started on a step-by-step guide about quantifying climate change impacts with examples. See Chapter 12  Quantification of Climate Change Impacts for more information.\nOverall correction of errors and typos throughout the text. Selected updated Figures with a focus on increased consistency.\n\nWe are aware that many more construction sites exist throughout the text. Furthermore, with the continuous advancements in watershed modeling, any future updates to the book should also reflect references to new technologies and approaches.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Modeling of Hydrological Systems in Semi-Arid Central Asia",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThe development of this book was supported by the Global Water Programme and the Blue Peace Central Asia Project by the Swiss Agency for Development and Cooperation (SDC).\n\n\n\n\n\n\n\n\nMany people indirectly contributed to the book through numerous discussions regarding its content. We especially thank Mr. Andrey Yakovlev for this critical feedback on the various book sections.\nThe book is dedicated to colleagues at the Central Asian National Meteorological and Hydrological Agencies whose tireless work in collecting and analyzing hydro-meteorological data in Central Asia has helped to significantly improve our understanding of the complex runoff generation processes at work in the region.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Modeling of Hydrological Systems in Semi-Arid Central Asia",
    "section": "License",
    "text": "License\n\n\n\nDOI\n\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.",
    "crumbs": [
      "About This Book"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface to the 2024.Q1 Edition",
    "section": "",
    "text": "This is a book and study guide about the hydrology of semi-arid Central Asia and applied hydrological modeling in the region. It is geared towards students and young professionals in Central Asia who are interested in learning modern hydrological modeling approaches. The book teaches by example and focuses on two example catchments in the Syr Darya and Amu Darya river basins as case studies. The methods demonstrated here can be applied elsewhere.\n\n\n\n\n\n\nThe What and Why of Hydrological Modeling\n\n\n\nHydrological models come in different incarnations and flavors. Hydrological water balance models, in this text also sometimes referred to as rainfall-runoff models, were developed to help us gain an understanding of the partitioning of available water into different fluxes and storage compartments over time in the natural system under consideration. This natural system normally consists of different interlinked compartments, including surface water and the unsaturated (soil moisture) and unsaturated (groundwater) zones. These models simulate the flow of water through these compartments. They are most often used in the context of water management and planning applications, i.e., for basin planning under climate change and population growth scenarios to allocate water between different uses and users on the one hand. On the other, these models are also used operationally to close supply-demand gaps in real-time management tasks and for short-term forecasting.\nWhere large amounts of data are available, empirical models can be implemented. These models strictly speaking do rely on the explicit simulation of the water balance of individual compartments. Rather, they learn patterns from measured time series of discharge and other relevant variables, such as temperature, precipitation, snow cover and then use these pattern between the time-ordered data for forecasting variable of interest (i.e., discharge, water levels, etc.).\n\n\nIn Part I of the book, key hydro-climatological characteristics of the region are presented. This Section draws inspiration from Victor Shults’ “Rivers of Middle Asia” (Shults 1965). Through the collection of a large number of in-situ hydrological data from all over the region and in combination with a plethora of newly available data, a grand modern regional perspective on Central Asian hydrology becomes possible in the tradition of Shults.\nTwo important basins are highlighted as in-depth case studies, i.e. the Gunt River Basins in the Amu Darya catchment and the Chirchik River basin in the Syr Darya. The analyses of these catchments draws on available data from the Central Asian Hydrometeorological Services and on global and entirely public hydro-climatological as well as land cover datasets. The goal of these introductory chapters is to familiarize the student with the first steps prior to any hydrological modeling, i.e. to obtain a robust understand of the system of interest through a thorough hydro-climatological characterization of the study area.\nPart II is a rather large section of the book which is then devoted to data where different open data sources are presented. Retrieval and data preparation in the context of hydrological modeling are discussed. These data include data on topography, land cover, climate reanalysis data and parameters on biophysical climate and climate projections data. The preparation of these data often requires significant work. Hence, a focus lies on demonstrating workflows to facilitate the handling and preparation of these type of data for hydrological modeling.\nIn Part III, three different modeling approach are presented and discussed. First, long-term water balance modeling using the Budyko framework is presented and discussed in depth with an application to a large dataset from the region. This approach can yield powerful insights for example on regional hydrological changes due to climatic changes and where the detailed hydrological-hydraulic modeling of a large number of rivers is impracticable. Second, detailed hydrological-hydraulic modeling of individual river basins is presented. These types of models are normally developed for tradeoff analysis between different water uses and users in a basin, i.e. in the planning context and also under different climate scenarios. They can also be run in operational mode to respond to real-time management challenges. Finally, the last modeling chapter introduces modeling through predictive inference where empirical data-driven models are setup for forecasting discharge at particular locations in a basin. These models rely on large amounts of measured discharge data and hence, their application is limited to places where such data are available.\nPart III also includes two Chapters on hydrological model applications where real-world model deployment is presented and discussed. A special emphasis here is aspects of operation and maintenance of deployed models in local agencies and options for this, also in relation to staff education and learning.\nThe book / study guide is accompanied by a Study Exercise Pack that encompasses data from 7 Central Asian catchments which can be used by students for learning and applying skills acquired to real-world examples in the region. The Exercise Pack can accessed and downloaded here. Furthermore, a dedicated R Package has been developed which implements many of the data analyses and processing steps shown in this book (see also Section for more information).\nWith everything that is presented, the focus is on the use of open source and free software. For data preparation and analysis as well as for water balance and empirical modeling, R and RStudio are utilized (R Core Team 2022). For the processing of geographic data, workflows in QGIS are demonstrated (QGIS Development Team 2021). For hydrological-hydraulic modeling, the free RS MINERVE is utilized which is a environment for the modeling of free surface runoff flow formation and propagation (Foehn et al. 2020; Garcia Hernandez et al. 2020). The reader is expected to have a basic understanding of R and QGIS and how to use these software for data analysis and processing.\nThe outlook having to learn hydrology together with quantitative geospatial analysis and programming may sound overwhelming at the beginning. Really the best way is just to dive into the book and learn through the many examples provided. All code with which the analysis and modeling is carried out is provided and can be thus adapted to any other local context or relevant task. So, this handbook on applied hydrological modeling is hopefully inviting students to learn through experimentation and not to get scared.\nBefore we get going, a small note on how to translate this text in any other language of interest. Should the reader struggle with the English language, there is a very easy way to translate this book into any of the local languages spoken in Central Asia, including Russian. The picture below shows a screenshot from the online book translated into Russian language via Google Chrome’s translation service. The screenshot shows how to activiate the translation panel (1). The translated book text then appears (2). Alternatively right-click anywhere on the page. Then, click Translate to [Language].\n\n\n\n\n\nFoehn, A., J. Garcia Hernandez, B. Roquier, J. Fluixa-Sanmartin, T. Brauchli, J. Paredes Arquiola, and G. De Cesare. 2020. “RS MINERVE - User Manual, V2.15.” ISSN 2673-2653. Switzerland: Ed. CREALP.\n\n\nGarcia Hernandez, J., A. Foehn, J. Fluixa-Sanmartin, B. Roquier, T. Brauchli, J. Paredes Arquiola, and De Cesare G. 2020. “RS MINERVE - Technical Manual, V2.25.” ISSN 2673-2661. Switzerland: Ed. CREALP.\n\n\nQGIS Development Team. 2021. QGIS Geographic Information System. QGIS Association.\n\n\nR Core Team. 2022. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nShults, Victor. 1965. Rivers of Middle Asia. 2nd Edition. Gidrometeoizdat, Leningrad.",
    "crumbs": [
      "Preface to the 2024.Q1 Edition"
    ]
  },
  {
    "objectID": "study_guide_materials.html",
    "href": "study_guide_materials.html",
    "title": "Study Guide and Materials",
    "section": "",
    "text": "Study Guide APPLIED MODELING\nAs part of this Applied Modeling Track, students are guided through implementing their own conceptual hydrological rainfall-runoff model of one of the Central Asian sample catchments they can choose from the Case Studies Pack.\nStudents are required to work through the chapters, including the occasional tasks that serve to deepen reflection on the course material and to do their daily homework assignments. As the final exam, the homework results are presented in a final student conference, for which the students have to submit a conference abstract before the conference.\nThis chapter explains how to use this course book.\nDifferent callout blocks appear throughout the text. These include Exercise, Tasks, and Take Home Messages. Caution and Warning callouts highlight possibly problematic issues.",
    "crumbs": [
      "Study Guide and Materials"
    ]
  },
  {
    "objectID": "study_guide_materials.html#sec-study-guide",
    "href": "study_guide_materials.html#sec-study-guide",
    "title": "Study Guide and Materials",
    "section": "",
    "text": "EXERCISE\n\n\n\nExercise boxes are highlighted in blue color. Hints and a link to the solution are provided with the exercise description. Exercises should be completed wherever they appear in the text before starting the next course chapter.",
    "crumbs": [
      "Study Guide and Materials"
    ]
  },
  {
    "objectID": "study_guide_materials.html#task",
    "href": "study_guide_materials.html#task",
    "title": "Study Guide and Materials",
    "section": "TASK",
    "text": "TASK",
    "crumbs": [
      "Study Guide and Materials"
    ]
  },
  {
    "objectID": "study_guide_materials.html#sec-materials",
    "href": "study_guide_materials.html#sec-materials",
    "title": "Study Guide and Materials",
    "section": "Materials",
    "text": "Materials\nIn the highly intensive hydrological modeling course at DKU, students must pass four graded exercises and complete the preparatory homework to be admitted to the final presentation. The following section describes the daily course content, the homework, and the graded exercises with links to the relevant supporting chapters in the course book. The descriptions of the graded exercises are highlighted with exercise boxes.\nDay 1: Introduction & Installation of Software\nRead Chapter 1: A Short History of Water in Central Asia and Chapter 2: Hydrological Systems in Semi-Arid Central Asia in the course book. Then, ensure the required software for this course is installed on your computer. Section Open-source resources of the Appendix includes installation instructions and the online learning material that can get you started with the software. Below is a quick summary:\n\n\nQGIS\n\nR\n\nRStudio\n\nRS Minerve\n\nIf you have not used the software above before, we recommend the following resources to get you started (remember, more detailed instructions for most tasks are available in the Appendix):\n\n\nQGIS training manual\n\n\nModern Dive for getting started with R and RStudio\n\nRS Minerve User Manual\n\nInevitably, you will also perform a lot of geocomputations with R in the future. After all, a GIS system like QGIS is nothing more than a nicely packed bunch of geocomputation algorithms and a window for visualizing geospatial assets. Well, rest assured, all of this can be done inside R. It is recommended, therefore, that you also consult the following excellent online resource Geocomputation with R.\n\n\n\n\n\n\nHOMEWORK\n\n\n\nDay 1 involves a lot of preparatory homework:\n- Reading the introductory chapters linked above and\n- Download and install the required software linked above.\nThe homework is not graded, but completion is required to work through the course.\n\n\nDay 2: Hydrological Modeling and Processes\nDay 2 involves a continued introduction to the hydrological modeling process, a deepening of the understanding of what hydrological models are used for, and a first part on hydrological processes (the partitioning of rainfall and water transfer through the hydrological compartments).\n\n\n\n\n\n\nHOMEWORK\n\n\n\n\nRole-play on model uses. Read the role-play exercise. You will be assigned a role. With your study colleagues, discuss the questions and take notes (about 15 min). One person per group will briefly (1 min) present the answers to the questions.\nIn preparation for the following lecture and the graded exercise, read the chapter on the case studies of Central Asian river basins and [Hydraulic-Hydrological Modeling] (#sec-hydraulic-hydrological-modeling).\n\nThe homework is not graded but supports reflection on the use of hydrological models and how to judge the quality of hydrological models on day 3.\n\n\nDay 3: Hydrological Modeling Concepts and Catchment Characterization\nFamiliarize yourself with the Geospatial Data. Do the catchment characterization of the basin you selected to work on by filling out the table below. If you have downloaded the entire folder on your local drive, you already have all the data for the analysis.\n\n\nTable 1: As an example, key relevant basin statistics for the Gunt River basin are shown with individual data sources indicated. Using the data in the data pack, you should characterize your case study basin similarly.\n\n\n\n\n\n\n\nATTRIBUTE\nVALUE\n\n\n\n\nGeography (“NASA Shuttle Radar Topography Mission (SRTM)” 2013)\n\n\n\n\nBasin Area \\(A\\)\n\n13’693 km2\n\n\n\nMinimum Elevation \\(h_{min}\\)\n\n2’068 masl\n\n\nMaximum Elevation \\(h_{max}\\)\n\n6’652 masl\n\n\nMean Elevation \\(h_{mean}\\)\n\n4’267 masl\n\n\nHydrology [Source: Tajik Hydromet Service]\n\n\n\nNorm hydrological year discharge \\(Q_{norm}\\)\n\n103.8 m3/s\n\n\nNorm cold season discharge (Oct. - Mar., Q4/Q1)\n19.8 m3/s\n\n\nNorm warm season discharge (Apr. - Sept., Q2/Q3)\n84.2 m3/s\n\n\nAnnual norm discharge volume\n3.28 km3\n\n\n\nAnnual norm specific discharge\n239 mm\n\n\nClimate\n\n\n\nMean basin temperature \\(T\\) (Karger et al. 2017)\n\n-5.96 deg. Celsius\n\n\nMean basin precipitation \\(P\\) (Beck et al. 2020)\n\n351 mm\n\n\nPotential Evaporation \\(E_{pot}\\) (Trabucco and Zomer 2019)\n\n929 mm\n\n\nAridity Index \\(\\phi = E_{pot} / P\\)\n\n2.7\n\n\nAridity Index (Trabucco and Zomer 2019)\n\n3.6\n\n\nLand Cover (Buchhorn et al. 2019)\n\n\n\nShrubland\n8 km2\n\n\n\nHerbaceous Vegetation\n4’241 km2\n\n\n\nCrop Land\n0.5 km2\n\n\n\nBuilt up\n4 km2\n\n\n\nBare / Sparse Vegetation\n8’410 km2\n\n\n\nSnow and Ice\n969 km2\n\n\n\nPermanent Water Bodies\n80 km2\n\n\n\nLand Ice\n\n\n\nTotal glacier area (RGI Consortium 2017)\n\n875 km2\n\n\n\nTotal glacier volume (calculated with (Erasov 1968))\n699 km3\n\n\n\n\n\n\n\nThere are four tutorial videos explaining how you can fill the above table. They are made available via the dedicated CAHAM YouTube Channel{target = “_blank”} accompanying this online textbook.\nThe first video explains how to process and extract geospatial raster and vector data in detail.\n\n\n\n\n\n\n\n\n\nThe second tutorial video shows how to extract relevant climate data for the case study basin.\n\n\n\n\n\n\n\n\n\nThe third video covers extracting information from land cover for the basin under consideration.\n\n\n\n\n\n\n\n\n\nFinally, the last video shows you how to extract the relevant glacier information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGRADED EXERCISE 1: Catchment characterization\n\n\n\nFollowing the video tutorial, fill in the table above with the characteristic numbers of your catchment together with your colleague. Compare your numbers to the ones of the Gunt catchment (table above). Note the submission deadline for Exercise 1 on Moodle.\n\n\nDay 4: Discharge and Climate Data\nYet more data preparation is required before you can start modeling. Hence, basin discharge and climate-forcing data are reviewed.\n\n\n\n\n\n\nGRADED EXERCISE 2: Discharge Characterization\n\n\n\nRead the chapters on discharge station data and climate data and, together with your colleague, perform a discharge characterization using the dedicated scripts in your case study’s ./CODE/discharge_characterization/ folder. Note the submission deadline for Exercise 2 on Moodle!\n\n\nDay 5: Discussion of Types of Hydrological Models\nHydrological models, in general, are discussed. Consult the introductory Section of Part III: Hydrological Modeling and Applications. All three types of modeling approaches will be presented but with a focus on hydraulic-hydrological rainfall-runoff modeling.\n\n\n\n\n\n\nHOMEWORK: RS Minerve tutorial\n\n\n\n\nRead the modeling chapter\n\nGo through the RS Minerve tutorial (TODO LINK)\n\nThis homework is not graded, but basic knowledge of RS Minerve is required for the second part of the course.\n\n\nDay 6 & 7: Model Calibration and Validation\nRead the chapter on Model calibration and validation and go through the example of the Nauvalisoy catchment, which illustrates the iterative model refinement process. As the ultimate goal, students will implement a hydrological model of their study catchment and calibrate it.\n\n\n\n\n\n\nGRADED EXERCISE 3: Model implementation and calibration in RS Minerve\n\n\n\n\nRead the modeling chapter\n\nImplementing a hydrological model of your study basin in RS Minerve.\n\nNote the submission deadline for Exercise 3 on Moodle!\n\n\n\n\n\n\n\n\nGRADED EXERCISE 4: Abstract submission for student conference\n\n\n\n\nCarefully read the abstract submission guidelines and write an abstract for your model.\n\nNote the submission deadline for Exercise 4 on Moodle!\n\n\nDay 8: Student Conference & Course Wrap Up\nThe last day of the course is organized as a student conference where students present their modeling work on their respective case study catchments. The groups need to prepare a presentation of 12 minutes duration. Each presentation will be followed by a 3-minute Q&A session. After all the groups have presented their work, impressions, and feedback will be shared by the teachers, followed by a group discussion.\nOnly students who pass the GRADED EXERCISES will be admitted to the student conference, which consists of the final exam.\nAt the end, students are invited to provide feedback about their impression of the course. A key question will be how the course can be further improved to reach future students more effectively.\n\n\n\n\n\n\nFINAL EXAM: Model presentation\n\n\n\n\nPresent an overview of your catchment, discharge characterization, and your model implementation and results at the student’s conference.\n\nNote that the presentations must be uploaded to the moodle before the start of the conference.",
    "crumbs": [
      "Study Guide and Materials"
    ]
  },
  {
    "objectID": "study_guide_materials.html#references",
    "href": "study_guide_materials.html#references",
    "title": "Study Guide and Materials",
    "section": "References",
    "text": "References\n\n\n\n\nBeck, Hylke E., Eric F. Wood, Tim R. McVicar, Mauricio Zambrano-Bigiarini, Camila Alvarez-Garreton, Oscar M. Baez-Villanueva, Justin Sheffield, and Dirk N. Karger. 2020. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1.\n\n\nBuchhorn, M., B. Smets, L. Bertels, B. De Roo, M. Lesiv, N. E. Tsendbazar, M. Herold, and S. Fritz. 2019. “Copernicus Global Land Service: Land Cover 100m: Collection 3: Epoch 2019: Globe.”\n\n\nErasov, N. V. 1968. “Method for Determining of Volume of Mountain Glaciers.” MGI, no. 14: 307–8.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\n“NASA Shuttle Radar Topography Mission (SRTM).” 2013. NASA. https://earthdata.nasa.gov/learn/articles/nasa-shuttle-radar-topography-mission-srtm-version-3-0-global-1-arc-second-data-released-over-asia-and-australia.\n\n\nRGI Consortium. 2017. “Randolph Glacier Inventory – a Dataset of Global Glacier Outlines: Version 6.0: Technical Report.” Global Land Ice Measurements from Space, Colorado, USA. Digital Media. https://doi.org/https://doi.org/10.7265/N5-RGI-60.\n\n\nTrabucco, Antonio, and Robert Zomer. 2019. “Global Aridity Index and Potential Evapotranspiration (ET0) Climate Database v2,” January. https://doi.org/10.6084/m9.figshare.7504448.v3.",
    "crumbs": [
      "Study Guide and Materials"
    ]
  },
  {
    "objectID": "hydrology_of_central_asia.html",
    "href": "hydrology_of_central_asia.html",
    "title": "Part I: Hydrology of Semi-Arid Central Asia",
    "section": "",
    "text": "The arid plains of Central Asia have been teeming with life thanks to the rivers cutting through them and bringing water to the parched plains when its most needed, in the hot and dry summer months. Why is it that the natural, undisturbed rivers swelled exactly during that time? How do we explain this phenomenon that has enabled for the Silk Road and unique cultural centers to emerge over the course of history and have brought riches to the communities there?\nThis Chapter describes the key elements of the regional hydrology of semi-arid Central Asia that will help to understand this coincidence of nature. Leaving the large scale perspective, 2 example river basins are discussed in greater detail with a special focus on the runoff generation mechanisms there.\nApart from looking at past data for explanation, a forward view will also be taken to discuss the recent and anticipated future hydrological changes. It will be highlighted how our current understanding of these changes will impact societies and the environment alike.",
    "crumbs": [
      "Part I: Hydrology of Semi-Arid Central Asia"
    ]
  },
  {
    "objectID": "hydrological_systems.html",
    "href": "hydrological_systems.html",
    "title": "1  Hydrological Systems in Semi-Arid Central Asia",
    "section": "",
    "text": "1.1 Regional Characteristics\nKey statistics of the Central Asia region are shown in Table 1.1. Central Asia is spreading over approximately 4.6 million km2, including the territories of Kazakhstan, Kyrgyzstan, Tajikistan, Turkmenistan, Uzbekistan and the Afghan Islamic Emirate (Central Intelligence Agency, n.d.). Populations are very unevenly distributed as is shown in Figure 1.2. One of the key population hotspot is the densely populated Fergana Valley that is shared between Uzbekistan, Kyrgyzstan and Tajikistan.\nThe average per capita annually renewable water availability is estimated to be around 1’500 m3. Thus, according to the United Nations definition, the region as a whole is water stressed. However, such regional view blends over significant differences between the upstream and downstream states and a more fine-grained per country water availability figures show the very uneven distribution of annually renewable water resources in a much clearer way (Figure 1.3).\nIn the vast plains that cover around 70 % of the total territory, there are only very few rivers which have scarcely any tributaries from the point they leave the mountain areas all the way to their mouth. Abundant solar radiation, high temperatures, small amounts of precipitation, a lack of humidity, unstable snow cover, slight slopes, geological structures etc., hinder the formation of surface flows in the plains of Central Asia, despite their big importance for the local agricultural production there. Only the largest rivers, such as Syr Darya, Amu Darya, and Ili are able to survive hundreds of kilometers of deserts and reach the most important landlocked reservoirs – the Aral Sea and Lake Balkhash (Shults 1965). The Aral Sea, of course, is no longer adequatly fed by the annual snow melt floods of the Syr Darya and Amu Darya and thus has gradually vanished over the second half of the 20th century (for more on this, see Section 1.4 below).\nIn fact, the inhomogeneity of the relief structure causes Central Asia to be the territory of immense contrasts Figure 1.4. Here, extreme aridity in the hot deserts of the plains and, only 100 km away, abundant humidity and snowfields in the mountains where precipitation levels can range between 1’000 mm/a up to 2’000 mm/a.\nThe uneven distribution of water bodies is striking (Figure 1.5). The mountains of Central Asia are riddled with an extremely branched river network consisting of more than ten thousand watercourses. These areas are called the zones of runoff formation.\nIn the flat foothill areas surrounding the mountain ranges, another branching river network is found which consists of irrigation channels, which do not contribute to the runoff of the core rivers, but rather divert the water from the river network and diffuse it in the irrigated oases where much of it gets evapotranspirated. Usually, these areas are referred to as zones of comples water distribution and use.\nAll Central Asian river basins are endorheic with no water draining out of the region but only evaporating back to the atmosphere. This emphasizes the importance of moisture transfer as an important mechanism in the region region since the formation of substantial watercourses in the mountains is followed by their complete dissipation in the plains, including in the irrigated oases and the terminal lakes, i.e. the Aral Sea and Lake Balkash (Shults 1965).",
    "crumbs": [
      "Part I: Hydrology of Semi-Arid Central Asia",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hydrological Systems in Semi-Arid Central Asia</span>"
    ]
  },
  {
    "objectID": "hydrological_systems.html#sec-regional-characteristics",
    "href": "hydrological_systems.html#sec-regional-characteristics",
    "title": "1  Hydrological Systems in Semi-Arid Central Asia",
    "section": "",
    "text": "Figure 1.2: Population density in the region of interest. Red colors show a high density indicated in the main population centers whereas the beige colors show very low densities over most of the territory. Data source: (International Earth Science Information Network - CIESIN - Columbia University 2018).\n\n\n\n\nTable 1.1: Overview statistics of the Central Asian Republics and the Islamic Emirat of Afghanistan. Source: World Population Review, World Bank and Wikipedia.\n\n\n\n\n\n\n\n\n\n\n\nCountry\nArea [km2]\nPopulation [Mio., 2018]\nPop. Density [Einw./km2]\nGDP [Mio. USD, 2017]\nPer Capita GDP [USD]\n\n\n\nKazakhstan\n2’724’900\n18.3\n6.3\n160’839\n8’780\n\n\nKyrgyzstan\n199’950\n6.3\n29.7\n7’061\n1’120\n\n\nTajikistan\n142’550\n9.1\n60.4\n7’146\n785\n\n\nTurkmenistan\n488’100\n5.9\n11.1\n37’926\n6’482\n\n\nUzbekistan\n448’978\n33.9\n69.1\n47’883\n1’412\n\n\nAfghanistan\n652’864\n32.9\n50.4\n21’657\n658\n\n\nTotal\n4’657’342\n106.4\n22.8\n282’512\n2’656\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.3: Per country water availability figures are shown. The striking difference between the very water rich upstream states and the downstream states that suffer from significant water scarcity is clearly visible.\n\n\n\n\n\n\n\n\n\nFigure 1.4: The river basins together with the main mountain ranges are highlighted (“NASA Shuttle Radar Topography Mission (SRTM)” 2013).\n\n\n\n\n\n\n\n\n\nFigure 1.5: The river network of the large rivers is shown. The basin shapes are adapted from (Lehner and G. 2013) and the rivers are geospatial assets from (GRDC, Koblenz, Germany: Federal Institute of Hydrology (BfG). 2020)",
    "crumbs": [
      "Part I: Hydrology of Semi-Arid Central Asia",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hydrological Systems in Semi-Arid Central Asia</span>"
    ]
  },
  {
    "objectID": "hydrological_systems.html#sec-hydrological-systems-climate",
    "href": "hydrological_systems.html#sec-hydrological-systems-climate",
    "title": "1  Hydrological Systems in Semi-Arid Central Asia",
    "section": "\n1.2 Climate",
    "text": "1.2 Climate\n\n1.2.1 Sources of Moisture\nIn the boreal summer, tropical air masses form in the plains of Central Asia. At that time, even cold air masses coming from the north heat quickly. There is no possibility for temperature differences between the lower and the middle troposphere to occur which explains the horizontal uniformity of high temperatures during summer (Shults 1965).\nContrary to this, the temperature differences in the region are highest in January during the peak of the boreal winter. As the territory of Central Asia is unprotected from the north, it is under the influence of dry, exceptionally cold air masses originating from the Arctic region and Siberia. These air masses can cause sharp frosts. The further the Siberian or Arctic air masses penetrate towards the west and the south, the more their temperature increase. This explains the big difference in air temperatures during winter between the north and the south of Central Asia. Cold air intrusions are often accompanied by the influx of warm air from the tropics. The cold waves taking turns with hot air masses cause unsteady frost in the plains and, together with generally low precipitation values, do not allow for the formation of a significant snow pack there (Shults 1965).\nAs is discussed further below, the winter snow cover in the high mountain ranges plays an essential role in runoff formation in the spring and summer months and is thus of key relevant to irrigation agriculture in the downstream and for hydropower production (see Section 1.3 below). Understanding the atmospheric mechanisms resulting in favorable conditions for winter precipitation is thus essential.\n(Gerlitz et al. 2018) discusses how the position of the westerly jet stream is connected with the frontal trajectories and the westerly disturbances which are the main moisture sources if the region. The relative position of these planetary wave tracks and their associated westerly flows to the orographic mountain barriers plays thus a key role. The main precipitation events migrate over the winter season from south to north. The southern parts of central Asia, particularly the windward slopes of the Karakorum and Hindu Kush mountain ranges, receive high amounts of winter precipitation (December-January-February), which reaches up to 60 % of the total annual precipitation. During spring the zone of maximum precipitation migrates northward, reaches the Pamir plateau in March, and continues to Tien Shan in April/May. The interaction of tropical air masses from the Arabian Golf with westerly flow in Central Asia is another important moisture source for the region.\nUsing data of the ERA-Interim reanalysis, (Gerlitz et al. 2018) classifies 8 weather types (WT) based on typical regional pressure field patterns over a domain covering 20- 60N and 50- 90E. Like this, large-scale features of winter circulation patterns in Central Asia can be captured. WT are analyzed with regard to the spatial anomalies of temperature and precipitation. Figure 1.6 and Figure 1.7 show the results. In these Figures, the individual plates WT 1 - 8 are labeled according to the main circulation feature over Central Asia, i.e. a Rossby ridge (R) or trough (T). Acronyms in the figures indicate the major features (and their centers of action) for each WT: anti-cyclonic anomaly (AC), cyclonic anomaly (C), central Asia ([CA]), Kazakhstan ([KAZ]), Mongolia ([MON]), and Indian Ocean and Indian subcontinent ([Indic]).\n\n\n\n\n\nFigure 1.6: Composite maps illustrating the averaged anomalies of ERA-Interim/Land 6-hourly temperature for each weather type (WT 1 - WT 8). Values are depicted in standard deviations for each grid cell, respectively. Arrows indicate anomalies of the 500-hPa ERA-Interim wind field (Gerlitz et al. (2018)).\n\n\n\n\n\n\n\nFigure 1.7: Composite maps illustrating the averaged anomalies of ERA-Interim/Land 6-hourly precipitation sums for each weather type. Values are depicted relative to the seasonal mean 6-hourly precipitation sum ((100)-1) for each grid cell, respectively. Arrows indicate anomalies of vertically integrated moisture fluxes (Gerlitz et al. (2018)).\n\n\nGenerally, it can be observed that configurations that are associated with a Rossby trough over Central Asia lead to an intensification of westerly moisture fluxes (WT 3, WT 7, and WT 8). If there is, however, a Rossby ridge type configuration as shown in WT 1, WT 2, and WT 4 panels of Figure 1.6 and Figure 1.7, moisture fluxes are northward-shifted and precipitation suppressed (Gerlitz et al. 2018).\n\n1.2.2 Distribution of Precipitation\nPrecipitation is extremely unevenly distributed in the region. 20% of the plain area receives less than 100 mm, while 91% of the territory receives less than 300 mm of precipitation a year with an overall average of 173 mm. The mountains are thus an important climatological and hydrological factor, since they are the places where the water condensates and where the rivers and groundwater originate. Although the range of precipitation levels is wide (60 mm - 2’500 mm), the mountains receive on average more than three times more precipitation than the plains, and the low temperatures favor its accumulation in the solid state (Shults 1965). Figure 1.8 shows the precipitation climatology over the Central Asia domain\n\n\n\n\n\nFigure 1.8: The annual precipitation climatology dervied from the CHELSA V21 high-resolution data set is shown (Karger et al. 2021).\n\n\n\n\n\n\n\nFigure 1.9: The cold season precipitation climatology dervied from the CHELSA V21 high-resolution data set is shown (Karger et al. 2021). With notable exceptions such as along the southern Gissar Alay range, precipitation occurs mostly on the flanks of the large mountain ranges with western to north-western exposure.\n\n\n\n\n\n\n\nFigure 1.10: The warm season precipitation climatology dervied from the CHELSA V21 high-resolution data set is shown (Karger et al. 2021). Compared to the cold season precipitation, the warm season precipitation is mostly intra-alpine and not of frontal characteristics.\n\n\nThe influence of the relief is notable also when speaking of precipitation distribution during the year ( Figure 1.9 and Figure 1.10). The high ground areas in Central Asia are witnessing an almost even distribution of precipitation on monthly basis, whereas at the same time, in the inner parts of high mountain ridges there is more precipitation in summer. Such a distribution of precipitation in the inner parts of mountain ridges is a consequence of high condensation levels in summer due to intense evaporation taking place in snow melting areas or, less often, on water surfaces.\nA typical example showing the influence of the local water vapor emission on annual distribution of precipitation could be the Issyk Kul Lake Basin. There the percentage of precipitation received during summer and the second half of spring, so from May to August, is sometimes reaching even 80% of the total annual precipitation amount, all thanks to the evaporation of the water from the lake and the emergence of thermal convection and subsequent moisture recycling.\nThe areas that are characterized by a predominant precipitation during summertime are the Central Tian Shan and Eastern Pamir, where the difference between the summer and the rest of the year is so big that during summer 60% of all annual precipitation is received.\nThe predominance of the precipitation during summer in case of mountains with steep slopes (15° - 30°) causes fast and abundant snowmelt runoff which is directed to the lower areas and then turns into a river network. Thanks to a large amount of precipitation, relatively low evaporation levels and steep slopes, all rivers of Central Asia, including the largest ones such as Amu Daria, Syr Daria, Ili or Zeravshan, arise in the mountains. Arising in the high ground area, these rivers are mainly fed by snow, glaciers and snow patches melting, as well as by groundwater that, again, were all formed by the same sources.\n\n1.2.3 A Changing Climate\nWith a warming climate and the associated increase of evapotranspiration in the downstream plains and the loss of glacier storage in the mountainous areas, a solid understanding of the snow pack formation becomes even more pertinent. This is for example evidenced by the fact that a discernible reduction of snow cover can be observed in some key basins in the lower to medium elevations between 1980 - 2010. Figure 1.11 shows this in a exemplary fashion for the Chirchik River basin (see also Chapter Chirchik River Basin for more information).\n\n\n\n\n\nFigure 1.11: Changes in fractional snow cover in the in the Western Tien Shan mountains, including the Chirchik River basin. Shown are percentage changes over the period 1980 - 2011. Changes in the snow cover fraction was computed at the pixel level using the high-resolution CHELSA V21 daily precipitation and temperature Karger et al. (2021) with a liquid-solid temperature threshold value of 1 deg. C.\n\n\nThrough the reduction of precipitation stored as snow in the mountain regions, there is a consensus that the discharge regimes in the region will be impacted. These impacts can be quantified with hydrological models, as we will show in this Coursebook. An example of such expected changes is provided in Figure 1.12. More information on the modeling approach is provided in the Chapter Hydrological-hydraulic Modeling whereas data and modeling requirements are discussed in Part II Data.\n\n\n\n\n\nFigure 1.12: Using the CMIP6 ssp585 climate scenario, the computed climate impacts on the discharge regime of Atbashy River (left tributary to the Naryn and located in Kyrgystan) is shown. The simulation results suggest that the peak discharge will shift towards spring and that peak flows will decrease in the basin for the different target periods in the future.",
    "crumbs": [
      "Part I: Hydrology of Semi-Arid Central Asia",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hydrological Systems in Semi-Arid Central Asia</span>"
    ]
  },
  {
    "objectID": "hydrological_systems.html#sec-zone-of-runoff-formation",
    "href": "hydrological_systems.html#sec-zone-of-runoff-formation",
    "title": "1  Hydrological Systems in Semi-Arid Central Asia",
    "section": "\n1.3 Zone of Runoff Formation",
    "text": "1.3 Zone of Runoff Formation\nAs is well known, the aridity index \\(\\phi\\) plays a key role in the partitioning of precipitation \\(P\\) into evapotranspiration \\(E\\) and runoff \\(Q\\) (see also Chapter 9 in Part III of this book for more information on this). It is defined as the ratio of mean potential evapotranspiration \\(E_{0}\\) to mean precipitation \\(P\\). For the analysis in semi-arid Central Asia, we use Aridity Index data from the CHELSA V21 High-Resolution Climatology data set Karger et al. (2021). Different aridity index classifications exist, such as the one from (McVicar, Roderick, Donohue, Li, Niel, et al. 2012), which is used here. Figure 1.13 highlights the zone of runoff formation where \\(\\phi &lt; 0.76\\) for the humid regions (blue-colored high mountain domains) and \\(\\phi \\geq 0.76\\) and \\(\\phi \\leq 1.3\\) for the transition regions (green-colored).\n\n\n\n\n\nFigure 1.13: The zone of runoff formation in High-Mountain Central Asia, as defined via the aridity index, is shown.\n\n\nThe Figure shows the sub-humid (green) and humid (blue) mountainous regions that contribute to the generation of surface runoff in the region. We refer to this region as the zone of runoff formation.\nDue to the presence of the vertical thermal gradient, the start of the positive air temperature season and, consequently, the start of the ice and snow melting season does not take place uniformly in the region at the same time of the year. Rather, the snow-melting process is of a protracted nature and the higher the mountains are in a particular catchment, the later the snow-melt floods take place on rivers that are emerging there.\nThe melting process starts last in the permanent snow and glacier regions. Because of this, the rivers, which are fed by snow melt in the upper parts of the catchment area, are of great importance for the irrigation of crops, since they are characterized by the most significant water runoff during July and August, at which point the irrigated plains experience severe drought and when irrigated crops have the highest water demand there.\nThe rivers having this kind of a runoff regime (nivo-glacial) are Pyandzh and Vakhsh Rivers, as well as the ones deriving from them, such as Amu Darya, Chu, Zeravshan, Talas and Ili Rivers with its numerous tributaries. These rivers all feature a small variability of annual runoff. This is partly a result of the regulating effect of the zone of eternal snow and ice and is very important from the perspective of agricultural production downstream. These rivers are thus particularly valuable, not only for irrigation but also as a source of hydroelectric power. Figure 1.14 shows example hydrographs of such rivers.\n\n\n\n\n\nFigure 1.14: Typical runoff regimes of nivo-glacial rivers in the Central Asia region. Hydrographs of different tributary of the Chirchik River are shown. Source: Yuri Ivanov (Uzbek Hydrometeorological Service), unpublished.\n\n\nRivers originating from the low mountains but being fed mainly by the snowmelt (nival regime rivers), are characterized by the early floods (March-May) and a sharp variability of annual runoff, since the amount of water is almost entirely determined by the snow reserves in the mountains which were accumulated in the previous winter season. Figure 1.15 shows example hydrographs of such rivers.\nFinally, the watercourses originating from the lowest parts of mountains or from low mountains, (nivo-pluvial regime rivers), which in comparison to other regimes receive much more liquid precipitation, are characterized by large amounts of water, often saturated by sediment, passing during short periods of time. These are so-called mudflows. These watercourses often dry up during summer because of a decrease in supplies from groundwater.\n\n\n\n\n\nFigure 1.15: Typical runoff regimes of nival-pluvial rivers in the Central Asia region. Hydrographs of different low-lying tributaries of the Chirchik River are shown. Source: Yuri Ivanov (Uzbek Hydrometeorological Service), unpublished.\n\n\nWhen entering the plains, the rivers of Central Asia form wide-spreading alluvial fans consisting of materials brought by them from the mountains. Here, the rivers are usually divided into several channels, and a large part of the water is filtered by these sediments. The large quantities of groundwater in these alluvial fans, which appear due to this process, mostly protrude from the earth’s surface at the edges of these alluvial fans, causing the small rivers that are fed by groundwater, so-called Karasu rivers, to emerge, which are also used for irrigation.\nThe relief thus has a powerful and many-sided impact on runoff formation processes. This influence is mediated through climatic factors, on which the recharge of the rivers, as well as the processes of thawing of snow and ice, etc., depend. In this regard, both the average water content, consistency of the annual runoff and its distribution over a year, and other characteristics of the river runoff cannot be considered independently from key relief factors, first and foremost altitude. All this demands a careful and comprehensive analysis of the impact of the relief on runoff processes.",
    "crumbs": [
      "Part I: Hydrology of Semi-Arid Central Asia",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hydrological Systems in Semi-Arid Central Asia</span>"
    ]
  },
  {
    "objectID": "hydrological_systems.html#sec-zone-of-water-distribution-and-use",
    "href": "hydrological_systems.html#sec-zone-of-water-distribution-and-use",
    "title": "1  Hydrological Systems in Semi-Arid Central Asia",
    "section": "\n1.4 Zone of Water Distribution and Use",
    "text": "1.4 Zone of Water Distribution and Use\nThe arid zones as shown in Figure 1.16 dominate the Central Asia region. As in the case of the definition of the zones of runoff formation (Section 1.3), one way to define the zones of complex water distribution and use is to use the definition of McVicar, Roderick, Donohue, Li, Van Niel, et al. (2012) to classify zones of high aridity in the following way\n\n\n\\(\\phi \\ge 1.3\\) and \\(\\phi &lt; 3\\): arid conditions\n\n\\(\\phi \\ge 3\\): hyperarid conditions\n\n\n\n\n\n\nFigure 1.16: The classification of the domain into arid and hyperarid zones is shown. Climatology data are from Karger et al. (2021) to compute the aridity index. For the definition of the humid and equidistant zones, we use McVicar, Roderick, Donohue, Li, Van Niel, et al. (2012).\n\n\nComparing Figure 1.13 with Figure 1.16, it is evident that the bulk of the domain is either arid or hyperarid and that the mountains are the regions where the annually renewable water resources are formed. In fact, these humid regions resemble floating islands in an enormous sea of aridity.\nFor this reason, it is not further astonishing that Central Asia is the home of some of the world’s largest irrigation systems (Figure 1.17). From the moment water leaves the mountainous zones of runoff formation in rivers, water gets diverted into irrigation channels for its diversion to supply irrigated areas. In the lowlands of Central Asia, it is for this reason impossible to talk about natural hydrology as man has engineered the hydrological landscape in its entirety with tens of thousands of kilometers of irrigation and drainage channels that, in some instances, even cross natural river basins.\nFor the interested readers, the history of these irrigation systems is provided by Peterson (2019). While the history of irrigation dates back millennia in the region, the large-scale push to extend irrigated areas beyond the historically established perimeters has happened during the Soviet era. It’s history is excellently recounted by Peterson (2019). As an example, ?fig-construction-of-big-fergana-canal shows archival footage from a Soviet propaganda film which shows in an impressive way the scale of the undertaking.\nNowadays, Uzbekistan has by far irrigates the largest area, i.e., almost half of the total irrigated area. Contrary to that, Tajikistan has the smallest fraction with only 7 % of the total irrigated area on its territory. Almost one-third of the total irrigated area is non-agricultural land including irrigated vegetation in cities and house gardens.\n\n\n\n\n\nFigure 1.17: Irrigated area in 2020 in semi-arid Central Asia, excluding Afghanistan. The pie chart in the upper left shows country-level statistics whereas the one in the upper right corner shows the type of irrigated area. Irrigated non-agricultural refers to irrigated areas in cities and housegardens. Source: hydrosolutions GmbH.\n\n\nThese data were derived with an unsupervised classification approach using Google Earth Engine and vast amounts of remotely sensed data (Ragettli, Herberz, and Siegfried (2018)). The interannual variability of irrigated areas is smaller than 5% between 2016 - 2020. This means that despite climate variability, the irrigation system is well buffered against below-normal water years. This certainly also has to do with the fact that many decadal to seasonal regulators/reservoirs can buffer water supply variability to a certain extent.\n\n\n\n\n\nFigure 1.18: Dams from the Global Georeferenced Database of Dams data are shown for the Central Asia region (Mulligan, Soesbergen, and Sáenz (2020)). The red dots indicate the main hydropower-producing dams, whereas the black dots show the locations of the decadal/seasonal regulators.\n\n\nThe region’s main planted crops for 2020 are shown in Figure 1.19. This figure is also derived from remotely sensed data and shows the crop disaggregated irrigated land (Source: hydrosolutions GmbH).\n\n\n\n\n\nFigure 1.19: Crop disaggregated irrigated areas. The main crops are shown color-encoded for the year 2020. Despite its declining significance, cotton remains the dominant irrigated crop in the region. Source: hydrosolutions GmbH.",
    "crumbs": [
      "Part I: Hydrology of Semi-Arid Central Asia",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hydrological Systems in Semi-Arid Central Asia</span>"
    ]
  },
  {
    "objectID": "hydrological_systems.html#sec-regional-water-balance",
    "href": "hydrological_systems.html#sec-regional-water-balance",
    "title": "1  Hydrological Systems in Semi-Arid Central Asia",
    "section": "\n1.5 Regional Water Balance",
    "text": "1.5 Regional Water Balance\nIt is instructive to show the opposite hydrological functions of mountain areas (zone of runoff formation) versus flat areas (zone of complex water distribution and use) of Central Asia by means of simple water balance considerations (Shults (1965)).\nThe water balance equation for mountain area, broadly defined, can be written as follows:\n\\[\np = e + q_{s} + q_{g}\n\\tag{1.1}\\]\nwhereby \\(p\\) represents the average long-term amount of precipitation and condensation of water vapor from the atmosphere, \\(e\\) the average long-term evaporation, \\(q_{s}\\) the average long-term surface outflow and \\(q_{g}\\) the average outflow of groundwater. This equation shows that the mountain area receives moisture only from the atmosphere and rainfall, which precipitates within its limits and evaporates only partially. The remainder part of it flows down in the form of surface and underground drainage. The sharp partition of a relief in the mountain area, and consequently, a deep natural drainage is the reason why groundwater is almost entirely connected to the river network already in the mountain area. The Meso-Cenozoic deposits, containing waterproof horizons, and the Paleozoic massifs on the border with the flat areas obstruct groundwater flows. Thus, the groundwater inflow to the flat areas makes no more than 10% - 15% of the surface one, and therefore, it can be neglected in the first equation.\nThen the water balance equation will have the following appearance:\n\\[\np = e + q_{s}\n\\tag{1.2}\\]\nBased on available data, the rate of surface outflow \\(q_{s}\\) can be calculated quite precisely: 155 billion m3 or 201 mm annually. River basin-specific surface runoff values are provided in Table. It is impossible to measure the amount of accumulated water vapor in the mountains by observation, so we have to proceed from the rate of the runoff, for which we need to know the value of the runoff coefficient. The last can be approximately considered as equal to 0.35 (see also next section for more information on this). Then, \\(p\\) equals 575 mm and \\(e\\), as follows, 575 mm - 201 mm = 374 mm.\n\nKey water balance basin statistics of selected large basins in Central Asia.\n\n\n\n\n\n\n\n\nBasin Name\nArea (km2)\nRunoff (m3/s)\nRunoff entering flatlands (m3/s)\nRunoff Coeff.\n(l/ (s km2))\n\n\n\nCaspian Sea\n29’700\n22\n12\n0.74\n\n\nEndhoreic Basins of TUK and AFG\n193’300\n180\n155\n0.93\n\n\nAmu Darya\n227’300\n2’500\n2’500\n11\n\n\nSyr Darya\n150’100\n1’200\n1’200\n8\n\n\nChu and Talas River Basin\n37’540\n190\n190\n5.1\n\n\nLake Issyk Kul\n12’600\n115\n-\n9.1\n\n\nSouthern Balkash Lake\n119’000\n800\n800\n6.7\n\n\nTotal\n769’600\n5’007\n4’857\n6.5\n\n\n\nIt should be noted that the rate of surface water inflow to the flatlands is lower than the runoff generated in the mountain areas as part of it is utilized in the mountain area for irrigation purposes (the rivers of Turkmenistan are, in this regard, an especially good example), or it evaporates from a surface like of the Lake Issyk Kul and other smaller lakes.\n\nThe equation of water balance for the flat area can be written in the form of\n\\[\np + q_{i} = e\n\\] {eq-regional-wb-3}\nwhere \\(q_{i}\\) represents the surface inflow of water.\nWe neglect underground outflow in the flat area as, even when it takes place, it is absolutely insignificant. The average amount of rainfall calculated by planimetering of the isohyetal map equals 173 mm. The inflow rate of water is equal to the outflow of water from mountain area, i. e. \\(q_{s} = q_{i} = 155 \\cdot 10^{9} \\text{ m} ^{3}\\). After making its way down to the flat area, which includes the surface of the Aral Sea and Lake Balkhash, the surface inflow of the rivers reaches 124 mm, and the evaporation is \\(e = p + q_{i} = 173 \\text{ mm} + 124 \\text{ mm} = 297 \\text{ mm}\\). Notably, from the entire moisture appearing in the flat area, 58% nevertheless is from atmospheric precipitation, despite its relatively insignificant absolute amount.\nComparing the two water balance equations shows that the mountain areas receive 575 mm of moisture from the atmosphere, of which 374 mm evaporates back to the atmosphere and 201 mm reach the downstream flat area as surface runoff. Conversely, the flat areas receive 297 mm of water from direct precipitation and from the inflow of mountain runoff. All of this water evaporates back to the atmosphere.\nTo summarize, it is clear that in the area of runoff formation, \\(p &gt; e\\), in the area of runoff losses \\(p &lt; e\\), and that in the area of runoff balance \\(p\\approx e\\). In each area where the runoff processes show the same orientation, its origin, distribution in time and space, and also the intensity of processes can, however, vary. In this sense, depending mainly on local topography (generally speaking, depending on the altitude, orientation and exposure of a reservoir to humid air masses), the specific runoff, the persistence of the annual runoff and its distribution over a year, as well as other characteristics of the river flow can sharply differ in different parts of the area of runoff formation, as it was already discussed above. On the other hand, the intensity of runoff losses, their distribution over a year, etc., within the area of runoff losses considerably depends on the economic activities and features of climatic conditions.",
    "crumbs": [
      "Part I: Hydrology of Semi-Arid Central Asia",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hydrological Systems in Semi-Arid Central Asia</span>"
    ]
  },
  {
    "objectID": "hydrological_systems.html#sec-hydrological-systems-references",
    "href": "hydrological_systems.html#sec-hydrological-systems-references",
    "title": "1  Hydrological Systems in Semi-Arid Central Asia",
    "section": "\n1.6 References",
    "text": "1.6 References\n\n\n\n\nCentral Intelligence Agency. n.d. “The World Factbook 2020.” https://www.cia.gov/library/publications/resources/the-world-factbook/index.html.\n\n\nGerlitz, Lars, Eva Steirou, Christoph Schneider, Vincent Moron, Sergiy Vorogushyn, and Bruno Merz. 2018. “Variability of the Cold Season Climate in Central Asia. Part I: Weather Types and Their Tropical and Extratropical Drivers.” Journal of Climate 31 (18): 7185–7207. https://doi.org/10.1175/jcli-d-17-0715.1.\n\n\nGRDC, Koblenz, Germany: Federal Institute of Hydrology (BfG). 2020. “Major River Basins of the World / Global Runoff Data Centre, GRDC. 2nd, Rev. Ext. Ed.” Shape.\n\n\nInternational Earth Science Information Network - CIESIN - Columbia University, Center for. 2018. “Gridded Population of the World, Version 4 (GPWv4): Population Density Adjusted to Match 2015 Revision UN WPP Country Totals, Revision 11.” Palisades, NY: NASA Socioeconomic Data; Applications Center (SEDAC). https://doi.org/10.7927/H4F47M65.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nKarger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-resolution monthly precipitation and temperature time series from 2006 to 2100.” Scientific Data 7 (1): 248. https://doi.org/10.1038/s41597-020-00587-y.\n\n\nKarger, Dirk Nikolaus, Adam M. Wilson, Colin Mahony, Niklaus E. Zimmermann, and Walter Jetz. 2021. “Global daily 1 km land surface precipitation based on cloud cover-informed downscaling.” Scientific Data 8 (1): 307. https://doi.org/10.1038/s41597-021-01084-6.\n\n\nLehner, B., and Grill G. 2013. “Global River Hydrography and Network Routing: Baseline Data and New Approaches to Study the World’s Large River Systems.” Hydrological Processes 27 (15): 2171–86.\n\n\nMcVicar, Tim R., Michael L. Roderick, Randall J. Donohue, Ling Tao Li, Thomas G. Van Niel, Axel Thomas, Jürgen Grieser, et al. 2012. “Global Review and Synthesis of Trends in Observed Terrestrial Near-Surface Wind Speeds: Implications for Evaporation.” Journal of Hydrology 416-417: 182–205. https://doi.org/https://doi.org/10.1016/j.jhydrol.2011.10.024.\n\n\nMcVicar, Tim R., Michael L. Roderick, Randall J. Donohue, Ling Tao Li, Thomas G. Van Niel, Axel Thomas, Jürgen Grieser, et al. 2012. “Global Review and Synthesis of Trends in Observed Terrestrial Near-Surface Wind Speeds: Implications for Evaporation.” Journal of Hydrology 416-417: 182–205. https://doi.org/https://doi.org/10.1016/j.jhydrol.2011.10.024.\n\n\nMulligan, Mark, Arnout van Soesbergen, and Leonardo Sáenz. 2020. “GOODD, a Global Dataset of More Than 38,000 Georeferenced Dams.” Scientific Data 7 (1): 31. https://doi.org/10.1038/s41597-020-0362-5.\n\n\n“NASA Shuttle Radar Topography Mission (SRTM).” 2013. NASA. https://earthdata.nasa.gov/learn/articles/nasa-shuttle-radar-topography-mission-srtm-version-3-0-global-1-arc-second-data-released-over-asia-and-australia.\n\n\nPeterson, Maya K. 2019. Pipe Dreams: Water and Empire in Central Asia’s Aral Sea Basin. Cambridge University Press.\n\n\nRagettli, Silvan, Timo Herberz, and Tobias Siegfried. 2018. “An Unsupervised Classification Algorithm for Multi- Temporal Irrigated Area Mapping in Central Asia.” Remote Sensing 10 (11): 1823. https://doi.org/10.3390/rs10111823.\n\n\nShults, Victor. 1965. Rivers of Middle Asia. 2nd Edition. Gidrometeoizdat, Leningrad.",
    "crumbs": [
      "Part I: Hydrology of Semi-Arid Central Asia",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Hydrological Systems in Semi-Arid Central Asia</span>"
    ]
  },
  {
    "objectID": "example_river_basins.html",
    "href": "example_river_basins.html",
    "title": "2  Case Study River Basins",
    "section": "",
    "text": "2.1 Gunt River Basin",
    "crumbs": [
      "Part I: Hydrology of Semi-Arid Central Asia",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Case Study River Basins</span>"
    ]
  },
  {
    "objectID": "example_river_basins.html#sec-example-gunt-river-basin",
    "href": "example_river_basins.html#sec-example-gunt-river-basin",
    "title": "2  Case Study River Basins",
    "section": "",
    "text": "2.1.1 Gunt Basin Characterization\nThe Gunt River basin is located in the Pamir mountains in the Gorno Badakhshan Autonomous Region in south-east Tajikistan. The basin covers approximately 14’000 km2. The Gunt River is a large right tributary of the upstream Pyandzh and joins the latter downstream of the town of Khorog. Mean elevation is approx 4’270 meters above mean sea level (masl) with an altitude range from 2’000 – 6’700 masl. The highest elevations in the catchment are Peak Karl Marx (6726 m) and Peak Engels (6510 m) at the southern border of the catchment.\nInformation on the available discharge and meteorological stations in the basin is provided in Table 2.1.\n\n\n\nTable 2.1: Details on meteorological and discharge measurement stations from which data is available.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStationName\nStationCode\nlat\nlon\neasting\nnorthing\nmasl\ntype\n\n\n\nBulunkul\n38953\n37.70417\n72.94583\n847890.5\n4180325\n3746\nMeteo\n\n\nKhorog\n38954\n37.50361\n71.51500\n722309.0\n4153714\n2075\nMeteo\n\n\nKhorog\n17050\n37.50361\n71.51500\n722309.0\n4153714\n2075\nDischarge Gauge\n\n\nJavshangoz\n38956\n37.39083\n72.29583\n791785.1\n4143330\n3438\nMeteo\n\n\nNavobod\n38950\n37.59417\n71.86556\n752995.5\n4164650\n2566\nMeteo\n\n\n\n\n\n\n\n\nA map of the basin is provided in Figure 2.2.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to experiment with the R code yourself, the data for this Chapter can be downloaded here (Note: size of data is &gt; 500 MB). You can store these data locally and then perform the analysis on your local computer by correspondingly adjusting the data path in the code block below before loading the data.\n\n\n\ndata_dir &lt;- \"../caham_data/AmuDarya/gunt_data/geospatial\"\n\n# Load vector data (shapefiles)\ngunt_basin_shp &lt;- st_read(file.path(data_dir, \"gunt_basin_shp.shp\"), quiet = TRUE)\ngunt_rivers_shp &lt;- st_read(file.path(data_dir,\"gunt_rivers_shp.shp\"), quiet = TRUE)\ngunt_subbasins_shp &lt;- st_read(file.path(data_dir, \"gunt_subbasins_shp.shp\"), quiet = TRUE)\n\n# Load raster data (.tif-files)\ngunt_dem &lt;- raster::raster(file.path(data_dir,\"gunt_dem.tif\"))\ngunt_dem_hillshade &lt;- raster::raster(file.path(data_dir,\"gunt_dem_hillshade.tif\"))\n\nAfter successfully loading the data, we can easily visualize it.\n\n# Visualize data\ntm_shape(gunt_dem_hillshade) +\n tm_raster(palette = gray(0:100 / 100), n = 100, legend.show = FALSE)  +\n  tm_shape(gunt_dem) +\n  tm_raster(alpha = 0.5, palette = terrain.colors(25), legend.show = TRUE, title = \"Elevation (masl)\") +\n  tm_shape(gunt_basin_shp) +\n  tm_polygons(\"area\", alpha = 0, legend.show = FALSE) +\n  tm_shape(gunt_subbasins_shp) +\n  tm_polygons(\"ID\", alpha = 0, legend.show = FALSE) +\n  tm_layout(legend.position = c(\"right\",\"bottom\")) +\n  tm_shape(gunt_rivers_shp) +\n  tm_lines(col = \"name\", scale = 3) + \n  tm_shape(meteo_stations_sf) +\n  tm_dots(col = \"black\", scale = 2) +\n  tm_shape(meteo_stations_sf) +\n  tm_text(\"StationName\", size = .75, auto.placement = TRUE, just = \"left\")\n\n\n\n\n\n\nFigure 2.2: The Gunt River basin, its topography and the main tributaries are shown (see corresponding legend). The discharge measurement station 17050 is located in Khorog on the Gunt Downstream, shortly after the confluence of Gunt Upstream and Shakara rivers.\n\n\n\n\nThe Table below summarizes key basin statistics that are relevant from the hydro-climatological perspective. Data from various sources are summarized here. These data are presented in Section II on Data and discussed in great detail there.\nThe basin area has been derived from the basin shapefile. Raster statistics of the SRTM digital elevation model (“NASA Shuttle Radar Topography Mission (SRTM)” 2013), the climate raster files as well as the land cover raster are calculated using the QGIS Raster Layer Statistics processing toolbox algorithm. The land ice total polygon area is computed with the Statistical Summary Option in QGIS.\nThe norm hydrological year discharge and the corresponding norm cold and warm season discharge values have been computed with data from the Tajik Hydrometeorological Service. The mean basin precipitation is computed using a state-of-the-art bias corrected high-resolution reanalysis product Beck et al. (2020). Such data will also be used for hydro-climatological modeling in later Chapters. Potential evaporation is from (Trabucco and Zomer 2019) using the Penman-Montieth equation.\n\n\nTable 2.2: Key relevant basin statistics for Gunt river basin. Individual data sources are indicated. Note that the hydrological year in Central Asia is defined as starting from October in year 1 and lasts until end of September in the subsequent year 2. More information on the relevance of hydrological year-based water accounting is given below.\n\n\n\n\n\n\n\nATTRIBUTE\nVALUE\n\n\n\n\nGeography (“NASA Shuttle Radar Topography Mission (SRTM)” 2013)\n\n\n\n\nBasin Area \\(A\\)\n\n13’693 km2\n\n\n\nMinimum Elevation \\(h_{min}\\)\n\n2’068 masl\n\n\nMaximum Elevation \\(h_{max}\\)\n\n6’652 masl\n\n\nMean Elevation \\(h_{mean}\\)\n\n4’267 masl\n\n\nHydrology [Source: Tajik Hydromet Service]\n\n\n\nNorm hydrological year discharge \\(Q_{norm}\\)\n\n103.8 m3/s\n\n\nNorm cold season discharge (Oct. - Mar., Q4/Q1)\n19.8 m3/s\n\n\nNorm warm season discharge (Apr. - Sept., Q2/Q3)\n84.2 m3/s\n\n\nAnnual norm discharge volume\n3.28 km3\n\n\n\nAnnual norm specific discharge\n239 mm\n\n\nClimate\n\n\n\nMean basin temperature \\(T\\) (Karger et al. 2017)\n\n-5.96 deg. Celsius\n\n\nMean basin precipitation \\(P\\) (Beck et al. 2020)\n\n351 mm\n\n\nPotential Evaporation \\(E_{pot}\\) (Trabucco and Zomer 2019)\n\n929 mm\n\n\nAridity Index \\(\\phi = E_{pot} / P\\)\n\n2.7\n\n\nAridity Index (Trabucco and Zomer 2019)\n\n3.6\n\n\nLand Cover (Buchhorn et al. 2019)\n\n\n\nShrubland\n8 km2\n\n\n\nHerbaceous Vegetation\n4’241 km2\n\n\n\nCrop Land\n0.5 km2\n\n\n\nBuilt up\n4 km2\n\n\n\nBare / Sparse Vegetation\n8’410 km2\n\n\n\nSnow and Ice\n969 km2\n\n\n\nPermanent Water Bodies\n80 km2\n\n\n\nLand Ice\n\n\n\nTotal glacier area (GLIMS and NSIDC 2005, updated 2018)\n\n875 km2\n\n\n\nTotal glacier volume (calculated with (Erasov 1968))\n699 km3\n\n\n\n\n\n\n\nWith the values provided in the table above, the discharge index \\(Q/P\\) is 68.5 % and the evaporative index \\(E/P\\) is 31.5 %. In other words, the long-term water balance shows that 3 precipitation units gets partitioned into 2 discharge units and 1 evaporation unit, approximately. The aridity index \\(\\phi\\) , when calculated using \\(P\\) from (Karger et al. 2020) and \\(E_{pot}\\) from (Trabucco and Zomer 2019) is 2.7. The aridity index from (Trabucco and Zomer 2019) is 3.6. These values indicate some uncertainty in relation to the global climate products used. Despite this, they confirm the highly arid characteristics of the basin.\n\n2.1.2 Gunt Basin Hydrology\nFor the analysis of the key hydro-climatological characteristics, we first load the available decadal and monthly station data1. The data used in this Chapter can be accessed downloaded from the following online repository.\nFirst, we load the available station data of the Gunt River basin into R. Note that the monthly data is available for one discharge station and 4 meteorological stations. See also Table 2.1 for more information on the stations.\n\ndata_dir &lt;- \"../caham_data/AmuDarya/gunt_data/station_data/\"\nfile_name = 'gunt_data_cleaned.Rds'\ngunt_station_data &lt;- read_rds(file.path(data_dir, file_name))\ngunt_station_data\n\n# A tibble: 23,352 × 10\n   date        data  norm units type  code  station     river basin  resolution\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;  &lt;fct&gt;     \n 1 1940-01-31  30.5  32.9 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 2 1940-02-29  27.3  30.1 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 3 1940-03-31  24.9  28.4 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 4 1940-04-30  26.4  30.7 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 5 1940-05-31  59    68.5 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 6 1940-06-30 309   232.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 7 1940-07-31 224   319.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 8 1940-08-31 201   237.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 9 1940-09-30 121   117.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n10 1940-10-31  60.8  63.1 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n# ℹ 23,342 more rows\n\n\nThis dataframe now contains all available data hydro-meteorological data from the basin. Most data are available at monthly time scales. As an example, the monthly discharge data from Gauge 17050 can be accessed and extracted from the Gunt dataset in the following way.\n\nq_17050_mon &lt;- gunt_station_data |&gt; filter(type == \"Q\" & code == '17050' & resolution == 'mon')\nq_17050_mon\n\n# A tibble: 972 × 10\n   date        data  norm units type  code  station     river basin  resolution\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;  &lt;fct&gt;     \n 1 1940-01-31  30.5  32.9 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 2 1940-02-29  27.3  30.1 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 3 1940-03-31  24.9  28.4 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 4 1940-04-30  26.4  30.7 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 5 1940-05-31  59    68.5 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 6 1940-06-30 309   232.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 7 1940-07-31 224   319.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 8 1940-08-31 201   237.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n 9 1940-09-30 121   117.  m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n10 1940-10-31  60.8  63.1 m3/s  Q     17050 Gunt_Khorog Gunt  Pyandz mon       \n# ℹ 962 more rows\n\n\nWhen we plot the data, we see that we have a near complete monthly record from 1940 onward (see Figure 2.3). The data gap in the 1990ies was during the Tajik civil war.\n\nq_17050_mon |&gt; \n  plot_time_series(date,\n    data,\n    .smooth        = FALSE,\n    .interactive   = TRUE,\n    .title         = \"\",    \n    .x_lab         = 'Year',\n    .y_lab         = 'Mean monthly Q [m3/s]',\n    .plotly_slider = TRUE)\n\n\n\n\n\n\nFigure 2.3: Visualized monthly discharge data at Gunt gauging station (17050)\n\n\n\nThere are visible changes in the winter low flow regime from 2007 onward. This is because of the hydropower production that started upstream at that time. Pamir Energy, the local generation company supplies hydropower electricity especially during the cold winter months to the communities in the valley. When hydropower is required, the water table of the Yashikul Lake in the Pamir plateau (Alishur catchment, see Figure 2.2) gets lowered to increase the discharge for energy production in the downstream.\n\n\n\n\n\nFigure 2.4: Since 2006, a run off the river hydropower plant operated by Pamir Energy produces hydropower to cover local electric energy demand. Lake Yashikul is used as a regulator. The increase in winter discharge from 2007 onwards is due to HPP operations.\n\n\nThe seasonal diagnostics of the monthly discharge time series is shown in @ref(fig-gunt-seasonal-diagnostics). As is easily visible, the peak discharge of Gunt River measured at Khorog station is in July.\n\nq_17050_mon |&gt; \n  plot_seasonal_diagnostics(.date_var      = date,\n                            .value         = data,\n                            .title         = \"\",\n                            .feature_set   = c(\"month.lbl\"),\n                            .interactive   = FALSE,\n                            .x_lab         = \"Year\",\n                            .y_lab         = \"Mean monthly Q [m3/s]\") +\n  scale_x_discrete(breaks = c(\"January\", \"February\", \"March\", \"April\", \"May\", \n                            \"June\", \"July\", \"August\", \"September\", \"October\", \n                            \"November\", \"December\", \"1\", \"2\", \"3\", \"4\"),\n                   labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \"J\", \"A\", \"S\", \"O\", \"N\", \"D\",\"1\", \"2\", \"3\", \"4\"))\n\n\n\n\n\n\nFigure 2.5: Seasonal diagnostics of the monthly discharge time series at the Gunt-Khorog gauging station (17050). The analysis can be easily carried out using the function plot_seasonal_diagnostics from the R package timetk.\n\n\n\n\nunlike in the lower lying Chirchik tributaries as shown in Figure 2.17 further below in Section 2.2.\n\n\n\n\n\n\nTip\n\n\n\nCompare the discharge seasonality of Gunt River with the seasonality of the large and small Chirchik River tributaries? Obtain the information of all the other rivers in the Case Study packs and their seasonality. What is the single most important determinant of peak discharge timing in Central Asia rivers?\n\n\nBelow in Figure 2.6, we are plotting changes to monthly flows over time by binning all available data in the corresponding monthly slots. The red lines are linear regression lines that indicate trends for the individual months. Over the observational record of approx. 80 years, changes in monthly discharge regimes are clearly visible. On the one hand, summer discharge of Gunt river during the third quarter (Q3), i.e., July, August and September, is decreasing whereas the cold season discharge in Q1 and Q4 is increasing. This is a clear indication that the basin hydrology is undergoing changes over the long run. These could either be climate-related or, as discussed above, also the result of man-made interventions such as the regulation of river discharge for winter hydropower energy production. However, the shift of discharge from the warm season (Q2 and Q3) towards the cold season (Q4 and Q1) has already happened before river regulation started and hence, it is likely that we see a compound effect here.\n\nq_17050_mon |&gt; \n  summarise_by_time(.date_var = date, \n                    .by       = \"month\",\n                    value     = mean(data)) |&gt; \n  tk_ts(frequency = 12) |&gt; \n  forecast::ggsubseriesplot(year.labels = FALSE) + \n              geom_smooth(method = \"lm\", color = \"red\") +\n              xlab('Month') +\n              ylab('Mean monthly Q [m3/s]') +\n  theme_bw()\n\n\n\n\n\n\nFigure 2.6: The Figure shows data from the entire record. All monthly data are binned in their corresponding month (black lines). A large interannual variability (year-to-year variations in discharge in the same months) is visible. The red lines show simple regression lines for each month separately.\n\n\n\n\nWhenever we analyze annual data and changes therein, we should work with data as observed during the hydrological year. The hydrological year in Central Asia is defined as:\n\nmonHY(Oct) = 1\nmonHY(Nov) = 2\n…\nmonHY(Sep) = 12\n\nThis also holds for meteorological data. Using this definition, we can further define cold and warm seasons easily where the cold season lasts from October through end of March (Q4 to Q1 the following year) and the warm season from April through end of September (Q2 and Q3). With this in mind, we can define the hydrological year discharge.\nGiven a time series of observations, the function convert2HYY() as part of the riversCentralAsia package provides a convenient way to compute hydrological year mean discharge, including for cold and warm seasons. For monthly mean temperatures mean(T), it computes hydrological year mean temperatures, including for cold and warm seasons. Finally, for precipitation, the function computes the hydrological year sum, including also for cold and warm season months. Figure 2.7 shows the discharge time series analysis for the Khorog gauging station.\n\nqHYY &lt;- convert2HYY(q_17050_mon,'17050','Q')\nqHYY |&gt;   pivot_longer(-hyYear) |&gt; \n  plot_time_series(hyYear,value,name,\n                   .title = '',\n                   .x_lab = 'Year',\n                   .y_lab = 'Mean monthly Q [m3/s]',\n                   .interactive = TRUE,\n                   .smooth = FALSE)\n\n\n\n\n\n\nFigure 2.7: Hydrological year discharge time series, incl. cold and warm season values. If data are not complete for all 12 months, the hydrological year statistics are not computed. Q_mean_ann: entire hydrological year discharge, Q_mean_cs: cold season Q1/Q4 discharge and Q_mean_ws: warm season Q2/Q3 discharge.\n\n\n\nFigure 2.7 confirms the findings from the seasonal analysis. However, it also shows that the first two decades of the 21st century show a marked decline in total discharge as compared to the period between 1960 to 2000.\nA common way to plot changes over time in hydro-meteorological time series is to plot annual deviations from corresponding long-term norms (long-term mean values). For this, we can use the plotNormDevHYY() function from the riversCentralAsia package. Given the three hydrological year annual time series, it computes long-term norms over the entire data set and subtracts actual annual values from the norm value. Like this, temporal changes and trends become even better visible. Figure 2.8 shows the results for the hydrological year data.\n\nplotNormDevHYY(qHYY,'Q','Khorog-Gunt 17050')\n\n\n\n\n\n\nFigure 2.8: Deviations from the corresponding long-term norms for the discharge time series at gauging station 17050. It should be noted that the values shown are deviations from the corresponding norms which are shown in the subtitles above the Figure plates.\n\n\n\n\nFigure 2.8 shows that, in absolute terms, the discharge in the high-flow season is undergoing a much greater reduction than an increase in the low-flow season. Hence, we cannot simply explain the decline of discharge in one season with the increase in the other. In other words, the early melting of the winter snow pack cannot alone explain the summer decline in water availability. Some other mechanism much be at work which we still need to better understand. One hypothesis could be that an increase in summer temperatures leads to higher evaporation over the basin thus leading to reduced discharge (see also Section 2.1.3 below).\nAlso and as mentioned above, winter discharge is influence by human regulation after 2006. This needs to be carefully taken into account when carrying out climate change impact analysis over the period of the observational record. For example, the cold season discharge deviation from the norm in 2006 and 2007 is 10 m3/s (see Figure 2.8) indicating that this amount of additional water was used for hydropower energy production during the winter.\nIn order to gauge whether there is a robust trend in discharge over the observed time period, we compute decadal (10 year means) and plot the results.\n\nmean10yearQ &lt;- qHYY |&gt; \n  filter(hyYear &lt; '2020-01-01') |&gt; \n  pivot_longer(-hyYear) |&gt; \n  group_by(name) |&gt; \n  summarise_by_time(hyYear,value, .by = \"10 year\", mean10yearQ = mean(value, na.rm = TRUE)) |&gt;\n  dplyr::select(-value) |&gt; \n  distinct() |&gt; \n  ungroup()\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\nℹ The deprecated feature was likely used in the timetk package.\n  Please report the issue at\n  &lt;https://github.com/business-science/timetk/issues&gt;.\n\nmean10yearQ |&gt; pivot_wider(names_from = name,values_from = mean10yearQ)\n\n# A tibble: 8 × 4\n  hyYear     Q_mean_ann Q_mean_cs Q_mean_ws\n  &lt;date&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 1940-01-01      111.       40.3      182.\n2 1950-01-01      108.       39.0      177.\n3 1960-01-01       96.2      35.7      156.\n4 1970-01-01      105.       35.9      174.\n5 1980-01-01      105.       37.5      171.\n6 1990-01-01      114.       41.0      188.\n7 2000-01-01      104.       43.8      165.\n8 2010-01-01       94.2      44.7      143.\n\nmean10yearQ |&gt;  plot_time_series(hyYear,mean10yearQ,name,\n                                  .smooth = FALSE,\n                                  .x_lab  = \"Year\",\n                                  .y_lab  = \"Q [m^3/s]\",\n                                  .title  = \"\")\n\n\n\n\n\n\nFigure 2.9: 10-year mean hydrological year discharge of Gunt River, including the cold and warm season components. The decadal mean values are related in time to the beginning of the corresponding decade in the Figure. The strongly declining trend in warm season discharge causes the overall observed decline in hydrological year discharge.\n\n\n\nThis is informative. From the 1990ies onwards, a strong reduction in mean hydrological year warm season discharge is observed of about -16 % relative to the mean 1940 - 1989 values. At the same time, 10-year mean hydrological year cold season discharge remained almost stable. As already mentioned, such types of findings are the a key motivation to study possible climate impacts in such basins in greater detail with hydrological modeling.\n\n2.1.3 Gunt Basin Climate\nA significant amount of meteorological station data are available. Some of these data are analyzed in this Section. While we mostly concentrate on mean monthly data for temperature, we should note that the available data record also contains data on absolute and mean minimum and maximum temperatures.\n\n# Extracting mean station data from the four stations.\nTmean_38954 &lt;- gunt_station_data |&gt; \n  filter(code == \"38954\" & type == 'mean(T)') |&gt; \n  filter(date &gt;= '1939-01-01') |&gt; \n  dplyr::select(date, data) |&gt; \n  rename(Tmean_38954 = data)\nTmean_38950 &lt;- gunt_station_data |&gt; \n  filter(code == \"38950\" & type == 'mean(T)') |&gt; \n  filter(date &gt;= '1939-01-01') |&gt; \n  dplyr::select(date, data) |&gt; \n  rename(Tmean_38950 = data)\nTmean_38953 &lt;- gunt_station_data |&gt; \n  filter(code == \"38953\" & type == 'mean(T)') |&gt; \n  filter(date &gt;= '1939-01-01') |&gt; \n  dplyr::select(date, data) |&gt; \n  rename(Tmean_38953 = data)\nTmean_38956 &lt;- gunt_station_data |&gt; \n  filter(code == \"38956\" & type == 'mean(T)') |&gt; \n  filter(date &gt;= '1939-01-01') |&gt; \n  dplyr::select(date, data) |&gt; \n  rename(Tmean_38956 = data)\n# Assembling the data. \nT &lt;- full_join(Tmean_38950, Tmean_38953, by = \"date\")\nT &lt;- full_join(T, Tmean_38954, by = \"date\")\nT &lt;- full_join(T, Tmean_38956, by = \"date\")\n# Plotting the dataframe \nT |&gt; pivot_longer(-date) |&gt; \n  filter(date &gt;= '1940-01-01') |&gt; \n  plot_time_series(date,\n                   value,\n                   name,\n                   .smooth = FALSE,\n                   .x_lab = 'Year',\n                   .y_lab = 'Mean monthly T [deg. C]',\n                   .title = \"\",\n                   .interactive = TRUE)\n\n\n\n\n\n\nFigure 2.10: Mean Monthly Temperature Climatology in the Gunt River Basin from 1940 - 2020. While first observations are available from the very beginning of the 20th century, data are only shown from 1940 onward which marks the start of a coherent record.\n\n\n# add a month identifier\nT &lt;- T |&gt; \n  mutate(mon = month(date))\n\nBecause of the high quality and the consistency of the long-term record of the data at Khorog station 39854, we focus the further climatological analysis there. Figure 2.11 shows deviations from norm mean temperatures over the last 120 years. The recent two decades stand out because of the pronounced warming observed at the station, especially during the cold season where norm deviations on average range between 1 - 2 degrees Celsius (deg. C.).\n\n# Station Khorog 38954\nmeanTHYY_38954 &lt;- gunt_station_data |&gt;\n  convert2HYY(38954,'mean(T)') |&gt; \n  filter(hyYear &gt;= \"1900-10-01\")\nplotNormDevHYY(meanTHYY_38954,'mean(T)','Khorog 38954')\n\n\n\n\n\n\nFigure 2.11: Annual devations from the norm of the mean temperature for the Khorog station 38954 record are shown for the entire hydrological year and for the corresponding cold and warm seasons. Note that the entire data record is taken into account here from the start of the 20th century.",
    "crumbs": [
      "Part I: Hydrology of Semi-Arid Central Asia",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Case Study River Basins</span>"
    ]
  },
  {
    "objectID": "example_river_basins.html#sec-example-chirchik-river-basin",
    "href": "example_river_basins.html#sec-example-chirchik-river-basin",
    "title": "2  Case Study River Basins",
    "section": "\n2.2 Chirchik River Basin",
    "text": "2.2 Chirchik River Basin\nThe Chirchik is a river in the Tashkent region of Uzbekistan. Its natural basin covers 13’112 km2, not accounting for the modern-time interbasin water transfers to the neighboring Akhangaran basin in the south (the outline of the basin is shown in Figure 2.12) and to the north. In terms of total runoff contribution, it is the biggest right tributary of the Syr Darya (see also further below in Section Section 2.2.1).\nThe river is formed by the confluence of the Chatkal and the Pskem rivers. They emerge at the south-western end of the Tien Shan mountains, i.e. the Talas Alatau, in the border region of Kyrgyzstan, Kazakhstan and Uzbekistan. The main tributaries are in clock-wise direction starting from north: Ugam, Pskem, Kosku and Chatkal. The Charvak reservoir receives water from these rivers. Ugam is the largest right tributary downstream of the reservoir and Aksak Ata the largest left-side tributary.\nBelow the Charvak hydroelectric power station, the river water gets diverted in numerous canals for irrigation in and around the Tashkent oasis and for interbasin water transfer to the Akhangaran basin in the south. As part of the Chirchik-Bozsuu cascade, several smaller dams along the river serve hydropower production and irrigation purposes.\n\n\n\n\n\nFigure 2.12: Overview over the Chirchik river basin with tributaries and the location of the main gauging stations in the zone of runoff formation and near the confluence with the Syr Darya.\n\n\nFigure 2.12 shows a comprehensive overview of the Chirchik river basin and its tributaries as well as relevant modern gauging stations. Gauges are indicated with the semi-round shapes and the corresponding five digit official code as utilized by the Uzbek Hydrometeorological Service (HMS) indicated. The gauge 16924 is not a real gauge in the sense that reservoir inflow is not measured at one point but rather is calculated from all contributing tributary flow components, i.e. the Chatkal river, the Pskem river, Nauvalisoy and the Koksu River.\nKoksu however, with a basin area of 392 km\\(^2\\), is ungauged. Its discharge contribution is calculated using an established empirical relationship between discharge in Chatkal River and discharge in Koksu. The empirical relationship is derived in Section Section 2.2.3. First, we now turn our attention to the description of key hydrological basin features.\n\n2.2.1 Chirchik Basin Characterization\nThis Section available data to characterize the Chirchik River Basin from the hydro-climatological perspective. Data access and modeling is further described in the Section on Data ?sec-part-ii-data-sources-retrieval-and-preparation in Part II and Part III Chapter 8 of this Book.\nThe available discharge data is shown in Figure 2.13. These are near complete historic records. See above Figure 2.12 for the station locations. Note that all available data has been converted to monthly time series for plotting.\n\nchirchik_river_data &lt;- ChirchikRiverBasin \n\nchirchik_river_data |&gt; \n  filter(type == 'Q') |&gt; \n  dplyr::select(date,data,river) |&gt; \n  group_by(river) |&gt; \n  summarise_by_time(.date_var = date, .by = 'month', data = mean(data)) |&gt; \n  plot_time_series(date,\n                   data,\n                   .facet_ncol      = 2,\n                   .interactive     = TRUE, \n                   .smooth          = FALSE,\n                   .title           = '',\n                   .color_var = river,\n                   .legend_show = FALSE,\n                   .x_lab = \"Date\",\n                   .y_lab = 'Discharge [m3/s]')\n\n\n\n\n\n\nFigure 2.13: Available discharge data of Chirchik River basin.\n\n\n\nThe discharge measurements at Gazalkent gauge (upper right panel, shown in red color) started already in 1900. It is one of the longest complete records available in Central Asia. The monthly record of the station is shown in Figure 2.14. You can zoom into the time series and investigate it in detail.\n\nchirchik_river_data |&gt; \n  filter(code == '16262') |&gt; \n  plot_time_series(date,data,\n    .interactive = TRUE,\n    .smooth = FALSE,\n    .title = \"\",\n    .x_lab = 'Date',\n    .y_lab = 'Discharge [m3/s]',\n    .plotly_slider = TRUE)\n\n\n\n\n\n\nFigure 2.14: Monthly discharge at Gauge 16262, Gazalkent.\n\n\n\nAs is easily visible, the June 1969 discharge was the historic monthly mean maximum with 1’220 m3/s. The time series features the typical snow-melt-driven runoff pattern with pronounced seasonality and interannual variability.\nAt Chinaz near the confluence of the Chirchik River with the Syr Darya (Gauge 16275), however, a changing discharge regime can be identified over time (see Figure 2.15). The drastic decrease in discharge there is due to two effects. First, water diversions and interbasin water transfers for irrigation purposes have greatly increased over the course of the 20th century. Second, the closure of the Charvak dam in 1974 and the subsequent filling of the dam decreased discharge during the filling period. Furthermore, the interannual variability of flows decreased from there onward due to the now regulated flow regime. This latter effect is also visible at the Gazalkent gauge (Figure 2.14). The non-stationarity in the discharge time series at these stations is thus explained by anthropogenic effects.\n\nchirchik_river_data |&gt; \n  filter(code == '16275') |&gt; \n  plot_time_series(date,data,\n    .interactive = TRUE,\n    .smooth = FALSE,\n    .title = \"\",\n    .x_lab = 'Date',\n    .y_lab = 'Discharge [m3/s]',\n    .plotly_slider = TRUE)\n\n\n\n\n\n\nFigure 2.15: Monthly discharge at Gauge 16275, Chinaz.\n\n\n\nThe effect of water diversion becomes even more apparent when the annual discharge at Gazalkent gauging station upstream of any major water diversion and at Chinaz gauge, which is in the very downstream of Chirchik River right before its confluence with the Syr Darya, are compared. The corresponding annual time series are shown in Figure 2.16 together with the difference of the two time series.\n\n\n\n\n\n\n\nFigure 2.16: Annual discharge at Gauge 16262, Gazalkent and Gauge 16275, Chinaz and the difference of the two timeseries. The difference of the two time series is from the allocation of water for human purposes, mostly for irrigation.\n\n\n\n\nFigure 2.16 shows the growing water allocation in the catchment from the 1930ies up to the end of the 20th century. Allocation grew almost 3-fold over this period. Interestingly, in the first decade of the 21st century, trends in allocation completely reversed and in 2009, roughly one third of the total flow at Gazalkent was allocated consumptively. The trend reversal might be due to a change in irrigation policy, problems with intake infrastructure between the two gauges, or both.\n\n\n\nKey statistics of Chirchik basin rivers.\n\ncode\nmean\nmin\nmax\nsd\n\n\n\n16262\n228.6\n48.7\n1220.0\n186.4\n\n\n16275\n104.9\n1.2\n912.0\n121.0\n\n\n16279\n115.7\n21.1\n729.0\n109.9\n\n\n16290\n79.4\n12.7\n438.0\n69.3\n\n\n16298\n3.8\n0.9\n21.1\n2.8\n\n\n16300\n22.4\n3.9\n114.0\n19.3\n\n\n16924\n205.3\n40.7\n1231.0\n183.2\n\n\n\n\n\nThe largest left tributary to Chirchik below the Charvak reservoir Aksak Ata. The gauging station on the river got dismantled a long time ago. An average long-term mean discharge of 2.35 m\\(^{3}\\)/s is a solid estimated of its contribution to the overall discharge of Chirchik. Thus, if we add up long-term average discharge at Gazalkent and the one from Aksak Ata, we obtain an annual norm discharge (total average water availability) of 231 m\\(^{3}\\)/s.\nChirchik river is thus the biggest right-tributary of the Syr Darya. Chatkal river contributes exactly half to it (115.7 m\\(^{3}\\)/s) and Pskem river approximately one third (34.4 % or 79.4 m\\(^{3}\\)/s). Nauvalisoy is only a very small river with 1.6 % runoff contribution (3.8 m\\(^{3}\\)/s). From the available data, the long-term average runoff contribution by the ungauged Koksu river can be estimated to be 6.4 m\\(^{3}\\)/s or 2.8 %. Downstream of the reservoir, Ugam river contributes an additional 9.7 % (22.4 m\\(^{3}\\)/s) to the total flow.\nLet us now turn our attention to the seasonality of the tributaries. We exclude both, the Chinaz Gauge and Gazalkent Gauge data in our analysis for the above-mentioned reason that flows there are no longer representing a natural runoff regimes but are influenced by human interference. For the analysis, we plot seasonalities of the key gauged and unregulated tributaries, i.e. Chatkal, Pskem, Nauvalisoy and Ugam rivers in Figure 2.17 and Figure 2.18 below.\n\nchirchik_river_data |&gt; \n  filter(type == 'Q', \n         code != \"16275\",\n         code != \"16262\",\n         code != \"16924\",\n         code != \"16298\",\n         code != \"16300\") |&gt; \n  na.omit() |&gt; \n  dplyr::select(date,data,code,river) |&gt; \n  group_by(code, river) |&gt; \n  plot_seasonal_diagnostics(.date_var = date,\n                            .value = data,\n                            .interactive = FALSE,\n                            .feature_set = c(\"week\",\"month.lbl\"),\n                            .title = \"\")\n\n\n\n\n\n\nFigure 2.17: Seasonality diagnostics of the two large tributaries, i.e. the Chatkal and Pskem rivers. Analyses of weekly (top row) and monthly data (bottom row) are shown. Year to year variability is pronounced during the peak runoff season in Q2 and Q3 where the amount of water in the rivers is critically determined by the amount of snow deposited in the mountains during the previous winter season.\n\n\n\n\nThe discharge seasonality of the gauging stations downstream of Charvak reservoir are shown below. Note that we only have monthly values for Ugam station which explains the appearance of the weekly plot in the upper right panel of Figure 2.18.\n\nchirchik_river_data |&gt; \n  filter(type == 'Q', \n         code != \"16275\",\n         code != \"16262\",\n         code != \"16924\",\n         code != \"16279\",\n         code != \"16290\") |&gt; \n  dplyr::select(date,data,code,river) |&gt; \n  group_by(code, river) |&gt; \n  plot_seasonal_diagnostics(.date_var = date,\n                            .value = data,\n                            .interactive = FALSE,\n                            .feature_set = c(\"week\",\"month.lbl\"),\n                            .title = \"\")\n\n\n\n\n\n\nFigure 2.18: Seasonality diagnostics of the two minor tributaries to the Chirchik River that are gauged.\n\n\n\n\nThe seasonality with the spring (small rivers) and summer (large tributaries) runoff peaks is striking in all the rivers. Nauvalisoy discharge peaks, on average, during or around week 20. Chatkal river discharge peaks around week 23 and Pskem river around week 26. These differences can be explained with the difference in mean catchment elevations which are as follows (ordered according to descending mean catchment elevation:\n\nPskem Catchment: 2’795 masl,\nChatkal catchment: 2’692 masl,\nNauvalisoy catchment: 2’160 masl,\nUgam catchment: 803 masl,\n\nwhere Ugam is the lowest lying and Pskem catchment the highest catchment when measured according to mean catchment elevation. Figure 2.19 shows the hypsometric curves of the main tributaries to the Chirchik River.\n\n\n\n\n\nFigure 2.19: Hypsometric curves of the tributaries to the Chirchik River Basin.\n\n\nUsing a LOESS smoother, we can remove discharge time series seasonality and catch a glimpse of the underlying long-term trends. This is shown for gauging station 16294, i.e. the inflow to the Charvak Reservoir, in Figure 2.20. If anything, a slightly increasing trend in mean discharge can be observed over the last 40 years. We will further discuss this finding also in the context of the analysis of the meteorological data record in the next Section.\n\nchirchik_river_data |&gt; \n  filter(code == \"16924\") |&gt; \n  na.omit() |&gt; \n  dplyr::select(date, data, code, river)  |&gt; \n  summarise_by_time(.date_var = date, .by = \"month\", value = mean(data)) |&gt; \n  plot_time_series(date, value, \n                   .x_lab = \"Date\", \n                   .y_lab = \"Discharge [m3/s]\",\n                   .interactive = FALSE,\n                   .title = \"\")\n\n\n\n\n\n\nFigure 2.20: Changes in mean monthly discharges are plotted with black lines over the entire observational record for Charvak Reservoir gauge (16924).The blue line shows a smoothed trend using a LOESS smoother.\n\n\n\n\nBut what about changes for particular seasons and months? To understand these changes, we plot monthly average data grouped together individually for all months. Figure 2.21 shows the resulting graphs together with their best fit regression lines for each month. Several interesting observations can be done.\n\nchirchik_river_data |&gt; \n  filter(code == \"16924\") |&gt; \n  dplyr::select(date, data, code, river)  |&gt; \n  summarise_by_time(.date_var = date, .by = \"month\", value = mean(data)) |&gt; \n  tk_ts(frequency = 12) |&gt; \n  forecast::ggsubseriesplot(year.labels = FALSE) + \n              geom_smooth(method = \"lm\", color = \"red\") +\n              xlab('Month') +\n              ylab('Discharge [m3/s]')\n\n\n\n\n\n\nFigure 2.21: Changes in mean monthly discharges are plotted with black lines over the entire observational record for Charvak Reservoir gauge (16924).The red lines are the per month best fit regression lines.\n\n\n\n\nFirst, cold season discharge in quarter 1 (Q1) and Q4 have a slightly increasing trend. Converse to this, the warm season quarterly trends are not uniform where Q2 trends are strongly increasing and Q3 trends are markedly decreasing. This is in line with what one would expect from a warming climate, i.e. that the snow-melt driven hydrograph peak flows shift in their timing towards earlier towards spring. At the same time, Q3 warm season discharge diminishes because of the earlier snow melt, assuming no changes in the precipitated water (compare also with the findings in Section 2.1 where a high elevation basin from the Pamir mountains is discussed). We will investigate the available climate and precipitation record of Chirchik River basin in the following section.\n\n\n\n\n\n\n\nFigure 2.22: The plates show mean, minimum and maximum quarterly discharges for Q1 (upper left plate), Q2 (upper right plate), Q3 (lower left plate) and Q4 (lower right plate). All values are in mean quarterly discharge per second.\n\n\n\n\nThe development of the quarterly minimum, maximum and mean discharge Q over the years for Gauge 16924 (Charvak reservoir inflow) is shown in Figure 2.22. The increasing trends in cold season discharge (Q1 and Q4) is confirmed. In these quarters, minimum, mean and maximum discharges appear to increase with a probably link to temperature increases during these quarters (see Section 2.2.2 for a discussion). In Q2, minimum and mean discharges have an increasing trend. In Q3, maximum discharge appears to decrease over time.\n\n2.2.2 Chirchik Basin Climate\nLong-term climate data from three different stations located in the vicinity and upstream of Charvak Reservoir is available. The stations are meteorological stations 38642 and 38339, both in Pskem River Basin, station 38471, Chatkal River Basin and station 38464 in the vicinity of the Charvak Reservoir (see also Figure 2.12 above for the locations of these stations.\nThe raw temperature data is shown in Figure 2.23 whereas the per month temperature trends are shown in Figure 2.24 and Figure 2.25. At both stations, a significant cold season warning trend is visible.\n\nchirchik_river_data |&gt; \n  filter(type == 'T') |&gt; \n  dplyr::select(date,data,station) |&gt; \n  group_by(station) |&gt; \n  plot_time_series(date,\n                   data,\n                   .facet_ncol      = 1,\n                   .interactive     = FALSE, \n                   .smooth          = TRUE,\n                   .title           = '',\n                   .x_lab = \"Date\",\n                   .y_lab = \"T [deg. C]\")\n\n\n\n\n\n\nFigure 2.23: Available decadal temperature records at Pskem and Chatkal meteorological stations. The record at the Kyrgyz Chatkal Meteo Station shows a large data gap in the post-transition years. The blue trend lines (LOESS smoother) indicate an increasing temperature trend at both mountain stations.\n\n\n\n\n\nchirchik_river_data |&gt; \n  filter(type == \"T\" & code == \"38462\") |&gt; \n  dplyr::select(date,data,code,river)  |&gt; \n  summarise_by_time(.date_var = date, .by = \"month\", value = mean(data)) |&gt; \n  tk_ts(frequency = 12) |&gt; \n  forecast::ggsubseriesplot(year.labels = FALSE) + \n              geom_smooth(method = \"lm\", color = \"red\") +\n              xlab('Month') +\n              ylab('T [deg. C]')\n\n\n\n\n\n\nFigure 2.24: Changes in mean monthly temperatures are plotted with black lines over the entire observational record for Pskem meteorological station.The red lines are the per month best fit regression lines.\n\n\n\n\n\nchirchik_river_data |&gt; \n  filter(type == \"T\" & code == \"38471\") |&gt; \n  dplyr::select(date, data, code, river)  |&gt; \n  summarise_by_time(.date_var = date, .by = \"month\", value = mean(data)) |&gt; \n  tk_ts(frequency = 12) |&gt; \n  forecast::ggsubseriesplot(year.labels = FALSE) + \n              geom_smooth(method = \"lm\", color = \"red\") +\n              xlab('month') +\n              ylab('deg. C.')\n\n\n\n\n\n\nFigure 2.25: Changes in mean monthly temperatures are plotted with black lines over the entire observational record for Chatkal meteorological station.The red lines are the per month best fit regression lines. The one spike in the September bin is an erroneous time series entry.\n\n\n\n\nThe increasing cold season temperatures have an impact on the snow fractions in the basins, i.e., on the fraction falling as solid precipitation versus total precipitation. Generally speaking, warming winter temperatures will lead to an increase in elevation of the snowline and thus reduce the area and volume of the winter snow deposits.\nSimilarly to the analysis carried out above for the development of quarterly flows, we can analyze the development of quarterly temperature statistics. Figure 2.26 shows the results.\n\nCodequarterT_mean &lt;- chirchik_river_data |&gt; \n  filter(type == \"T\" & code == \"38471\") |&gt; \n  dplyr::select(date, data, code, river)  |&gt; \n  summarise_by_time(.date_var = date, .by = \"quarter\",value = mean(data, na.rm = TRUE)) |&gt; \n  rename(mean = value) |&gt; \n  na.omit()\n\nquarterT_max &lt;- chirchik_river_data |&gt; \n  filter(type == \"T\" & code == \"38471\") |&gt; \n  dplyr::select(date,data,code,river)  |&gt; \n  summarise_by_time(.date_var = date, .by = \"quarter\",value = max(data, na.rm = TRUE)) |&gt; \n  rename(max = value) |&gt; \n  na.omit()\n\nquarterT_min &lt;- chirchik_river_data |&gt; \n  filter(type == \"T\" & code == \"38471\") |&gt; \n  dplyr::select(date, data, code, river)  |&gt; \n  summarise_by_time(.date_var = date,.by = \"quarter\",value = min(data, na.rm = TRUE)) |&gt; \n  rename(min = value) |&gt; na.omit()\n\nquarterT &lt;- left_join(quarterT_mean, quarterT_max, by = 'date')\nquarterT &lt;- left_join(quarterT, quarterT_min, by = 'date')\n\nquarterT &lt;- \n  bind_cols(quarterT, quarterT$date |&gt; \n              tsibble::yearquarter()) |&gt; \n  rename(quarter = '...5')\nquarterT$quarter &lt;- quarterT$quarter |&gt; \n  format(., format = \"Q%q\")\n\nq1T &lt;- quarterT |&gt; filter(quarter == 'Q1') |&gt; dplyr::select(-quarter) |&gt; pivot_longer(-date)\nq2T &lt;- quarterT |&gt; filter(quarter == 'Q2') |&gt; dplyr::select(-quarter) |&gt; pivot_longer(-date)\nq3T &lt;- quarterT |&gt; filter(quarter == 'Q3') |&gt; dplyr::select(-quarter) |&gt; pivot_longer(-date)\nq4T &lt;- quarterT |&gt; filter(quarter == 'Q4') |&gt; dplyr::select(-quarter) |&gt; pivot_longer(-date)\n\npQ1 &lt;- q1T  |&gt; \n  plot_time_series(.date_var = date,.value = value,.color_var = name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',.title = \"Quarter Q1\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"Date\", .y_lab = \"T [deg. C]\",.legend_show = TRUE)\npQ2 &lt;- q2T  |&gt; \n  plot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',.title = \"Quarter Q2\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"Date\",.y_lab = \"T [deg. C]\",.legend_show = FALSE)\npQ3 &lt;- q3T  |&gt;\nplot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',.title = \"Quarter Q3\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"Date\",.y_lab = \"T [deg. C]\", .legend_show = FALSE)\npQ4 &lt;- q4T  |&gt; na.omit() |&gt;\nplot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',.title = \"Quarter Q4\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"Date\",.y_lab = \"T [deg. C]\",.legend_show = FALSE)\n\npQ1 + pQ2 + pQ3 + pQ4 + plot_layout(guides = 'collect') & theme(legend.position = 'bottom')\n\n\n\n\n\n\nFigure 2.26: Development of mean, minimum and maximum quarterly temperatures for Q1 at Station 38462.\n\n\n\n\nApart from a generally increasing trend in temperature, especially in the cold seasons, there are also significant positive anomalies since the year 2000 at Station 38462 in the maximum winter (Q1) temperatures. Changes in the fraction of precipitation falling as snow are expected under such developments.\nWe can in fact compute these changes with high resolution daily climate fields that are nowadays available (see Karger et al. (2017), Karger et al. (2020) and Karger et al. (2021) and also more on these in the corresponding Section on Snow and Glacier Data further below). Figure 2.27 shows the corresponding results.\n\n\n\n\n\nFigure 2.27: Changes in fractional snow cover in the in the Western Tien Shan mountains, including the Chirchik River basin. Shown are percentage changes over the period 1980 - 2011. Changes in snow cover fraction was computed using CHELSA V21 (Karger et al. 2017) daily precipitation and temperature with a liquid-solid temperature threshold value of 1 deg. Celsius. Source: hydrosolutions GmbH.\n\n\nThe atmospheric warming between 1980 and 2011 (this is period for which daily precipitation and temperature fields are available at 1 km2 resolution) has lead to a reduction of the fraction of the total precipitation falling as snow between 5 - 24 % within the elevation range from 800 masl to 1’500 masl, approx. Changes for pixels where the trend is not significant at the 95 % confidence level are not shown, i.e., they are transparent. In the region shown, no significant positive trends could be identified over the period of consideration. Finally, at higher elevations in the Chirchik River basin, no significant changes can be seen in most places.\nNow we look into the development of precipitation over the available record of station data.\n\nchirchik_river_data |&gt; \n  filter(type == 'P') |&gt; \n  dplyr::select(date,data,station) |&gt; \n  group_by(station) |&gt; \n  plot_time_series(date,\n                   data,\n                   .facet_ncol      = 1,\n                   .interactive     = FALSE, \n                   .smooth          = FALSE,\n                   .title           = '')\n\n\n\n\n\n\nFigure 2.28: Available decadal and monthly data records from different meteorological stations that are located in the zone of runoff formation. As in the case of temperature, the precipitation record at the Kyrgyz Chatkal Meteo Station shows a large data gap in the post-transition years.\n\n\n\n\n\nCodequarterP_mean &lt;- chirchik_river_data |&gt; \n  filter(type == \"P\" & code == \"38464\") |&gt; \n  dplyr::select(date,data,code,river)  |&gt; \n  summarise_by_time(.date_var = date, .by = \"quarter\",value = mean(data,na.rm = TRUE)) |&gt; \n  rename(mean = value) |&gt; na.omit()\n\nquarterP_max &lt;- chirchik_river_data |&gt; \n  filter(type == \"P\" & code == \"38464\") |&gt; \n  dplyr::select(date,data,code,river)  |&gt; \n  summarise_by_time(.date_var = date, .by = \"quarter\",value = max(data,na.rm = TRUE)) |&gt; \n  rename(max = value) |&gt; na.omit()\n\nquarterP_min &lt;- chirchik_river_data |&gt; \n  filter(type == \"P\" & code == \"38464\") |&gt; \n  dplyr::select(date,data,code,river)  |&gt; \n  summarise_by_time(.date_var = date,.by = \"quarter\",value = min(data,na.rm = TRUE)) |&gt; \n  rename(min = value) |&gt; na.omit()\n\nquarterP &lt;- left_join(quarterP_mean, quarterP_max, by = 'date')\nquarterP &lt;- left_join(quarterP, quarterP_min, by = 'date')\n\nquarterP &lt;- \n  bind_cols(quarterP,quarterP$date |&gt; \n              tsibble::yearquarter()) |&gt; \n  rename(quarter = '...5')\nquarterP$quarter &lt;- quarterP$quarter |&gt; \n  format(., format = \"Q%q\")\n\nq1P &lt;- quarterP |&gt; filter(quarter == 'Q1') |&gt; dplyr::select(-quarter) |&gt; pivot_longer(-date)\nq2P &lt;- quarterP |&gt; filter(quarter == 'Q2') |&gt; dplyr::select(-quarter) |&gt; pivot_longer(-date)\nq3P &lt;- quarterP |&gt; filter(quarter == 'Q3') |&gt; dplyr::select(-quarter) |&gt; pivot_longer(-date)\nq4P &lt;- quarterP |&gt; filter(quarter == 'Q4') |&gt; dplyr::select(-quarter) |&gt; pivot_longer(-date)\n\npQ1 &lt;- q1P  |&gt; #group_by(name) |&gt; \n  plot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',\n      .title = \"Quarter Q1\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"Date\", .y_lab = \"P [mm/10 days]\",.legend_show = TRUE\n      )\npQ2 &lt;- q2P  |&gt; #group_by(name) |&gt; \n  plot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',\n      .title = \"Quarter Q2\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"Date\",.y_lab = \"P [mm/10 days]\",.legend_show = FALSE\n      )\npQ3 &lt;- q3P  |&gt; #group_by(name) |&gt; \nplot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',\n      .title = \"Quarter Q3\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"Date\",.y_lab = \"P [mm/10 days]\", .legend_show = FALSE\n      )\npQ4 &lt;- q4P  |&gt; na.omit() |&gt; #group_by(name) |&gt; \nplot_time_series(date,value,name,\n      .smooth = TRUE, .smooth_degree = 1, .smooth_period = 'auto',\n      .title = \"Quarter Q4\", .interactive = FALSE, .facet_ncol = 1,\n      .x_lab = \"Date\",.y_lab = \"P [mm/10 days]\",.legend_show = FALSE\n      )\n\npQ1 + pQ2 + pQ3 + pQ4 + plot_layout(guides = 'collect') & theme(legend.position = 'bottom')\n\n\n\n\n\n\nFigure 2.29: Development of mean, minimum and maximum quarterly precipitation at Station 38462 in the Chirchik River basin.\n\n\n\n\nFigure 2.29 shows that at the meteorological station 38462, precipitation levels have been increasing until the 1980ies for Q3 and Q4. Contrary to that, there is a slightly increasing trend in Q1 since the year 2000.\nIn summary, the general global warming trend is clearly visible in in-situ station records as well as in high-resolution climatologies. As our quick analysis of the changes in snow cover fractions show, the warming translates into less snow cover in intermediate elevation ranges but not in the high-mountain zones where the critical snow storage for warm season runoff is found. This might be one of the possible explanations that we do not see any reduction in observed discharge in the basin.\n\n2.2.3 Discharge Estimation from the Ungauged Kosku Tributary\nThe construction of the Charvak reservoir and the subsequent water impoundment of the rivers water in the reservoir led to a destruction of the previous gauge that measured Chatkal discharge, including the contribution from the Koksu right-tributary. To be able to estimate the Koksu discharge contribution to the Charvak reservoir, even after the impoundment, an empirical relationship between Chatkal river discharge and Koksu was established. The procedure allows for the more or less precise estimation of Koksu discharge, even in the absence of direct measurements, as described below.\n\n\n\n\n\n\nNote\n\n\n\nThe model shown here to estimate Koksu discharge using measured quantities at Khudaydod is a simple type of an empirical model. You will learn more about such types of models in Chapter 10.\n\n\nBefore the closure of the Charvak dam and the subsequent filling of the reservoir in and after 1974, the Uzbek experts from the Hydrometeorological Agency started a detailed 3-years measurement comparison campaign at Charvak gauge and at gauge 16279 in Khudaydod (see Figure 2.12). Both are located on Chatkal river\nThe confluence of Koksu river with Charvak river is just upstream of the former Charvak gauge and hence between gauge 16279 and the former Charvak gauge. Using daily data from the measurement comparisons campaign, Charvak gauge discharge was related to Khudaydod discharge using a linear relationship. At the same time, they were now able to relate Koksu discharge to the discharge at Chatkal River in Khudaydod as shown in Equation 2.1.\n\\[\nQ_{Koksu} \\propto Q_{Charvak} - Q_{16279}\n\\tag{2.1}\\]",
    "crumbs": [
      "Part I: Hydrology of Semi-Arid Central Asia",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Case Study River Basins</span>"
    ]
  },
  {
    "objectID": "example_river_basins.html#footnotes",
    "href": "example_river_basins.html#footnotes",
    "title": "2  Case Study River Basins",
    "section": "",
    "text": "The data was kindly provided by the Tajik Hydrometeorological Agency located in Dushanbe, Tajikistan.↩︎",
    "crumbs": [
      "Part I: Hydrology of Semi-Arid Central Asia",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Case Study River Basins</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Part II: Data Sources, Retrieval and Preparation",
    "section": "",
    "text": "In this Chapter and its Sections, we will discuss how to retrieve, prepare and process the data that is required for hydrological modeling. Data include\n\ngeospatial data,\nin-situ station data,\nsnow and glacier data, and\nclimate reanalysis and projections data.\n\nAs will become clear, the preparation of these data requires a substantial amount of work, local storage space and, in some instances, computational power. However, modern GIS tools and data processing libraries in Python and R make the preparation of these data more accessible than ever before. In some cases, free online tools can be used to retrieve and process data.\n\n\n\n\n\n\nImportant\n\n\n\nData needs vary according to the modeling approach. However, they always entail first a definition of the geometry of the river basin under consideration, second a subdivision of the basin into hydrological response units (HRUs) and third, the production of relevant time series for the individual HRUs, including climate forcing, glacier contributions, information on snow cover, etc..\n\n\nAn overview of all data used is provided in the Chapter 3 Sources of Relevant Data. The preparation of the discharge data as described in Chapter 4 Discharge Station Data demonstrates the necessary steps for the quality control of the observed station data. This is an important step prior to any hydrological modeling, independent of the modeling approach used.\nThe Chapter 5 on Geospatial Data is a chapter on using GIS tools to prepare for hydrological modeling. It shows the necessary geospatial analysis steps for basin delineation and the generation of the required input files for physically-based modeling using RSMinerve. These geospatial assets can then be loaded and processed in RSMinerve as shown in PartIII Chapter 8 Hydrological-Hydraulic Modeling. The can also be used in other contexts.\nChapter 6 introduces available snow and glacier data that can be used for hydrological modeling. It also demonstrates the generation of time series data for individual hydrological response units with regard to glacier contributions. The Chapter 7 Climate Data discussed state-of-the-art high-resolution climate data for past observations of precipitation and temperature as well as climate projections and the processing and preparation of these data for hydrological modeling.\nAs an example, the following diagram shows an entire modeling chain for hydrological modeling for climate change impact assessments. It shows that the data preparation step involves many interrelated components that partially depend on each other in a sequential way. This Chapter aims at carefully working through this modeling chain to demonstrate the preparation of all the relevant data for the subsequent modeling application.\n\n\n\n\n\n\nFigure 1: Example hydrological modeling chain that include data preparation, the implementation of the hydrological model and the analysis of results. The data preparation steps are a very important part of hydrological modeling.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation"
    ]
  },
  {
    "objectID": "data_sources.html",
    "href": "data_sources.html",
    "title": "3  Sources of Relevant Data",
    "section": "",
    "text": "3.1 Gauge Location & Norm Discharge Data\nThe Central Asia Hydrometeorological Organizations are the organizations administering discharge data. Unfortunately, the organizations do not have a good understanding of most of the past and current locations where they monitor this discharge. This can be verified by checking the website Hydrometeorological Services in Central Asia where many of the shown gauges are not correctly georeferenced.\nThis is problematic for many reasons, especially for GIS analysis and hydrological modeling. With a focus on the mountain rivers, we developed a workflow to verify gauge locations communicated by Hydrometeorological Organizations in a consistent and comprehensive manner. With the goal of establishing a comprehensive archive, this workflow was applied to map operational stations and past stations.\nFirst, data from all existing gauges and locations were entered into a data frame with the following attributes:\nSecond, data on historic stations were obtained from the Soviet compendia Surface Water Resources, Vol 14 Issues 1 and 3. Third, data from northern Afghan rivers’ stream flow characteristics and the location of gauging stations there were obtained from (Olson and Williams-Sether 2010). Norm discharge was calculated from the available data.\nAll stations were manually located in a Geographic Information System (GIS) using the relevant Soviet Military Topographic maps (1:200’000) from the corresponding region. The maps were downloaded from https://maps.vlasenko.net and subsequently georeferenced in QGIS with the Raster/Georeferencr tool there (QGIS Development Team 2021). Where possible, high-resolution satellite imagery from Microsoft® Bing™ Maps was used to identify measurement bridges and for final validation of the identified locations.\nThe upstream catchments for each gauge were then mapped in R using the WhiteboxTools v.2.0.0. Like this, we have compiled a list of 297 gauges with their contributing catchments. Figure 3.1 shows the result. For more information, please see (Marti et al. 2023).\nFor all basins, mean statistics were computed from the digital elevation model, the land cover data, climatological and glacier data, as well as from 1979 through 2012 for mean average monthly temperature and precipitation, and finally, the monthly snow cover fraction derived from MODIS Terra and Aqua Snow Cover Data Product which is available from 2001 through the end of 2021.\nThe norm discharge data presented here are used for the long-term water balance modeling presented in Chapter 9 in Part III of the book. For selected stations and river basins, we have time series available that can be used to calibrate time-dependent models. Some of these data and their processing will be presented and discussed in Chapter 4.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sources of Relevant Data</span>"
    ]
  },
  {
    "objectID": "data_sources.html#sec-discharge-gauge-location",
    "href": "data_sources.html#sec-discharge-gauge-location",
    "title": "3  Sources of Relevant Data",
    "section": "",
    "text": "CODE: Official gauge code in use\n\nEASTING and NORTHING: Coordinates in UTM42N\n\nLON and LAT: Longitude and latitude\n\nNAME_ENG: Translated English station name\n\nNAME_RU: Original station name\n\nRIVER: Name of river\n\nBASIN: Name of the larger basin\n\nCOUNTRY: The country the gauge is located in\n\nQ_NORM_M3S: Long-term norm discharge\n\nOBS_PERIOD: Observation period of long-term norm discharge\n\nSOURCE: Source of information on norm discharge\n\n\n\n\n\n\n\n\n\nFigure 3.1: Geographic overview over mapped basins. The pastel colors encode the mountainous catchments of the corresponding large basins. See Siegfried et al. (2023) for more information.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sources of Relevant Data</span>"
    ]
  },
  {
    "objectID": "data_sources.html#sec-vector-data-sources",
    "href": "data_sources.html#sec-vector-data-sources",
    "title": "3  Sources of Relevant Data",
    "section": "\n3.2 Vector data",
    "text": "3.2 Vector data\nFor our purpose, we define the area of interest (AOI) as 55 deg. E - 85. deg. E and 30 deg. N - 50 deg. N..\nGlobal political boundaries can be obtained from the Global Administrative Divisions database at gadm.org. Except for Turkmenistan, data on first (Oblast) and second-level (Rayon) administrative divisions is available for all Central Asian states.\nShapefiles of large river basins can be retrieved from the Global Runoff Data Center. Note that for the Central Asian region, the flat downstream areas of these basins are delineating natural hydrological borders. They do not account for man-made inter-basin transfers and thus would need to be corrected where necessary.\nThe river network can be obtained from the Global Runoff Data Center WMOBB data that was released in 2021. It can be downloaded via this link. With 161 MB, approx., data covering the whole globe can be downloaded in a straightforward manner. The global data can be clipped easily with the bounding box defined above.\nData for the large rivers can be extracted from the layer wmobb_rivnets_Q09_10 (containing line sections representing an upland area above 4’504 km2). The layer called wmobb_rivnets_Q08_09 contains line sections representing an upland area between 1’150 km2 and 4’504 km2 and, finally, the wmobb_rivnets_Q07_08 (containing line sections representing an upland area above between 487 and 1’150 km2) (GRDC, Koblenz, Germany: Federal Institute of Hydrology (BfG). 2020). Smaller rivers can be further added to a QGIS project on a case-per-case basis using the additional data sets in the files obtained from GRDC.\nPermanent water bodies and courses can be obtained from the global HydroLakes Database (Messager et al. 2016). It can be downloaded via this link.\nGlacier data can be taken from the Randolph Glacier Inventory (RGI) 6.0. The inventory contains a global archive of glacier outlines and can be obtained via this website. Information from 16’617 glaciers is available in the AOI. The corresponding data on glacier thickness and glacier thinning rates is available as complementary data Hugonnet et al. (2021). More information on these data can be found in Chapter 6, whereas glacier models are discussed in a separate Section in the Chapter on Hydrological Modeling.\nData on dams is available from the GOODD data set. Information from 88 dams in the region of interest is mapped (Mulligan, Soesbergen, and Sáenz 2020). The data is available from Global Dams Watch.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sources of Relevant Data</span>"
    ]
  },
  {
    "objectID": "data_sources.html#sec-raster-data-sources",
    "href": "data_sources.html#sec-raster-data-sources",
    "title": "3  Sources of Relevant Data",
    "section": "\n3.3 Raster Data",
    "text": "3.3 Raster Data\nThe NASA SRTM digital elevation model 1 Arc-second (30 m) global product is used as a DEM (“NASA Shuttle Radar Topography Mission (SRTM)” 2013). There are many ways to access these data, some more, some less convenient. An easy way to access these data is in QGIS by using the SRTM-Downloader Plugin. For web-based access, it is recommended to utilize the NASA EarthExplorer. Sample instructions on how to download DEM data from the EarthExplorer can, for example, be found by watching the following YouTube tutorial.\n\n\n\n\n\n\n\n\n\nLand cover information can be obtained from the Copernicus Global Land Service: Land Cover 100m: collection 3: epoch 2019: Globe data (Buchhorn et al. 2019). The Global Land Cover Viewer allows to access, view and download annual land cover data from 2015 - 2019.\nHigh-resolution climate data can be obtained from the Climatologies at High Resolution for the Earth’s Land Surface Areas (CHELSA) dataset via www.chelsa-climate.org. For the Version 2.1 product, climatologies for the periods 1981 - 2010, 2011 - 2040, 2041 - 2070, and 2071 - 2100 for a large number of variables are available for download as GeoTiff-files. With regard to the daily data, it is recommended to use the Global daily 1km land surface precipitation based on cloud cover-informed downscaling. This precipitation product reflects actual conditions in High-Mountain Asia in a much better way than the precipitation from the CHELSA-W5E5 V1.1 product. For daily temperature, the data from the CHELSA-W5E5 V1.1 product can be downloaded for a given domain of interest via this link.\n\n\n\n\n\n\nWarning\n\n\n\nNote that the daily high-resolution climate fields for the entire Central Asia domain require a lot of storage space. Their processing for later analysis is computationally intensive.\n\n\nThe FLO1K, global maps of mean, maximum and minimum annual stream flow at 1 km resolution from 1960 through 2015 can be retrieved from this web,site. FLO1K delivers relevant data for water resources analyses at a global scale and yet high spatial resolution (Barbarossa et al. 2018). These data can be helpful for long-term water balance assessments and for studying the hydropower potential in the high mountain regions where flow measurements are sparse.\nThe CHELSA V21 global daily high-resolution climatology, available from 01-01-1979 until 31-12-2011 was processed over the Central Asia domain to map climate trends, including on temperature, precipitation, and snow fraction. The data is available upon request from this site: https://chelsa-climate.org Karger et al. (2021). The CHELSA V21 product is corrected for snow undercatch in the high elevation ranges and thus can represent better actual high mountain precipitation than other available global climatologies (Beck et al. 2020). The aridity index (AI) fields were taken from the bio-climate CHELSA V21 data set and compared with the CGIAR AI product (Trabucco and Zomer 2019). Data on an additional 70 bio-climatic indicators were downloaded from the CHELSA V21 1980 - 2010 climatology, and statistics were extracted for each gauged catchment, together with the AI.\nHigh-resolution crop disaggregated irrigated areas were mapped over the entire Central Asia domain by hydrosolutions GmbH (see also (Ragettli, Herberz, and Siegfried 2018) for more information). Like this, 30 m crop maps were produced with Google Earth Engine using unsupervised classification for 2016 - 2020. These maps, in conjunction with estimates of irrigation water intake volumes and estimates of actual evapotranspiration help in irrigation scheme performance assessments. In hydrological modeling, these data can be used to introduce sectoral consumption estimates and help develop sound and effective basin planning.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sources of Relevant Data</span>"
    ]
  },
  {
    "objectID": "data_sources.html#sec-data-sources-references",
    "href": "data_sources.html#sec-data-sources-references",
    "title": "3  Sources of Relevant Data",
    "section": "\n3.4 References",
    "text": "3.4 References\n\n\n\n\nBarbarossa, Valerio, Mark A. J. Huijbregts, Arthur H. W. Beusen, Hylke E. Beck, Henry King, and Aafke M. Schipper. 2018. “FLO1K, Global Maps of Mean, Maximum and Minimum Annual Streamflow at 1 Km Resolution from 1960 Through 2015.” Scientific Data 5 (1): 180052. https://doi.org/10.1038/sdata.2018.52.\n\n\nBeck, Hylke E., Eric F. Wood, Tim R. McVicar, Mauricio Zambrano-Bigiarini, Camila Alvarez-Garreton, Oscar M. Baez-Villanueva, Justin Sheffield, and Dirk N. Karger. 2020. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1.\n\n\nBuchhorn, M., B. Smets, L. Bertels, B. De Roo, M. Lesiv, N. E. Tsendbazar, M. Herold, and S. Fritz. 2019. “Copernicus Global Land Service: Land Cover 100m: Collection 3: Epoch 2019: Globe.”\n\n\nFarinotti, Daniel, Matthias Huss, Johannes J. Fürst, Johannes Landmann, Horst Machguth, Fabien Maussion, and Ankur Pandit. 2019. “A Consensus Estimate for the Ice Thickness Distribution of All Glaciers on Earth.” Nature Geoscience 12 (3): 168–73. https://doi.org/10.1038/s41561-019-0300-3.\n\n\nGRDC, Koblenz, Germany: Federal Institute of Hydrology (BfG). 2020. “Major River Basins of the World / Global Runoff Data Centre, GRDC. 2nd, Rev. Ext. Ed.” Shape.\n\n\nHugonnet, Romain, Robert McNabb, Etienne Berthier, Brian Menounos, Christopher Nuth, Luc Girod, Daniel Farinotti, et al. 2021. “Accelerated Global Glacier Mass Loss in the Early Twenty-First Century.” Nature 592 (7856): 726–31. https://doi.org/10.1038/s41586-021-03436-z.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nKarger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-resolution monthly precipitation and temperature time series from 2006 to 2100.” Scientific Data 7 (1): 248. https://doi.org/10.1038/s41597-020-00587-y.\n\n\nKarger, Dirk Nikolaus, Adam M. Wilson, Colin Mahony, Niklaus E. Zimmermann, and Walter Jetz. 2021. “Global daily 1 km land surface precipitation based on cloud cover-informed downscaling.” Scientific Data 8 (1): 307. https://doi.org/10.1038/s41597-021-01084-6.\n\n\nMarti, Beatrice, Andrey Yakovlev, Dirk Nikolaus Karger, Silvan Ragettli, Aidar Zhumabaev, Abdul Wakil Wakil, and Tobias Siegfried. 2023. “CA-Discharge: Geo-Located Discharge Time Series for Mountainous Rivers in Central Asia.” Scientific Data 10 (1): 579. https://doi.org/10.1038/s41597-023-02474-8.\n\n\nMessager, M. L., B. Lehner, Grill G., I. Nedeva, and O. Schmitt. 2016. “Estimating the Volume and Age of Water Stored in Global Lakes Using a Geo-Statistical Approach.” Nature Communications 13603.\n\n\nMulligan, Mark, Arnout van Soesbergen, and Leonardo Sáenz. 2020. “GOODD, a Global Dataset of More Than 38,000 Georeferenced Dams.” Scientific Data 7 (1): 31. https://doi.org/10.1038/s41597-020-0362-5.\n\n\n“NASA Shuttle Radar Topography Mission (SRTM).” 2013. NASA. https://earthdata.nasa.gov/learn/articles/nasa-shuttle-radar-topography-mission-srtm-version-3-0-global-1-arc-second-data-released-over-asia-and-australia.\n\n\nOlson, S. A., and T. Williams-Sether. 2010. “Streamflow Characteristics at Streamgages in Northern Afghanistan and Selected Locations.” U.S. Geological Survey Data Series 529. USGS.\n\n\nQGIS Development Team. 2021. QGIS Geographic Information System. QGIS Association.\n\n\nRagettli, Silvan, Timo Herberz, and Tobias Siegfried. 2018. “An Unsupervised Classification Algorithm for Multi- Temporal Irrigated Area Mapping in Central Asia.” Remote Sensing 10 (11): 1823. https://doi.org/10.3390/rs10111823.\n\n\nSiegfried, Tobias, Aziz Ul Haq Mujahid, Beatrice Marti, Peter Molnar, Dirk Nikolaus Karger, and Andrey Yakovlev. 2023. “Unveiling the Future Water Pulse of Central Asia: A Comprehensive 21st Century Hydrological Forecast from Stochastic Water Balance Modeling.” https://doi.org/https://doi.org/10.21203/rs.3.rs-3611140/v1.\n\n\nTrabucco, Antonio, and Robert Zomer. 2019. “Global Aridity Index and Potential Evapotranspiration (ET0) Climate Database v2,” January. https://doi.org/10.6084/m9.figshare.7504448.v3.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sources of Relevant Data</span>"
    ]
  },
  {
    "objectID": "discharge_station_data.html",
    "href": "discharge_station_data.html",
    "title": "4  Discharge Station Data",
    "section": "",
    "text": "4.1 Available Data\nThe riversCentralAsia Package provides available data on the gauging and meteorological stations in the Chirchik River Basin (where other data are used, and their source and access options are indicated). This is the time to install and load the package with the following commands.\ndevtools::install_github(\"hydrosolutions/riversCentralAsia\") # download package from github\nlibrary('riversCentralAsia') # load package\nBefore starting any type of modeling, it is important to get a good understanding of the data that we are dealing with and whether there exist problems with the raw data that need to be addressed prior to the modeling step. This is actually also one of the more hidden agendas when doing a basin characterization, i.e. to detect such possible issues present in the available data.\nProblems in real-world data usually include data gaps and outliers as data records that one obtains are usually neither complete nor cleaned (of errors).\nThe steps performed here are thus required steps for any type of successful modeling and should be performed with great care prior to starting hydrological modeling.\nWe concentrate our efforts here on discharge records and data from meteorological stations in the Chirchik River Basin for demonstration purposes. The techniques shown here for decadal (10-days) data naturally extend to monthly data and also, to data from other basins and other sources.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discharge Station Data</span>"
    ]
  },
  {
    "objectID": "discharge_station_data.html#sec-available-data",
    "href": "discharge_station_data.html#sec-available-data",
    "title": "4  Discharge Station Data",
    "section": "",
    "text": "Garbage in - Garbage out\n\n\n\nThe importance of good quality data for modeling cannot be overstated. It can very easily be summarized in the following way\n\nData \\(\\rightarrow\\) Model \\(\\rightarrow\\) Results\n\nIf the underlying data is erroneous, then this translated into\n\nGarbage in \\(\\rightarrow\\) Model \\(\\rightarrow\\) Garbage out",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discharge Station Data</span>"
    ]
  },
  {
    "objectID": "discharge_station_data.html#sec-gap-filling-discharge-data",
    "href": "discharge_station_data.html#sec-gap-filling-discharge-data",
    "title": "4  Discharge Station Data",
    "section": "\n4.2 Gap Filling Discharge Data",
    "text": "4.2 Gap Filling Discharge Data\nIn the following, we will work with decadal discharge data from the two main tributaries of the Chirchik River, i.e. the Chatkal River (Gauge 16279) and the Pskem River (Gauge 16290) as well as on the data of the inflow to the Charvak reservoir (Gauge 16924). The goal is to analyze the data and prepare for modeling. First, let us load the relevant discharge data. The data are stored in the ChirchikRiverBasin object.\n\ndata &lt;- ChirchikRiverBasin # load data\nq_dec_tbl &lt;- data |&gt; filter(code == '16279' | code == '16290' | code == '16924') # Note for the new name of the data object, we use snake notation. We choose to add periodicity (_dec_) and data type (_tbl for tibble/dataframe) to the data name. This just helps to stay organized and is good practice in R programming.\nq_dec_tbl\n\n# A tibble: 9,072 × 14\n   date        data  norm units type  code  station   river   basin   resolution\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;fct&gt;     \n 1 1932-01-10  48.8  38.8 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 2 1932-01-20  48.4  37.5 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 3 1932-01-31  42.4  36.6 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 4 1932-02-10  43.7  36.4 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 5 1932-02-20  44.2  36.3 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 6 1932-02-29  47.7  36.9 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 7 1932-03-10  54.1  39.4 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 8 1932-03-20  63.2  47.6 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 9 1932-03-31 103    60.5 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n10 1932-04-10 103    86.4 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n# ℹ 9,062 more rows\n# ℹ 4 more variables: lon_UTM42 &lt;dbl&gt;, lat_UTM42 &lt;dbl&gt;, altitude_masl &lt;dbl&gt;,\n#   basinSize_sqkm &lt;dbl&gt;\n\n\nYou can get more information about the available data by typing ?ChirchikRiverBasin. Note that the original time series data has been packaged in this format by the riversCentralAsia::loadTabularData() function which takes a simple .csv file as input.\nIt is advisable to check at this stage for missing data in time series and to fill gaps where present. Are there missing data? How can these be filled so as to arrive at complete time series that are required for hydrological modeling? These are the key questions that we will address in the following.\nAs can be seen in Figure 4.1, close inspection of the time series indeed reveals some missing data in the 1940ies. The missing data are indicated by gaps in the time series. The gaps are clearly visible in the time series of the Pskem River (Gauge 16290) and the Charvak reservoir (Gauge 16924). The inflow to the Chatkal River (Gauge 16279) time series appears complete and does not show any missing data. However, we will see that this is not the case.\n\n\n\n\n\n\nTip\n\n\n\nNote, Figure 4.1 is an interactive figure where you can zoom in. Try it and zoom into the 1940ies to visualize the missing data fore clearly. You can zoom out again by clicking the Autoscale hover over. For the visualization of time series, we normally use the excellent timetk R Package. Check it out and try yourself!\n\n\n\nq_dec_tbl |&gt; plot_time_series(date,data,\n                               .facet_vars  = code,\n                               .smooth      = FALSE,\n                               .interactive = TRUE,\n                               .x_lab       = \"year\",\n                               .y_lab       = \"m^3/s\",\n                               .title       = \"\"\n                               )\n\n\n\n\n\n\nFigure 4.1: Discharge data of selected gauges in the upstream zone of runoff formation in the Chirchik River Basin. Data Source: Uzbek Hydrometeorological Service.\n\n\n\nMissing data are also confirmed by the warning that the function timetk::plot_time_series() throws (suppressed here). Statistics of the missing data can be easily obtained. As the Table below shows, we can do this analysis for each discharge station separately.\n\nq_dec_tbl |&gt; group_by(code) |&gt; \n  summarize(n.na = sum(is.na(data)), na.perc = n.na/n()*100)\n\n# A tibble: 3 × 3\n  code   n.na na.perc\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;\n1 16279    15   0.496\n2 16290    39   1.29 \n3 16924    42   1.39 \n\n\nSummarizing the number of observation with missing data reveals that 15 data points for station 16279 (0.5 % of total record length) and 39 for station 16290 (1.3 % of total record length) are missing. As there are only very few gaps in the existing time series, we use a simple method to fill these. Wherever there is a gap, we fill in the corresponding decadal norm as stored in the norm column in the object q_dec_tbl at the time stamp of the missing data. The visualization of the results confirms that our simple gap filling approach is indeed satisfactory (see Figure 4.2).\n\n# Make a copy of the original data\nq_dec_filled_tbl &lt;- q_dec_tbl\n\n# Actual gap filling step\nq_dec_filled_tbl$data[is.na(q_dec_filled_tbl$data)] = \n  q_dec_filled_tbl$norm[is.na(q_dec_filled_tbl$data)] \n\n# Inspect results\nq_dec_filled_tbl |&gt; plot_time_series(date, data, \n                                      .facet_vars  = code, \n                                      .smooth      = FALSE,\n                                      .interactive = TRUE,\n                                      .x_lab       = \"year\",\n                                      .y_lab       = \"m^3/s\",\n                                      .title       = \"\"\n                                      )\n\n\n\n\n\n\nFigure 4.2: Gap filled Pskem and Chatkal river discharges.\n\n\n\nAll missing data are gone now as can easily be validated. The number of missing data points is zero for all stations. The results are summarized in the Table below.\n\nq_dec_filled_tbl |&gt; group_by(code) |&gt; \n  summarize(n.na = sum(is.na(data)), na.perc = n.na/n()*100)\n\n# A tibble: 3 × 3\n  code   n.na na.perc\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;\n1 16279     0       0\n2 16290     0       0\n3 16924     0       0\n\n\nA note of caution here. This simple gap filling technique reduces variance in the time series. It should only be used when the percentage of missing data is low. As will be discussed in the next Section Section 4.3 below, more sophisticated techniques should be utilized when there exist substantial gaps and in the case of less regular data.\nFinally, we discard the data that we no longer need, including the norm data, which we used for gap filling of the missing discharge data and convert the data to wide format (see Table 4.1 below) to add to it meteorological data in the next Section.\n\n\nTable 4.1\n\nq_dec_filled_wide_tbl &lt;- q_dec_filled_tbl |&gt; # again we use the name convention of objects as introduced above\n  mutate(code = paste0('Q',code |&gt; as.character())) |&gt; # Since we convert everything to long form, we want to keep information as compact as possible. Hence, we paste the type identifier (Q for discharge here) in from of the 5 digit station code.\n  dplyr::select(date,data,code) |&gt; # ... and then ditch all the remainder information\n  pivot_wider(names_from = \"code\",values_from = \"data\") # in order to pivot to the long format, we need to make a small detour via the wide format.\n\nq_dec_filled_long_tbl &lt;- q_dec_filled_wide_tbl |&gt; pivot_longer(-date) # and then pivot back\nq_dec_filled_wide_tbl\n\n# A tibble: 3,024 × 4\n   date       Q16279 Q16290 Q16924\n   &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 1932-01-10   48.8   38.3   87.1\n 2 1932-01-20   48.4   37.7   86.1\n 3 1932-01-31   42.4   36.2   78.6\n 4 1932-02-10   43.7   35.6   79.3\n 5 1932-02-20   44.2   35     79.2\n 6 1932-02-29   47.7   37.1   84.8\n 7 1932-03-10   54.1   43.1   97.2\n 8 1932-03-20   63.2   47    110  \n 9 1932-03-31  103     72.1  175  \n10 1932-04-10  103     73.2  176  \n# ℹ 3,014 more rows\n\n\n\n\nAs a result, we now have a complete record of decadal discharge data for the two main tributaries of the Chirchik river and the inflow time series to Charvak Reservoir from the beginning of 1932 until and including 2015, i.e. 84 years. The same type of preparatory analysis will now be carried out for the meteorological data but in a slightly more sophisticated way.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discharge Station Data</span>"
    ]
  },
  {
    "objectID": "discharge_station_data.html#sec-gap-filling-meteorological-data",
    "href": "discharge_station_data.html#sec-gap-filling-meteorological-data",
    "title": "4  Discharge Station Data",
    "section": "\n4.3 Gap Filling Meteorological Data",
    "text": "4.3 Gap Filling Meteorological Data\nHere, we use precipitation and temperature data from Pskem (38462), Chatkal (38471) and Charvak Reservoir (38464) Meteorological Stations (see Section 2.2 for more information on these stations). We also have data from Oygaing station (Station Code 38339) but the record only starts in 1962 and the time resolution is monthly. Therefore, we do not take this station into account here for the time being.\nWe start with precipitation and plot the available data.\n\np_dec_tbl &lt;- data |&gt; filter(type == \"P\" & code != \"38339\") \np_dec_tbl |&gt; plot_time_series(date,data,\n                               .facet_vars  = code,\n                               .interactive = TRUE,\n                               .smooth      = FALSE,\n                               .title       = \"\",\n                               .y_lab       = \"mm/decade\",\n                               .x_lab       = \"year\"\n                               )\n\n\n\n\n\n\nFigure 4.3: Raw decadal precipitation data from Pskem (38462), Charvak Reservoir (38471) and Chatkal Meteo Station (38471).\n\n\n\nThe precipitation data from these 3 stations shows some significant data gaps. The Chatkal Meteorological Station that is located in Kyrgyzstan apparently did not work in the post-transition years as continuous measurements were only resumed there in 1998.\nLet us see what happens if we were to use the same simple gap filling technique that we introduced above for discharge.\n\np_dec_filled_tbl &lt;- p_dec_tbl\np_dec_filled_tbl$data[is.na(p_dec_filled_tbl$data)] = p_dec_filled_tbl$norm[is.na(p_dec_filled_tbl$data)]\np_dec_filled_tbl |&gt; plot_time_series(date,data,\n                                      .facet_vars  = code,\n                                      .interactive = TRUE,\n                                      .smooth      = FALSE,\n                                      .title       = \"\",\n                                      .y_lab       = \"mm/decade\",\n                                      .x_lab       = \"year\"\n                                      )\n\n\n\n\n\n\nFigure 4.4: Precipitation Data gap-filled with norms. The filled values from 1990 - 2000 in the case of the Station 38471 indicate that the norm-filling technique is not adequate for this type of data.\n\n\n\nClosely inspect the significant data gap in the 1990ies at Station 38741. Play around and zoom into the time series in the 1990ies in Figure 4.3 and compare it with the resulting gap-filled time series in Figure 4.4. We see that our technique of gap filling with long-term norms is not suitable for this type of data and the significant gap size. The effect of variance reduction is clearly visible.\nHence, we resort to a more powerful gap filling technique that uses a (regression) model to impute the missing values from existing ones at the neighboring stations, i.e. Stations 38462 and 38464. To do so, we utilize the simputation R package. Please note that if you do not have the required package installed locally, you should install it prior to its use with the following command install.packages('simputation')\n\nlibrary(simputation)\n# First, we bring the data into the suitable format. \np_dec_wide_tbl &lt;- p_dec_tbl |&gt; \n  mutate(code = paste0('P',code |&gt; as.character())) |&gt; \n  dplyr::select(date,data,code) |&gt; \n  pivot_wider(names_from = \"code\",values_from = \"data\")\n\n# Second, we impute missing values.\np_dec_filled_wide_tbl &lt;- p_dec_wide_tbl  |&gt; \n  impute_rlm(P38471 ~ P38462 + P38464) |&gt; # Imputing precipitation at station 38471 using a robust linear regression model\n  impute_rlm(P38462 ~ P38471 + P38464) |&gt; # Imputing precipitation at station 38462 using a robust linear regression model\n  impute_rlm(P38464 ~ P38462 + P38471) # Imputing precipitation at station 38464 using a robust linear regression model\n\np_dec_filled_long_tbl &lt;- p_dec_filled_wide_tbl |&gt; pivot_longer(c('P38462','P38464','P38471')) \n\np_dec_filled_long_tbl |&gt; plot_time_series(date,value,\n                                          .facet_vars  = name,\n                                          .interactive = TRUE,\n                                          .smooth      = FALSE,\n                                          .title       = '',\n                                          .y_lab       = \"mm/decade\",\n                                          .x_lab       = \"year\"\n                                          )\n\n\n\n\n\n\nFigure 4.5: Precipitation Data gap filled with a robust linear regression modeling approach\n\n\n\nAs you can see, we use simple linear regression models to impute missing value in the target time series using observations from the neighboring stations. This is of course only possible where data is not missing across the time series, as we will discuss below.\nThrough simple visual inspection, it becomes clear that this type of regression model for gap filling is better suited than the previous approach chosen. Let us check whether we could successfully fill all gaps with this robust linear regression approach.\n\np_dec_filled_long_tbl |&gt; \n  group_by(name) |&gt; \n  summarize(n.na = sum(is.na(value)), n.na.perc = n.na / n() * 100)\n\n# A tibble: 3 × 3\n  name    n.na n.na.perc\n  &lt;chr&gt;  &lt;int&gt;     &lt;dbl&gt;\n1 P38462    12     0.402\n2 P38464    12     0.402\n3 P38471     3     0.100\n\n\nIt turns out that we still have very few gaps to deal with. We can see them by simply visualizing the wide tibble. The problem persisted at times when two or more values were missing across the available stations at the same time and where thus the linear regression could not be carried out. Let us look at the start of the record…\n\np_dec_filled_wide_tbl |&gt; \n  head(10)\n\n# A tibble: 10 × 4\n   date       P38462 P38464 P38471\n   &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 1933-01-10     NA   NA        2\n 2 1933-01-20     NA   NA       10\n 3 1933-01-31     NA   NA        5\n 4 1933-02-10     NA   NA       33\n 5 1933-02-20     NA   NA        8\n 6 1933-02-28     NA   NA       10\n 7 1933-03-10     NA   NA       31\n 8 1933-03-20     NA   NA       50\n 9 1933-03-31     NA   NA        6\n10 1933-04-10     23   21.3     13\n\n\n… and the end of the record. The missing values are easily spotted.\n\np_dec_filled_wide_tbl |&gt; \n  tail()\n\n# A tibble: 6 × 4\n  date       P38462 P38464 P38471\n  &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 2015-11-10     72     81     19\n2 2015-11-20    122     76     43\n3 2015-11-30      7      2      3\n4 2015-12-10     NA     NA     NA\n5 2015-12-20     NA     NA     NA\n6 2015-12-31     NA     NA     NA\n\n\nWe can solve the issues related to the missing values at the start of the observation record by using the same technique as above and by only regressing P38462 and P38464 on P38471.\n\np_dec_filled_wide_tbl &lt;- \n  p_dec_filled_wide_tbl  |&gt; \n  impute_rlm(P38462 ~ P38471) |&gt; # Imputing precipitation at station 38462 using a robust linear regression model\n  impute_rlm(P38464 ~ P38471) # Imputing precipitation at station 38464 using a robust linear regression model\np_dec_filled_wide_tbl |&gt; head(10)\n\n# A tibble: 10 × 4\n   date       P38462 P38464 P38471\n   &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 1933-01-10   5.60   5.08      2\n 2 1933-01-20  18.3   16.7      10\n 3 1933-01-31  10.4    9.46      5\n 4 1933-02-10  54.9   50.3      33\n 5 1933-02-20  15.2   13.8       8\n 6 1933-02-28  18.3   16.7      10\n 7 1933-03-10  51.8   47.3      31\n 8 1933-03-20  82.0   75.0      50\n 9 1933-03-31  12.0   10.9       6\n10 1933-04-10  23     21.3      13\n\n\nConverse to this, the complete set of observations is missing for December 2015. We will thus remove these non-observations from our tibble. This can be done once and for all with na.omit() as shown in the code block below.\n\np_dec_filled_wide_tbl &lt;- p_dec_filled_wide_tbl |&gt; na.omit()\np_dec_filled_wide_tbl |&gt; tail()\n\n# A tibble: 6 × 4\n  date       P38462 P38464 P38471\n  &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 2015-10-10      5      1      0\n2 2015-10-20     89    108     58\n3 2015-10-31     34     40     12\n4 2015-11-10     72     81     19\n5 2015-11-20    122     76     43\n6 2015-11-30      7      2      3\n\np_dec_filled_long_tbl &lt;-  p_dec_filled_wide_tbl |&gt; pivot_longer(-date)\n\nInspecting the temperature data, we see similar data issues as in the precipitation data set and can proceed accordingly for gap filling.\n\nt_dec_tbl &lt;- data |&gt; filter(type == \"T\") \nt_dec_tbl |&gt; plot_time_series(date,data,\n                               .facet_vars  = code,\n                               .interactive = TRUE,\n                               .smooth      = FALSE,\n                               .title       = '',\n                               .y_lab       = \"deg. Celsius\",\n                               .x_lab       = \"year\"\n                               )\n\n\n\n\n\n\nFigure 4.6: Raw temperature data from the meteorological stations Pskem (38462) and Chatkal (38471)\n\n\n\n\n# First, we bring the data into the suitable format. \nt_dec_wide_tbl &lt;- t_dec_tbl |&gt; \n  mutate(code = paste0('T',code |&gt; as.character())) |&gt; \n  dplyr::select(date,data,code) |&gt; \n  pivot_wider(names_from = \"code\",values_from = \"data\")\n\n# Second, we impute missing values.\nt_dec_filled_wide_tbl &lt;- t_dec_wide_tbl  |&gt; \n  impute_rlm(T38471 ~ T38462) |&gt; # Imputing precipitation at station 38471 using a robust linear regression model\n  impute_rlm(T38462 ~ T38471) # Imputing precipitation at station 38462 using a robust linear regression model\n\nt_dec_filled_long_tbl &lt;- t_dec_filled_wide_tbl |&gt; \n  pivot_longer(c('T38462','T38471')) \n\nt_dec_filled_long_tbl |&gt; \n  plot_time_series(date,value,\n                   .facet_vars  = name,\n                   .interactive = TRUE,\n                   .smooth      = FALSE,\n                   .title       = '',\n                   .y_lab       = \"deg. Celsius\",\n                   .x_lab       = \"year\"\n                   )\n\n\n\n\n\n\nFigure 4.7: Temperature data gap filled with robust linear regression modeling.\n\n\n\nThere are some irregularities in the temperature time series of Chatkal Meteorological Station in the first decade of the 20th century (tip: zoom in to see these more clearly). Note that these were not introduced by the gap filling technique that we used but are most likely wrong temperature readings or recordings. We will return to these in the outlier analysis below in Section 4.4.\nAny missing values left in the temperature time series? Let’s check!\n\nt_dec_filled_long_tbl |&gt; \n  group_by(name) |&gt; \n  summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100)\n\n# A tibble: 2 × 3\n  name    n.na n.na.perc\n  &lt;chr&gt;  &lt;int&gt;     &lt;dbl&gt;\n1 T38462     3     0.100\n2 T38471     3     0.100\n\n\nTo see where the missing value are, we find them easily again by looking at the head and tail of the tibble.\n\nt_dec_filled_wide_tbl |&gt; head()\n\n# A tibble: 6 × 3\n  date       T38462 T38471\n  &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 1933-01-10   -6.9  -16.6\n2 1933-01-20   -6.1  -15.5\n3 1933-01-31   -6.3  -15.6\n4 1933-02-10   -2     -8.6\n5 1933-02-20   -3.3  -12.5\n6 1933-02-28   -0.1   -8.5\n\n\n\nt_dec_filled_wide_tbl |&gt; tail()\n\n# A tibble: 6 × 3\n  date       T38462 T38471\n  &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 2015-11-10    2.4   -2.5\n2 2015-11-20    2     -2.2\n3 2015-11-30    4.6   -3.7\n4 2015-12-10   NA     NA  \n5 2015-12-20   NA     NA  \n6 2015-12-31   NA     NA  \n\n\nFinally, we remove these non observations again as above with the function na.omit().\n\nt_dec_filled_wide_tbl &lt;- t_dec_filled_wide_tbl |&gt; na.omit()\nt_dec_filled_long_tbl &lt;- t_dec_filled_wide_tbl |&gt; pivot_longer(-date)\n\nTo deal with the missing values at the end of the observational record, we could also have used any other technique. Using the norm values however would have artificially reduced the variance in both cases as explained above. Furthermore and at least in the case of temperature, it is also questionable to what extent a norm calculated over the last 84 years is still representative given global warming. We will look in this important and interesting topic in the next section.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discharge Station Data</span>"
    ]
  },
  {
    "objectID": "discharge_station_data.html#sec-anomalies-and-outliers",
    "href": "discharge_station_data.html#sec-anomalies-and-outliers",
    "title": "4  Discharge Station Data",
    "section": "\n4.4 Anomalies and Outliers",
    "text": "4.4 Anomalies and Outliers\nWe use the function timetk::plot_anomaly_diagnostics() to investigate these anomalies in the time series. For discharge, we first log-transform the raw data with the following transformation to reduce the variance of the original data.\n\\[\n\\hat{q}(t) = log(q(t) + 1)\n\\] where \\(\\hat{q}(t)\\) denotes the transformed discharge. Prior to the log transformation, 1 is added so as to avoid cases where discharge would be 0 and the logarithmic transform thus undefined. The transformation can easily be done with the log1p() function in R. Back-transformation is then via the function expm1() simply involves taking the exponent and subtracting 1 from the result. Figure 4.8 shows the result.\nResults are shown in Figure 4.8, Figure 4.9 and Figure 4.11 below.\nThe exceptionally wet year 19169 shows up as anomalous in the Chatkal River Basin and at the downstream Charvak Reservoir inflow gauge.\n\nq_dec_filled_long_tbl |&gt; \n  plot_anomaly_diagnostics(date,\n                           value |&gt; log1p(),\n                           .facet_vars  = name,\n                           .frequency = 36,\n                           .interactive = TRUE,\n                           .title = \"\")\n\n\n\n\n\n\nFigure 4.8: Anomaly diagnostics of discharge data. The transparent grey band shows the width of the normal range. The highly anomalous wet year of 1969 is clearly visible in the discharge record of the Chatkal river basin (Station 16279).\n\n\n\nThe investigation of precipitation anomalies shows a succession of regular anomalous wet events over time. It is interesting to see that the winter 1968/69 regularly anomalous at all three stations (Figure 4.9, zoom in to investigate).\n\np_dec_filled_long_tbl |&gt; \n  plot_anomaly_diagnostics(date,\n                           value,\n                           .facet_vars  = name,\n                           .interactive = TRUE,\n                           .title = \"\")\n\n\n\n\n\n\nFigure 4.9: Anomaly diagnostics of precipitation data. The transparent grey band shows the width of the normal range. The winter 1968/69 is regularly anomalous at all three stations.\n\n\n\nWhile intuitively, we would have expected an exceptionally mild winter in 1968/69 due to the precipitation excess, the corresponding anomaly does not show up in the temperature record as shown in Figure 4.11.\nWe have set a quite conservative range for the precipitation anomalies in Figure 4.9. We might want to experiment with a larger .alpha value in the plot_anomaly_diagnostics() function to see if the winter 1968/69 is still considered as an anomaly.\n\np_dec_filled_long_tbl |&gt; \n  plot_anomaly_diagnostics(date,\n                           value,\n                           .facet_vars  = name,\n                           .interactive = TRUE,\n                           .title = \"\",\n                           .alpha = .03)\n\n\n\n\n\n\nFigure 4.10: Anomaly diagnostics of precipitation data. The width of the normal range has been much increased and only the very extrem decades are now considered as anomalous. The cluster of extremes in 1968/69 appears more clearly like this.\n\n\n\nNow we look at the temperature anomalies.\n\nt_dec_filled_long_tbl |&gt;  \n  plot_anomaly_diagnostics(date,value,\n                           .facet_vars  = name,\n                           .interactive = TRUE,\n                           .title = \"\")\n\n\n\n\n\n\nFigure 4.11: Anomaly diagnostics of temperature data.\n\n\n\nApart from the identification of extreme periods since as the 1969 discharge year in the Chatkal river basin, the diagnostics of anomalies also helps to identify likely erroneous data records. In Figure 4.11 for example, when we zoom into the data of the series T38471 in the first decade of the 21st century, problems in relation to positive anomalies during the winter are visible in 4 instances. One explanation would be that in at least some instances, the data are erroneously recorded as positive values when in fact they were negative temperatures (see dates ‘2002-01-31’, ‘2005-01-10’ and ‘2007-02-28’, Chatkal Station 38471).\nObvious errors can be spotted like this and corrected. However, non-obvious data errors should be communicated with the data producing agency and replacement strategy jointly defined. If this is not possible, the values could be set to NA and then imputed as shown above.\nThe discharge data is now ready to be used for modelling and we can move on to the next Chapter on Geospatial Data.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discharge Station Data</span>"
    ]
  },
  {
    "objectID": "discharge_station_data.html#sec-discharge-station-data-references",
    "href": "discharge_station_data.html#sec-discharge-station-data-references",
    "title": "4  Discharge Station Data",
    "section": "\n4.5 References",
    "text": "4.5 References",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discharge Station Data</span>"
    ]
  },
  {
    "objectID": "geospatial_data.html",
    "href": "geospatial_data.html",
    "title": "5  Geospatial Data",
    "section": "",
    "text": "5.1 Geospatial Data Prerequisites\nHere, a local installation of QGIS and a basic understanding of Geographic Information Systems are required. Please see ?sec-study-guide-materials for more information on how to install QGIS.\nThere are very many online resources that can be consulted for learning QGIS. You can Google them or start with a video tutorial like the following one.\nTo process the data for your case study pack, make sure that you downloaded the corresponding files via this link. Depending on the catchment you have chosen, the files are either in the folder ./AMUDARYA, ./CHIRCHIK, ./CHU or ./SYRDAYA.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "geospatial_data.html#sec-geospatial-data-prerequisites",
    "href": "geospatial_data.html#sec-geospatial-data-prerequisites",
    "title": "5  Geospatial Data",
    "section": "",
    "text": "Tip\n\n\n\nAppendix C in the Appendix walks you through in a detailed manner of many of the required steps in the Chapter. Therefore, please also consult the resources there.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "geospatial_data.html#sec-catchment-delineation",
    "href": "geospatial_data.html#sec-catchment-delineation",
    "title": "5  Geospatial Data",
    "section": "\n5.2 Catchment Delineation",
    "text": "5.2 Catchment Delineation\nAssuming you have downloaded the catchment data folder of your choice, the corresponding DEM file should be loaded in a new, empty QGIS project. Before doing this, make sure that the project’s projection is in UTM (How to change the projection of a project). Check the projection of the DEM layer and reproject it if necessary (how to).\nIf you do not have a DEM available to start with, the process of downloading one is straightforward. Probably the easiest way is to install the QGIS SRTM3 plugin (how to). An alternative way is to download it via the USGS Earth Explorer (how to). Both solutions require you to register an Earth Explorer Account (how to register). Finally, after downloading the DEM tiles for the region of interest, the tiles need to be merged (how to merge DEM tiles).\nTypically, geospatial data from open sources is stored in the standard projection WGS84 (EPSG:4326). The WGS84 is in degrees, minutes, and seconds, but having spatial layers in the UTM projection for hydrological analysis is more convenient for hydrological analysis. Look up in which UTM Zone your catchment lies and re-project to that zone. In the example of the Chirchiq basin, the preferred UTM zone is 42N, i.e., EPSG:32642.\nAs a next step, we will load the shapefile of the discharge gauge station location. This file contains the point location where discharge is measured and from which we want to delineate the upstream contributing area. The data is stored in the ./GaugeData folder and is called XXXXX_Gauge.shp, where the XXXXX are placeholders for the five-digit code uniquely identifying your station. Once the shapefile is loaded, check in Properties/Information that it is correctly projected. If this is not the case, reproject to the relevant UTM zone as described above.\nNow, we are ready to start with the catchment delineation.\nInstead of just loading the existing catchment shapefile from the corresponding ./Basin folder (how to add a vector layer to a QGIS project), it is better to actually go through the steps step-by-step to learn how to derive them. The steps are also described in this online tutorial:\n\n\n\n\n\n\n\n\n\nTo derive the catchment area upstream of the discharge gauge that we have loaded in the previous step, the DEM is traced upstream until elevation values no longer increase, i.e., until the watershed’s boundary is reached. The SRTM DEM contains the rounded average elevation in each cell of about 25 - 30 meters [m] resolution. Due to the averaging and rounding it may happen, that in a SRTM DEM, an upstream river cell contains lower elevations than the downstream river cell. If the catchment delineation algorithm reaches such a situation, it stops, thinking it has reached the watershed’s boundary. To avoid this, cells that form local depressions should be filled to create a river bed that is continuously increasing in elevation in the upstream direction until it reaches the watershed boundary.\nIn QGIS, several methods can fill terrain depressions. We are using the r.fill.dir algorithm to perform this task. Figure 5.2 shows the process in detail. First, we make sure that we have loaded the DEM in the correct projection. Second, we open the Processing Toolbox and type r.fill.dir in the search bar, and then double-click on the corresponding algorithm. Third, in the open window, we ensure that the DEM is selected under the Elevation header. On top of that, the Flow Direction and Problem Areas output files are not needed, and their checkboxes can be left unchecked.\n\n\n\n\n\nFigure 5.2: Filling sinks in a DEM with the r.fill.dir algorithm.\n\n\nIf the algorithm has run, a new entry for the Depressionless DEM will appear in the layers panel with the correct DEM. This raster can now be used to delineate the basin area. The following steps need to be carried out for this;\n\nUse the r.watershed algorithm to obtain the flow accumulation and drainage direction rasters.\nEnsure that the gauge is correctly located on the DEM\nUse the r.water.outlet algorithm to delineate the upstream area\nConvert the resulting Basin raster into a polygon via the Conversion/Polygonize method.\n\nWe show the process in detail here.\n\n\n\n\n\nFigure 5.3: Running the r.watershed algorithm. Select the sink-filled DEM, set the minimum size of the exterior watershed to 200’000 and check the box Use positive flow accumulation even for likely underestimation.\n\n\nFigure 5.3 shows step-by-step how to run the r.watershed algorithm. First, make sure that the Depressionless DEM raster is selected in the Elevation field. Fill in 200’000 as the minimum size of the exterior watershed (this is a guessed number that normally yields good results). Also, check the box to use positive flow accumulation even for likely underestimates. Finally, just select the top two output files, i.e., Number of cell that drain through each cell and Drainage direction. The remaining resulting rasters are not required during the next steps.\nOnce you click run, the resulting raster layers will be computed. The flow accumulation raster (called Number of cells that drain through each cell) contains a large range of values. The largest value is obviously in the one cell that drains the largest area of the DEM.\n\n\n\n\n\n\nTip\n\n\n\nUse the raster calculator to compute the logarithm of the flow accumulation raster. This helps to visualize the raster better. This raster can ensure that the gauge from which the upstream area will be delineated is actually correctly located on the streams. If misplacement is evident, we can edit the gauge shapefile and relocate the gauge to the correct river stretch.\n\n\nOnce it is ensured that the gauge is correctly located on the stream, we are ready to delineate the upstream area by using the r.water.outlet algorithm. For this, zoom in to display a close-up of the gauge and select and open the r.water.outlet window. The algorithm only needs two inputs, i.e., the Drainage direction raster and the coordinates of the outlet point. Instead of manually entering them, you can just click on the map to select the precise point of the gauge location. This action will transfer the coordinates to the algorithm’s interface. After pressing run, the Basin raster will be computed. Figure 5.4 shows the process.\n\n\n\n\n\nFigure 5.4: Basin delineation using the r.water.outlet algorithm. The resulting Basin raster can easily be polygonized.\n\n\nAs a final step, you should polygonize the Basin raster. We are now ready to run the process model, which generates the required shapefiles for further processing in RSMinerve. The resulting vectorized basin can be stored in the corresponding folder on the local computer.\nThe process can be repeated for any other basin in a similar manner. Note also that if you have several gauges to map the upstream areas from, the Flow accumulation and the Drainage direction rasters can be computed once, stored, and reused again for all gauges.\nWe now have all the necessary files to process them in the Graphical Modeler model that is provided with the learning pack.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "geospatial_data.html#sec-preparation-of-rsminerve-input-files",
    "href": "geospatial_data.html#sec-preparation-of-rsminerve-input-files",
    "title": "5  Geospatial Data",
    "section": "\n5.3 Preparation of RSMinerve Input files",
    "text": "5.3 Preparation of RSMinerve Input files\nYou can download the Graphical Modeler model from the Students_CaseStudyPacks online repository. The model file is called rsminerve_gis_files_preparation_2022_win_os.model3 and should be downloaded in your local working directory. The model can then be loaded via the Processing/Graphical Modeler menu in QGIS. Once you have loaded the model, you should see the model pop up on your screen, as is shown in a similar fashion in Figure 5.5.\nNote that the present version of the model file has been developed under QGIS 3.24 and may not run error-free in a different QGIS release.\n\n\n\n\n\n\nWarning\n\n\n\nWarning (14.02.2023): The model does currently not run on QGIS 3.34. Efforts are underway to address and resolve the issue.\n\n\n\n\n\n\n\nFigure 5.5: The Graphical Modeler model rsminerve_gis_files_preparation_2022_win_os.model3 is shown in a graphical manner in the right window (1). When you click Run, the parameters specification window on the left will pop up (2). See text for further explanations\n\n\nFigure 5.5 shows nothing more than a graphical representation of a script. This script is like a recipe to execute algorithms in QGIS sequentially where an output of one algorithm feeds as input into the following algorithm. The yellow highlighted elements in window (1) are input parameters that are specified in the window (2) before the execution of the script. The green elements in window (1) are the results that are stored during and after the execution of the algorithm and available for further processing then.\nAs explained above, the rsminerve_gis_files_preparation_2022_win_os.model3 model, in a nutshell, prepares input files for RSMinerve using the DEM, the basin shapefile, and the gauge’s location as input. The key output elements are called SUBBASINS, RIVERS, and JUNCTIONS.\nBefore the execution of the model (script), the parameters have to be set in a careful manner. For some of the parameter values, a first best guess might produce outcomes that are not at the required level of detail or, alternatively, too detailed. Depending on the basin under consideration, an iterative approach typically must be followed to arrive at the desired output, as explained in the following.\nIn Figure 5.6, we show a meaningful parameter selection for the Chatkal River basin in the Case Studies pack (Gauge 16279). The basin and gauge shapefiles, as well as the DEM, are the geospatial assets that need to be specified, as shown in (1), (5), and (4). The following parameters are important to set correctly.\n\n\n\n\n\nFigure 5.6: Parameter selection menu of the Graphical Modeler rsminerve_gis_files_preparation_2022_win_os.model3 model. The detailed description of the input elements as well as parameter values chosen is explained in the main text.\n\n\n\nBasinShapeBuffer_meters (2): This is a buffer value to be specified in meters. The default value of 500 m is a value that can be used for a wide range of different catchment sizes and does not need to be changed.\nChannel Network Cutoff Value (3): This is a crucial parameter for determining the subbasin granularity. If many the main basin is to be subdivided into a larger number of subbasins, this value needs to be chosen to be around 107 - 108. For Chatkal, as our example shows, a perfect value for the subdivision into the main four subbasins is 109.\nElevation Bands Table (4): Figure 5.7 shows the table that can be customized according to user needs and wants. The table specifies the granuarity with which the basin domain will be subdivided into elevation bands (hydrological response units, i.e. HRUs). As a rule of thumb, in climate change impact studies and for snow melt-dominated basins, a spacing of 100 m - 200 m is an optimal choice for HBV models in RSMinerve. Note, however, that the finer the domain is, the larger the number of HRUs to be modeled. This proportionally increases the computational demand of the hydrological model.\nRiver Network Level (8): Here, you set a number between 1 and 8. The lower the number, the higher the number of tributaries and sub-tributaries that will be delineated for the RIVERS output shapefile. If you choose 8, this means that normally, only the river’s main stem will be delineated. This is relevant for the case where no further subdivisions into small subbasins are desired for modeling in RSMinerve. 7 is a robust choice if only the main tributaries should be delineated.\n\n\n\n\n\n\nFigure 5.7: The elevation bands table is shown. It shows the user-customizable elevation band intervals. The example shows the specification of 9 elevation bands (HRUs) of 500 m spacing between each. For each HRU and each subbasin, a corrsponding hydrological model will be configured in RSMinerve\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou will likely have to iterate and run the model a couple of times to achieve the desired results. Try it yourself! Note that if you get an error message that POLYGONS.shp was not found, try increasing the Channel Network Cutoff Value by 50 % - 70 %.\n\n\nSample output results for our demonstration catchment are shown in Figure 5.8. The resulting layers for this demonstration are available in the data pack. We are, however, not quite done yet.\n\n\n\n\n\nFigure 5.8: Resulting output of the Graphical Modeler model for Chatkal River basin. The next step is to clean up manually the SUBBASINS, RIVERS and JUNCTIONS shapefiles.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "geospatial_data.html#sec-post-processing-results-shapefiles",
    "href": "geospatial_data.html#sec-post-processing-results-shapefiles",
    "title": "5  Geospatial Data",
    "section": "\n5.4 Post-Processing Results Shapefiles",
    "text": "5.4 Post-Processing Results Shapefiles\nAs mentioned in the previous Section, we need to post-process the SUBBASINS, RIVERS, and JUNCTIONS shapefiles so that they correctly contain the required fields, as shown in Figure 5.1. A good way to post-process the files is to start with the RIVERS layer, continue with the JUNCTIONS layer, and end with the SUBBASINS layer.\n\n5.4.1 JUNCTIONS and RIVER Layers\nIn editing the junctions layer, the basic idea is to edit the junctions layer in a way to remove junctions that are not needed, given the desired subdivision of the larger basin into subbasins. For the demo catchment shown in Figure 5.9, the situation is explained in greater detail.\n\n\n\n\n\nFigure 5.9: The Figure shows the output files of the Graphical Modeler model. For this demonstration catchment, the only junction that we want to keep (apart from the outflow junction located at the gauge) is the one highlighted with the red arrow which marks the confluence of the two major upstream tributaries.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you struggle with shapefile editing, please consult the detailed step-by-step guide as shown in the corresponding Quick Guide in the Appendix.\n\n\nOnce we have removed the junctions that are not needed, we can very easily also remove the river lines that are not required. Generally, we do not need river sections above the most upstream junctions for any given subbasin within the river basin or the river basin itself.\nIf we follow these guides, we end up with the results shown in Figure 5.10. After editing the attribute tables of these shapefiles, the files are ready for import in RSMinerve.\n\n\n\n\n\nFigure 5.10: Final edited shapefiles with the unnecessary junctions and river stretches removed. The two red arrows point to the two remaining junctions, and the blue arrow shows the river stretch that is left.\n\n\nWhat is left now is to ensure that the attribute tables contain the correct fields and that these fields contain the correct labels. Figure 5.1 shows the required fields. These are\n\nJUNCTIONS: Junctions Name (junct_name), Junctions ID (junct_id), Rivers ID (riv_id)\nRIVERS: Rivers Name (riv_name), Rivers ID (riv_id), Junctions ID (junct_id)\n\nThe abbreviated field names are used in QGIS since field names are limited in shapefiles. The fields and values of the final junctions shapefile are shown in Figure 5.11. Compare this with the edited final attribute table of the rivers shapefile, as shown in Figure 5.12.\n\n\n\n\n\nFigure 5.11: The final attribute table of the demo junctions shapefile is shown. (1) denotes the fields for the donwstream outlet junction and (2) shows the values for the upstream junction. The naming convention has to be understood in the context of the naming convention using in the RIVERS shapefile\n\n\n\n\n\n\n\nFigure 5.12: The final rivers shapefile attribute table is shown with the only river highlighted (1). Very importantly, the junt_id field referes to the downstream junction where the river drains into.\n\n\n\n5.4.2 SUBBASINS Layer\nNow, we are working on the subbasins. The major subbasins are highlighted in the following Figure 5.13. We call them the Chatkal River downstream, the Chatkal River upstream, and the Sandalash River. These big subbasins are the hydrological response units (HRUs) for hydrological modeling.\n\n\n\n\n\nFigure 5.13: The major subbasins of the Chatkal River basin above Gauge 16279 are shown. (1) is Chatkal downstream, (2) is Sandalash River, and (3) is Chatkal upstream.\n\n\nHowever, do not forget that we have classified the entire basin into zones of elevation bands. These are shown in the Figure 5.13 as polygons with black outlines from the shapefile layer BasinElevationBands_poly_fixed.shp. Exactly 7 elevation bands (or, in other words, HRUs) were delineated with the Graphical Modeler like this. We must somehow intersect the subbasin HRUs with the elevation bands to generate elevation band HRUs for each subbasin. We will show the steps required to achieve this in the following.\nFirst, we prepare the attribute table of the subbasins shapefile. The way this is done correctly for the demonstration catchment is shown in Figure 5.14. The attribute table contains two fields, i.e., basin_name and junct_id. We name the downstream and upstream sections of the Chatkal River with chatkal_ds and chatkal_us correspondingly. The cells of the field junct_id are populated with the name of the corresponding downstream junction where the subbasin drains. Since both, the subbasin chatkal_us and sandalash drain into the junction upstream_junct, the latter appears in both cells of the junct_id field for these rivers.\n\n\n\n\n\nFigure 5.14: Subbasins attribute table after adding the required fields and entires as per the requirements of RSMinerve. (1) is for the downstream part of Chatkal River, (2) is for the upstream part of Chatkal River, and (3) is for Sandalash River.\n\n\nSecond, we clean up and prepare the attribute table for intersecting with the subbasins layer in a third step. The attribute table of the layer BasinElevationbands_Poly_fixed contains three fields, i.e., ID, VALUE, and NAME, from which you can safely delete the ID and VALUE fields. What remains is the NAME field that contains a unique identifier for the elevation bands in ascending order, starting from the lowest. Figure 5.15 shows the attribute table.\n\n\n\n\n\nFigure 5.15: The attribute table of the elevation bands shapefile is shown after the fields ID and VALUE have been removed (1). When an elevation band is selected in the edit mode, the corresponding geometry is highlighted on the map as is shown by the arrows (2).\n\n\nThird, we must intersect the elevation bands shapefile with the subbasins shapefile. This can easily be accomplished using the Vector/Geoprocessing/Intersection tool in QGIS. When you select the Intersection Algorithm, make sure that you set the elevation bands shapefile as the Input layer and set subbasins shapefile as the Overlay Layer. When executed, you receive an attribute table that looks like the one shown in Figure 5.16.\n\n\n\n\n\nFigure 5.16: The attribute table resulting from the Intersection Algorithm. You now have to manually add the numbers of the Name field as a suffix to the existing names of the basin_name field.\n\n\nAs a final step, you must manually add the numbers stored in the NAME field to the basin names in the field basin_name. Alternatively, you can open the field calculator and overwrite the basin_name field using the formula: basin_name + ‘_’ + NAME. After this is done, you can safely delete the NAME field and have proudly completed the required steps to set up the GIS shapefiles that can be used as input to the RSMinerve hydrological-hydraulic modeling tool. Figure 5.17 shows the result.\n\n\n\n\n\nFigure 5.17: The final attribute table of the subbasins file is shown. The elevation band identifiers have been added as suffixes with “_x” to the subbasin names where x denotes the number of the subbasin-specific elevation band.\n\n\nUsing the tool Zonal Statistics from the Processing Toolbox, you can extract the mean elevation for each hydrological response unit. Save the elevation in a field named Z. The resulting GIS layers produced with the above-described steps are available in the data pack\nIntersecting with glacier outlines\nIf glaciers are present in the basin, discharge from glacier melt can be simulated using the GSM model in RS MINERVE. The SUBBASINS layer can be intersected with the glacier outlines from the Randolph Glacier Inventory (RGI) v6.0 data set see section on glacier outlines. The following step-by-step instructions show how to further process the SUBBASINS layer to for later automated model generation.\nIn the data pack of the book, you will find a GIS layer named 16279_RGI_Glaciers with the glacier outlines from the Randolph Glacier Inventory (RGI), clipped to the case study basin. The RGI glacier outlines are not always consistent with the basin boundaries derived using the process modeler in QGIS as described above because the two data sets are based on different data sources and spatial resolution. We, therefore, start out by estimating the glacier area and volume for each of the RGI glaciers in the basin. Open the field calculator in the attribute table of the RGI glaciers layer and estimate the glacier area in \\(m^2\\) for each glacier using the formula $area (see Figure 5.18).\n\n\n\n\n\nFigure 5.18: Calculate the glacier area using the field calculator. Use descriptive output field names and add units where possible (in this example, we choose the name Area_m2). Make sure to select decimal numbers for the output field type\n\n\nYou can then use an empirical function to estimate glacier volume in the field calculator; for example, following Erasov (1968): \\(V = 0.027 \\cdot A^{1.5}\\) with Volume \\(V\\) in \\(km^3\\) and Area \\(A\\) in \\(km^2\\)) as shown in ?fig-glacier-volume-field-calculator.\n We now have an estimate of the water volume stored in each glacier. The goal is to estimate the glacier volume in each of the subbasins and to edit the elevation bands layer so that automated model creation in RS MINERVE can create an appropriate GSM object for each glaciated sub-basin. In the first step, we clip the glacier areas from the elevation bands layer BasinElevationbands_Poly_fixed using the tool Vector/Geoprocessing/Difference. That layer now only contains HRUs for HBV model objects. You can save this layer to your GIS folder, as it will later be merged with the layers for the GSM objects and be your final layer for import to RS MINERVE.\nWe now prepare the layer for the GSM objects. Intersect the glaciers layer with the elevation bands layer BasinElevationbands_Poly_fixed using the tool Vector/Geoprocessing/Intersection. From the glaciers layer you only need the fields RGIId, Area_m2, and Volume_m3. This will give you a layer with the glacier outlines and the elevation bands on the glacier areas. For each of the glaciers, you now see in which elevation band it lies.\nWe now aggregate the glacier area and volume in each elevation band using the field calculator and the formulas sum(Area_m2, group_by := basin_name) and sum(Volume_m3, group_by := basin_name). The layer is then dissolved using the tool Vector/Geoprocessing/Dissolve with the dissolve field basin_name. This merges all features with the same basin_name into one geometric feature of the layer. The fields RGIId, Area_m2, and Volume_m3 are now obsolete and can be removed. Thus, we now have the total glacier area and volume for each glaciated HRU in the study basin. For simulating runoff formation with the GSM model, we still need an initial glacier thickness, which we obtain using the field calculator and dividing the total glacier volume in the HRU by the total glacier area in the HRU. This gives us a total glacier thickness in the basin, which is available for melting.\nNow, we can clean up the layer and merge it with the HBV elevation bands layer we prepared and saved earlier. You may want to edit the names of the features for the GSM objects with the prefix glacier or gl so that you recognize which hydrological model to assign to which HRU in RS MINERVE (see Section 8.6.2). You then merge the two vector layers and are ready to import them into RS MINERVE for automated model generation. We save the layer under the name 16297_Chatkal_RSM_HRU.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "geospatial_data.html#sec-geospatial-data-references",
    "href": "geospatial_data.html#sec-geospatial-data-references",
    "title": "5  Geospatial Data",
    "section": "\n5.5 References",
    "text": "5.5 References\n\n\n\n\nErasov, N. V. 1968. “Method for Determining of Volume of Mountain Glaciers.” MGI, no. 14: 307–8.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "snow_and_glacier_data.html",
    "href": "snow_and_glacier_data.html",
    "title": "6  Snow and Glacier Data",
    "section": "",
    "text": "6.1 Introduction\nThe runoff of Syr Darya and Amu Darya consists of 65 % - 75 % snow melt, 23 % precipitation, and 2 % - 8 % glacier melt approximately (Armstrong et al. 2019). Seasonal snowmelt is the primary driver of discharge and, thus, the main source of irrigation water in late spring. At first glance, glacier discharge may seem unimportant. While glacier runoff is a minor contributor to the annual runoff of large rivers in Central Asia, it is seasonally relevant as it covers the irrigation demand in summer when snow melt is over (Kaser, Grosshauser, and Marzeion 2010). Further, it is a much more critical contribution to discharge in the highly glaciated tributaries to the large rivers (Khanal et al. 2021). Generally, the cryosphere is a major contributor to the water balance in Central Asia (Barandun et al. 2020). Very little is known about the volume of permafrost in Central Asia, but the impact of permafrost loss is expected to be of large magnitude (Gruber et al. 2017).\nAmong other things, climate impacts translate into long-term changes in runoff formation fractions and the distribution of runoff formation within the hydrological year. Figure Figure 6.1 shows a simplified illustration of the changes in runoff regime as glaciers shrink (for the example of a heavily glaciated basin) (Hock et al. 2019). The considerable reduction in river discharge as glaciers disappear from the catchment is noteworthy, especially in the late summer period, which coincides with a period of high irrigation demand in Central Asia. Further, discharge variability is expected to increase; with little to no discharge in late summer when glacier storage is gone and occasional high precipitation runoff due to high intensity rainfall events.\nBy the end of the century, an average reduction of 50 % to 80 % of glacier mass has to be expected in Central Asia (Marzeion et al. 2020), depending on the socio-economic pathway. Glaciers at low elevations will completely disappear. As the cryosphere acts as a natural seasonal to multi-year water reservoir, the loss of this storage capacity has a large impact on downstream food and energy production and the environment, with associated economic loss (Rasul and Molden 2019).\nAs a demonstration site, we use the catchment of the gauging station on the Atbashy River, a tributary to the Naryn River in Central Asia. If you’d like to reproduce the examples presented in this Chapter, you can download the zipped data in the example data set available here. You can extract the downloaded data into a location of your choice and adapt the reference path below. The rest of the code will run as it is, provided you have the required r packages installed. The size of the data package is 14.1 GB.\nPlease note that new (highly relevant and public) glacier data are released ever more frequently. The summary provided here refers to the latest data sets at the time of writing in February 2022.\nlibrary(tmap, quietly = TRUE)\nlibrary(sf, quietly = TRUE)\nlibrary(raster, quietly = TRUE)\nlibrary(tidyverse, quietly = TRUE)\nlibrary(lubridate, quietly = TRUE)\n\ndevtools::install_github(\"hydrosolutions/riversCentralAsia\", quiet = TRUE)\nlibrary(riversCentralAsia, quietly = TRUE)\n\n# Path to the data directory downloaded from the download link provided above. \n# Here the data is extracted to a folder called atbashy_glacier_demo_data\ndata_path &lt;- \"../caham_data/SyrDarya/Atbashy/\"",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Snow and Glacier Data</span>"
    ]
  },
  {
    "objectID": "snow_and_glacier_data.html#sec-introduction-snow-glacier-data",
    "href": "snow_and_glacier_data.html#sec-introduction-snow-glacier-data",
    "title": "6  Snow and Glacier Data",
    "section": "",
    "text": "Figure 6.1: Illustration of the development of glacier melt in a heavily glaciated catchment (Image source: Hock and colleagues: High Mountain Areas. In: IPCC Special Report on the Ocean and Cryosphere in a Changing Climate)",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Snow and Glacier Data</span>"
    ]
  },
  {
    "objectID": "snow_and_glacier_data.html#sec-snow-cover-and-snow-water-equivalent",
    "href": "snow_and_glacier_data.html#sec-snow-cover-and-snow-water-equivalent",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.2 Snow Cover and Snow Water Equivalent",
    "text": "6.2 Snow Cover and Snow Water Equivalent\nWe start with the upper-most cryosphere layer: the snow cover. A large amount of data sets are available and the following section presents a suitable product for the calibration of the snow module in the HBV model objects.\n\n6.2.1 High Mountain Asia Snow Reanalysis (HMASR) Product\nYufei Liu, Fang, and Margulis (2021) provide a daily reanalysis product for snow-covered area and snow water equivalent (SWE) in High Mountain Asia between October 1, 1999 and September 30, 2017. The spatial resolution is 16 arc-seconds (approximately 400 m) and the spatial coverage is N: 45, S: 27, E: 105, W: 60 so the northern-most headwater basins of the Illi River catchment are unfortunately not covered. Other SWE products have a coarser spatial resolution (e.g. the Daily Snow Depth Analysis Data by the Canadian Meteorological Center, which is available at 24 km grid resolution). They are thus less suited for calibration of hydrological models of small to medium sized headwater basins.\nThe HMASR data is available via NSIDC (Y. Liu, Fang, and Margulis 2021). Their SWE can directly be compared to the SWE computed in hydrological models like HBV. More on this in the model calibration section.From the downloaded data, only the SWE and the validity mask (showing the pixels where the snow water equivalent product is valid) is required. Please note that the warning is due to the .nc file format and need not concern you. The SWE data is read correctly from the .nc files.\n\ndem &lt;- raster(paste0(data_path, \"GIS/16076_DEM.tif\"))\nbasin &lt;- st_read(paste0(data_path, \"GIS/16076_Basin_outline.shp\"), quiet = TRUE)\n\n# Load one example file and display SWE for a random date in the cold season. \nfilespath &lt;- paste0(data_path, \"SNOW/\")\nyear &lt;- 1999\n\n# Load non-seasonal snow mask\nfilepart &lt;- \"_MASK.nc\"\nindex = sprintf(\"%02d\", (year - 1999))\n\n# The Atbashy basin is covered by two raster stacks\nmask_w &lt;- raster::brick(paste0(filespath, \n                               \"HMA_SR_D_v01_N41_0E76_0_agg_16_WY\", \n                               year, \"_\", index, filepart), \n                     varname = \"Non_seasonal_snow_mask\")\nraster::crs(mask_w) = raster::crs(\"+proj=longlat +datum=WGS84 +no_defs\")\nmask_e &lt;- raster::brick(paste0(filespath,\n                               \"HMA_SR_D_v01_N41_0E77_0_agg_16_WY\", \n                               year, \"_\", index, filepart), \n                     varname = \"Non_seasonal_snow_mask\")\nraster::crs(mask_e) = raster::crs(\"+proj=longlat +datum=WGS84 +no_defs\")\n\n# The rasters need to be rotated\ntemplate &lt;- raster::projectRaster(from = mask_e, to= mask_w, alignOnly = TRUE)\n\n# template is an empty raster that has the projected extent of r2 but is \n# aligned with r1 (i.e. same resolution, origin, and crs of r1)\nmask_e_aligned &lt;- raster::projectRaster(from = mask_e, to = template)\nmask_w &lt;- flip(t(mask_w), direction = 'x')\nmask_e_aligned &lt;- flip(t(mask_e_aligned), direction = 'x')\nmask &lt;- merge(mask_w, mask_e_aligned, tolerance = 0.1) \nmask = raster::projectRaster(from = mask, \n                             crs = crs(\"+proj=utm +zone=42 +datum=WGS84 +units=m +no_defs\"))\n\n# Load snow data\nvarname = \"SWE_Post\"\nfilepart &lt;- \"_SWE_SCA_POST.nc\"\nsca_w &lt;- raster::brick(paste0(filespath, \n                              \"HMA_SR_D_v01_N41_0E76_0_agg_16_WY\", \n                              year, \"_\", index, filepart), \n                       varname = varname, level = 1)\n\n[1] \"vobjtovarid4: **** WARNING **** I was asked to get a varid for dimension named Day BUT this dimension HAS NO DIMVAR! Code will probably fail at this point\"\n\nraster::crs(sca_w) = raster::crs(\"+proj=longlat +datum=WGS84 +no_defs\")\nsca_e &lt;- raster::brick(paste0(filespath,\n                              \"HMA_SR_D_v01_N41_0E77_0_agg_16_WY\", \n                              year, \"_\", index, filepart), \n                       varname = varname, level = 1)\n\n[1] \"vobjtovarid4: **** WARNING **** I was asked to get a varid for dimension named Day BUT this dimension HAS NO DIMVAR! Code will probably fail at this point\"\n\nraster::crs(sca_e) = raster::crs(\"+proj=longlat +datum=WGS84 +no_defs\")\ntemplate &lt;- raster::projectRaster(from = sca_e, to = sca_w, alignOnly = TRUE)\n# template is an empty raster that has the projected extent of r2 but is \n# aligned with r1 (i.e. same resolution, origin, and crs of r1)\nsca_e_aligned&lt;- raster::projectRaster(from = sca_e, to = template)\nsca_w &lt;- flip(t(sca_w), direction = 'x')\nsca_e_aligned &lt;- flip(t(sca_e_aligned), direction = 'x')\nsca &lt;- raster::merge(sca_w, sca_e_aligned, tolerance = 0.1)\nsca &lt;- projectRaster(from = sca, \n                     crs = crs(\"+proj=utm +zone=42 +datum=WGS84 +units=m +no_defs\"))\n\nsca_masked &lt;- mask(sca, mask, maskvalue = 1)\nsca_masked &lt;- mask(sca_masked, basin)\n\n# Visualize snow water equivalent\ntmap_mode(\"view\")\ntm_shape(max(sca_masked, na.rm = TRUE)) + \n  tm_raster(n = 6,\n            palette = \"Blues\",\n            alpha = 0.8,\n            legend.show = TRUE, \n            title = \"SWE (m)\") + \n  tm_shape(basin) + \n  tm_borders(col = \"black\", lwd = 0.6)\n\n\n\n\n\n\nFigure 6.2: The maximum daily snow water equivalent in the hydrological year 1999 in the Atbashy basin (Kyrgyzstan). (Data source: HMASR, NSIDC).\n\n\n\n\n6.2.2 Operational SLF Model Data on Snow Height and Snow Water Equivalent\nFor operational hydrological modelling, different possibilities exist to obtain near-real time snow cover, height and information on the snow water equivalent (SWE). One such option is to use data from the operational SLF Model.\n\nFebruary 2024: More information coming soon!",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Snow and Glacier Data</span>"
    ]
  },
  {
    "objectID": "snow_and_glacier_data.html#sec-glacier-outlines",
    "href": "snow_and_glacier_data.html#sec-glacier-outlines",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.3 Glacier Outlines",
    "text": "6.3 Glacier Outlines\nThe Randolph Glacier Inventory (RGI) v6.0 (RGI Consortium 2017) makes a consistent global glacier data base publicly available1. It includes geolocated glacier geometry and some additional parameters like elevation, length, slope and aspect. For water resources modeling in Central Asia, RGI regions 13 (Central Asia) and 14 (South Asia West) are relevant. The glacier geometries for all RGI regions are available from the GLIMS RGI v6.0 website. For this demo, the data for the Atbashy River basin is available from the data download link given above.\n\n# Loading the data\nrgi &lt;- st_read(paste0(data_path, \"GIS/16076_Glaciers_per_subbasin.shp\"), \n               quiet = TRUE) |&gt; \n  st_transform(crs = crs(dem))\n\n# Generation of figure\ntmap_mode(\"view\")\ntm_shape(dem, name = \"DEM\") +\n  tm_raster(n = 6, \n            palette = terrain.colors(6),\n            alpha = 0.8,\n            legend.show = TRUE, \n            title = \"Elevation (masl)\") + \n  tm_shape(rgi, name = \"RGI v6.0\") + \n  tm_polygons(col = \"lightgray\", lwd = 0.2)\n\n\n\n\n\n\nFigure 6.3: DEM & Glaciers (light gray) of the demo basin.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Snow and Glacier Data</span>"
    ]
  },
  {
    "objectID": "snow_and_glacier_data.html#glacier-thickness",
    "href": "snow_and_glacier_data.html#glacier-thickness",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.4 Glacier Thickness",
    "text": "6.4 Glacier Thickness\nFarinotti et al. (2019) make distributed glacier thickness maps available for each glacier in the RGI v6.0 data set. As the data set is large, we have downloaded the required maps of glacier thickness for you and made them available in the download link above. We will refer to this data set as the glacier thickness data set or the Farinotti data set.\nPlease note that Farinotti et al. (2019) provide ice thickness. This can be converted to water equivalents by assuming a ice density of 900 kg/m3, e.g. 100 m glacier thickness translates to a water column of about 90 m.\nThe original glacier thickness data set is available from the data collection of Farinotti et al. (2019) which is available from the data section of their online article.\nThe following code chunks demonstrates how to extract glacier thickness data from the Farinotti data set.\n\n6.4.1 How to Extract Glacier Thickness\n\n# Get a list of all files in the glacier thickness data set. The files are named \n# after the glacier ID in the RGI v6.0 data set (variable RGIId).  \nglacier_thickness_dir &lt;- paste0(data_path, \"GLACIERS/Farinotti/\") \nfilelist &lt;- list.files(path = glacier_thickness_dir, pattern = \".tif$\", \n                       full.names = TRUE)\n\n# Filter the glacier thickness file list for the glacier ids in the catchment of \n# interest. \nfilelist &lt;- filelist[sapply(rgi$RGIId, grep, filelist)]\n\n# Get the maximum glacier thickness for each of the glaciers in filelist. \n# Note: this works only for small catchments as the origin of the rasters to be \n# mosaiced needs to be consistent. For a larger data set you will need to implement \n# a loop over all glaciers to extract the thickness per glacier or per elevation \n# band. This operation can take a while. \nglacier_thickness &lt;- Reduce(function(x, y) raster::mosaic(x, y, fun = max),\n                            lapply(filelist, raster::raster)) \n\n# For plotting, clip the glacier thickness raster of the basin to the basin boundary\nglacier_thickness &lt;- mask(glacier_thickness |&gt; \n                            projectRaster(crs = crs(dem)), basin)\n\n\ntmap_mode(\"view\")\ntm_shape(dem, name = \"DEM\") + \n  tm_raster(n = 6, \n            palette = terrain.colors(6),\n            alpha = 0.8,\n            legend.show = TRUE, \n            title = \"Elevation (masl)\") + \n  tm_shape(glacier_thickness, name = \"Farinotti\") +\n  tm_raster(n = 6, \n            palette = \"Blues\",\n            legend.show = TRUE, \n            title = \"Glacier thickness\\n(m)\") + \n  tm_shape(rgi, name = \"RGI v6.0\") + \n  tm_borders(col = \"gray\", lwd = 0.4) + \n  tm_shape(basin, name = \"Basin outline\") + \n  tm_borders(col = \"black\", lwd = 0.6)\n\n\n\n\n\n\nFigure 6.4: Glacier thickness by Farinotti et al., 2019\n\n\n\nA more recent glacier thickness data set by Millan et al. (2022) estimates much larger ice reservoirs in the Himalayan region but similar goodness of fit for the glaciers in the Central Asian region as the Farinotti data set. The Millan et al. (2022) data set is not included in the present workflow yet but should be considered as an alternative for the Farinotti data set.\nThe glacier thickness data set by Farinotti et al. (2019) can for example be combined with the RGI v6.0 data set for the Central Asia region to validate the well-known glacier area-volume relationship by Erasov (1968) (see section Area-Volume scaling).",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Snow and Glacier Data</span>"
    ]
  },
  {
    "objectID": "snow_and_glacier_data.html#glacier-thinning-rates",
    "href": "snow_and_glacier_data.html#glacier-thinning-rates",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.5 Glacier Thinning Rates",
    "text": "6.5 Glacier Thinning Rates\nHugonnet et al. (2021) provide annual estimates of glacier thinning rates for each glacier in the RGI v6.0 data set. The authors advise to not to rely on the annual data but rather on an average over at least 5 years to get reliable thinning rates for individual glaciers. We compare trends in glacier thinning rates to trends in glacier thickness during model calibration. We will refer to this data set as the thinning rates data set or the Hugonnet data set. A copy of the Hugonnet thinning rates is included in the download link above.\nThe original per-glacier time series of thinning rates can be downloaded from the data repository as described in the github site linked under the code availability section of the online paper of Hugonnet et al. (2021).\n\nhugonnet &lt;- read_csv(paste0(data_path, \"/GLACIERS/Hugonnet/dh_13_rgi60_pergla_rates.csv\"))\n# Explanation of variables:\n# - dhdt is the elevation change rate in meters per year,\n# - dvoldt is the volume change rate in meters cube per year,\n# - dmdt is the mass change rate in gigatons per year,\n# - dmdtda is the specific-mass change rate in meters water-equivalent per year.\n\n# Filter the basin glaciers from the Hugonnet data set. \nhugonnet &lt;- hugonnet |&gt; \n  dplyr::filter(rgiid %in% rgi$RGIId) |&gt; \n  tidyr::separate(period, c(\"start\", \"end\"), sep = \"_\") |&gt; \n  mutate(start = as_date(start, format = \"%Y-%m-%d\"), \n         end = as_date(end, format = \"%Y-%m-%d\"), \n         period = round(as.numeric(end - start, units = \"days\")/366))\n\n# Join the Hugonnet data set to the RGI data set to be able to plot the thinning \n# rates on the glacier geometry. \nglaciers_hugonnet &lt;- rgi |&gt; \n  left_join(hugonnet |&gt; dplyr::select(rgiid, area, start, end, dhdt, err_dhdt, \n                                      dvoldt, err_dvoldt, dmdt, err_dmdt, \n                                      dmdtda, err_dmdtda, period),  \n            by = c(\"RGIId\" = \"rgiid\")) \n\n# Visualization of data\ntmap_mode(\"view\")\ntm_shape(dem, name = \"DEM\") + \n  tm_raster(n = 6, \n            palette = terrain.colors(6),\n            alpha = 0.8, \n            legend.show = TRUE, \n            title = \"Elevation (masl)\") + \n  tm_shape(glaciers_hugonnet |&gt; dplyr::filter(period == 20), name = \"Hugonnet\") +\n  tm_fill(col = \"dmdtda\", \n          n = 6, \n          palette = \"RdBu\",\n          midpoint = 0, \n          legend.show = TRUE, \n          title = \"Glacier thinning\\n(m weq/a)\") + \n  tm_shape(rgi, name = \"RGI v6.0\") + \n  tm_borders(col = \"light blue\", lwd = 0.4) + \n  tm_shape(basin, name = \"Basin outline\") + \n  tm_borders(col = \"black\", lwd = 0.6)\n\n\n\n\n\n\nFigure 6.5: Average glacier mass change by Hugonnet et al., 2021.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Snow and Glacier Data</span>"
    ]
  },
  {
    "objectID": "snow_and_glacier_data.html#glacier-discharge",
    "href": "snow_and_glacier_data.html#glacier-discharge",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.6 Glacier Discharge",
    "text": "6.6 Glacier Discharge\nMiles et al. (2021) ran specific mass balance calculations over many glaciers larger than 2 km2 in High Mountain Asia (HMA). They provide the average glacier discharge between 2000 and 2016. We will refer to this data set as the glacier discharge data set or the Miles data set. A copy of the glacier discharge data is available from the data download link provided above. The glacier ablation data can be used as a third party data set to validate glacier discharge rates simulated using the GSM model implemented in RS MINERVE.\nThe original data is available from the data repository linked in the online version of the paper.\n\nglaciers_miles &lt;- read_csv(paste0(data_path, \"GLACIERS/Miles/Miles2021_Glaciers_summarytable_20210721.csv\")) |&gt; \n  dplyr::filter(VALID == 1, \n                RGIID %in% glaciers_hugonnet$RGIId) \n\nglaciers_miles &lt;- rgi |&gt; \n  left_join(glaciers_miles |&gt; \n              dplyr::select(-c(VALID, CenLat, CenLon)),  \n            by = c(\"RGIId\" = \"RGIID\")) \n\n# Data visualization\ntmap_mode(\"view\")\ntm_shape(dem, name = \"DEM\") + \n  tm_raster(n = 6, \n            palette = terrain.colors(6),\n            alpha = 0.8, \n            legend.show = TRUE, \n            title = \"Elevation (masl)\") + \n  tm_shape(glaciers_miles, name = \"Miles\") + \n  tm_fill(col = \"totAbl\", \n          n = 6, \n          palette = \"RdBu\", \n          midpoint = 0, \n          legend.show = TRUE, \n          title = \"Glacier ablation\\n(m3/a)\") + \n  tm_shape(rgi, name = \"RGI v6.0\") + \n  tm_borders(col = \"gray\", lwd = 0.4) + \n  tm_shape(basin, name = \"Glacier outline\") + \n  tm_borders(col = \"black\", lwd = 0.6)\n\n\n\n\n\n\nFigure 6.6: Glacier ablation rates by Miles et al., 2021.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Snow and Glacier Data</span>"
    ]
  },
  {
    "objectID": "snow_and_glacier_data.html#glacier-discharge-projections",
    "href": "snow_and_glacier_data.html#glacier-discharge-projections",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.7 Glacier Discharge Projections",
    "text": "6.7 Glacier Discharge Projections\nProjected glacier and off-glacier discharge for the 21th century are provided by Rounce, Hock, and Shean (2020a) through NSIDC. Monthly per-glacier discharge rates for RGI v6.0 region 13 have been produced using the Python Glacier Evolution Model (PyGEM) (Rounce, Hock, and Shean 2020b). PyGEM glacier discharge projections are done using the CMIP 5 climate model ensemble and emission scenarios. These are thus not 1-to-1 compatible with the CMIP6 models and SSP used as climate forcing in this book. Riahi et al. (2017) compare radiative forcing (an important driver for glacier melt) for CMIP5 and CMIP6 scenarios and show that the lower emission CMIP5 scenarios (RCP2 2.4 and RCP 4.5) underestimate radiative forcing as compared to the CMIP6 SSP counterparts (SSP 1 and SSP 2.). This means that also glacier melt simulated using the CMIP5 model ensembles may underestimate actual glacier melt for the lower emission scenarios. RCP 8.5 seems to be, however, fairly compatible with SSP 5. There is no counterpart for SSP 3 in the CMIP5 scenarios.\nIn 2023, glacier discharge projections using CMIP6 models and scenarios were released (Rounce et al. 2023). These can directly be used in climate impact modeling assessments. We use these PyGEM glacier discharge projections for estimating future glacier melt in the entire Central Asian region.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Snow and Glacier Data</span>"
    ]
  },
  {
    "objectID": "snow_and_glacier_data.html#a-note-on-the-uncertainties-of-the-glacier-data-sets",
    "href": "snow_and_glacier_data.html#a-note-on-the-uncertainties-of-the-glacier-data-sets",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.8 A Note on the Uncertainties of the Glacier Data Sets",
    "text": "6.8 A Note on the Uncertainties of the Glacier Data Sets\nThe geometries of the RGI v6.0 data set are generally very good. If you simulate glacier discharge in a small catchment with few glaciers it is advisable to visually check the glacier geometries and make sure, all relevant glaciers in the basin are included in the RGI data set. You may have to manually add missing glaciers or correct the geometry.\nFor some regions in Central Asia, OpenStreetMap is an excellent reference for glacier locations and names in Central Asia. You can import the map layer in QGIS or also download individual GIS layers.\nThe glacier thickness data set is validated only at few locations as measurements of glacier thickness are typically not available. Farinotti et al. (2019) list an uncertainty range for the volume estimate in regions RGI 13 and 14 of 26 % each.\nHugonnet et al. (2021) & Miles et al. (2021) provide the uncertainties of their estimates for per-glacier glacier thinning & discharge rates in the data set itself. They typically lie around \\(\\pm\\) 150%.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Snow and Glacier Data</span>"
    ]
  },
  {
    "objectID": "snow_and_glacier_data.html#permafrost-extent",
    "href": "snow_and_glacier_data.html#permafrost-extent",
    "title": "6  Snow and Glacier Data",
    "section": "\n6.9 Permafrost Extent",
    "text": "6.9 Permafrost Extent\nWhile glaciers are intensively researched, also in Central Asia, little is known about the extent of permafrost in High Mountain Asia and how climate change will impact permafrost regions (Barandun et al. 2020). A high-resolution (1 km grid resolution, average between 2000 and 2016) estimate of permafrost extent is available from the ESA GlobPermafrost program (Obu et al. 2019). Data can be downloaded from the Arctic Permafrost Geospatial Center (APGC). Figure Figure 6.7 shows the permafrost probability extent, i.e. the estimated fraction of grid cell covered by permafrost. Glaciated areas are excluded.\nThe permafrost probability extent can for example be used to inform model zonation.\n\n# Load the data and cut to the Atbashy basin\npermafrost &lt;- raster(paste0(data_path, \"PERMAFROST/Permafrost_extent.tif\")) \n\ntmap_mode(\"view\")\ntm_shape(permafrost, name = \"Permafrost\") +\n  tm_raster(n = 6, \n            palette = \"Blues\",\n            legend.show = TRUE, \n            title = \"Perfafrost prob.\\nextent\") + \n  tm_shape(rgi, name = \"RGI v6.0\") + \n  tm_borders(col = \"gray\", lwd = 0.4) + \n  tm_shape(basin, name = \"Basin outline\") + \n  tm_borders(col = \"black\", lwd = 0.6)\n\n\n\n\n\n\nFigure 6.7: Permafrost probability extent by Obu et al.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Snow and Glacier Data</span>"
    ]
  },
  {
    "objectID": "snow_and_glacier_data.html#footnotes",
    "href": "snow_and_glacier_data.html#footnotes",
    "title": "6  Snow and Glacier Data",
    "section": "",
    "text": "A new Version 7 of the dataset has been published in 2023. For more information, see here{target =” _blank”}.↩︎\nRCP stands for Representative Concentration Pathway. RCPs are predecessors of the shared socio-economic pathways used in the CMIP6 phase.↩︎",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Snow and Glacier Data</span>"
    ]
  },
  {
    "objectID": "climate_data.html",
    "href": "climate_data.html",
    "title": "7  Climate Data",
    "section": "",
    "text": "7.1 Historical Climate Data",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Climate Data</span>"
    ]
  },
  {
    "objectID": "climate_data.html#sec-historical-climate-data",
    "href": "climate_data.html#sec-historical-climate-data",
    "title": "7  Climate Data",
    "section": "",
    "text": "7.1.1 CHELSA V21 High-Resolution Climate Data\nTo obtain information on past precipitation (P) and temperature (T) in the Central Asia region, we use the very high-resolution daily temperature climate product from CHELSA. CHELSA (Climatologies at high resolution for the earth’s land surface areas) is a global downscaled climate data set currently hosted by the Swiss Federal Institute for Forest, Snow and Landscape Research (WSL). It is built to provide free access to high resolution climate data for research and application, and is constantly updated and refined. CHELSA data are in the process of revolutionizing the field of hydrology in data-poor regions, among many other application domains.\nCHELSA includes climate layers for various time periods and variables, ranging from the Last Glacial Maximum, to the present, to several future scenarios. CHELSA is based on a mechanistic statistical downscaling of global reanalysis data for the historical observations (hist_obs) or global circulation model output for future simulations (fut_sim) (see also www.chelsa-climate.org for more information). For more technical information, please consult the following document.\nThe historical observations for CHELSA precipitation and temperature are available from 1979 through 2016 for daily time steps and at 1 km resolution. These are derived from ERA-INTERIM reanalysis model outputs, among other things. ERA-INTERIM is a global atmospheric reanalysis product with a spatial resolution of 80 km, approximately. A good introduction about what a reanalysis product is can be found on this website.\nDaily gridded precipitation fields are generated merging data from the ERA5 reanalysis precipitation and the MODIS monthly cloud cover. The CHELSA algorithm that is used for downscaling takes into account orographic predictors such as wind, topographic exposition and boundary layer height Karger et al. (2021). When compared with other products, the resulting data shows excellent performance, also in complex high mountain terrain. Temperature observations are available over the same period and are taken from (“CHELSA-W5E5 V1.0: W5E5 V1.0 Downscaled with CHELSA V2.0” 2022).\n\n\n\n\n\n\nWarning\n\n\n\nThe data for the Central Asia domain (55 deg. E - 85 deg. E and 30 deg. N - 50 deg. N) is very large and is not provided here for download (total storage requirements &gt; 1 TB). Please contact Tobias Siegfried via siegfried@hydrosolutions.ch for more information on how to obtain access the data of the entire domain.\n\n\nThe high-resolution climate data derived with the CHELSA algorithm is corrected for the problem of snow-undercatch in the high mountain regions as described by (Beck et al. 2020). What is snow-undercatch? Measuring precipitation correctly in high altitude regions is complex because of sublimation and blowing snow. An example of this is shown in Figure 7.1 for high elevation gauges in Spain. In a recent inter-comparison project carried out in Spain, it has been shown that undercatch poses significant problems in accurately measuring solid precipitation (Buisán et al. 2017) in mountainous regions. Both, ERA-INTERIM and CHELSA themselves assimilate station data in their models and hence are affected by these erroneous measurements.\n\n\n\n\n\nFigure 7.1: Measured snow undercatch values in high-mountain stations in Spain. The values were determined within the World Meteorological Organization Solid Precipitation Intercomparison Experiment (WMO-SPICE). See text for more information and reference.\n\n\n(Beck et al. 2020) has recognized this and released monthly correction factors that are taken into account in the CHELSA algorithm (see Figure 7.2).\n\n\n\n\n\nFigure 7.2: Figure from (Beck et al. 2020), Supplementary Material. Plate d): Best estimate of global bias correction factors. Plate e): Lower bound estimate of global bias correction factors. Plate f): Upper bound of global bias correction factors. As is clearly visible, bias correction factors in high-mountain Asia, including the parts of Central Asia are significant.\n\n\nThe annual precipitation climatology, i.e. the long-term mean annual precipitation, from 1979 - 2011 is shown in Figure 7.3. As is easily visible and not further surprising, the mountainous regions receive the bulk of the precipitation, on average.\n\n\n\n\n\nFigure 7.3: The CHELSA V21 Precipitation climatology in 6 large basins Central Asian basins is shown, including Amu Darya, Syr Darya, Talas River, Chu River, Issq Kul and Ily River is shown. Light blue colors indicate very little preciptiation whereas red colors indicate high annual norm precipitation amounts.\n\n\nThe following Figure 7.4 and Figure 7.5 show cold and warm season precipitation amounts. The cold season is defined to encompass the months October (previous year) - March (preceding year) whereas the warm season lasts from April through September.\n\n\n\n\n\nFigure 7.4: The cold season precipitation climatology is shown. This Figure should also be compared with the the warm season precipitation climatology as shown in Figure 7.5.\n\n\n\n\n\n\n\nFigure 7.5: The warm season precipitation climatology is shown. This Figure should also be compared with the the cold season precipitation climatology as shown in Figure 7.4.\n\n\nFigure 7.4 shows that winter precipitation is mainly concentrated on the western fringes of the mountain ranges where moisture gets precipitated via westerly circulations and associated frontal systems. Compared to this, the main warm season precipitation locations move further to the east and to inner mountain range locations where summer convective storms cause this (see Figure 7.5).\nThe climatological data used to produce these Figures is available via this Dropbox link{target =” _blank”} in the climate/chelsa_v21/climatologies/ sub-folder. There data over the historical observation period from 1981 - 2010 has been prepared for the norm annual and cold as well as warm season temperatures (tas_…) has been prepared. Similarly, data on precipitation (pr_…) and potential evapotranspiration (pet_…) is available and on the aridity index which is defined as \\(\\phi = PET/P\\) where \\(PET\\) is the potential evapotranspiration climatology and \\(P\\) is the precipitation climatology.\n\n\n\n\n\n\nTip\n\n\n\nTry it yourself! Download the data for the Central Asia domain here{target =” _blank”} and visualize the climatologies for your case study catchment and extract mean statistics for the basins. As a reminder, the case study basins can be accessed via this link{target =” _blank”}.\n\n\n\n7.1.2 Assessment of CHELSA V21 Data Quality in a Sample Catchment\nHow can the quality of the CHELSA data in the complex Central Asia domain be assessed? With try to answer this question by looking at one of the case study basins provided as part of the Student Case Study Pack. Specifically, we want to answer the following questions for the Gunt River basin in the Pamir mountains:\n\ndoes the magnitude of the precipitation yield physically meaningful results, and\ndoes the climatology adequately reproduce the seasonal cycle observed one at the stations?\n\nLong-term Annual Norm Discharge\nLet us address the first question by investigating long-term norm CHELSA precipitation values and comparing these long-term norm values of the specific discharge of Gunt River. If \\(P &gt;Q\\), where \\(P\\) is the long-term mean precipitation and \\(Q\\) is the long-term mean discharge, we can confidently say that the bias corrected CHELSA precipitation product is meaningful from a water balance perspective. The long-term water balance is simply\n\\[\nQ = P - E\n\\tag{7.1}\\]\nwhere \\(Q\\) is the specific discharge [mm], \\(P\\) is the long-term mean precipitation [mm] and \\(E\\) is the long-term mean evapotranspiration \\(E\\) [mm] (see also the Chapter on Long-term water balance modeling for more information). Hence, if, over the long run, \\(P&gt;E\\) and under the assumption that storage changes i.e. from glacier melt are not present, the water balance is valid and the product from that perspective validated.\nWe can compute the average long-term precipitation in the catchment in a simple way. The code block below shows how.\n\n# load required libraries\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(exactextractr)\nlibrary(raster)\nlibrary(tmap)\nlibrary(tmaptools)\n\n# load basin shapefile. note that if you want to replicate \n# the code below on your own computer, make sure that you \n# adjust the data paths accordingly, depending on where \n# you stored the downloaded data!\nbasin_shp_path &lt;- \n  \"../caham_data/AmuDarya/17050_Gunt/GIS/17050_basin_latlon.shp\"\nbasin_shp &lt;- sf::st_read(basin_shp_path, quiet = TRUE)\n\n# load climatology and extract values\np_clim_path &lt;- \n  \"../caham_data/central_asia_domain/climate/chelsa_v21/climatologies/hist_obs/pr_chelsa_climatology_ann.tif\"\np_clim &lt;- raster(p_clim_path)\n\n# mask and crop\np_clim_gunt &lt;- p_clim |&gt; \n  mask(basin_shp) |&gt; \n  crop(basin_shp)\n\n# plotting climatology\ntmap::tmap_mode(\"view\")\n#tmap::tm_basemap(tm_tiles(\"Stamen.Terrain\")) +\ntm_tiles(\"Stamen.Terrain\") +\n  tm_shape(p_clim_gunt) +\n  tm_raster(style = \"quantile\", n = 7, \n            palette = get_brewer_pal(\"Blues\", n = 7, plot = FALSE),\n            title = \"Annual precipation (mm)\") +\n  tm_shape(basin_shp) +\n  tm_borders(\"black\") +\n    tm_layout(legend.position = c(\"right\", \"bottom\"))\n\nWarning: basemap Stamen.Terraindoes not exist in the providers list nor does it\nseem a valid url\n\n\n\n\n\n\n\nFigure 7.6: CHELSA precipitation climatology in Gunt River Basin. Values are in millimeters per year (mm/a). Note, the Figure is interactive and you can zoom in and out to see more details. The above code is shown to demonstrate the generation of such a Figure.\n\n\n\nThe mean norm precipitation can be easily computed as follows.\n\n# extracting mean value over Gunt River basin. \n# note the resulting value is in mm.\np_clim_mean &lt;- p_clim |&gt;  exact_extract(basin_shp,'mean')\np_clim_mean\n\n[1] 384.4959\n\n\nWe thus get 384 mm for the 30 year period from 1981 through 2010. Let us compare this to the long-term discharge at Gunt Khorog discharge station. For this, we load the corresponding data frame from which we extract the corresponding data.\n\n# load discharge data from Gunt Khorog gauging station. note that if you want to replicate the code below on your own computer, make sure that you adjust the data paths accordingly, depending on where you stored the downloaded data!\nstation_data &lt;- \n  readRDS(\"../caham_data/AmuDarya/17050_Gunt/GaugeData/17050_Q.Rds\")\n\n# extract data for station 17050 and discharge between 1981 and 2010\ndischarge_data_17050 &lt;- \n  station_data |&gt;\n  filter(type == \"Q\") |&gt; \n  filter(code == \"17050\") |&gt; \n  filter(date &gt;= ymd(\"1981-01-01\")) |&gt; \n  filter(date &lt;= ymd(\"2010-12-31\"))\n  \n# compute long-term discharge\ndischarge_data_17050_norm &lt;- \n  discharge_data_17050$data |&gt; \n  mean(na.rm = TRUE)\ndischarge_data_17050_norm\n\n[1] 108.7257\n\n\nFor the long-term discharge at the station 17050, we thus get 109 m3/s. To compute the annual norm specific discharge, we need to compute the total discharge volume for a year and then divide by the total basin area.\n\nbasin_area &lt;- st_area(basin_shp) |&gt; as.numeric()\ndischarge_data_17050_norm_vol &lt;- \n  discharge_data_17050_norm * 3600 * 24 * 365 / basin_area * 1000\ndischarge_data_17050_norm_vol\n\n[1] 250.5378\n\n\nHence, for the period 1981 - 2010, we obtain for the specific discharge 250 mm. Clearly, \\(Q&lt;P\\) and we can calculate that, on average, 2 parts of the total precipitation are discharged via the river whereas 1 part is evaporated without contribution to runoff at the gauge 17050 in Khorog town.\nAs an aside, the bias corrected precipitation climatology shows an interesting feature of the Gunt river basin (see Figure 7.6). Namely, there is a stark precipitation gradient between the western part of the basin where the bulk of the precipitation is observed and the hyper-arid Pamir plateau region to the east, where annual precipitation is below 200 mm at a mean altitude of above 4’000 meters above sea level [masl]. This place thus can be classified as alpine desert. This is an orographic effect as most of the moisture is washed out of the atmosphere before it enters the region of the plateau arriving from the westerly direction.\nDischarge Seasonality\nWhat about the seasonality of the CHELSA precipitation? Can it adequately reproduce the observed precipitation seasonality? If this would not be the case, we would have to reject the validity of the product and explore other high-resolution climatologies such as WorldClim V2 or CHPClim V1 (see (Beck et al. 2020) for more information on these products). Let us explore again the data of the Gunt River basin to answer this question.\nFirst, we load and prepare all the required station precipitation and geospatial data. Then, we compute the monthly norms of these data for the period 1981-01-01 through 2010-12-31. Note that the meteorological station at Khorog is located at the same place as the discharge gauging station and has the 5-digit index 38954 (see also the dedicated Example Catchments Chapter for more information).\n\nstation_data &lt;- \n  readRDS(\"../caham_data/AmuDarya/17050_Gunt/GaugeData/17050_Q.Rds\")\n\n# extract the precipitation data for station 17050 between 1981 and 2010\npr_data_38954 &lt;- \n  station_data |&gt;\n  filter(type == \"P\") |&gt; \n  filter(code == \"38954\") |&gt; \n  filter(date &gt;= ymd(\"1981-01-01\")) |&gt; \n  filter(date &lt;= ymd(\"2010-12-31\"))\n\n# add a month identifier to the dataframe and group by months\npr_data_38954 &lt;- pr_data_38954 |&gt; \n  mutate(month = month(date)) |&gt; \n  group_by(month) |&gt; dplyr::select(date, data, month)\n\n# compute monthly mean values\np_monthly_mean_38954 &lt;- pr_data_38954 |&gt; \n  summarize(data = mean(data, na.rm = TRUE)) |&gt; \n  add_column(data_source = \"Meteo Station 39954\")\n\n# plot the resulting monthly time series\np_monthly_mean_38954 |&gt; \n  ggplot(aes(x = month, y = data), color = data_source) + \n  geom_line() + \n  xlab(\"Month\") + \n  ylab(\"mm/month\") +\n  ggtitle(\"Monthly precipitation climatology from 1981 - 2010, Station 38954\") + \n  theme_bw() + \n  scale_x_continuous(breaks = c(1:12))\n\n\n\n\n\n\n\nWe can extract the mean monthly values from the CHELSA SpatRaster data and then compare it to station data.\n\np_clim_monthly_path &lt;- \n  \"../caham_data/central_asia_domain/climate/chelsa_v21/climatologies/hist_obs/monthly/pr_monthly_55_85_30_50.tif\"\np_clim_monthly &lt;- terra::rast(p_clim_monthly_path)\n\np_monthly_mean_CHELSA_data &lt;- \n  p_clim_monthly |&gt; exact_extract(basin_shp,'mean') |&gt; \n  as.numeric()\n\np_monthly_mean_CHELSA &lt;- \n  p_monthly_mean_38954\np_monthly_mean_CHELSA$data &lt;- \n  p_monthly_mean_CHELSA_data\np_monthly_mean_CHELSA$data_source &lt;- \n  \"CHELSA\"\n\np_monthly_mean &lt;- \n  p_monthly_mean_38954 |&gt; \n  add_row(p_monthly_mean_CHELSA)\n\n# plot the resulting monthly time series\np_monthly_mean |&gt; \n  ggplot(aes(x = month, y = data, color = data_source)) + \n  geom_line() + \n  xlab(\"Month\") + \n  ylab(\"mm/month\") +\n  ggtitle(\"Comparison of station and CHELSA precipitation climatologies\") + \n  theme_bw() + \n  scale_x_continuous(breaks = c(1:12))\n\n\n\n\n\n\nFigure 7.7\n\n\n\n\nAs is evident by looking at Figure 7.7, the CHELSA product can adequatly reproduce the seasonality of the local precipitation climatology and is only slightly overestimation absolute values. However, with regard to the later, this argument is not necessarily valied as we compare local point measurements with raster data with a resolution of 1 km2. A more thorow comparsion would generate an interpolated climatology field from station data and then compare these fields. However, as data is very scarce in this large basin, we do not have the means to perform such analysis.\n\n\n\n\n\n\nEXERCISE\n\n\n\nTry it yourself. Conduct the same analysis with the monthly temperature climatologies for the meteorological station and for the CHESLA data. You can very easily carry this out while reusing code blocks from above, also for any of the other basins available in the Case Study pack.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Climate Data</span>"
    ]
  },
  {
    "objectID": "climate_data.html#sec-climate-projections",
    "href": "climate_data.html#sec-climate-projections",
    "title": "7  Climate Data",
    "section": "\n7.2 Climate Projections",
    "text": "7.2 Climate Projections\nThe daily CHELSA V21 climate forcing data can be using for hydrological modeling from 1979 - 2010. But what about the future? After all, one of the main goals of this course book is to demonstrate how to use hydrological modeling to quantify future climate impacts in the Central Asian river basins. For this purpose, we will demonstrate in this Section how to download and process future climate data for studying hydrological changes.\n\n\n\n\n\nFigure 7.8: The time arrow is shown point from left to the right. The availability of corresponding data is indicated for what we call the historical reference period (hist_…) and the future scenario period (fut_…).\n\n\nTo start with, we need to divide the total period of interest into a historic period and a future period. The historic period in Figure 7.8 is highlighted in dark grey color and ranges from 1979 through 2010. It is the period for which we have observed climate forcing data from the CHELSA dataset and gauge data available at the same time. As will be discussed in the Chapter on hydrological modeling, it is also the period which we use to calibrate and validate our hydrological model.\nWhat we call the future period is highlighted by the light gray arrow in Figure 7.8. Admittedly, the years from 2011 through 2022 are not in our current future (this edition of the book is written and published in 2022), so it really is just a matter of definition. It is this period over which we want to study climate impacts under different scenarios.\n\n7.2.1 Global Circulation Models\nThese scenarios are describing different increasing greenhouse concentration pathways are are computed with large-scale numerical models called General Circulation Models or GCMs. They globally represent physical processes in the atmosphere, the oceans, the cryosphere and the land surface. GCMs compute geographically distributed and physically consistent estimate of regional climate change and thus are the key inputs to different types of impact analyses. A stylized schematic structure of a GCM is shown in Figure 7.9.\n\n\n\n\n\nFigure 7.9: Schematic structure of a GCM model. Source: Penn State University, David Bice.\n\n\nFigure 7.9 shows that GCMs discretize the atmosphere, ocean and land columns into a three dimensional grid with differing numbers of vertical layers that is a function of the model and the compartment under consideration (atmosphere, land, ocean). With a typical horizontal resolution between 250 km - 600 km, the spatial resolution of the GCMs is coarse relative to what is needed for detailed impact studies. Furthermore, many key physical processes such as cloud formation happen at sub-grid resolution. These can thus only indirectly be represented by a process called parameterization and it represents a major source of uncertainty in GCM-based future climate simulations. Furthermore, since every GCM model represents processes and feedback mechanisms in the model in a different way, there is also inter-model uncertainty where different models generate different climate responses despite the same scenario-based forcing. Being cognisant of these uncertainties is important in impact studies. For more information, see also this website.\nFor illustration, ?fig-comparison-chelsa-gcm-resolution shows the temperature fields for 01. January 1979 over the Central Asia domain as provided by the CHELSA V21 data set (left panel) and as computed by the GCM GFDL-ESM4 model under the historic run. Note that there is no particular reason why we choose this model, it is just to serve as an example here. The complete list of models which we use for the climate change impact analysis will be presented and discussed further below.\nThe difference in resolution is striking with the CHELSA data having having a horizontal resolution of 1 km, approx., and the GCM model having a resolution of 1.5 degrees x 1 degrees which corresponds to 166.5 km x 111 km on the equator, approximately. Why is GCM resolution so coarse? It is, simply put, limited by restrictions given by the computational power of the powerful super computers where these models are run on.\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.10: Comparison of CHELSA temperature climatology (left) and the GCM climate field of the historical run of the model GFDL-ESM4 (right).\n\n\n\n\n\n\n\n\n\nFigure 7.11: Comparison of CHELSA temperature climatology (left) and the GCM climate field of the historical run of the model GFDL-ESM4 (right).\n\n\n\n\n\nAs is indicated in Figure 7.9, GCM runs from the historic period from 1979 through 2010 are also available. These historic GCM runs are very important as we shall see in a minute when it comes to bias correcting and downscaling GCM runs onto the spatial units of interest, i.e. hydrological response units (HRUs) in our case.\n\n7.2.2 CMIP6 Climate Scenarios\nWhat are the future scenarios that we are interest in? The global climate science community has worked hard under the Phase 6 of the Coupled Model Intercomparison Project (CMIP6) the define relevant future scenarios that are describing different climate forcing trajectories. The paper by (O’Neill et al. 2016) is the reference source with regard to detailed descriptions of the scenarios. We are interested to cover and study a broad range of possible hydrological future states and thus select 4 distinct shared socioeconomic pathway (SSP) scenarios that cover this entire possible range.\nThe SSPs are based on five narratives describing broad socioeconomic trends that possibly shape future society. These are intended to span the range of plausible futures. The narratives are (taken from (Riahi et al. 2017)):\n\nSSP1 Sustainability – Taking the Green Road (Low challenges to mitigation and adaptation): The world shifts gradually, but pervasively, toward a more sustainable path, emphasizing more inclusive development that respects perceived environmental boundaries. Management of the global commons slowly improves, educational and health investments accelerate the demographic transition, and the emphasis on economic growth shifts toward a broader emphasis on human well-being. Driven by an increasing commitment to achieving development goals, inequality is reduced both across and within countries. Consumption is oriented toward low material growth and lower resource and energy intensity.\nSSP2 Middle of the Road (Medium challenges to mitigation and adaptation): The world follows a path in which social, economic, and technological trends do not shift markedly from historical patterns. Development and income growth proceeds unevenly, with some countries making relatively good progress while others fall short of expectations. Global and national institutions work toward but make slow progress in achieving sustainable development goals. Environmental systems experience degradation, although there are some improvements and overall the intensity of resource and energy use declines. Global population growth is moderate and levels off in the second half of the century. Income inequality persists or improves only slowly and challenges to reducing vulnerability to societal and environmental changes remain.\nSSP3 Regional Rivalry – A Rocky Road (High challenges to mitigation and adaptation): A resurgent nationalism, concerns about competitiveness and security, and regional conflicts push countries to increasingly focus on domestic or, at most, regional issues. Policies shift over time to become increasingly oriented toward national and regional security issues. Countries focus on achieving energy and food security goals within their own regions at the expense of broader-based development. Investments in education and technological development decline. Economic development is slow, consumption is material-intensive, and inequalities persist or worsen over time. Population growth is low in industrialized and high in developing countries. A low international priority for addressing environmental concerns leads to strong environmental degradation in some regions.\nSSP4 Inequality – A Road Divided (Low challenges to mitigation, high challenges to adaptation): Highly unequal investments in human capital, combined with increasing disparities in economic opportunity and political power, lead to increasing inequalities and stratification both across and within countries. Over time, a gap widens between an internationally-connected society that contributes to knowledge- and capital-intensive sectors of the global economy, and a fragmented collection of lower-income, poorly educated societies that work in a labor intensive, low-tech economy. Social cohesion degrades and conflict and unrest become increasingly common. Technology development is high in the high-tech economy and sectors. The globally connected energy sector diversifies, with investments in both carbon-intensive fuels like coal and unconventional oil, but also low-carbon energy sources. Environmental policies focus on local issues around middle and high income areas.\nSSP5 Fossil-fueled Development – Taking the Highway (High challenges to mitigation, low challenges to adaptation): This world places increasing faith in competitive markets, innovation and participatory societies to produce rapid technological progress and development of human capital as the path to sustainable development. Global markets are increasingly integrated. There are also strong investments in health, education, and institutions to enhance human and social capital. At the same time, the push for economic and social development is coupled with the exploitation of abundant fossil fuel resources and the adoption of resource and energy intensive lifestyles around the world. All these factors lead to rapid growth of the global economy, while global population peaks and declines in the 21st century. Local environmental problems like air pollution are successfully managed. There is faith in the ability to effectively manage social and ecological systems, including by geo-engineering if necessary.\n\nFigure 7.12 shows the underlying population and GDP developments for the corresponding SSPs and Table 7.1 details about the forcing in these scenarios.\n\n\n\n\n\nFigure 7.12: Global population and GDP developments under the CMIP6 shared socioeconomic pathways (taken from this source).\n\n\nThe description of the Tier 1 scenarios below that we are focussing on is taken from the aforementioned publication.\n\n\nTable 7.1: Scenarios, their forcing category and the effective radiative forcing by the year 2100 (O’Neill et al. 2016).\n\n\n\n\n\n\n\n\nScenario\nForcing Category\n2100 forcing [W/m2]\n\n\n\nShared Socioeconomic Pathway SSP5-8.5\nHigh\n8.5\n\n\nShared Socioeconomic Pathway SSP3-7.0\nHigh\n7\n\n\nShared Socioeconomic Pathway SSP2-4.5\nMedium\n4.5\n\n\nShared Socioeconomic Pathway SSP1-2.6\nLow\n2.6\n\n\n\n\n\n\nFor each scenario, we select 4 high priority GCM models for the preparation of downscaled climate forcings for the basins under consideration. The following Table 7.2 shows overview information about the models which are used in this course. Output of the GCM models shown in the table is available at daily timescales until 2100 and also for the historic runs.\n\n\nTable 7.2: Model names, models and host institution where the GCM models have been developed.\n\n\n\n\n\n\n\n\nName\nModel\nInstitution\n\n\n\nGFDL-ESM4\ngfdl- esm4\nNational Oceanic and Atmospheric Administration, Geophysical Fluid Dynamics Laboratory, Princeton, NJ 08540, USA\n\n\nUKESM1-0-LL\nukesm1- 0-ll\nMet Office Hadley Centre, Fitzroy Road, Exeter, Devon, EX1 3PB, UK\n\n\nMPI-ESM1-2-HR\nmpi- esm1-2- hr\nMax Planck Institute for Meteorology, Hamburg 20146, Germany\n\n\nIPSL-CM6A-LR\nipsl- cm6a-lr\nInstitut Pierre Simon Laplace, Paris 75252, France\n\n\nMRI-ESM2-0\nmri-esm2-0\nMeteorological Research Institute, Tsukuba, Ibaraki 305-0052, Japan\n\n\n\n\n\n\nGCM Model data can be downloaded from the climate store on the dedicated Copernicus website. :::{.callout-caution} Some of the GCM models used in CMIP6 show high sensitivity to warming processes (Zelinka et al. 2020) and a larger than average uncertainty of the transient climate response (TCR). TCR a metric for “the amount of global warming in the year in which atmospheric CO2 concentrations have finally doubled after having steadily increased by 1% every year” (Hausfather et al. 2022)). Hausfather et al. (2022) suggest to exclude models with high TCR beyond the 66% likelihood range (given for example in Tokarska et al. (2020)) from a climate projection study. The UK model we use (see above) is among the models with high TCR. We keep the model in our ensemble for now, being aware of its limitations.\n:::\n\n\n\n\n\n\nImportant\n\n\n\nEach GCM model is different from the others. Some model leap days, some don’t. Some assume that each month has 30 days, other don’t. The preprocessing steps required to make the outputs of these models is very time consuming and is not recommendened for beginners. Therefore, please download the pre-processed climate scenarios for the Central Asia domain from this online repository.\n\n\n\n7.2.3 Downscaling and Bias Correction Using Quantile Mapping\nGCM model data can be subject to systematic biases (e.g. Kotlarski et al. 2014) and their coarse resolution does not allow us to directly use these data in hydrological modeling studies. For this reason, a large number of downscaling and bias correction techniques have been developed, inclduing the well-known delta change method (see e.g. Feigenwinter et al. 2018 and references therein).\nThe daily CHELSA V21 data that became available in 2021, together with daily GCM data from CMIP6 now allows a relatively straight forward application of empirical quantile mapping to downscale to and statistically correct GCM for hydrological response units (HRUs).\nFeigenwinter et al. (2018) explains clearly how empirical quantile mapping works. For the historical period (also called calibration period in the context of the discussion here), simulated model output (in our case, GCM hist_sim data, as shown in Figure 7.8) is corrected with a correction function towards an observational reference (here, the high-resolution CHELSA climatology) and systematic model biases are partly removed.\n\n\n\n\n\nFigure 7.13: Overview on the bias correction approach: a bias correction function is calibrated by comparing raw climate model output to observations in a common historical reference period. The calibrated correction function is then applied to the entire raw model output in order to produce a bias-corrected time series out into the future scenario period (taken from (Feigenwinter et al. 2018)).\n\n\nIn a climate change context, the so-called correction function (or transfer function), established in the historical calibration period, can then be applied to the simulated future time series in order to produce bias-corrected scenario time series.\n\n\n\n\n\nFigure 7.14: The nature of empirical quantile mapping is shown (source: (Feigenwinter et al. 2018)). Left panel: Example based on the probability density function (PDF). Right panel: example based on the cumulative distribution function (CDF).\n\n\nFigure 7.14 explains the bias correction approach graphically. A biased simulated distribution (blue) is corrected towards an observed distribution (black). In the example shown the raw simulated distribution is subject to both a bias of the mean and a bias in variance. The resulting bias-corrected distribution (dashed red) approximates the observed one but is typically not identical to it (e.g. due to the sampling uncertainty during the calibration of the correction function or details of the specific quantile mapping implementation).\nAs we explained above, we investigate 4 climate scenarios (ssp126, ssp245, ssp 370 and ssp585) for which we have 4 GCM model runs each (GFDL-ESM4, UKESM1-0-LL, MPI-ESM1-2-HR and MRI-ESM2-0). Hence, we have 16 scenario-model combination and the same amounts of correction functions. The best way to achieve this is show the necessary steps by means of an example catchment. While each catchment is unique, the steps to pre-process and later export the corresponding climate files for modeling in RSMinerve are not and are thus generalizable.\nThe only caveat is that the processing of the CHELSA high-resolution climate files requires the very large raw files to be available locally. Due to the size of these files for the entire Central Asia domain, sharing these files is not easy and we are working on ways to make these files more readily available in the future so that they can be processes locally.\nFor the moment, each case study catchment in the student pack contains the precomputed climate files with which RSMinerve hydrological models can be run in a straight forward manner. These files are stored in the RS_Minerve folder. The following files are available:\n\n\nhist_obs_rsm.csv: This is the .csv-files that contains the CHELSA V21 temperature and precipitation forcing for each of the elevation bands as specified in the /GIS/XXXXX_hru.shp file where XXXXX is the placeholder for the corresponding gauge code.\n\nhist_sim_….csv: For the 4 climate models investigated, the simulated history is stored in these file. They are used for bias correction and not directly used in hydrological modeling.\n\nfut_sim_….csv: We have generated 16 such files which consist of 4 climate scenarios for each of the 4 climate models. These are the future temperature and precipitation time series that were bias corrected using the quantile mapping method as explained above.\n\nfut_sim_bcsd_….csv: These are the 16 final bias corrected and downscaled future climate forcing files. These are the .csv-files that are read into RSMinerve for the study of climate impacts on the river basin under consideration.\n\nThe processes of generating these files is shown in the following at the example of Chon Kemin catchment in Kyrgyzstan. For each catchment in the Students’ Case Study Pack, the code to process the files can be found in the corresponding CODE folder.\nFirst, the necessary libraries need to be loaded.\n\n# Tidy data wrangling\nlibrary(tidyverse) # includes readr and ggplot2\nlibrary(lubridate)\nlibrary(timetk)\n\n# plotting add-ons to ggplot2\nlibrary(patchwork)\n\n# Our own package for load and processing local data\ndevtools::install_github(\"hydrosolutions/riversCentralAsia\")\nlibrary('riversCentralAsia')\n\n# Spatial data processing\nlibrary(raster)\nlibrary(terra)\nlibrary(sf)\nlibrary(stars)\nlibrary(exactextractr)\n\n# quantile mapping\nlibrary(qmap)\n\nThen, we can simply configure the details of the catchment at hand (note the location installation dependent specification of paths!).\n\n# River\nriver_name &lt;- \"ChonKemin\"\nbasin_name &lt;- \"Chu\"\n\n# Gauge\ngauge_name &lt;- '15149_gauge'\ngauge_code &lt;- '15149'\nq_path &lt;- \"../caham_data/student_case_study_basins/15149_ChonKemin/GaugeData/\"\nq_name &lt;- paste0(gauge_code,\"_Q.csv\")\ndata_type_Q &lt;- \"Q\"\nunits_Q &lt;- \"m3/s\"\n\n# GIS \n#Important naming convention. We assume that GIS-files adhere to the following naming convention:\n#- Basin Shapefile: paste0(gauge_code,\"_basin.shp\")\n#- River Shapefile: paste0(gauge_code,\"_river.shp\")\n#- Junctions Shapefile: paste0(gauge_code,\"_junctions.shp\")\n#- HRU Shapefile: paste0(gauge_code,\"_hru.shp\")\n#- DEM Raster: paste0(gauge_code,\"_dem.tif\")\ngis_path &lt;- \"../caham_data/student_case_study_basins/15149_ChonKemin/GIS/\"\ndem_file &lt;- paste0(gauge_code,'_dem.tif')\ncrs_project &lt;- 4326 #latlon WGS84\n\nThere are a number of parameters to be set before the modeling which can be done as follows.\n\n# Time zone\ntz &lt;-  \"UTC\"\n\n# GCM Climate Models and Simulations/Experiments and data paths\nhist_obs_dir &lt;- \"../../../../../../../../../../../../Documents/ca_large_files/CA_CLIMATE_PROJECTIONS/CHELSA_V21_1979_2018/\" \nhist_sim_dir &lt;- \"../../../../central_asia_domain/climate/hist_sim/\"\nfut_sim_dir &lt;-  \"../../../../central_asia_domain/climate/fut_sim/\"\n\ngcm_Models &lt;- c(\"GFDL-ESM4\", \"IPSL-CM6A-LR\", \"MRI-ESM2-0\", \"UKESM1-0-LL\")\ngcm_Scenarios &lt;- c(\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\")\n\ngcm_Models_Scenarios &lt;- base::expand.grid(gcm_Models,gcm_Scenarios) |&gt;\n        dplyr::mutate(model_scenario_combination = paste0(Var1,\"_\",Var2)) |&gt;\n        dplyr::select(model_scenario_combination) |&gt; unlist() |&gt; as.character()\n\n# Historical Observations\nhist_obs_start &lt;- 1979\nhist_obs_end &lt;- 2011\nhist_obs_dates &lt;- riversCentralAsia::generateSeqDates(hist_obs_start,hist_obs_end,'day')\nhist_obs_dates &lt;- as_date(hist_obs_dates$date) |&gt; as_tibble() |&gt; rename(Date = value)\n\n# Historical GCM Simulations\nhist_sim_start &lt;- hist_obs_start\nhist_sim_end &lt;- hist_obs_end\nhist_sim_dates &lt;- hist_obs_dates\n\n# Future GCM Simulations\nfut_sim_start &lt;- 2012\nfut_sim_end &lt;- 2099\nfut_sim_dates &lt;- riversCentralAsia::generateSeqDates(fut_sim_start,fut_sim_end,'day')\nfut_sim_dates &lt;- as_date(fut_sim_dates$date) |&gt; as_tibble() |&gt; rename(Date = value)\n\n# Climate Data Observation Frequency\nobs_freq &lt;- \"day\"\n\n# RSMinverve\nmodel_dir &lt;- \"../../RS_MINERVE/\"\n\n## Dates\n\nhist_sim_dates &lt;- hist_obs_dates\nfut_sim_dates &lt;- riversCentralAsia::generateSeqDates(fut_sim_start,fut_sim_end,'day')\nfut_sim_dates &lt;- as_date(fut_sim_dates$date) |&gt; as_tibble() |&gt; rename(Date = value)\n\n\n\n\n\n\n\nIMPORTANT\n\n\n\nNote that you would have to set the data_paths in the above code block according to your own local installation. However, since the CHELSA raster stack files are not available in the Students’ Case Study directory, the code here serves just as a demonstration. It should be further noted that this below is an advanced section that requires a good understanding of the R programming language.\n\n\nAfter setting the parameters, we can generate the hydrological response units. As the adept reader realizes, we are creating the elevation bands in R/RStudio and do not resort to QGIS as has been shown in the previous Geospation Data Section.\n\n# Parameter definition for the generation of the elevation bands\nband_interval &lt;- 300 # in meters. Note that normally you want to work with band intervals of 100 m to 200 m. To make the model less computationally demanding, we work with a coarser resolution of 300 m. \nholeSize_km2 &lt;- .1 # cleaning holes smaller than that size\nsmoothFact &lt;- 2 # level of band smoothing\ndemAggFact &lt;- 2 # dem aggregation factor (carefully fine-tune this)\n## Delineation\nhru_shp &lt;- gen_basinElevationBands(gis_path,dem_file,demAggFact,band_interval,holeSize_km2,smoothFact)\n# Control output\nhru_shp |&gt; plot()\n\n\n\n\n\n\n\nIn other words, the function gen_basinElevationBands() from the riversCentralAsia R Package creates elevation bands (HRUs) as per the parameter values. In the above example, we are generating elevation bands with a 300 meters [m] bands interval. The smaller this number, the higher the number of elevation bands that will be generated and the higher the computational requirements will be of the hydrological model.\nAs a next step, we intersect the subbasins with elevtion bands. In the example of Chon Kemin, the basin corresponds to the one subbasin.\n\npath2subbasin_shp &lt;- paste0(gis_path,gauge_code,\"_basin.shp\")\nsubbasins_shp &lt;- st_read(path2subbasin_shp, quiet = TRUE)\nsubbasins_hru_shp &lt;- st_intersection(hru_shp,subbasins_shp)\n\nWe can extract and add mean elevation data for each HRU in the following way.\n\ndem &lt;- raster::raster(paste0(gis_path,dem_file))\nzonalStat_Z &lt;- exactextractr::exact_extract(dem, subbasins_hru_shp, 'mean', progress = FALSE)\nsubbasins_hru_shp$Z &lt;- zonalStat_Z\n\nIn a final step, we add unique subbasin names and remove the shapefile fields that are no longer needed in the next steps. The plot shows the resulting shapefile.\n\nfor (idxSubBasin in seq(length(subbasins_shp$name))) {\n  \n  subbasin_sel &lt;- subbasins_hru_shp |&gt; dplyr::filter(name == subbasins_shp$name[idxSubBasin])\n  subbasin_sel &lt;- subbasin_sel |&gt; dplyr::arrange(Z)\n  subbasin_sel$hru_num &lt;- (1:base::nrow(subbasin_sel))\n  subbasin_sel$name &lt;- paste0(subbasin_sel$name,'_',subbasin_sel$hru_num)\n  \n  if (idxSubBasin == 1) {\n    res_subbasins &lt;- subbasin_sel\n  } else {\n    res_subbasins &lt;- res_subbasins |&gt; dplyr::add_row(subbasin_sel)\n  }\n  \n}\n\nsubbasins_hru_shp &lt;- res_subbasins |&gt; dplyr::select(-layer,-hru_num)\nsubbasins_hru_shp |&gt; plot()\n\n\n\n\n\n\n\nThe resulting shapefile can then be written to local storage.\n\nsf::st_write(subbasins_hru_shp, paste0(gis_path, gauge_code, '_hru', '.shp'), append = FALSE)\n\nThe geometry of the hydrological modeling approach is now defined and we can start to extract and generate the climate forcing data. First, the historical observations (hist_obs) of temperature and precipitation for each HRU need to be defined. We show how this can be done in a straight forward manner using again helper functions from the riversCentralAsia package.\n\n# Parameters\nclimate_data_type &lt;- \"hist_obs\"\n\n# Load HRU shapefile\nsubbasins_hru_shp &lt;- \n  sf::st_read(paste0(gis_path,gauge_code,'_HRU','.shp'))\n\n# List CHELSA climate files\nclimate_files_tas &lt;- \n  list.files(hist_obs_dir,pattern = \"tas_\",full.names = TRUE)\nclimate_files_pr &lt;-  \n  list.files(hist_obs_dir,pattern = \"pr_\",full.names = TRUE)\n\n# Restrict years range\nn_years &lt;- \n  hist_obs_start:hist_obs_end\nclimate_files_tas &lt;- \n  climate_files_tas[1:length(n_years)]\nclimate_files_pr &lt;- \n  climate_files_pr[1:length(n_years)]\n\n# Temperature data processing\ntemp_or_precip &lt;- \"Temperature\"\nhist_obs_tas &lt;- riversCentralAsia::gen_HRU_Climate_CSV_RSMinerve(climate_files_tas,\n                                              river_name,\n                                              temp_or_precip,\n                                              subbasins_hru_shp,\n                                              hist_obs_start,\n                                              hist_obs_end,\n                                              obs_freq,\n                                              climate_data_type,\n                                              crs_project)\n\n# Precipitation data processing\ntemp_or_precip &lt;- \"Precipitation\"\nhist_obs_pr &lt;- riversCentralAsia::gen_HRU_Climate_CSV_RSMinerve(climate_files_pr,\n                                              river_name,\n                                              temp_or_precip,\n                                              subbasins_hru_shp,\n                                              hist_obs_start,\n                                              hist_obs_end,\n                                              obs_freq,\n                                              climate_data_type,\n                                              crs_project)\n\n# Combine extract climate tibbles.\nhist_obs_rsm &lt;- hist_obs_tas |&gt; add_column(hist_obs_pr |&gt; \n                                              dplyr::select(-Station),.name_repair = 'unique')\n\n# Add Discharge Data (monthly)\nq_dec &lt;- riversCentralAsia::loadTabularData(q_path,\n                                              q_name,\n                                              gauge_code,\n                                              gauge_name,\n                                              river_name,\n                                              basin_name,\n                                              data_type_Q,\n                                              units_Q)\nfunc_type_lib &lt;- list(mean = \"Q\")\nq_mon &lt;- aggregate_to_monthly(q_dec,func_type_lib)\n\nq_mon &lt;- q_mon |&gt; \n  mutate(date = floor_date(as.POSIXct(.$date,tz = tz), unit = \"day\")) \n# Note, the function above outputs dates as date class. \n# This causes problems down the road. \n# We address these by converting the date values to dttm class.\n\nq_mon &lt;- q_mon |&gt; dplyr::select(date, data) |&gt; \n  dplyr::filter(date &gt;= ymd(paste0(hist_obs_start, \"-01-01\"))) |&gt; \n  dplyr::filter(date &lt;= ymd(paste0(hist_obs_end, \"-12-31\")))\n\ndates_char_Q &lt;- \n  riversCentralAsia::posixct2rsminerveChar(q_mon$date, tz = \"UTC\") |&gt; \n  rename(Station = value) |&gt; \n  tibble::add_column(Q = (q_mon$data |&gt; as.character))\n\n# Get gauge location and elevation\ngauge_shp &lt;- sf::st_read(paste0(gis_path,gauge_code,\"_gauge.shp\"))\ndem &lt;- raster::raster(paste0(gis_path,dem_file))\ngauge_coord &lt;- gauge_shp |&gt; sf::st_coordinates()\ngauge_Z &lt;- exactextractr::exact_extract(dem,gauge_shp,'mean')\n\n# Combine everything\nhist_obs_rsm &lt;- dplyr::full_join(hist_obs_rsm,dates_char_Q, by = 'Station') \n\n# now finish off by giving the required attributes in the table for the discharge station\nhist_obs_rsm$Q[1] = data_type_Q\nhist_obs_rsm$Q[2] = gauge_coord[1]\nhist_obs_rsm$Q[3] = gauge_coord[2]\nhist_obs_rsm$Q[4] = 550\nhist_obs_rsm$Q[5] = data_type_Q\nhist_obs_rsm$Q[6] = 'Flow'\nhist_obs_rsm$Q[7] = units_Q\nhist_obs_rsm$Q[8] = 'Constant after'\n\n# Write final file to disk\nreadr::write_csv(hist_obs_rsm,paste0(model_dir,climate_data_type,\"_rsm.csv\"),na = \"NA\",col_names = FALSE)\n\nThe last line of code writes the result back to the disk. Please check in your Case Study Pack the folder /RS_MINERVE/ where the corresponding hist_obs_rsm.csv is stored. The file format corresponds to import requirements from the side of RSMinerve. You can open the text file either with a Spreadsheet program such as Excel or with a text editor. The daily time series of temperature, precipitation for each HRU and the discharge for the gauging station are stored in the columns with a header section. Since observed discharge data is only available on a monthly basis, the time series is filled with NA where there are no obervations available.\nFor the monthly mean discharge values, the following convention is adhered to. Monthly mean discharge values are written/stored in the first day of the month.\nNext, we can process the historical GCM runs in a similar fashion.\n\n# Parameters\noutput_file_dir &lt;- \"../../RS_MINERVE/\"\nclimate_data_type &lt;- \"hist_sim\"\nclimate_files_tas &lt;- list.files(hist_sim_dir,pattern = \"tas_\",full.names = TRUE)\nclimate_files_pr &lt;- list.files(hist_sim_dir,pattern = \"pr_\",full.names = TRUE)\n\n# Extract GCM Model-Specfic Data\nhist_sim_rsm &lt;- vector(mode = \"list\", length = length(gcm_Models))\nnames(hist_sim_rsm) &lt;- gcm_Models\n\nfor (idxGCM in seq(length(gcm_Models))) {\n  \n  temp_or_precip &lt;- \"Temperature\"\n  gcm_model &lt;- gcm_Models[idxGCM]\n  hist_sim_T &lt;- riversCentralAsia::gen_HRU_Climate_CSV_RSMinerve(climate_files_tas[idxGCM],\n                                              river_name,\n                                              temp_or_precip,\n                                              subbasins_hru_shp,\n                                              hist_sim_start,\n                                              hist_sim_end,\n                                              obs_freq,\n                                              climate_data_type,\n                                              crs_project)\n \n  temp_or_precip &lt;- \"Precipitation\"\n  hist_sim_P &lt;- riversCentralAsia::gen_HRU_Climate_CSV_RSMinerve(climate_files_pr[idxGCM],\n                                              river_name,\n                                              temp_or_precip,\n                                              subbasins_hru_shp,\n                                              hist_sim_start,\n                                              hist_sim_end,\n                                              obs_freq,\n                                              climate_data_type,\n                                              crs_project)\n  \n  hist_sim_rsm[[idxGCM]] &lt;- hist_sim_T |&gt; \n    tibble::add_column(hist_sim_P |&gt; dplyr::select(-Station),.name_repair = 'unique')\n  write_csv(hist_sim_rsm[[idxGCM]],\n            paste0(model_dir,climate_data_type,\"_\",gcm_model,\"_\",\n                   gauge_code,\"_\",hist_sim_start,\"_\",hist_sim_end,\".csv\"),\n            col_names = FALSE)\n  \n}\n\nThe same applies to the future climate scenario runs.\n\nclimate_data_type &lt;- \"fut_sim\"\n\n# Load HRU shapefile\nsubbasins_hru_shp &lt;- sf::st_read(paste0(gis_path,gauge_code,'_HRU','.shp'))\n\n# Process and Extract GCM Model-Specific Data\nfut_sim_rsm &lt;- base::vector(mode = \"list\", length = length(gcm_Models_Scenarios))\nnames(fut_sim_rsm) &lt;- gcm_Models_Scenarios # now we have a named list\n\ndebug_T &lt;- fut_sim_rsm\n\nfor (idxGCM in seq(length(gcm_Models_Scenarios))) {\n\n  # GCM Model and Scenario\n  str2proc &lt;- gcm_Models_Scenarios[idxGCM]\n  gcm_model &lt;- substr(str2proc, 1, nchar(str2proc) - 7)\n  gcm_scenario &lt;- substr(str2proc, nchar(str2proc) - 6 + 1, nchar(str2proc))\n\n  # Process files - tas\n  temp_or_precip &lt;- \"Temperature\"\n  climate_file_tas &lt;- \n    list.files(fut_sim_dir,pattern = paste0(\"tas_day_\",gcm_Models_Scenarios[idxGCM]),full.names = TRUE)\n  fut_sim_T &lt;- gen_HRU_Climate_CSV_RSMinerve(climate_file_tas,\n                                             river_name,\n                                             temp_or_precip,\n                                             subbasins_hru_shp,\n                                             fut_sim_start,\n                                             fut_sim_end,\n                                             obs_freq,\n                                             climate_data_type,\n                                             crs_project)\n  \n  # Process files - pr\n  temp_or_precip &lt;- \"Precipitation\"\n  climate_file_pr &lt;- \n    list.files(fut_sim_dir,pattern = paste0(\"pr_day_\",gcm_Models_Scenarios[idxGCM]),full.names = TRUE)\n  fut_sim_P &lt;- gen_HRU_Climate_CSV_RSMinerve(climate_file_pr,\n                                             river_name,\n                                             temp_or_precip,\n                                             subbasins_hru_shp,\n                                             fut_sim_start,\n                                             fut_sim_end,\n                                             obs_freq,\n                                             climate_data_type,\n                                             crs_project)\n  \n  # Final dataframe\n  fut_sim_rsm[[idxGCM]] &lt;- fut_sim_T |&gt; \n    tibble::add_column(fut_sim_P |&gt; dplyr::select(-Station),.name_repair = 'unique')\n  # Write result to disk\n  readr::write_csv(fut_sim_rsm[[idxGCM]],\n                   paste0(model_dir,climate_data_type,\"_\",gcm_model,\"_\",\n                          gcm_scenario,\"_\",river_name,\"_\",fut_sim_start,\"_\",\n                          fut_sim_end,\".csv\"),\n                   col_names = FALSE)\n}\n\nIn a final step, we can produce the bias corrected future climate scenarios with the following code.\n\n# Preparations\n## HRUs\nsubbasins_hru_shp &lt;- \n  sf::st_read(paste0(gis_path,gauge_code,'_hru','.shp'))\nn_hru &lt;- subbasins_hru_shp |&gt; nrow()\nhru_names &lt;- subbasins_hru_shp$name\n\n# ================\n# Prepare hist_obs\n# ================\nclimate_data_type &lt;- \"hist_obs\"\nhist_obs_path &lt;- paste0(model_dir,climate_data_type,\"_rsm.csv\")\nhist_obs_orig &lt;- hist_obs_path |&gt; \n  readr::read_csv(col_types = cols(.default = col_character())) |&gt;\n  dplyr::select(-Station,-Q)\n\n# Extract data by groups and convert T to deg. K\nhist_obs_T &lt;- hist_obs_orig[,1:n_hru] |&gt; slice(-1:-7) |&gt; \n  type_convert() |&gt; \n  mutate(across(.cols = everything(), ~ . + 273.15))\nhist_obs_P &lt;- \n  hist_obs_orig[, (n_hru + 1):(2 * n_hru)] |&gt; \n  slice(-1:-7) |&gt;  \n  type_convert()\n\n# Fix row names\nnames(hist_obs_T) &lt;- hru_names\nnames(hist_obs_P) &lt;- hru_names\n\nhist_obs_T_df &lt;- hist_obs_T |&gt; as.data.frame()\nrow.names(hist_obs_T_df) &lt;- hist_obs_dates$Date |&gt; as.character()\nhist_obs_P_df &lt;- hist_obs_P |&gt; as.data.frame()\nrow.names(hist_obs_P_df) &lt;- hist_obs_dates$Date |&gt; as.character()\n\n# ================\n# Prepare hist_sim\n# ================\nhist_sim_T_list &lt;- \n  base::vector(mode = \"list\", length = length(gcm_Models))\nhist_sim_P_list &lt;- \n  base::vector(mode = \"list\", length = length(gcm_Models))\nnames(hist_sim_T_list) &lt;- \n  gcm_Models # now we have a named list\nnames(hist_sim_P_list) &lt;- \n  gcm_Models # now we have a named list\n\nfor (idxGCM in 1:length(gcm_Models)) {\n  hist_sim_path &lt;- \n    list.files(model_dir,\n               pattern = paste0(\"hist_sim_\",gcm_Models[idxGCM]),full.names = TRUE)\n  hist_sim_orig &lt;- hist_sim_path |&gt; \n    readr::read_csv(col_types = cols(.default = col_character())) |&gt; \n    dplyr::select(-Station)\n  \n  hist_sim_T &lt;- hist_sim_orig[,1:n_hru] |&gt; slice(-1:-7) |&gt; \n    type_convert() |&gt; \n    mutate(across(.cols = everything(), ~ . + 273.15))\n  hist_sim_P &lt;- hist_sim_orig[,(n_hru+1):(2*n_hru)] |&gt; \n    slice(-1:-7) |&gt; \n    type.convert()\n  \n  # Fix row names\n  names(hist_sim_T) &lt;- hru_names\n  names(hist_sim_P) &lt;- hru_names\n  \n  hist_sim_T_df &lt;- hist_sim_T |&gt; as.data.frame()\n  row.names(hist_sim_T_df) &lt;- hist_sim_dates$Date |&gt; as.character()\n  hist_sim_P_df &lt;- hist_sim_P |&gt; as.data.frame()\n  row.names(hist_sim_P_df) &lt;- hist_sim_dates$Date |&gt; as.character()\n  \n  hist_sim_T_list[[idxGCM]] &lt;- hist_sim_T_df\n  hist_sim_P_list[[idxGCM]] &lt;- hist_sim_P_df\n}\n\n# ===============\n# Prepare fut_sim\n# ===============\nfut_sim_T_list &lt;- base::vector(mode = \"list\", length = length(gcm_Models_Scenarios))\nfut_sim_P_list &lt;- base::vector(mode = \"list\", length = length(gcm_Models_Scenarios))\nnames(fut_sim_T_list) &lt;- gcm_Models_Scenarios # now we have a named list\nnames(fut_sim_P_list) &lt;- gcm_Models_Scenarios\nfut_sim_T_list_bcsd &lt;- fut_sim_T_list\nfut_sim_P_list_bcsc &lt;- fut_sim_P_list\n\nfor (idxGCM in 1:length(gcm_Models_Scenarios)) {\n  fut_sim_orig_path &lt;- \n    list.files(model_dir,pattern = \n                 paste0(\"fut_sim_\",gcm_Models_Scenarios[idxGCM]),full.names = TRUE)\n  fut_sim_orig &lt;- fut_sim_orig_path |&gt; \n    readr::read_csv(col_types = cols(.default = col_character())) |&gt; \n    dplyr::select(-Station)\n  \n  fut_sim_T &lt;- fut_sim_orig[,1:n_hru] |&gt; \n    slice(-1:-7) |&gt; \n    type_convert() |&gt; \n    mutate(across(.cols = everything(), ~ . + 273.15))\n  fut_sim_P &lt;- fut_sim_orig[,(n_hru+1):(2*n_hru)] |&gt; \n    slice(-1:-7) |&gt; \n    type.convert()\n  \n  # Fix row names\n  names(fut_sim_T) &lt;- hru_names\n  names(fut_sim_P) &lt;- hru_names\n  \n  fut_sim_T_df &lt;- fut_sim_T |&gt; as.data.frame()\n  row.names(fut_sim_T_df) &lt;- fut_sim_dates$Date |&gt; as.character()\n  fut_sim_P_df &lt;- fut_sim_P |&gt; as.data.frame()\n  row.names(fut_sim_P_df) &lt;- fut_sim_dates$Date |&gt; as.character()\n  \n  fut_sim_T_list[[idxGCM]] &lt;- fut_sim_T_df\n  fut_sim_P_list[[idxGCM]] &lt;- fut_sim_P_df\n}\n\n# ===================\n# Do quantile mapping\n# ===================\n\n# --- Debugging\nfut_sim_rsm_qmapped &lt;- \n  base::vector(mode = \"list\", length = length(gcm_Models_Scenarios))\nnames(fut_sim_rsm_qmapped) &lt;- gcm_Models_Scenarios # now we have a named list\n# -----\n\nfor (idxGCM in 1:length(gcm_Models_Scenarios)) {\n  \n  # Preparation\n  str2proc &lt;- gcm_Models_Scenarios[idxGCM]\n  gcm_model &lt;- substr(str2proc, 1, nchar(str2proc) - 7)\n  gcm_scenario &lt;- substr(str2proc, nchar(str2proc) - 6 + 1, nchar(str2proc))\n  \n  # Bias correction\n  hist_sim_T_df_gcmModel &lt;- hist_sim_T_list[[gcm_model]]\n  hist_sim_P_df_gcmModel &lt;- hist_sim_P_list[[gcm_model]]\n  \n  fut_sim_T_df_gcmModel &lt;- fut_sim_T_list[[idxGCM]]\n  fut_sim_P_df_gcmModel &lt;- fut_sim_P_list[[idxGCM]]\n  \n  qmap_param_T_gcm &lt;- fitQmap(hist_obs_T_df, hist_sim_T_df_gcmModel, method = \"QUANT\")\n  qmap_param_P_gcm &lt;- fitQmap(hist_obs_P_df, hist_sim_P_df_gcmModel, method = \"QUANT\")\n  \n  # T bias correction\n  fut_sim_T_df_gcmModel_qmapped &lt;- \n    doQmap(fut_sim_T_df_gcmModel,qmap_param_T_gcm)\n  # Fill 0s where present. Occasionally, there are 0s resulting from the quantile mapping these \n  # are ironed out here by filling the missing values using the ones from the preceeding observation. \n  fut_sim_T_df_gcmModel_qmapped[fut_sim_T_df_gcmModel_qmapped == 0] &lt;- NA\n  fut_sim_T_df_gcmModel_qmapped &lt;- \n    zoo::na.locf(fut_sim_T_df_gcmModel_qmapped)\n  # P bias correction\n  fut_sim_P_df_gcmModel_qmapped &lt;- \n    doQmap(fut_sim_P_df_gcmModel,qmap_param_P_gcm)\n\n  # go back to tibble and convert back to deg. C\n  fut_sim_T_gcmModel_qmapped &lt;- \n    fut_sim_T_df_gcmModel_qmapped |&gt; as_tibble() |&gt; \n    add_column(Date = fut_sim_dates$Date,.before = 1)\n  fut_sim_T_gcmModel_qmapped &lt;- \n    fut_sim_T_gcmModel_qmapped |&gt; mutate(across(-Date, ~ . - 273.15))\n  fut_sim_P_gcmModel_qmapped &lt;- \n    fut_sim_P_df_gcmModel_qmapped |&gt; as_tibble() |&gt; \n    add_column(Date = fut_sim_dates$Date,.before = 1)  \n  \n  fut_sim_T_list[[idxGCM]] &lt;- fut_sim_T_gcmModel_qmapped\n  fut_sim_P_list[[idxGCM]] &lt;- fut_sim_P_gcmModel_qmapped\n  \n  # Export to .csv-file\n  fut_sim_qmapped &lt;- \n    fut_sim_T_gcmModel_qmapped |&gt; #dplyr::select(-Date) |&gt; \n    tibble::add_column(fut_sim_P_gcmModel_qmapped |&gt; \n                         dplyr::select(-Date),.name_repair = \"universal\") |&gt; \n    mutate(across(.cols = everything(),~ as.character(.))) |&gt; \n    dplyr::select(-Date)\n  \n  # --- Debugging\n  fut_sim_rsm_qmapped[[idxGCM]] &lt;- fut_sim_qmapped\n  # ---\n  \n  fut_sim_orig_path &lt;- \n    list.files(model_dir,pattern = paste0(\"fut_sim_\",gcm_Models_Scenarios[idxGCM]),\n               full.names = TRUE)\n  fut_sim_orig &lt;- \n    fut_sim_orig_path |&gt; readr::read_csv(col_types = cols(.default = col_character())) \n  \n  fut_sim_orig_header &lt;- fut_sim_orig |&gt; dplyr::select(-Station) |&gt; \n    dplyr::slice(1:7,) \n  fut_sim_qmapped &lt;- fut_sim_orig_header  |&gt; bind_rows(fut_sim_qmapped) |&gt; \n    add_column(Station = fut_sim_orig$Station,.before = 1)\n\n  # Write result to disk\n  climate_data_type &lt;- \"fut_sim_bcsd\"\n  readr::write_csv(fut_sim_qmapped,\n                   paste0(model_dir,climate_data_type,\"_\",gcm_model,\"_\",\n                          gcm_scenario,\"_\",river_name,\"_\",fut_sim_start,\"_\",\n                          fut_sim_end,\".csv\"),\n                   col_names = FALSE)\n}\n\nElevation dependent impact of climate change\nThe CHELSA v2.1 data set includes elevation dependent changes of forcing over time. That is, temperatures in high elevation bands (higher elevation band numbers in figure below) heat up more quickly than in lower elevation bands. Similarly is the change in precipitation more extreme in high elevation bands than in low elevation bands. Elevation dependent warming is discussed for example in Mountain Research Initiative EDW Working Group (2015).\n\nforcing &lt;- load_minerve_input_csv(\"../caham_data/student_case_study_basins/15149_ChonKemin/RS_MINERVE/fut_sim_bcsd_GFDL-ESM4_ssp585_ChonKemin_2012_2099.csv\")\n\nRows: 7 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (23): X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nforcinga &lt;- forcing |&gt; \n  mutate(year = hyear(date)) |&gt; \n  group_by(year, basin, type) |&gt; \n  summarise(value = ifelse(type == \"T\", mean(value), sum(value))) |&gt; \n  ungroup() |&gt;  \n  distinct() |&gt; \n  dplyr::filter(year &gt; min(year) & year &lt; max(year)) |&gt; \n  separate(basin, into = c(\"nameA\", \"nameB\", \"band\"), sep = \"_\") |&gt; \n  mutate(band = factor(band, levels = c(1:11))) |&gt; \n  unite(\"name\", c(nameA, nameB), sep = \"_\")\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n`summarise()` has grouped output by 'year', 'basin', 'type'. You can override\nusing the `.groups` argument.\n\nforcingatrend &lt;- forcinga |&gt; \n  group_by(name, band, type) |&gt; \n  mutate(diff = value - first(value)) |&gt; \n  ungroup() |&gt; \n  mutate(type = ifelse(type == \"P\", \"P [mm/a]\", \"T [deg C]\"))\n\nggplot(forcingatrend) +  \n  geom_line(aes(year, diff, colour = band)) + \n  facet_wrap(\"type\", scales = \"free_y\") +\n  scale_colour_viridis_d() + \n  ylab(\"Variable(t) - Variable(t=1)\") + \n  theme_bw()\n\n\n\nChanges in temperature and precipitation over time are elevation dependent in the Chon Kemin basin. Data source: CHELSA v2.1.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Climate Data</span>"
    ]
  },
  {
    "objectID": "climate_data.html#sec-climate-data-references",
    "href": "climate_data.html#sec-climate-data-references",
    "title": "7  Climate Data",
    "section": "\n7.3 References",
    "text": "7.3 References\n\n\n\n\nBeck, Hylke E., Eric F. Wood, Tim R. McVicar, Mauricio Zambrano-Bigiarini, Camila Alvarez-Garreton, Oscar M. Baez-Villanueva, Justin Sheffield, and Dirk N. Karger. 2020. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1.\n\n\nBuisán, Samuel T., Michael E. Earle, José Luı́s Collado, John Kochendorfer, Javier Alastrué, Mareile Wolff, Craig D. Smith, and Juan I. López-Moreno. 2017. “Assessment of Snowfall Accumulation Underestimation by Tipping Bucket Gauges in the Spanish Operational Network.” Atmospheric Measurement Techniques 10 (3): 1079–91.\n\n\n“CHELSA-W5E5 V1.0: W5E5 V1.0 Downscaled with CHELSA V2.0.” 2022. ISIMIP Repository. https://doi.org/10.48364/ISIMIP.836809.2.\n\n\nFeigenwinter, I., S. Kotlarski, A. Casanueva, A. M. Fischer, C. Schwierz, and M. A. Liniger. 2018. “Exploring Quantile Mapping as a Tool to Produce User-Tailored Climate Scenarios for Switzerland.” Technical Report 270. Federal Office of Meteorology; Climatology MeteoSwiss.\n\n\nHausfather, Zeke, Kate Marvel, Gavin A. Schmidt, John W. Nielsen-Gammon, and Mark Zelinka. 2022. “Climate Simulations: Recognize the ‘Hot Model’ Problem.” Nature 605 (7908): 26–29. https://doi.org/10.1038/d41586-022-01192-2.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nKarger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-resolution monthly precipitation and temperature time series from 2006 to 2100.” Scientific Data 7 (1): 248. https://doi.org/10.1038/s41597-020-00587-y.\n\n\nKarger, Dirk Nikolaus, Adam M. Wilson, Colin Mahony, Niklaus E. Zimmermann, and Walter Jetz. 2021. “Global daily 1 km land surface precipitation based on cloud cover-informed downscaling.” Scientific Data 8 (1): 307. https://doi.org/10.1038/s41597-021-01084-6.\n\n\nKotlarski, S., K. Keuler, O. B. Christensen, A. Colette, M. Déqué, A. Gobiet, K. Goergen, et al. 2014. “Regional climate modeling on European scales: a joint standard evaluation of the EURO-CORDEX RCM ensemble.” Geoscientific Model Development 7 (4): 1297–1333. https://doi.org/10.5194/gmd-7-1297-2014.\n\n\nMountain Research Initiative EDW Working Group. 2015. “Elevation-Dependent Warming in Mountain Regions of the World.” Nature Climate Change 5 (5): 424–30. https://doi.org/10.1038/nclimate2563.\n\n\nO’Neill, Brian C., Claudia Tebaldi, Detlef P. van Vuuren, Veronika Eyring, Pierre Friedlingstein, George Hurtt, Reto Knutti, et al. 2016. “The Scenario Model Intercomparison Project (ScenarioMIP) for CMIP6.” Geoscientific Model Development 9 (9): 3461–82. https://doi.org/10.5194/gmd-9-3461-2016.\n\n\nRiahi, Keywan, Detlef P. van Vuuren, Elmar Kriegler, Jae Edmonds, Brian C. O’Neill, Shinichiro Fujimori, Nico Bauer, et al. 2017. “The Shared Socioeconomic Pathways and their energy, land use, and greenhouse gas emissions implications: An overview.” Global Environmental Change 42: 153–68. https://doi.org/10.1016/j.gloenvcha.2016.05.009.\n\n\nTokarska, Katarzyna B., Martin B. Stolpe, Sebastian Sippel, Erich M. Fischer, Christopher J. Smith, Flavio Lehner, and Reto Knutti. 2020. “Past Warming Trend Constrains Future Warming in CMIP6 Models.” SCIENCE ADVANCES 6 (12): 14. https://doi.org/10.1126/sciadv.aaz9549.\n\n\nZelinka, Mark D., Timothy A. Myers, Daniel T. McCoy, Stephen Po-Chedley, Peter M. Caldwell, Paulo Ceppi, Stephen A. Klein, and Karl E. Taylor. 2020. “Causes of Higher Climate Sensitivity in CMIP6 Models.” Geophysical Research Letters 47 (1): e2019GL085782. https://doi.org/10.1029/2019GL085782.",
    "crumbs": [
      "Part II: Data Sources, Retrieval and Preparation",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Climate Data</span>"
    ]
  },
  {
    "objectID": "hydrological_modeling.html",
    "href": "hydrological_modeling.html",
    "title": "Part III: Hydrological Modeling & Applications",
    "section": "",
    "text": "This part of the book focuses on different types of hydrological modeling approaches and applications. The Chapter on hydrological modeling using rainfall-runoff models introduces modeling using the free-to-use RSMinerve Software Suite in a hands-on manner. These types of models are foundational, for example, for basin planning exercises where tradeoffs between water for different sectoral allocations need to be quantified in a specific context.\nSuch models are also crucial for detailed climate impact studies regularly used to study these. The idea is simple, i.e., to utilize available climate model output over the 21st century as forcing and investigating changes in the hydrographs at stations of interest over time. When different models and scenarios are run, a band of uncertainty relevant to any decision-making context can be specified.\nFinally, the design of hydropower infrastructure depends on hydrological assessments with such types of models. The model outputs, i.e., simulated (modeled) discharge at a particular location, can be used to compute cumulative flow duration curves, which are essential for assessing the hydropower potential of a site and critically informing infrastructure sizing.\nThe relevance of these types of models for water planners and managers in the global drylands cannot be overstated. Therefore, one of the primary goals of this course is to familiarize the students well with such types of models.\nThe Chapter on long-term hydrological modeling using the Budyko framework looks at the greater semi-arid Central Asia region compared to individual catchments. It is at this scale and over many smaller catchments where interesting steady-state patterns of the partitioning of available water into evaporation and runoff can be studied under current and future climate states. Among other applications, such models can help to inform large-scale questions about the current and future interstate water distribution.\nFinally, the Chapter on time series modeling using predictive inference discusses models that learn from past patterns to predict the future without explicit water balance constraints. Through the learning of patterns in time-ordered (time series) data, possibly also with the help of auxiliary data such as snow cover, snow water equivalent, or climate forcing and forecasts, it has been shown that time series models can be powerful in predicting discharge at particular locations for different lead times, from hours to seasons. The section will present such approaches in the context of the seasonal forecasting of river flows in Central Asia.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications"
    ]
  },
  {
    "objectID": "hydraulic_hydrological_modeling.html",
    "href": "hydraulic_hydrological_modeling.html",
    "title": "8  Hydrological-Hydraulic Modeling",
    "section": "",
    "text": "8.1 Prerequisites\nBefore delving into this section of the course material, you should have at least:\nYou may have to dig deeper into the user manual and the technical manual within the frame of the course but the above points are the minimum requirement to get started.\nIf you fulfill the prerequisites, you should be able to do the tutorial with the minimal description in the modeling section. However, detailed step-by-step descriptions are linked for each task.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hydrological-Hydraulic Modeling</span>"
    ]
  },
  {
    "objectID": "hydraulic_hydrological_modeling.html#section-rsminerve-prerequisites",
    "href": "hydraulic_hydrological_modeling.html#section-rsminerve-prerequisites",
    "title": "8  Hydrological-Hydraulic Modeling",
    "section": "",
    "text": "RS Minerve installed How to install RS Minerve. This tutorial was written using RS Minerve 2.9.1.0.\nRead chapters 1 and 2 from the RS Minerve user manual (Foehn et al. 2020) (the manual can be downloaded from the CREALP website, see Section How to install RS Minerve).\nFamiliarize yourself with RS Minerve by following Example 1 in the RS Minerve User Manual.\nRead the HBV model description in the RS Minerve technical manual (Chapter 2.7) (Garcia Hernandez et al. 2020).",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hydrological-Hydraulic Modeling</span>"
    ]
  },
  {
    "objectID": "hydraulic_hydrological_modeling.html#section-general-modelling-process",
    "href": "hydraulic_hydrological_modeling.html#section-general-modelling-process",
    "title": "8  Hydrological-Hydraulic Modeling",
    "section": "\n8.2 General Modeling Process",
    "text": "8.2 General Modeling Process\nHydrological modeling is an iterative process (see Figure 8.2). At the beginning, the purpose of the model has to be defined. The model goal determines the spatial and temporal resolution of the hydrological model. E.g. for analyzing the impact of climate change on seasonal, regional flows we are interested in the seasonal flow volumes whereas a model used for early warnings from floods requires high temporal resolution and a high model performance for threshold overflows. See for example Blöschl and Sivapalan (1995) for a thorough discussion of scales in hydrological modeling. Please note that the definition of the model goal also determines the performance criteria the model has to fulfill in order to be judged suitable for its purpose. As a next step, data about the catchment is gathered (see for example basin characterization in the Chapter Case Studies) and a first conceptual model of the dominant flow processes in the basin is drafted (e.g. low peaks and significant base flow point to large storage capacity in the basin whereas high peaks and low base flow indicate small storage capacity in the basin). The conceptual model of the river catchment is then implemented with a mathematical model (e.g. the HBV model in RS Minerve) and parameterized as well as possible (i.e. using reasonable estimates for initial parameter values). The model is then calibrated and validated (i.e. tested with a previously unseen data set) as described further below If the performance criteria for the model calibration are satisfied, a conscientious modeler performs a sensitivity analysis to gain confidence in the model. From each of these steps, the modeler can (and generally has to) go back to a previous step to gather more data, modify the conceptual model, adapt the model implementation, re-calibrate the model and/or re-evaluate the model sensitivity. Also, the modeler is aware of the inherent uncertainties of the data and the model (see for example Refsgaard et al. (2007) for a discussion on how to include uncertainties in the modeling process). The described steps produce a model (or an ensemble of models) that satisfy the performance criteria, only now the modeler can start actually using the model for the purpose it was implemented for.\n\n\n\n\n\n\n\nFigure 8.2: The modelling process is iterative.\n\n\n\n\nModeling is an involved process requiring highly specialized knowledge and skills. A modeler has the responsibility to clearly state the underlying assumptions, uncertainties and limitations of their model, especially if it is to be used in decision making. This book chapter will guide you through the modeling process with the example of the Nauvalisoy catchment.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hydrological-Hydraulic Modeling</span>"
    ]
  },
  {
    "objectID": "hydraulic_hydrological_modeling.html#section-linear-reservoir-model",
    "href": "hydraulic_hydrological_modeling.html#section-linear-reservoir-model",
    "title": "8  Hydrological-Hydraulic Modeling",
    "section": "\n8.3 The Simplest Model - The Linear Reservoir Model",
    "text": "8.3 The Simplest Model - The Linear Reservoir Model\nWhy do we do hydrological modeling? Typically, we want to know how much river flow we can expect at a given time in the future so we can plan our water consumption ahead, harness against floods or implement measures against droughts. So how can we forecast river discharge? For example, one could use the long term seasonal average discharge as the likely future discharge. However, in some years we have high discharge and in some years we have low discharge (Figure 8.3). How to tell when you have which discharge?\n\n\n\n\n\n\n\nFigure 8.3: Seasonal discharge of the Nauvalisoy river illustrating the variability of the discharge.\n\n\n\n\nLet’s go back to the water balance: Where does the water in the river come from? Precipitation. When we compare precipitation and discharge time series we see that the two are related (Figure 8.4).\n\n\n\n\n\n\n\nFigure 8.4: Discharge (red) and precipitation (grey) in the Nauvalisoy catchment.\n\n\n\n\nHigher precipitation in the catchment is positively correlated with higher discharge in the river with a delay. Or, in other words: The discharge is the catchments hydrological response to precipitation. The relationship is can be expressed as follows:\n\\[\nQ \\propto P \\cdot K\n\\]\nIn words, this reads: Discharge \\(Q\\) is proportional to precipitation \\(P\\) times a transfer function \\(K\\) that defines the delay of the signal. The transfer function \\(K\\) describes how precipitation becomes river discharge. It depends on the catchment characteristics, importantly on the storage capacity of the catchment. In reality, the transformation of precipitation to discharge is quite complex but we can simplify the reality and come up with a basic conceptual model of our river catchment: a large bath tube with the area of the catchment and an outlet at the bottom. Such a conceptual model is called the linear reservoir model (Figure 8.5).\n\n\n\n\n\n\n\nFigure 8.5: From the real-life hydrologicl system (left) to a linear reservoir (right).\n\n\n\n\nThe linear reservoir model describes the discharge from the reservoir as linearly proportional to the storage in the reservoir:\n\\[\nQ(t) = 1/k \\cdot S(t)\n\\]\nWhere \\(Q\\) is discharge, \\(S\\) is the storage and \\(k\\) is a constant called storage coefficient depending on the storage capacity of the reservoir. If \\(k\\) is small, the stored water will run out quickly, if \\(k\\) is large, the stored water will run out slowly. Time-dependent variables are indicated with \\((t)\\), i.e. variables that change over time.\nSubstitute the linear reservoir equation into the water balance:\n\\[\nP = Q + dS/dt \\Leftrightarrow P = Q + k \\cdot dQ/dt\n\\]\nWhere \\(P\\) is the excess precipitation, i.e. the precipitation that is not intercepted by plants or ponds and not evapotranspirated back to the atmosphere but contributes to discharge. The above is a differential equation and can be rearranged:\n\\[(P-Q) \\cdot dt = k \\cdot dQ\\]\nand integrated and solved for \\(Q(t)\\) (dig out your math skills!). In the example in the box below, we discretized the water balance with the linear relationship between discharge and storage change in the reservoir:\n\\[\\frac{P(t_1)+P(t_1)}{2} - \\frac{Q(t1)+Q(t_2)}{2} = \\frac{k}{t_2-t_1} \\cdot (Q(t_2)-Q(t_1))\\]\nand solved the equation for \\(Q(t_2)\\) (Inspiration from Pedersen, Peters, and Helweg (1980)).\nWe assume that at the beginning of our experiment, we don’t have a discharge as the bucket is empty.\n\\[Q(t) = P(t) \\cdot (1-e^{(-t/k)})\\]\nThis is the equation for the rising discharge curve. Once you turn off the recharge of the reservoir (once you stop pouring water into the bucket), the discharge from the bucket can be described as:\n\\[Q(t) = Q^* \\cdot e^{-\\tau/k}\\]\nWhich describes the receding limb of the hydrograph (What is a hydrograph) with \\(\\tau\\) being the time since the recharge stopped.\nUnderstanding the linear reservoir model is the first stepping stone to hydrological modeling as many conceptual models build up on linear reservoirs. This includes the model presented next, i.e., the HBV model.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hydrological-Hydraulic Modeling</span>"
    ]
  },
  {
    "objectID": "hydraulic_hydrological_modeling.html#section-hbv-model",
    "href": "hydraulic_hydrological_modeling.html#section-hbv-model",
    "title": "8  Hydrological-Hydraulic Modeling",
    "section": "\n8.4 The HBV Model",
    "text": "8.4 The HBV Model\nThe HBV (Hydrologiska Byråns Vattenavdelning) model is a conceptual rainfall-runoff model which is suitable for many snow-fed river catchments (Bergström 1980; Lindström et al. 1997). We recommend you to study the short description on the HBV model in the RS Minerve technical manual (Garcia Hernandez et al. 2020). Keep these 4 pages at hand for the following exercises. Like this, you can look up how the parameters influence the flow of water through the HBV model. A brief reminder of the HBV model implemented in RS Minerve is given in Figure 8.6.\nIn RS Minerve, the catchment area is typically subdivided into areas with similar hydrological properties, so-called hydrological response units or HRUs, for which the discharge is computed individually. Several HRUs can be connected. This approach is often numerically more efficient than a gridded representation of the catchment area via a domain discretization.\n\n\n\n\n\n\n\nFigure 8.6: The HBV model concept (source: RS MINERVE Technical Manual).\n\n\n\n\nWater enters the HBV model in the form of precipitation \\(P\\), which is a model forcing provided by the user. Precipitation is separated into solid (snow fall \\(SF\\)) and liquid (rainfall \\(RF\\)) depending on the threshold temperature \\(T\\), the second model forcing provided by the user. The uppermost component of the HBV model is the snow model (the two top-most blue reservoirs in Figure 8.6). A degree-day-melt model melts snow and refreezes water (\\(M_{sn}\\)), depending on the meteorological conditions.\nThe rainfall plus the snow melt are accumulated to the precipitation equivalent \\(P_{eq}\\) which enters the uppermost soil layer represented in HBV by the soil humidity reservoir of the root zone. This non-linear reservoir represents the loss of water to direct evaporation and to plant evaporation. Potential evaporation \\(ETP\\) thereby is another model forcing, albeit optional, as it can be provided by the user or calculated based on temperature and latitude by RS Minerve.\nWater that is not evaporated or held in the root zone is recharged to deeper soil layers, the upper reservoir. The upper reservoir constitutes the unsaturated zone below the root zone and produces the direct runoff or near-surface runoff (\\(Q_r\\)) that occurs when the soils are saturated and the interflow (\\(Q_u\\)) which is a lateral discharge in the unsaturated zone.\nThrough deep percolation (\\(i_{Perc}\\)) water reaches the lower reservoir which corresponds to the saturated zone, the groundwater layer, and produces the baseflow or the lateral discharge of groundwater (\\(Q_l\\)). By the way, the lower reservoir is very similar to the linear reservoir discussed above with the exception of an upper storage limit (\\(SL\\)).\nThe several reservoirs of the HBV model come with a multitude of parameters (Figure 8.7) which have to be adjusted such that the model reproduces the measured discharge of the basin, a process called model calibration which will be discussed in more detail further on in this chapter. Here, we’d like you to note the number of model states and parameters or degrees of freedom the HBV model has and to compare it to the amount of measurements of model states that is actually available to constrain the model parameters.\n\n\n\n\n\n\n\nFigure 8.7: The HBV model parameters (source RS MINERVE Technical Manual).\n\n\n\n\nIn the HBV model, 6 parameters need to be constrained alone for the snow model (Figure 8.7). Typically, measurements of the states of the model reservoirs (\\(SWE\\) and \\(WH\\), \\(Hum\\), \\(SU\\), \\(SL\\)) are not available. Thus, more often than not, parameters of hydrological models are solely calibrated based on the aggregated total discharge of a basin. A lot of care should be given to trying to constrain the model parameters using combinations of different approaches:\n\nMinimize the number of parameters in your model. This is done by fixing the least sensitive parameters to literature values.\nCompare to parameter values from the literature for similar catchments.\nLook for data sets that may be used to calibrate parts of the model.\nDo not only minimize the error between the measured and the simulated discharge but look at all the models states and fluxes and make sure they are physically meaningful.\n\nLuckily, research into suitable data products to further constrain hydrological models exists. The simulated snow water equivalent may be compared to the snow water equivalent (SWE) available from a) the High Mountain Asia Snow Reanalysis or the SLF operational snow model product as mentioned earlier (see Section 6.2 for more information). Like this, additional data can be used to further constrain model parameters in the calibration step.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hydrological-Hydraulic Modeling</span>"
    ]
  },
  {
    "objectID": "hydraulic_hydrological_modeling.html#the-gsm-model",
    "href": "hydraulic_hydrological_modeling.html#the-gsm-model",
    "title": "8  Hydrological-Hydraulic Modeling",
    "section": "\n8.5 The GSM Model",
    "text": "8.5 The GSM Model\nSimilar to the HBV model, the GSM model separates precipitation into solid and liquid precipitation, depending on temperatures, and features a snow pack module. In addition to the HBV snow module, the GSM snow module allows seasonal varying snow melt coefficients using a sinusoidal modulation. Details can be read in the RS MINERVE technical manual (Garcia Hernandez et al. 2020) (see Figure 8.8).\n\n\n\n\n\n\n\nFigure 8.8: The GSM model concept (source: RS MINERVE Technical Manual).\n\n\n\n\nThe melting of the snow pack yields the so called equivalent precipitation \\(P_{eq}\\). Runoff from snow melt is subsequently produced through release from a linear reservoir. Glacier area is assumed to be constant in the GSM model and glacier melt only occurs when the snow cover above the glacier has melted away. Also the glacier melt coefficient can be modulated according to a sinusoidal function, analogue to the snow melt coefficient. Runoff from glacier melt is then produced after transformation of the glacier melt signal through a linear reservoir.\nThe GSM model has a large number of model parameters which need to be calibrated. Snow height data on glaciers is generally not available from in-situ measurements but in recent years, estimates on glacier thickness, thinning rates or glacier discharge rates have become publicly available (see Chapter 6). These data may be used to guide GSM model calibration (see Section 8.7).\n\n\n\n\n\n\nRS Minerve GSM model limitations\n\n\n\nRS Minerve does not implement a glacier mass balance with GSM. Significant factors like the change of the glacier geometry as a glacier is melting, are neglected. Further, the GSM implementation of RS Minerve assumes an infinitive glacier reservoir. That means that glacier melt is produced even if glacier thickness becomes negative. This is typically not an issue during short term simulations of a couple of decades as long as glaciers do not disappear during the simulation period. For climate change impact studies however, smaller and lower-lying glaciers are expected to disappear completely. The currently implemented GSM version cannot deal with this situation.\nFor simple, linear models with no downstream water uses implemented, the glacier discharge can be set to zero after the simulation run. For more complex models with downstream water uses however, a simulation needs to be stopped regularly to check the status of the glacier thickness. If the glacier volume in a HRU has melted away, glacier melt in that HRU needs to be turned off. This can for example be done by setting the melt coefficients to 0.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hydrological-Hydraulic Modeling</span>"
    ]
  },
  {
    "objectID": "hydraulic_hydrological_modeling.html#sec-the-rs-minerve-modeling-environment",
    "href": "hydraulic_hydrological_modeling.html#sec-the-rs-minerve-modeling-environment",
    "title": "8  Hydrological-Hydraulic Modeling",
    "section": "\n8.6 The RS Minerve Modeling Environment",
    "text": "8.6 The RS Minerve Modeling Environment\nThe reader should already be familiar with RS Minerve through completion of Example 1 in the RS Minerve user guide (Foehn et al. 2020). The following paragraphs demonstrate how to automatically implement a hydrological model in RS Minerve using an example basin from Central Asia. The basin of the river Chon Kemin hereby serves as a demonstration site. All data are available in the Student Case Study Pack.\n\n8.6.1 Loading GIS Data\nOpen RS Minerve and move to the GIS tab (1). Click on the Add Layers button in the top right corner of the menu bar (2). An explorer window will pop-up where you select the layers you want to load (3). You require the HRU, junction and river layers which have been prepared for you in the student case study pack.\n Click Open (4) to close the explorer window and to load the selected GIS layers into RS Minerve. The result is shown in Figure 8.9.\n\n\n\n\n\nFigure 8.9: GIS data loaded in RS Minerve\n\n\nRS Minerve offers limited GIS utilities. You can, for example, change the colors of the GIS layers in the layer properties. The usage is similar to QGIS. You can further inspect the layer attributes by selecting a layer (1) and clicking the Table Info button (2) to open the attribute table.\n\n\n\n\n\nFigure 8.10: View the attribute table of an example layer in RS Minerve.\n\n\n\n8.6.2 Automatic Model Generation\nTo automatically generate the model structure, make sure, the 3 required GIS layers are loaded (see Section above), then click on the Creation button in the Model Management toolbox (1). The model creation configuration window will pop up where you select the appropriate attributes for each model layer from the drop-down menus (2, 4). Tick the Altitude (Z) option in the Subbasins properties and select the Z attribute (3) to let RS Minerve know the altitude of each HRU.\n\n\n\n\n\nFigure 8.11: Configuration of the model creation utilities. Selecting the appropriate attributes from the GIS layers.\n\n\nThen select all subbasins in the Allocate Subbasins Type window (5) and click on the model object selector (6).\nSelect the HBV model (1) and click the Allocate Subbasins button (2).\n\n\n\n\n\nFigure 8.12: Configuration of the model creation utilities. Selecting hydrological models for each HRU.\n\n\nAll the subbasins should now display the red HBV icon as shown in Figure 8.14. If you have glaciers in your basin and prepared your subbasins layer accordingly (see Section 5.3), you now select the glacier HRUs and allocate GSM objects to them (Figure 8.13).\n\n\n\n\n\nFigure 8.13: Configuration of the model creation utilities when glacier runoff is simulated using GSM. Assign HBV model objects to all non-glacierised HRUs intended for modelling with HBV and GSM model objects for glacierised HRUs.\n\n\nNext, select the river stretch ChonKemin (1), select the Kinematic Wave in the drop-down menu (2) and press Allocate Rivers (3). The icon of the rivers sections should subsequently be colored in the Kinematic Wave colors (not shown).\n\n\n\n\n\nFigure 8.14: Configuration of the model creation utilities. Selecting hydrodynamic stream flow models for each river section.\n\n\nFinally, press the Create Model button in the lower right corner of the RS Minerve window (4). If the model creation process is successful, the message Creation model finished. appears in the lower left corner of the RS Minerve window (5). If the model creation process is not successful, an error message will appear instead at the same location which will support you in identifying the problem.\nYou can now switch to the Model tab and look at the automatically generated model layout (Figure 8.15).\n\n\n\n\n\nFigure 8.15: Automatically generated model layout of the Chon Kemin catchment.\n\n\nThe river stretch is no longer required. Delete it by selecting it with a left-click on the river object and then pressing delete in the Editing Tools menu in the header bar.\nYou can beautify the model view by loading a background image of the Chon Kemin catchment (for example a screen shot from QGIS, Figure 8.16) and by re-arranging the HRUs (click and drag) and climate stations for better readability.\n\n\n\n\n\nFigure 8.16: Load a background image into RS Minerve by clicking Background (1) Load (2) , selecting the image (3) and pressing Open (4). Exit the background image editing mode by clicking (1) again.\n\n\nIf you have generated GSM model objects, each of them will have a virtual weather station assigned to it. We approximate the weather forcing on the glaciers with the weather forcing in the respective HBV HRU for which we have already prepared the input file (see Chapter 7). We delete the automatically generated station objects connected to the GSM objects and connect the station objects of the appropriate HBV object (same elevation layer) to the GSM objects. Stations at high elevations in glaciated basins thus are connected to both HBV and GSM model objects (Figure 8.17). Attention, do not forget to specify the initial glacier thickness in the initial conditions of the GSM model objects.\n\n\n\n\n\nFigure 8.17: If GSM model objects are created, delete the automatically generated virtual weather stations connected to the GSM model objects and connect the GSM model objects to the virtual weather station objects of the HBV model objects in the same elevation band and sub-basin.\n\n\nNow we need to add Comparator and Source Objects in order to be able to compare the simulated river discharge to the observed discharge. Step-by-step, you need to click on the Source Object in the model object list (1). A small faucet icon will appear in lieu of your customary cursor icon. Move your cursor to an appropriate location in the map window and click to place the source object (2). Do the same for the Comparator Object and activate it by clicking the object (3) and place it with a click in the map window (4).\n\n\n\n\n\nFigure 8.18: Adding source and comparator object to the model layout.\n\n\nThe new objects now need to be connected to the rest of the model objects. Activate the Connections mode in the Editing tools box of the header bar (5). Then draw a line from the source object to the comparator object (6). In the pop-up window, specify that the source object is your reference (7). Then acknowledge by clicking Ok.\nIn the same manner, draw a connection from the Junction Object (15149_gauge) to the Comparator (1) and specify that the Junction represents your simulated discharge (2).\n\n\n\n\n\nFigure 8.19: Connecting the new objects to the rest of the model objects.\n\n\nDeactivate the connection mode by clicking the Select button (3). You can rename the new objects by clicking on their names. The basic model layout is now finished but there are still a few required steps before we can run the model.\nWith the present model, we can now simulate discharge from rainfall and snowmelt. We can further simulate the evolution of the water storage in different compartments in the catchments. What about glacier melt? Glacier melt can be simulated in RS Minerve with the GSM model. However, the GSM model is only marginally suitable for long-term scenario simulation. We therefore suggest a different glacier modeling strategy where discharge from glacier melt is simulated externally and then included in RS Minerve as a source. The process is described in Chapter Chapter 11.\n\n8.6.3 Loading Climate Data\nThe hydrological model needs forcing data, i.e., precipitation and temperature. The following sections illustrate how to load the preprocessed climate data into RS Minerve.\nNavigate to the Database tab (1) and click the Open button (2). This opens an explorer pop-up window where you choose CSV files in the drop-down menu (3) and then select the file hist_obs_rsm.csv. Close the explorer window by pressing the Open button (hidden beneath the drop-down menu in ?fig-rsm-load-db-01).\nIn the navigation window on the left, a database icon will appear. Explore the Database by clicking on the triangle next to the icon. You can display the data in the database by clicking on the Sensors (1), typically T (temperature), P (precipitation) and Q (discharge), in the navigation window. A time series plot will then appear in the main window. You can also explore the data in the table format by switching to the Values tab (2).\nNote the time range of the forcing data (from Jan 1, 1979 0:00 to Dec. 21, 2011 0:00). The time range of the forcing determines the time range of the simulation. Also note the time interval between the data (daily data for the forcing and decadal data for the discharge).\nClick on New group (1, not visible because the name is already changed in Figure 8.20) and change the name of the group to Climate (2). In the dropdown menu, select Input for the data category (3).\n\n\n\n\n\nFigure 8.20: Loading climate forcing into RS Minerve\n\n\nWe now move the discharge time series to a separate data group: Select Database (1) then click Add (2). A new group appears at the bottom of the data list (3). Select New group (3) and change the name (4) to Discharge and select Input from the dropdown menu (5).\n\n\n\n\n\nFigure 8.21: Loading climate forcing into RS Minerve\n\n\nNavigate to the discharge time series Q (1) and do a right-click. In the pop-up window that appears select Cut (2).\n\n\n\n\n\nFigure 8.22: Cut the discharge time series from the climate data group.\n\n\nSelect Discharge (1) and click Add (2) to add a new data set (3). Right-click on New Dataset and select Paste to put the discharge sensor data at the new location. You can rename New Dataset.\n\n\n\n\n\nFigure 8.23: Paste the discharge time series to the discharge data group.\n\n\nThe database is now ready, next we need to link the data to the model objects. Navigate to the Model tab. Under Data source select the appropriate data groups for the station data objects and for the source objects (1) and select the appropriate Dataset (2). The names in the database tab should already be consistent with the ones in the objects and the linking for the weather stations works automatically.\nThen adapt the Start and End of the simulation period (4). Start cannot be earlier than the first forcing in the database and End cannot be later than the last forcing in the database. The simulation period can, however, be shorter than the forcing time series. The simulation time step is 1 day and the recording time step is 1 month (4).\nThe for the linking of the discharge data with the source we still need to double-click on the source object in the map (1) and select Q * Q in the drop-down menu (3).\n\n\n\n\n\nFigure 8.24: Linking the discharge with the source object\n\n\n\n8.6.4 Configuration\nLast but not least we need to tell RS Minerve to compute the evaporation flux for us. This step needs to be done only once for each model and is described in the Quick guide. Don’t forget to save your model.\n\n8.6.5 First Model Run\nNow click Validation (1) to let RS Minerve test if the model is valid.\nIn case the model is valid it will say so in the lower left corner of the RS Minerve window (2). A more detailed report appears in the output window on the right (3). The warning can be ignored as the discharge of the source is not a required input for the model simulation.\nNow press Start (1). The simulation is done fairly quickly. With a double-click on the comparator object in the center window the output window fills with a lot of information. The Results table shows a number of indicators of model goodness. Here we will only briefly mention the most commonly encountered2.\nThe relative root mean squared error (RSME) ranges from 0 to \\(\\infty\\) and should be minimized. \\[\n\\text{RRMSE} = \\frac{ \\sqrt{ \\frac{1}{N} \\cdot \\sum_{t=1}^N \\Big( Q_{sim}(t) - Q_{obs}(t)\\Big)^2}}{\\overline{Q_{obs}(t)}}\n\\]\nThe Nash-Sutcliffe efficiency (Nash) ranges from -\\(\\infty\\) to 1 and should be maximized. Nash of 0 means that the model has the same performance as the long-term average of the measured data. \\[\n\\text{Nash} = 1 − \\frac{\\sum_{t=1}^{N} \\Big( Q_{sim}(t) - Q_{obs}(t) \\Big)^2}{\\sum_{t=1}^{N} \\Big( Q_{sim}(t) - \\overline{Q_{obs}(t)} \\Big)^2}\n\\] Where \\(t\\) is the simulation time step, \\(N\\) is the number of simulation time steps, \\(Q_{sim}(t)\\) is the simulated discharge and \\(Q_{obs}(t)\\) is the observed discharge at time \\(t\\). \\(\\overline{Q_{obs}(t)}\\) refers to the average of the observed discharge.\nIf your customer is especially interested in low discharge periods, the Nash coefficient for logarithm values is recommended (see Garcia Hernandez et al. (2020)). There are further performance indicators focusing on the discharge volume (Relative Volume Bias) or on the exceedance of a threshold flow, which can be specified in the parameters of the comparator.\n\n\n\n\n\nFigure 8.25: First model run.\n\n\nThe graph (4) shows the measured discharge in blue (QReference) and the simulated discharge in green (QSimulation).\nNote that QSimulation starts with 0. This is because RS Minerve initializes the initial water content of the HBV modules with 0 unless something else is specified. You should click Conditions on Dec. 12, 2011 (2), apply and re-run the model to obtain a properly initialized model.\nTo view the default initial conditions (IC) and parameters of a model object (2), double-click on the model object in the main model window (1). You can edit the IC and parameters in the tables of the result window (2). You can view properties of several objects the IC viewer (and similarly in the parameters viewer) by clicking on the Initial conditions button in the Model Properties toolbar (3). Select the object type (4) and the zone (5) and scroll through the IC table (6).\n\n\n\n\n\nFigure 8.26: Viewing initial model states and model parameters\n\n\nIt is recommended to save a backup of the initial parameter set. During model model calibration experiments the modeler can sometimes end up with unrealistic parameter combinations and then it is appropriate to reset the model and start afresh. Click on Export P in the Model Properties toolbar to save the default parameters as a text file which can be imported using the Import P button.\nThe model produces the correct seasonality but the water balance is way off. The RS Minerve model produces more discharge than is observed at the monitoring station. We fix this by calibrating the model. This is a big task and will be treated in the next chapter.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hydrological-Hydraulic Modeling</span>"
    ]
  },
  {
    "objectID": "hydraulic_hydrological_modeling.html#sec-model-calibration",
    "href": "hydraulic_hydrological_modeling.html#sec-model-calibration",
    "title": "8  Hydrological-Hydraulic Modeling",
    "section": "\n8.7 Model Calibration",
    "text": "8.7 Model Calibration\nThe large number of parameters of the HBV model can be adjusted within reasonable ranges to make the simulated river discharge more similar to the measured discharge. This model calibration process is sometimes also called history matching because past or historical measurements are used to adapt the model parameters. Mathematically speaking, model calibration is an optimization problem where the value of a performance indicator (or a combination of several performance indicators) is minimized. An example: The model calibration changes the parameters of the model such that the error between the observed and simulated discharge is minimized.\nThe simplest form of such an equation is given below where for each time step \\(t\\), the difference between the simulated (\\(Q_{sim}\\)) and observed (\\(Q_{obs}\\)) discharge is computed. The simulated discharge depends on the parameters \\(\\boldsymbol{P}\\) (note: \\(\\boldsymbol{P}\\) is a vector of parameters). The differences between the simulated and observed discharge are summed up to result in one value \\(\\boldsymbol{E}\\), depending on \\(\\boldsymbol{P}\\) which can be minimized by changing the parameters \\(\\boldsymbol{P}\\).\n\\[\n\\min_{\\boldsymbol{P}} \\sum_{t=1}^{N} \\Big( Q_{sim}(t, \\boldsymbol{P}) - Q_{obs}(t)  \\Big) = \\min_{\\boldsymbol{P}} \\boldsymbol{E(\\boldsymbol{P})}\n\\]\nRS Minerve features implementations of several different performance indicators. The choice of the minimization function influences the calibration outcome. Two commonly used performance indicators are described [above]{Section 8.6.5}. Please read the Chapter on the performance indicators of the RS Minerve technical manual (Garcia Hernandez et al. 2020) to also understand the other performance indicators that are available.\nHydrological models are typically not well defined, i.e. multiple sets of parameter combinations will yield reasonable model performance. This property of hydrological models is sometimes called the equifinality problem, i.e. several pathways lead to the same outcome. Modelers just have to live with this but there are several strategies to minimize the negative impact of the equifinality.\nCalibration and Validation\nAn important strategy to gain confidence in a model is to split the simulation period into a calibration period and a validation period. During calibration, the model parameters are adjusted to get a satisfying fit with the measured discharge. During the validation period the calibrated model is compared to discharge data that the model hasn’t seen before, i.e. the model’s ability to handle potentially different model input than it has seen during calibration (also called its ability to generalize) is revealed. For example, 30 years of discharge time series can be split into the first 20 years for model calibration (i.e. model calibration is done only over the first 20 years of the time series) and into the last 10 years for model validation. A good model should perform satisfactorily not only during calibration but also during validation. If the model validation period shows better model performance than the calibration period, the modeler should take a close look at the forcing and try to explain this.\nIn hydrology, it is recommended to do cross-validation, i.e. to calibrate and validate a model with several combinations of calibration and validation periods and to summarize the overall model performance and the resulting parameter combinations.\nInterested readers are referred to the literature for a more thorough discussion of the topic. For example, Bergstroem (Bergström 1991) gives a thorough discussion of model calibration and validation based on the HBV model. Albeit a bit dated, the discussion is still valid today.\nWarm-up Period\nTo overcome issues with the initial water content of the catchment, a warm-up or spin-up period is typically defined. For small basins with small storage (like the mountainous basins in Central Asia) one year of model warm up is typically sufficient to fill the models reservoir with an appropriate amount of water. The warm-up period is specified in the comparator properties and excluded from the automatic calibration process.\n\n\n\n\n\nFigure 8.27: Set up a spin-up period which is excluded from the model calibration. Open the object properties with a double-click on the comparator object in the main model window (1) and specify a 1-year spin-up period in seconds (2).\n\n\nFixing of Non-Sensitive Parameters\nBefore starting a full automated calibration of the model, it is advisable to identify parameters which are not sensitive and to fix them to reasonable values. In a fist step, parameters of all HRUS in the catchments are assumed to be the same. Then, one by one, the parameters are varied (simultaneously in all HRUs) to assess the impact of the parameters on the resulting discharge. If one parameter value is not sensitive, i.e. if changing the parameter value does not significantly impact the discharge, it is fixed to a literature value. The following figures illustrate the procedure:\nRemember the value of a performance indicator, for example the Nash efficiency. Then, open the Parameters window by pressing the Parameters button in the Model Properties toolbar (1), select HBV (2) and the default zone A (3) in the drop-down menu, and double the Melting factor (\\(\\text{CFMax}\\)) (4). Apply the changes (5) and re-run the model.\nIf the performance indicator becomes worse (e.g. more negative in the case of Nash), change the parameter to half of the original parameter value (2.5 in the present example and re-run the model). In the Chon Kemin example shown here, Nash will become less negative. We conclude that \\(\\text{CFMax}\\) is a sensitive parameter that should be calibrated. If the change of the parameter value does not produce a similar relative change of the performance indicator, the model is not highly sensitive to that specific parameter and it does not need to be calibrated.\nCross-Validation\nIt is good practice to validate your results with data from the literature and with other, publicly available data sets. Literature values may be useful to limit parameter ranges for the calibration and to verify the conceptual hydrological model. Data sets like the High Mountain Asia Snow Reanalysis data set can be used to verify the snow water balance (automated calibration of snow water equivalents is not currently possible in RS Minerve) and for flatter areas, soil remotely sensed moisture observations and evaporation products may be used to constrain the parameters of the root zone and upper soil box.\nCalibration Procedure\nIt is advisable to not calibrate all parameters at the same time but to calibrate module by module from the top-most layer to the bottom-most layer (along the flow path of the water). I.e., in the case of the HBV model, start by calibrating the sensitive parameters of the snow module, then proceed to the root zone module, the upper soil module and last calibrate the parameters of the groundwater module. This procedure may not yield an absolute minimum of the objective function (the best performance indicators possible) but it will leave you with physically meaningful parameters which is what is important.\nIn between calibrations of individual parameters, switch to the model tab or to the selection and plot tab and look at the results of individual model components. Make sure you don’t have accumulation of snow in the lower HRUs. Accumulation of snow may only be justified in glacierized HRUs. Also other water reservoirs may accidentally be filling up or emptying out, e.g. storage of the lower reservoir. Typically, the storage components in a hydrological system are more or less in steady-state (except for the glaciers). That means, that over several years, the water storage does not change much. If your model shows storage changes, they may be caused either by infrastructure or management changes (research) or by bad parameter sets. You should further update the initial conditions of the model from time to time to make sure that they are consistent with the model parameters.\nYou may perform several calibrations using different performance indicators and different calibration periods. This will give you a set of parameters which partially represent the uncertainty of the hydrological model.\nManual Calibration\nAutomated calibration is available in RS Minerve for discharge. However, inexperienced modelers should always start with an exploration of the sensitivity of the parameter space. Further, not all model states and fluxes can be calibrated using the automated calibration tools in RS Minerve, for example snow water equivalent. If automated calibration is not available, simulation results can be exported from RS Minerve and be compared to observed data (or simulation results from the literature) in R.\nAs mentioned above, publicly available data like snow water equivalents, snow covered area, glacier thinning rates or simulated glacier runoff data may be used to manually calibrate parameters of the GSM model objects. Such data typically has large uncertainties, sometimes in the order of 100 %, but it at least allows to make sure that simulated glacier runoff is in a reasonable order of magnitude.\nThe selection of simulation results in large models can be a tedious process. RS Minerve allows the import of a so called check node file in the xml format in which a selection of simulation results can be specified. The package riversCentralAsia offers the function writeSelectionCHK which facilitates the writing of such a check node file. The model calibration should start with the snow modules of the HBV model objects or the glacier thinning and discharge of the GSM model objects, then proceed to the soil moisture, upper and lower reservoir modules of the HBV model objects.\nCalibration of Snow Water Equivalent\nAt the time of writing, RS Minerve does not yet support the automatic calibration of snow water equivalent (SWE). However, snow melt is a major contribution to discharge in Central Asia. Measurements of SWE are only very rarely available and only at very few locations. Thus, products such as the SWE SnowMapper which operationally monitors snow height and SWE in Central Asia (see Section 6.2.2) or reanalysis products like the High Mountain Asia Snow Reanalysis (HMASR) product (see Section 6.2.1) are valuable resources to validate hydrological models.\nIn the snow and glacier data section we have demonstrated how to extract average SWE from the HMASR product for each elevation band in the Atbashy basin. It is highly recommended to study the technical manual of each data set prior to using it and to review the data set for its quality. We will show here how to extract simulated SWE from RS Minerve and how it may be compared to the HMASR data set.\nThe code snipped below demonstrates how to write a check node file for the HBV model objects in the hydrological model of the Atbashy river basin.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(ncdf4)\nlibrary(riversCentralAsia)\n\n# Path to the data directory downloaded from the download link provided above. \ndata_path &lt;- \"../caham_data\"\n\n# Read in a list of HBV objects in the model\nObject_IDs &lt;- \n  st_read(file.path(data_path, \"SyrDarya/Atbashy/GIS/16076_HRU.shp\")) |&gt; \n  st_drop_geometry() \n\n# Define the model objects and variables for the selection\ndata &lt;- tibble(\n  Model = rep(\"Model Atbashy\", length(Object_IDs$name)), \n  Object = c(rep(\"HBV92\", length(Object_IDs$name))), \n  ID = Object_IDs$name, \n  Variable = rep(\"SWE (m)\")\n)\n\n# Write the check node file which can be imported to RS MINERVE\nwriteSelectionCHK(\n  # To reproduce the example, make sure the below filepath exists\n  filepath = file.path(data_path, \"SyrDarya/Atbashy/RSMINERVE/SWE.chk\"), \n  data = data, \n  # The name of the selection will appear in RS MINERVE\n  name = \"SWE\"  \n)\n\nThe check node file is read in to RS Minerve under the Selection and plots tab, using the Import button. A new selection named SWE will appear which can be exported to a csv file using the button Export Results to… The following code snipped demonstrates how to read in the simulated SWE to R.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(ncdf4)\nlibrary(riversCentralAsia)\n\nswe_sim &lt;- readResultCSV(\n  file.path(data_path, \n         \"SyrDarya/Atbashy/RSMINERVE/Atbaschy_Results_SWE_uncalibrated.csv\")) |&gt;  \n  mutate(Subbasin = gsub(\"\\\\_Subbasin\\\\_\\\\d+$\", \"\", model), \n         # Extract the elevation band level from the HRU names, for plotting\n         \"Elevation band\" = str_extract(model, \"\\\\d+$\") |&gt; as.numeric(), \n         \"Elevation band\" = factor(`Elevation band`, levels = c(1:20)))\n\nggplot(swe_sim) + \n  geom_point(aes(date, value, colour = `Elevation band`), \n             alpha = 0.4, size = 0.4) + \n  scale_colour_viridis_d() + \n  facet_wrap(\"Subbasin\") + \n  ylab(\"SWE [m]\") + \n  xlab(\"Date\") + \n  theme_bw()\n\n\n\n\n\n\nFigure 8.28: Simulated SWE from an uncalibrated hydrological model of the Atbashy river catchment in each elevation band in each sub-basin.\n\n\n\n\nNow we import the SWE extracted from the HMASR product for each elevation band and compare the observed SWE to the simulated one.\n\n# Loads SWE extracted from HMASR per HRU in the Atbashy model.\nload(file.path(data_path, \"SyrDarya/AtBashy/SNOW/SWE.RData\"))\n\ncompare_swe &lt;- swe_sim |&gt; \n  rename(Sim = value) |&gt; \n  dplyr::select(date, model, Sim, Subbasin, `Elevation band`) |&gt; \n  left_join(swel |&gt; \n              dplyr::select(Date, Name, SWE) |&gt; \n              rename(Obs = SWE), \n            by = c(\"date\" = \"Date\", \"model\" = \"Name\")) |&gt;\n  mutate(Month = month(date), \n         Year = hyear(date), \n         Month_str = factor(format(date, \"%b\"), \n                            levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \n                                       \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \n                                       \"Nov\", \"Dec\")), \n         \"Obs-Sim\" = Obs - Sim)\n\n# Calculate the performance indicator RMSE. Many other indicators may be used. \nRMSE = sqrt(mean(compare_swe$`Obs-Sim`, na.rm = TRUE))\n\nggplot(compare_swe) +\n  geom_abline(intercept = 0, slope = 1) + \n  geom_point(aes(Obs, Sim, colour = `Elevation band`), size = 0.4) +\n  scale_color_viridis_d() + \n  xlab(\"Observed SWE [m]\") + \n  ylab(\"Simulated SWE [m]\") + \n  facet_wrap(\"Month_str\") + \n  coord_fixed() + xlim(0, 1.5) + ylim(0, 1.5) + \n  theme_bw()\n\n\n\n\n\n\nFigure 8.29: Monthly simulated SWE from an uncalibrated hydrological model of the Atbashy river catchment versus SWE extracted from the HMASR product (aka observed SWE, source: Liu et al. 2021) in each elevation band.\n\n\n\n\nThe comparison of simulated and observed SWE shows biases, indicating an underestimation of SWE in the model which can be adjusted by adapting the parameters of the snow modules in the HBV model objects in RS MINERVE. The package riversCentralAsia allows you to edit parameter files from R using the functions readRSMParameters() and writeRSMParameters() (please see the package documentation for examples)3.\nIn places where SWE observations are not available or where operational data on SWE is required, a binary variable of SWE larger than a threshold per hydrological response unit (HRU) can be compared to snow covered area (a MODIS product) in each HRU (see for example Parajka and Blöschl (2008)).\nCalibration of the Glacier Melt Model\nThe following code chunk demonstrates how a check node file can be prepared to read out simulated glacier discharge and glacier thickness from an uncalibrated version of the Gunt River model.\nPlease note that, in addition to a visual comparison of simulated vs. observed model states of fluxes, the computation of performance indicators is recommended. Tracking changes of performance indicators makes it straight forward to see, if a change in a parameter leads to the desired adjustment of the model outcome or not.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(ncdf4)\nlibrary(riversCentralAsia)\n\n# Read in the HRU names\nObject_IDs &lt;- \n  st_read(file.path(data_path,\"AmuDarya/17050_Gunt/GIS/17050_hru_GSM.shp\")) |&gt; \n  st_drop_geometry() \n\n# Get a list of glacierised HRUs (their name starts with \"gl\")\ngsm_objects &lt;- Object_IDs |&gt; \n  dplyr::filter(str_detect(name, \"gl_\"))\n\n# Prepare a tibble with the variables to be written to the check node file\ndata &lt;- tibble(\n  Model = rep(\"Model Gunt\", (length(gsm_objects$name))*2), \n  Object = rep(\"GSM\", (length(gsm_objects$name)*2)), \n  ID = c(gsm_objects$name, gsm_objects$name), \n  Variable = c(rep(\"Qglacier (m3/s)\", length(gsm_objects$name)), \n               rep(\"Hglacier (m)\", length(gsm_objects$name)))\n)\n\n# Write the check node file\nwriteSelectionCHK(\n  filepath = \n    file.path(data_path,\"AmuDarya/17050_Gunt/RS_MINERVE/HQ_glaciers.chk\"), \n  data = data, \n  name = \"HQglaciers\"  # Name of the selection that will appear in RS MINERVE\n)\n\nIn the Selection and plots tab in RS MINERVE, click on Import and select the file HQ_glaciers.chk. You will see a new selection now called HQglaciers. You can now save the simulation results as csv by clicking the Export Results to… button. We call the file Gunt_Results_HQglaciers.csv and read it in as demonstrated below:\n\nhqgl &lt;- \n  readResultCSV(\n    file.path(\n      data_path,\n      \"AmuDarya/17050_Gunt/RS_MINERVE/Gunt_Results_HQglaciers.csv\")) |&gt; \n  mutate(object = ifelse(str_detect(model, \"gl_\"), \"GSM\", \"HBV\"), \n         # Extract ID of elevation band for plotting\n         layer = str_extract(model, \"\\\\d+\"),  \n         layer = factor(layer, levels = c(1:11)))\n\nggplot(hqgl |&gt; \n         mutate(Hyear = hyear(date)) |&gt; \n         group_by(Hyear, layer, variable, unit) |&gt; \n         summarise(date = last(date), \n                   value = ifelse(variable == \"Qglacier\", \n                                  mean(value), \n                                  last(value))) |&gt; \n         ungroup()) + \n  geom_line(aes(date, value, colour = layer), alpha = 0.6, size = 0.8) + \n  scale_colour_viridis_d() + \n  facet_wrap(c(\"variable\", \"unit\"), scales = \"free_y\") + \n  theme_bw()\n\n\n\n\n\n\nFigure 8.30: Simulated glacier height and discharge from an uncalibrated model of the Gunt river basin illustrating that glacier elevations can become negative.\n\n\n\n\nThe above plot show how glacier thickness can go below 0. As this hydrological model is very simple, we can correct the glacier thickness and discharge after the simulation.\n\nhqgl_corrected &lt;- hqgl |&gt; \n  pivot_wider(values_from = value, names_from = c(variable, unit), \n              names_sep = \"|\") |&gt; \n  mutate(\"Qglacier|m3/s\" = ifelse(`Hglacier|m` &lt;= 0, 0, `Qglacier|m3/s`), \n         \"Hglacier|m\" = ifelse(`Hglacier|m` &lt;= 0, 0, `Hglacier|m`)) |&gt; \n  pivot_longer(c(`Qglacier|m3/s`, `Hglacier|m`), values_to = \"value\", \n               names_to = c(\"variable\", \"unit\"), names_sep = \"\\\\|\")\n\nggplot(hqgl_corrected |&gt; \n         mutate(Hyear = hyear(date)) |&gt; \n         group_by(Hyear, layer, variable, unit) |&gt; \n         summarise(date = last(date), \n                   value = ifelse(variable == \"Qglacier\", \n                                  mean(value), \n                                  last(value))) |&gt; \n         ungroup()) + \n  geom_line(aes(date, value, colour = layer), alpha = 0.6, size = 0.8) + \n  scale_colour_viridis_d() + \n  facet_wrap(c(\"variable\", \"unit\"), scales = \"free_y\") + \n  theme_bw()\n\n\n\n\n\n\nFigure 8.31: Corrected, simulated glacier height and discharge from an uncalibrated model of the Gunt river basin. Glacier thickness cannot become negative.\n\n\n\n\nGlacier discharge as well as glacier thickness can be compared to literature data. By manually adjusting the GSM model parameters, both simulated glacier discharge and glacier thickness can become more similar to the literature data.\nComparing Computed to Observed Glacier Thinning Rates\nSimulated glacier thinning rates may be compared to observed thinning rates by Hugonnet et al. (2021). The observed thinning rates are available by glacier while the simulated thinning rates are by HRU. We aggregate both data sets to the basin level (see @ fig-obs-hugonnet-gunt-raw).\n\n# Read in the RGI glaciers shape cut to the Gunt basin to filter the Hugonnet\n# data set\n# Read in glacier geometries\nrgi &lt;- \n  st_read(\n    file.path(data_path,\n              \"AmuDarya/17050_Gunt/Glaciers/17050_RGI_glaciers.shp\")) |&gt; \n  dplyr::select(RGIId, Area_m2, Volume_m3) \n\nReading layer `17050_RGI_glaciers' from data source \n  `/Users/tobiassiegfried/hydrosolutions Dropbox/Tobias Siegfried/1_HSOL_PROJECTS/PROJECTS/[2020-06-TW] Currricula Strengthening CA/CourseMaterials/Handbook/caham_data/AmuDarya/17050_Gunt/Glaciers/17050_RGI_glaciers.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1347 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 729359.9 ymin: 4093677 xmax: 939336.3 ymax: 4215699\nProjected CRS: WGS 84 / UTM zone 42N\n\n# Glacier volume was already estimated in QGIS. You can also do this in R using \n# the following example code:  \n# demo &lt;- rgi |&gt; \n#   mutate(Area_m2 = as.numeric(st_area(rgi)), \n#          Volume_m3 = glacierVolume_Erasov(Area_m2/10^6)*10^9)\n\nhugonnet &lt;- \n  read_csv(file.path(data_path, \"/central_asia_domain/glaciers/Hugonnet/dh_13_rgi60_pergla_rates.csv\"))\n# Explanation of variables (see documentation by Hugonnet et al., 2021):\n# - dhdt is the elevation change rate in meters per year,\n# - dvoldt is the volume change rate in meters cube per year,\n# - dmdt is the mass change rate in gigatons per year,\n# - dmdtda is the specific-mass change rate in meters water-equivalent per year.\n\n# Filter the basin glaciers from the Hugonnet data set. \nhugonnet &lt;- hugonnet |&gt; \n  dplyr::filter(rgiid %in% unique(rgi$RGIId)) |&gt; \n  tidyr::separate(period, c(\"start\", \"end\"), sep = \"_\") |&gt; \n  mutate(start = as_date(start, format = \"%Y-%m-%d\"), \n         end = as_date(end, format = \"%Y-%m-%d\"), \n         period = round(as.numeric(end - start, units = \"days\")/366))\n\n# Join the Hugonnet data set to the RGI data set to be able to plot the thinning\nglaciers_hugonnet &lt;- rgi |&gt; \n  left_join(hugonnet |&gt; \n              dplyr::select(rgiid, start, dmdtda, err_dmdtda, period) |&gt; \n              dplyr::filter(start == as_date(\"2000-01-01\"), \n                            period == 10),  \n            by = c(\"RGIId\" = \"rgiid\")) |&gt; \n  group_by(start, period) |&gt; \n  summarise(dmdtda = mean(dmdtda, na.rm = TRUE), \n            err_dmdtda = mean(err_dmdtda, na.rm = TRUE)) |&gt; \n  ungroup() \n\nsim_delta_Hglacier &lt;- hqgl_corrected |&gt; \n  # Filter the simulated glacier thickness to the same period as the observed\n  dplyr::filter(variable == \"Hglacier\", \n                date &gt;= as_date(\"2000-01-01\") & date &lt; as_date(\"2011-01-01\"))|&gt; \n  mutate(Hyear = hyear(date)) |&gt; \n  group_by(Hyear, model, unit) |&gt; \n  summarise(Hglacier = last(value)) |&gt;\n  ungroup() |&gt; \n  # Calculate glacier thickness change\n  pivot_wider(names_from = model, values_from = Hglacier) |&gt; \n  mutate(across(starts_with(\"gl\"), ~ .x - lag(.x))) |&gt; \n  drop_na() |&gt; \n  pivot_longer(-c(Hyear, unit), names_to = \"model\", values_to = \"dHgl/dt\") |&gt; \n  group_by(model) |&gt; \n  summarise(\"sd(dHgl/dt)\" = sd(`dHgl/dt`, na.rm = TRUE), \n            \"dHgl/dt\" = mean(`dHgl/dt`, na.rm = TRUE)) |&gt; \n  ungroup() |&gt; \n  mutate(model = gsub(\"gl_\", \"\", model)) |&gt; \n  # Aggregate by basin\n  summarise(\"sd(dHgl/dt)\" = mean(`sd(dHgl/dt)`, na.rm = TRUE), \n            \"dHgl/dt\" = mean(`dHgl/dt`, na.rm = TRUE))\n\n# Combine observed and simulated average glacier thinning in one variable \ncompareH &lt;- glaciers_hugonnet |&gt; \n  st_drop_geometry() |&gt; \n  dplyr::select(dmdtda, err_dmdtda) |&gt; \n  transmute(Source = \"Obs\", \n            \"dH/dt [m]\" = dmdtda, \n            lb = dmdtda - err_dmdtda, \n            ub = dmdtda + err_dmdtda) |&gt; \n  add_row(sim_delta_Hglacier |&gt; \n            mutate(Source = \"Sim\", \n                   lb = `dHgl/dt` - `sd(dHgl/dt)`, \n                   ub = `dHgl/dt` + `sd(dHgl/dt)`) |&gt;\n            rename(\"dH/dt [m]\" = `dHgl/dt`) |&gt; \n            select(-`sd(dHgl/dt)`)) \n\nggplot(compareH) + \n  geom_errorbar(aes(Source, ymin = lb, ymax = ub), \n                width = 0.2) + \n  geom_col(aes(Source, `dH/dt [m]`, fill = Source), \n           colour = \"black\", position = \"dodge\") + \n  geom_hline(yintercept = 0, size = 0.4) + \n  xlab(\"\") + \n  theme_bw()\n\n\n\n\n\n\nFigure 8.32: Average observed (Source: Hugonnet et al., 2021) in the Gunt river basin and simulated glacier thinning rates from an uncalibrated hydrological model of the Gunt river basin.\n\n\n\n\nThe average observed thinning rate is 2 cm/a and the simulated thinning rate is 34 cm/a. The simulated thinning remains within the uncertainty of the observed thinning rates but may be overestimated in the model. Let’s have a look ad glacier discharge data sets.\nGlacier Discharge Comparison\nGlacier discharge estimates in High Mountain Asia by Miles et al. (2021) are among the best currently available as they calculate the specific mass balance of the glacier.\n\n# Load glacier ablation data by Miles et al., 2021 and filter to the Central \n# Asian domain\n# Limitation: Miles et al. report average total Ablation in m3/a for the years \n# 2000 to 2016. Our historical forcing time series only goes until the end of \n# 2011, that means that the averaging periods for the comparison of the \n# observed and simulated glacier discharge do not overlap.  \n# Further, Miles et al. only cover glaciers with areas &gt; 2 km2. The observed \n# glacier discharge over the basin is therefore likely underestimated. \nmiles &lt;- \n  read_csv(\n    file.path(data_path,\n              \"/central_asia_domain/glaciers/Miles/Miles2021_Glaciers_summarytable_20210721.csv\")) |&gt;\n  dplyr::filter(grepl(\"RGI60-13.\", RGIID), \n                VALID == 1) |&gt; \n  dplyr::select(RGIID, totAbl, totAblsig)\n\nglaciers_miles &lt;- rgi |&gt; \n  st_drop_geometry() |&gt; \n  left_join(miles, by = c(\"RGIId\" = \"RGIID\")) |&gt; \n  summarise(totAbl = sum(-totAbl, na.rm = TRUE), \n            totAblsig = sum(totAblsig, na.rm = TRUE)) |&gt; \n  ungroup() \n\nsim_Qglacier &lt;- hqgl |&gt; \n  dplyr::filter(variable == \"Qglacier\", \n                date &gt;= as_date(\"2000-01-01\"))|&gt; \n  mutate(Hyear = hyear(date)) |&gt; \n  group_by(Hyear, model, unit) |&gt; \n  summarise(Qglacier = mean(value), \n            sdQgl = sd(value)) |&gt;\n  ungroup() |&gt; \n  group_by(model) |&gt; \n  summarise(Qglacier = sum(Qglacier)*60*60*24*365, # m3/s to m3/a\n            sdQgl = sum(sdQgl)*60*60*24*365) |&gt; \n  ungroup() |&gt; \n  mutate(model = gsub(\"_Gl\", \"\", model)) |&gt; \n  summarise(Qglacier = sum(Qglacier), \n            sdQgl = mean(sdQgl))\n\ncompareQ &lt;- glaciers_miles |&gt; \n  transmute(Source = \"Obs\", \n            \"Q [10^6 m3/a]\" = totAbl, \n            lb = totAbl - totAblsig, \n            ub = totAbl + totAblsig) |&gt; \n  add_row(sim_Qglacier |&gt; \n            transmute(Source = \"Sim\", \n                      \"Q [10^6 m3/a]\" = Qglacier, \n                      lb = Qglacier - sdQgl, \n                      ub = Qglacier + sdQgl)) |&gt; \n  mutate(`Q [10^6 m3/a]` = `Q [10^6 m3/a]`*10^(-6), \n         lb = lb/10^6, \n         ub = ub/10^6)\n\nggplot(compareQ) + \n  geom_errorbar(aes(Source, ymin = lb, ymax = ub), \n                width = 0.2) + \n  geom_col(aes(Source, `Q [10^6 m3/a]`, fill = Source), \n           colour = \"black\", position = \"dodge\") + \n  geom_hline(yintercept = 0, size = 0.4) + \n  xlab(\"\") + \n  theme_bw()\n\n\n\n\n\n\nFigure 8.33: Average observed (Source: Miles et al., 2021) in the Gunt river basin and simulated glacier discharge rates from an uncalibrated hydrological model of the Gunt river basin.\n\n\n\n\nThe observed glacier discharge is estimated at about 30 million m3/a whereas the simulated glacier discharge is estimated at about 1.5 billion m3/a. In the case of glacier discharge, Rounce, Hock, and Shean (2020b) simulate glacier discharge in High Mountain Asia using the Python Glacier Evolution Model (PyGEM) and make their data publicly available (Rounce, Hock, and Shean 2020a). Let’s compare our simulation results to theirs. The below code snipped demonstrates how the PyGEM simulation results can be extracted for a given basin. We have precomputed the data for the Gunt catchment.\n\nglaciers &lt;- rgi$RGIId\n\n# Here we read in all nc files in a folder. For this example, we only use one \nncfilelist &lt;- list.files(path = file.path(data_path, \"/central_asia_domain/glaciers/PyGEM\"), pattern = \".nc$\", \n                         full.names = TRUE)\nncfilelist &lt;- ncfilelist[str_detect(ncfilelist, \"_R13_\")]\n\nglac_melt_basin &lt;- NULL\nglac_runoff_basin &lt;- NULL\noffglac_runoff_basin &lt;- NULL\nglac_volume_basin &lt;- NULL\n\nfor (ncfile in ncfilelist) {\n  \n  # Get the rcp identifyer from the file name\n  rcp &lt;- str_extract(ncfile, \"rcp\\\\d{2}\")\n  \n  # Open a file for reading\n  file &lt;- nc_open(ncfile)\n  \n  # Get the RGI IDs and the glacier indices used in the PyGEM data set\n  rgiid &lt;- ncvar_get(file, \"RGIId\")\n  pygem_indices &lt;- ncvar_get(file, \"glac\")\n  \n  # Identify the indices of the glaciers we are interested in in the nc file\n  rgi_indices &lt;- which(rgiid %in% glaciers)\n  \n  # Get dates for the monthly time series. \n  # The time series starts on October 1, 1999. \n  time &lt;- ncvar_get(file, \"time\")  # In days\n  date &lt;- as_date(\"1999-10-01\") + time\n  \n  # Get dates for the annual time series. \n  year &lt;- ncvar_get(file, \"year_plus1\")\n  date_annual &lt;- as_date(paste(year, \"10\", \"01\", sep = \"-\"))\n  \n  # Get the glacier volume and discharge for the selected glaciers\n  glac_melt_monthly &lt;- ncvar_get(file, \"glac_melt_monthly\")[, rgi_indices] |&gt; \n    as_tibble(.name_repair = ~ glaciers) |&gt; \n    mutate(date = as.character(date)) |&gt; \n    tidyr::pivot_longer(-date, names_to = \"RGIId\", values_to = \"glac_melt_mweq\") |&gt; \n    mutate(rcp = rcp) \n  glac_melt_monthly_std &lt;- ncvar_get(file, \"glac_melt_monthly_std\")[, rgi_indices] |&gt; \n    as_tibble(.name_repair = ~ glaciers) |&gt; \n    mutate(date = as.character(date)) |&gt; \n    pivot_longer(-date, names_to = \"RGIId\", values_to = \"glac_melt_std_mweq\") |&gt; \n    mutate(rcp = rcp) \n  \n  glac_runoff_monthly &lt;- ncvar_get(file, \"glac_runoff_monthly\")[, rgi_indices] |&gt;\n    as_tibble(.name_repair = ~ glaciers) |&gt;\n    mutate(date = as.character(date)) |&gt;\n    pivot_longer(-date, names_to = \"RGIId\", values_to = \"glac_runoff_m3\") |&gt;\n    mutate(rcp = rcp)\n  glac_runoff_monthly_std &lt;- ncvar_get(file, \"glac_runoff_monthly_std\")[, rgi_indices] |&gt;\n    as_tibble(.name_repair = ~ glaciers) |&gt;\n    mutate(date = as.character(date)) |&gt;\n    pivot_longer(-date, names_to = \"RGIId\", values_to = \"glac_runoff_std_m3\") |&gt;\n    mutate(rcp = rcp)\n  \n  offglac_runoff_monthly &lt;- ncvar_get(file, \n                                      \"offglac_runoff_monthly\")[, rgi_indices] |&gt;\n    as_tibble(.name_repair = ~ glaciers) |&gt;\n    mutate(date = as.character(date)) |&gt;\n    pivot_longer(-date, names_to = \"RGIId\", values_to = \"offglac_runoff_m3\") |&gt;\n    mutate(rcp = rcp) \n  offglac_runoff_monthly_std &lt;- ncvar_get(file, \n                                      \"offglac_runoff_monthly_std\")[, rgi_indices] |&gt;\n    as_tibble(.name_repair = ~ glaciers) |&gt;\n    mutate(date = as.character(date)) |&gt;\n    pivot_longer(-date, names_to = \"RGIId\", values_to = \"offglac_runoff_std_m3\") |&gt;\n    mutate(rcp = rcp) \n  \n  # Aggregate to annual date for the basin\n  glac_melt_basin &lt;- rbind(glac_melt_basin, \n    glac_melt_monthly |&gt; \n      left_join(glac_melt_monthly_std, by = c(\"date\", \"RGIId\", \"rcp\")) |&gt; \n      mutate(date = as_date(paste(hyear(date), \"10\", \"01\", sep = \"-\"))) |&gt; \n      group_by(date, rcp) |&gt; \n      summarise(mean = sum(glac_melt_mweq, na.rm = TRUE), \n                lb = sum(glac_melt_mweq - glac_melt_std_mweq, na.rm = TRUE), \n                ub = sum(glac_melt_mweq + glac_melt_std_mweq, na.rm = TRUE)) |&gt; \n      ungroup())\n  \n  glac_runoff_basin &lt;- rbind(glac_runoff_basin, \n    glac_runoff_monthly |&gt; \n      left_join(glac_runoff_monthly_std, by = c(\"date\", \"RGIId\", \"rcp\")) |&gt; \n      mutate(date = as_date(paste(hyear(date), \"10\", \"01\", sep = \"-\"))) |&gt; \n      group_by(date, rcp) |&gt; \n      summarise(mean = sum(glac_runoff_m3, na.rm = TRUE), \n                lb = sum(glac_runoff_m3 - glac_runoff_std_m3, na.rm = TRUE), \n                ub = sum(glac_runoff_m3 + glac_runoff_std_m3, na.rm = TRUE)) |&gt; \n      ungroup())\n  \n  offglac_runoff_basin &lt;- rbind(offglac_runoff_basin, \n    offglac_runoff_monthly |&gt; \n      left_join(offglac_runoff_monthly_std, by = c(\"date\", \"RGIId\", \"rcp\")) |&gt; \n      mutate(date = as_date(paste(hyear(date), \"10\", \"01\", sep = \"-\"))) |&gt; \n      group_by(date, rcp) |&gt; \n      summarise(mean = sum(offglac_runoff_m3, na.rm = TRUE), \n                lb = sum(offglac_runoff_m3 - offglac_runoff_std_m3, na.rm = TRUE), \n                ub = sum(offglac_runoff_m3 + offglac_runoff_std_m3, na.rm = TRUE)) |&gt; \n      ungroup())\n  \n  \n  \n  # Read annual volume development and standard deviation\n  glac_volume_annual &lt;- ncvar_get(file, \"glac_volume_annual\")[, rgi_indices] |&gt; \n    as_tibble(.name_repair = ~ glaciers) |&gt; \n    mutate(date = date_annual) |&gt; \n    pivot_longer(-date, names_to = \"RGIId\", values_to = \"glac_volume_km3ice\") |&gt; \n    mutate(rcp = rcp)\n  glac_volume_annual_std &lt;- ncvar_get(file, \n                                      \"glac_volume_annual_std\")[, rgi_indices] |&gt; \n    as_tibble(.name_repair = ~ glaciers) |&gt; \n    mutate(date = date_annual) |&gt; \n    pivot_longer(-date, names_to = \"RGIId\", \n                 values_to = \"glac_volume_std_km3ice\") |&gt; \n    mutate(rcp = rcp)\n  \n  # Aggregate to basin level\n  glac_volume_basin &lt;- rbind(glac_volume_basin, \n    glac_volume_annual |&gt; \n      left_join(glac_volume_annual_std, by = c(\"date\", \"RGIId\", \"rcp\")) |&gt; \n      group_by(date, rcp) |&gt; \n      summarise(mean = sum(glac_volume_km3ice, na.rm = TRUE), \n                lb = sum(glac_volume_km3ice - glac_volume_std_km3ice, na.rm = TRUE), \n                ub = sum(glac_volume_km3ice + glac_volume_std_km3ice, na.rm = TRUE)) |&gt; \n      ungroup())\n  \n}\n\nsave(glac_runoff_basin, \n     glac_volume_basin, \n     file = file.path(data_path,\n                      \"/AmuDarya/17050_Gunt/Glaciers/17050_PyGEM_demo.RData\"))\n\n\nload(\n  file.path(data_path,\n    \"/AmuDarya/17050_Gunt/Glaciers/17050_PyGEM_demo.RData\"))\n\nglac_runoff_basin &lt;- glac_runoff_basin |&gt; \n  dplyr::filter(year(date) &lt;= 2011) |&gt;  # Filter same period as simulation\n  group_by(rcp) |&gt; \n  summarise(mean = mean(mean), \n            lb = mean(lb), \n            ub = mean(ub)) |&gt; \n  ungroup()\n\ncompareQ2 &lt;- compareQ |&gt; \n  dplyr::filter(Source == \"Sim\") |&gt; \n  add_row(tibble(\n    Source = \"Obs\", \n    \"Q [10^6 m3/a]\" = glac_runoff_basin$mean/10^6, \n    lb = glac_runoff_basin$lb/10^6, \n    ub = glac_runoff_basin$ub/10^6\n  ))\n\nggplot(compareQ2) + \n  geom_errorbar(aes(Source, ymin = lb, ymax = ub), \n                width = 0.2) + \n  geom_col(aes(Source, `Q [10^6 m3/a]`, fill = Source), \n           colour = \"black\", position = \"dodge\") + \n  geom_hline(yintercept = 0, size = 0.4) + \n  xlab(\"\") + \n  theme_bw()\n\n\n\n\n\n\nFigure 8.34: Average observed (Source: Rounce et al., 2020) in the Gunt river basin and simulated glacier thinning rates from an uncalibrated hydrological model of the Gunt river basin.\n\n\n\n\nThe uncalibrated hydrological model grossly overestimates glacier discharge. It is necessary to adjust the model parameters to produce less glacier discharge.\nAutomatic Model Calibration with RS Minerve\nRS Minerve offers several interesting expert tools, including the Calibrator. Open the calibrator tab by clicking the Expert button in the Modules toolbar and select Calibrator. A calibrator tab will open up (Figure 8.35). Follow the steps to set up your first calibration: Select the current calibration configuration (1). You may rename it and save it (Export) for later use. Then, select HBV (2) and Zone A (3) to tell the calibrator the objects that you want to calibrate parameters for. You also need to select the Comparator object based on which performance indicators will be computed (4). In the Parameters window, select one of your sensitive parameters and tick the box. Then you specify the performance indicator(s) that should be used in the calibration. You can combine several indicators with user defined weights, according to your preferences and needs. In this demo, we use Nash with weight 1 (6) and Relative Volume Bias with weight 1 (7). Subsequently you specify the calibration period (8). Remember that you need to divide your modeling period in a calibration and a validation period. Once ready, press Start (9) and the calibrator starts spinning. You can monitor the progress of the calibration in the Summary results window (10) and in the Graphic results window (11).\n\n\n\n\n\nFigure 8.35: Setting up a model calibration.\n\n\nYou proceed in the same way for all sensitive parameters and maybe recalibrate some of the most sensitive ones. The best parameter sets are transferred to your model tab but make sure to save a copy of the calibrated parameters.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hydrological-Hydraulic Modeling</span>"
    ]
  },
  {
    "objectID": "hydraulic_hydrological_modeling.html#sec-hydrological-hydraulic-modeling-references",
    "href": "hydraulic_hydrological_modeling.html#sec-hydrological-hydraulic-modeling-references",
    "title": "8  Hydrological-Hydraulic Modeling",
    "section": "\n8.8 References",
    "text": "8.8 References\n\n\n\n\nBergström, Sten. 1980. “Development and Application of a Conceptual Runoff Model for Scandinavian Catchments.” {SMHI} {Report} Nr Rho 7. Lund Institute of Technology/University of Lund.\n\n\n———. 1991. “Principles and Confidence in Hydrological Modelling.” Hydrology Research 22 (2): 123–36. https://doi.org/10.2166/nh.1991.0009.\n\n\nBlöschl, G., and M. Sivapalan. 1995. “Scale Issues in Hydrological Modelling: A Review.” Hydrological Processes 9 (3-4): 251–90. https://doi.org/https://doi.org/10.1002/hyp.3360090305.\n\n\nCREALP. 2021. “RS MINERVE.” https://www.crealp.ch/fr/accueil/outils-services/logiciels/rs-minerve/telechargement-rsm.html.\n\n\nFoehn, A., J. Garcia Hernandez, B. Roquier, J. Fluixa-Sanmartin, T. Brauchli, J. Paredes Arquiola, and G. De Cesare. 2020. “RS MINERVE - User Manual, V2.15.” ISSN 2673-2653. Switzerland: Ed. CREALP.\n\n\nGarcia Hernandez, J., A. Foehn, J. Fluixa-Sanmartin, B. Roquier, T. Brauchli, J. Paredes Arquiola, and De Cesare G. 2020. “RS MINERVE - Technical Manual, V2.25.” ISSN 2673-2661. Switzerland: Ed. CREALP.\n\n\nHugonnet, Romain, Robert McNabb, Etienne Berthier, Brian Menounos, Christopher Nuth, Luc Girod, Daniel Farinotti, et al. 2021. “Accelerated Global Glacier Mass Loss in the Early Twenty-First Century.” Nature 592 (7856): 726–31. https://doi.org/10.1038/s41586-021-03436-z.\n\n\nLindström, Göran, Barbro Johansson, Magnus Persson, Marie Gardelin, and Sten Bergström. 1997. “Development and Test of the Distributed HBV-96 Hydrological Model.” Journal of Hydrology 201 (1-4): 272–88. https://doi.org/10.1016/S0022-1694(97)00041-3.\n\n\nMiles, Evan, Michael McCarthy, Amaury Dehecq, Marin Kneib, Stefan Fugger, and Francesca Pellicciotti. 2021. “Health and Sustainability of Glaciers in High Mountain Asia.” Nature Communications 12 (2868): 10. https://doi.org/https://doi.org/10.1038/s41467-021-23073-4.\n\n\nParajka, J., and G. Blöschl. 2008. “The Value of MODIS Snow Cover Data in Validating and Calibrating Conceptual Hydrologic Models.” Journal of Hydrology 358 (3-4): 240–58. https://doi.org/10.1016/j.jhydrol.2008.06.006.\n\n\nPedersen, John T., John C. Peters, and Otto J. Helweg. 1980. “Hydrographs by Single Linear Reservoir Model.” {Technical Report} TP-74. US Army Corps of Engineers, Institute of Water Resources, Hydrologic Engineering Center.\n\n\nRefsgaard, Jens Christian, Jeroen P. van der Sluijs, Anker Lajer Højberg, and Peter A. Vanrolleghem. 2007. “Uncertainty in the Environmental Modelling Process – A Framework and Guidance.” Environmental Modelling & Software 22 (11): 1543–56. https://doi.org/10.1016/j.envsoft.2007.02.004.\n\n\nRounce, David R., Regine Hock, and David E. Shean. 2020a. “High Mountain Asia PyGEM Glacier Projections with RCP Scenarios.” Data Set. NASA National Snow and Ice Data Center Distributed Active Archive Center. https://doi.org/10.5067/190Y84IGLIQH.\n\n\n———. 2020b. “Glacier Mass Change in High Mountain Asia Through 2100 Using the Open-Source Python Glacier Evolution Model (PyGEM).” Frontiers in Earth Science 7 (January): 331. https://doi.org/10.3389/feart.2019.00331.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hydrological-Hydraulic Modeling</span>"
    ]
  },
  {
    "objectID": "hydraulic_hydrological_modeling.html#footnotes",
    "href": "hydraulic_hydrological_modeling.html#footnotes",
    "title": "8  Hydrological-Hydraulic Modeling",
    "section": "",
    "text": "The reader is advised to consult the user manual and familiarizes themselves with the walk through examples that are discussed and presented there. They give a solid first hands-on introduction about the basic functionalities of the modeling environment.↩︎\nPlease read Chapter 3 on the performance indicators in the technical manual (Garcia Hernandez et al. 2020) for the equations of each available indicator.↩︎\nAdvanced modelers can use the package RSMinerveR, available from github, to run the hydrological model from R.↩︎",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hydrological-Hydraulic Modeling</span>"
    ]
  },
  {
    "objectID": "long_term_water_balance_modeling.html",
    "href": "long_term_water_balance_modeling.html",
    "title": "9  Long-term Water Balance Modeling",
    "section": "",
    "text": "9.1 Introduction\nThe general water balance of a catchment can be written as\n\\[\n\\Delta S = P - E - Q\n\\tag{9.1}\\]\nwhere \\(\\Delta S\\) is net storage change in millimeters [mm], \\(P\\) is precipitation in mm, \\(E\\) is evaporation in mm, and \\(Q\\) is specific discharge in mm. Evaporation is when a substance is converted from its liquid into its vapor phase, independently of where it lies in nature (Miralles et al. 2020). This definition of evaporation encompasses evaporation from inside leaves (transpiration), evaporation from bare soils, evaporation from intercepted precipitation (interception loss), evaporation from open water surfaces, and finally, evaporation over ice- and snow-covered surfaces (often referred to as sublimation).\nOver the period of a hydrological year and longer time scales, we expect \\(\\Delta S\\) to be 0 since neither water storage nor destorage happens over longer periods. This would not be true for catchments where, for example, man-made storage infrastructure was built over the period under consideration or for catchments with ongoing glacier melt over a prolonged time. If \\(\\Delta S = 0\\), the above Equation @ref(eq:WB1) can be rewritten as\n\\[\nQ = P - E\n\\tag{9.2}\\]\nDividing by \\(P\\), we get\n\\[\n\\frac{Q}{P} = 1 - \\frac{E}{P}\n\\tag{9.3}\\]\nwhere \\(Q/P\\) can be called the runoff index and \\(E/P\\) is the evaporation index or evaporative fraction.\nFor a catchment, the annual mean \\(E\\) and \\(Q\\) are governed by the total water supply \\(P\\) and the total available energy, which is expressed as potential evaporation \\(E_{pot}\\) and which denotes the (atmospheric) water demand. If \\(E_{pot}\\) is small, the discharge \\(Q\\) usually is bigger than evaporation \\(E\\). Similarly, if the available radiative energy is very high, the water demand \\(E_{pot}\\) is very large and \\(Q \\ll E\\) (Arora 2002). \\(E_{pot}\\) and \\(P\\) are thus the critical determinants of annual or longer timescale runoff and evaporation rates. Michael Budyko has termed the ratio \\(E_{pot} / P\\) aridity index (Budyko 1974).\nAs explained above, water demand is determined by energy. Solar radiation is the primary energy source for the earth-atmosphere system and the key driver of the hydrological cycle. At the earth’s surface, the net radiative flux \\(R_N\\) is the energy that is available for a) heating and cooling of the soil (ground heat flux), b) changing the phase of water (latent heat flux), and c) heating or cooling air in the boundary layer thus causing atmospheric dynamics (sensible heat flux).\nThis can be formalized with the following relationship\n\\[\nR_{N} = H_{S} + H_{L} + \\Delta H_{G}\n\\tag{9.4}\\]\nwhere \\(R_{N}\\) is the net radiation [in W/m2 = kg/s3], \\(H_{S}\\) is the upward sensible heat flux, \\(H_{L}\\) is the latent heat flux and \\(\\Delta H_{G}\\) the net ground heat flux. The latent heat flux is directly proportional to evaporation \\(E\\). Thus, \\(H_{L} = L \\cdot E\\) where \\(L = 2.5 \\cdot 10^{6}\\) J/kg [= m2/s2] is the latent heat of vaporization and \\(E\\) is the actual evaporation in [m/s]. As in the case of the water balance, at the annual or longer time scales, we can neglect the heat storage effect in the ground and get\n\\[\nR_{N} = H_{S} + L \\cdot E\n\\tag{9.5}\\]\nThe Bowen ratio is defined as the fraction of the sensible heat flux divided by the latent heat flux, i.e.\n\\[\n\\gamma = \\frac{H_{S}}{H_{L}} = \\frac{H_{S}}{L \\cdot E }\n\\tag{9.6}\\]\nBy rearranging the terms, the long-term energy balance in Equation Equation 9.5 can simply be rewritten as\n\\[\nR_{N} = (1 + \\gamma)L E\n\\tag{9.7}\\]\nUsing the fact that \\(R_{N} = L E_{pot}\\), where \\(E_{pot}\\) is the potential evaporation, and dividing by precipitation, we can rewrite the above Equation 9.7 as\n\\[\n\\frac{E_{pot}}{P} = (1 + \\gamma) \\frac{E}{P}\n\\tag{9.8}\\]\nwhere the left-hand side is called the aridity index, i.e. \\(\\phi = E_{pot}/P\\) and \\(E/P\\) is called the evaporative fraction or evaporation index, as mentioned above. With this, Equation 9.8 can be written as a function of the Bowen ratio and the aridity index, i.e.\n\\[\n\\frac{E}{P} = 1 - \\frac{Q}{P} = \\frac{\\phi}{(1 + \\gamma)}\n\\tag{9.9}\\]\n\\(Q/P\\) is again the runoff index. Since the Bowen ratio is also water supply and energy demand limited, it too is a function of the aridity index and we can thus rewrite Equation 9.9 as\n\\[\n\\frac{E}{P} = \\frac{\\phi}{1 + f(\\phi)} = F[\\phi]\n\\tag{9.10}\\]\nThe Budyko relationship thus allows for a simple parameterization of how the aridity index \\(\\phi\\) controls the long-term mean partitioning of precipitation into stream-flow and evaporation and it is capable of capturing the behavior of thousands of catchments around the world. This explains its growing popularity over recent years (Berghuijs, Gnann, and Woods 2020).\nFigure @ref(fig:budykoSpace) shows a plot of data from catchments in the US for which consistent long-term hydro-climatological data records are available. Individual catchments’ aridity indices are plotted against evaporative fractions, averaged over many years. The catchment data plots along the Budyko curve in the two-dimensional Budyko space as indicated in the Figure where the Budyko curve is defined as\n\\[\n\\frac{E}{P} = \\left[ \\frac{E_{pot}}{P} \\text{tanh} \\left( \\frac{P}{E_{pot}} \\right) \\left( 1 - \\text{exp} \\left( - \\frac{E_{pot}}{P} \\right) \\right) \\right]^{1/2}\n\\tag{9.11}\\]\nThis non-parametric relationship between the aridity index and the evaporative fraction was developed by M. Budyko (Budyko 1951).\nThe Budyko space is delineated by the demand and supply limits. Catchments within the space should theoretically fall below the supply limit (\\(E/P = 1\\)) and the demand limit (\\(E/E_{pot} = 1\\)), but tend to approach these limits under very arid or very wet conditions (Berghuijs, Gnann, and Woods 2020). The data from the US shows that a large percentage of in-between catchment variability can be explained by the Budyko curve. After the seminal work Budyko in the last century, the evidence for a strong universal relationship between aridity and evaporative fraction via the Budyko curve has since grown. As catchment hydrology still lacks a comprehensive theory that could explain this simple behavior across diverse catchments Gentine et al. (2012), the ongoing debate about the the underlying reasons for this relationship continues (see e.g. (Padron et al. 2017; Berghuijs, Gnann, and Woods 2020)).\nWhile almost all catchments plot within a small envelope of the original Budyko curve, systematic deviations are nevertheless observed from the original Budyko curve. Several new expressions for \\(F[\\phi]\\) were therefore developed to describe the long-term catchment water balance with one parameter (see e.g. Budyko (1974); Sposito (2017); Choudhury (1999)). One popular equation using only 1 parameter is the Choudhury equation which relates the aridity index \\(\\phi\\) to the evaporative fraction \\(E/P\\) in the following way\n\\[\n\\frac{E}{P} = \\left[ 1 + \\left( \\frac{E_{pot}}{P} \\right) ^{-n} \\right]^{1/n}\n\\tag{9.12}\\]\nwhere \\(n\\) is a catchment-specific parameter which accounts for factors such as vegetation type and coverage, soil type and topography, etc. (see e.g. Zhang et al. (2015) for more information). In other words, \\(n\\) integrates the net effects of all controls of of the evaporative fraction other than aridity. The Figure @ref(fig:ChoudhuryEquationStateSpace) shows the control of \\(n\\) over the shape of the Budyko Curve.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Long-term Water Balance Modeling</span>"
    ]
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-budyko-data-and-methods",
    "href": "long_term_water_balance_modeling.html#sec-budyko-data-and-methods",
    "title": "9  Long-term Water Balance Modeling",
    "section": "9.2 Data and Methods",
    "text": "9.2 Data and Methods\n\n9.2.1 Data\nA large number of geospatial data were collected for the Central Asia region. The domain of interest was defined as 55 deg. E - 85. deg. E and 30 deg. N - 50 deg. N.. Shapefiles from the large river basin were retrieved from the Global Runoff Data Center and extracted for the following basins: Amu Darya, Chu, Issy Kul, Murghab-Harirud, Syr Darya and Talas. Where necessary, the polygons of the downstream flat areas were corrected to account for man-made water transfers via large canal systems and corresponding flow alterations across basins there. These large river basins define the area of interest (AOI).\nFor the selected basins, the WMOBB River Network data was extracted from the layers wmobb_rivnets_Q09_10 (containing line sections representing an upland area above 4’504 km2), wmobb_rivnets_Q08_09 (containing line sections representing an upland area between 1’150 km2 and 4’504 km2) and wmobb_rivnets_Q07_08 (containing line sections representing an upland area above between 487 and 1’150 km2) (GRDC, Koblenz, Germany: Federal Institute of Hydrology (BfG). 2020). Permanent water bodies and courses were taken from the global HydroLakes Database (Messager et al. 2016). Information on land cover were taken from the Copernicus Global Land Service: Land Cover 100m: collection 3: epoch 2019: Globe data (Buchhorn et al. 2019). The NASA SRTM digital elevation model 1 Arc-second (30 m) global product was used as a DEM (“NASA Shuttle Radar Topography Mission (SRTM)” 2013).\nIn total, data from 277 gauging stations from Afghanistan, Kyrgyzstan, Kazakhstan, Uzbekistan and Tajikistan could be obtained from the local Hydrometeorological Organization, public reports and the Soviet compendia Surface Water Resources, Vol 14 Issues 1 and 3. Except for the Afghan stations, all stations were manually located in a Geographic Information System (GIS) using the relevant Soviet Military Topographic maps (1:200’000) from the corresponding region. The maps were downloaded from https://maps.vlasenko.net and subsequently geo-referenced in QGIS (QGIS Development Team 2021). Data from northern Afghan rivers’ stream flow characteristics and the location of these gauging stations was taken from (Olson and Williams-Sether 2010).\nFor each gauge, the contributing area was delineated in R with the WhiteboxTools v2.0.0 and long-term norm mean discharge was obtained over variable observation periods between 1900 and 2018 was acquired. For a few selected stations, monthly and decadal time series data are available over the entire observational record. The FLO1K, global maps of mean, maximum and minimum annual stream flow at 1 km resolution from 1960 through 2015 were retrieved (Barbarossa et al. 2018). The goodness of the FLO1K product in the Central Asia domain was validated at the locations of the 277 gauges through linear regression.\nGeospatial information on glaciers was taken from the Randolph Glacier Inventory (RGI) 6.0. Information from 16’617 glaciers was retrieved, together with glacier length, thickness and glacier thinning rates Hugonnet et al. (2021).\nThe CHELSA V21 global daily high-resolution climatology, available from 01-01-1979 until 31-12-2011 was processed over the Central Asia domain to map climate trends, including on temperature, precipitation, snow fraction. The data is available upon request from this site: https://chelsa-climate.org Karger et al. (2021). The CHELSA V21 product is corrected for snow undercatch in the high elevation ranges and thus is able to better represent actual high mountain precipitation than other available global climatologies (Beck et al. 2020). The aridity index (AI) fields were taken from the bio-climate CHELSA V21 data set and compared with the CGIAR AI product (Trabucco and Zomer 2019). Data on an additional 70 bio-climatic indicators were downloaded from the CHELSA V21 1980 - 2010 climatology and statistics extracted for each of the 277 gauged catchments, together with the AI.\nHigh-resolution crop disaggregated irrigated areas were mapped over the entire Central Asia domain (Ragettli, Herberz, and Siegfried 2018). Like this 30 m crop maps were produced with Google Earth Engine using unsupervised classification for the years 2016 - 2020. Vector information on the irrigation systems in the Chu and Talas River basins as well as from the Uzbek Fergana Oblast, including the land cadaster there, are available.\nFinally, data from the GOODD data set was used to retrieve information from 88 dams in the region of interest (Mulligan, Soesbergen, and Sáenz 2020).\n\n\n9.2.2 Methods\nA strategy for hydrological modeling of the regional Central Asian hydrology using the Budyko framework was devised. The Budyko principle posits that, over the long-run, runoff at a particular location is governed by the long-term availability of water (supply) and energy (demand) there (Budyko M., 1974). Under this assumption, the evaporative fraction of a basin, i.e. the long-term mean actual evaporation divided by long-term mean precipitation, can be expressed as a function of the aridity index (long-term mean potential evaporation divided by long-term mean precipitation).",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Long-term Water Balance Modeling</span>"
    ]
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-budyko-results",
    "href": "long_term_water_balance_modeling.html#sec-budyko-results",
    "title": "9  Long-term Water Balance Modeling",
    "section": "9.3 Results",
    "text": "9.3 Results\nThis Section is currently under active development. Please check back later.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Long-term Water Balance Modeling</span>"
    ]
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-discussion-and-conclusions",
    "href": "long_term_water_balance_modeling.html#sec-discussion-and-conclusions",
    "title": "9  Long-term Water Balance Modeling",
    "section": "9.4 Discussion and Conclusions",
    "text": "9.4 Discussion and Conclusions\nThis Section is currently under active development. Please check back later.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Long-term Water Balance Modeling</span>"
    ]
  },
  {
    "objectID": "long_term_water_balance_modeling.html#sec-long-term-water-balance-modeling-references",
    "href": "long_term_water_balance_modeling.html#sec-long-term-water-balance-modeling-references",
    "title": "9  Long-term Water Balance Modeling",
    "section": "9.5 References",
    "text": "9.5 References\n\n\n\n\nArora, Vivek K. 2002. “The Use of the Aridity Index to Assess Climate Change Effect on Annual Runoff.” Journal of Hydrology 265 (1): 164–77. https://doi.org/https://doi.org/10.1016/S0022-1694(02)00101-4.\n\n\nBarbarossa, Valerio, Mark A. J. Huijbregts, Arthur H. W. Beusen, Hylke E. Beck, Henry King, and Aafke M. Schipper. 2018. “FLO1K, Global Maps of Mean, Maximum and Minimum Annual Streamflow at 1 Km Resolution from 1960 Through 2015.” Scientific Data 5 (1): 180052. https://doi.org/10.1038/sdata.2018.52.\n\n\nBeck, Hylke E., Eric F. Wood, Tim R. McVicar, Mauricio Zambrano-Bigiarini, Camila Alvarez-Garreton, Oscar M. Baez-Villanueva, Justin Sheffield, and Dirk N. Karger. 2020. “Bias Correction of Global High-Resolution Precipitation Climatologies Using Streamflow Observations from 9372 Catchments.” Journal of Climate 33 (4): 1299–1315. https://doi.org/10.1175/JCLI-D-19-0332.1.\n\n\nBerghuijs, W. R., S. J. Gnann, and R. A. Woods. 2020. “Unanswered Questions on the Budyko Framework.” Hydrological Processes.\n\n\nBuchhorn, M., B. Smets, L. Bertels, B. De Roo, M. Lesiv, N. E. Tsendbazar, M. Herold, and S. Fritz. 2019. “Copernicus Global Land Service: Land Cover 100m: Collection 3: Epoch 2019: Globe.”\n\n\nBudyko, M. I. 1951. “On Climatic Factors of Runof (in Russian).” Problemy Fiz Geeografii 16: 41–48.\n\n\n———. 1974. Climate and Life. Academic Press.\n\n\nChoudhury, B. J. 1999. “Evaluation of an Empirical Equation for Annual Evaporation Using Field Observations and Results from a Biophysical Model.” Journal of Hydrology 216: 99–110.\n\n\nFarinotti, Daniel, Matthias Huss, Johannes J. Fürst, Johannes Landmann, Horst Machguth, Fabien Maussion, and Ankur Pandit. 2019. “A Consensus Estimate for the Ice Thickness Distribution of All Glaciers on Earth.” Nature Geoscience 12 (3): 168–73. https://doi.org/10.1038/s41561-019-0300-3.\n\n\nGentine, P., P. D’Odorico B. R. Lintner, G. Sivandran, and G. Salvucci. 2012. “Interdependence of Climate, Soil, and Vegetation as Constrained by the Budyko Curve.” Geophysical Research Letters 39 (19).\n\n\nGLIMS, and NSIDC. 2005, updated 2018. Global Land Ice Measurements from Space Glacier Database. Compiled and made available by the international GLIMS community and the National Snow and Ice Data Center, Boulder CO, U.S.A. DOI:10.7265/N5V98602.\n\n\nGRDC, Koblenz, Germany: Federal Institute of Hydrology (BfG). 2020. “Major River Basins of the World / Global Runoff Data Centre, GRDC. 2nd, Rev. Ext. Ed.” Shape.\n\n\nHugonnet, Romain, Robert McNabb, Etienne Berthier, Brian Menounos, Christopher Nuth, Luc Girod, Daniel Farinotti, et al. 2021. “Accelerated Global Glacier Mass Loss in the Early Twenty-First Century.” Nature 592 (7856): 726–31. https://doi.org/10.1038/s41586-021-03436-z.\n\n\nKarger, Dirk Nikolaus, Olaf Conrad, Jürgen Böhner, Tobias Kawohl, Holger Kreft, Rodrigo Wilber Soria-Auza, Niklaus E. Zimmermann, H. Peter Linder, and Michael Kessler. 2017. “Climatologies at high resolution for the earth’s land surface areas.” Scientific Data 4 (1): 170122. https://doi.org/10.1038/sdata.2017.122.\n\n\nKarger, Dirk Nikolaus, Dirk R. Schmatz, Gabriel Dettling, and Niklaus E. Zimmermann. 2020. “High-resolution monthly precipitation and temperature time series from 2006 to 2100.” Scientific Data 7 (1): 248. https://doi.org/10.1038/s41597-020-00587-y.\n\n\nKarger, Dirk Nikolaus, Adam M. Wilson, Colin Mahony, Niklaus E. Zimmermann, and Walter Jetz. 2021. “Global daily 1 km land surface precipitation based on cloud cover-informed downscaling.” Scientific Data 8 (1): 307. https://doi.org/10.1038/s41597-021-01084-6.\n\n\nMessager, M. L., B. Lehner, Grill G., I. Nedeva, and O. Schmitt. 2016. “Estimating the Volume and Age of Water Stored in Global Lakes Using a Geo-Statistical Approach.” Nature Communications 13603.\n\n\nMiralles, D. G., W. Brutsaert, A. J. Dolman, and J. H. Gash. 2020. “On the Use of the Term \"Evapotranspiration\".” Water Resources Research 56 (https://doi.org/10.1029/2020WR028055).\n\n\nMulligan, Mark, Arnout van Soesbergen, and Leonardo Sáenz. 2020. “GOODD, a Global Dataset of More Than 38,000 Georeferenced Dams.” Scientific Data 7 (1): 31. https://doi.org/10.1038/s41597-020-0362-5.\n\n\n“NASA Shuttle Radar Topography Mission (SRTM).” 2013. NASA. https://earthdata.nasa.gov/learn/articles/nasa-shuttle-radar-topography-mission-srtm-version-3-0-global-1-arc-second-data-released-over-asia-and-australia.\n\n\nOlson, S. A., and T. Williams-Sether. 2010. “Streamflow Characteristics at Streamgages in Northern Afghanistan and Selected Locations.” U.S. Geological Survey Data Series 529. USGS.\n\n\nPadron, R. S., L. Gudmundsson, P. Greve, and S. Seneviratne. 2017. “Largescale Controls of the Surface Water Balance over Land: Insights from a Systematic Review and Meta-Analysis.” Water Resources Research.\n\n\nQGIS Development Team. 2021. QGIS Geographic Information System. QGIS Association.\n\n\nRagettli, Silvan, Timo Herberz, and Tobias Siegfried. 2018. “An Unsupervised Classification Algorithm for Multi- Temporal Irrigated Area Mapping in Central Asia.” Remote Sensing 10 (11): 1823. https://doi.org/10.3390/rs10111823.\n\n\nSposito, Garrison. 2017. “Understanding the Budyko Equation.” Water 9 (4): 236. https://doi.org/10.3390/w9040236.\n\n\nTrabucco, Antonio, and Robert Zomer. 2019. “Global Aridity Index and Potential Evapotranspiration (ET0) Climate Database v2,” January. https://doi.org/10.6084/m9.figshare.7504448.v3.\n\n\nZhang, D., Z. Cong, G. Ni, D. Yang, and S. Hu. 2015. “Effects of snow ratio on annual runoff within the Budyko framework.” Hydrology and Earth System Sciences 19 (4): 1977–92. https://doi.org/10.5194/hess-19-1977-2015.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Long-term Water Balance Modeling</span>"
    ]
  },
  {
    "objectID": "modeling_using_predictive_inference.html",
    "href": "modeling_using_predictive_inference.html",
    "title": "10  Modeling Using Predictive Inference",
    "section": "",
    "text": "10.1 Prerequisites\nTime series-based hydrological models use past discharge observations and auxilliary variables such as snow cover in the catchment and/or precipitation to predict the future from learned past patterns. Currently, no ready-made hydrological modeling packages can be used for this purpose. Such types of models rather draw on available time series-based modeling algorithms that are implemented and available in dedicated packages for programming environments like R.\nWe will work with R and RStudio and the following packages that need to be installed to replicate and run the models shown here. Note that such a package installation step only has to be performed once.\n# Core Libraries\ninstall.packages('tidyverse')    # Meta - dplyr, ggplot2, purrr, tidyr, stringr, forcats\ninstall.packages('lubridate')    # date and time\ninstall.packages('timetk')       # Time series data wrangling, visualization and preprocessing\ninstall.packages('modeltime')    # latest and greatest time series modeling package for R\ninstall.packages('tidymodels')   # The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles.\n\n# Sample data from Central Asia river basins and helper functions\nlibrary('devtools')\ndevtools::install_github(\"hydrosolutions/riversCentralAsia\")\nThe packages can then be loaded and made available in your R session.\nlibrary(devtools)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(timetk)\nlibrary(tidymodels)\nlibrary(riversCentralAsia)\nWhen other additional packages are needed, they will be loaded in the corresponding Sections below.\nPlease also remember the following rules when working with R dataframes in the tidyverse:\nA final note. In all of the following, we mostly use the powerful data manipulation and visualization techniques for time series data as provided by the timetk package. This package greatly facilitates any work with time series data as it, among other things, nicely integrates with the R ‘tidyverse’.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Modeling Using Predictive Inference</span>"
    ]
  },
  {
    "objectID": "modeling_using_predictive_inference.html#sec-em-prerequisites",
    "href": "modeling_using_predictive_inference.html#sec-em-prerequisites",
    "title": "10  Modeling Using Predictive Inference",
    "section": "",
    "text": "Every column is variable.\nEvery row is an observation.\nEvery cell is a single value.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Modeling Using Predictive Inference</span>"
    ]
  },
  {
    "objectID": "modeling_using_predictive_inference.html#sec-em-forecasting-using-predictive-inference",
    "href": "modeling_using_predictive_inference.html#sec-em-forecasting-using-predictive-inference",
    "title": "10  Modeling Using Predictive Inference",
    "section": "\n10.2 Forecasting Using Predictive Inference",
    "text": "10.2 Forecasting Using Predictive Inference\nIn this Section, we are concerned with predictive inference using observed data to predict future data that is not known yet but that is important to forecast with high confidence and low uncertainty. In other words, it is assumed that we can encapsulate historic patterns in a model for learning about the future with such model.\nIn hydrology, we are dealing with time series, i.e. ordered observations in time. A generic model structure thus can be specified in the following way\n\\[\ny(t+\\Delta t) = f(y(t),x(t)) + \\epsilon(t)\n\\]\nwhere \\(y(t+\\Delta t)\\) is called the forecast target (discharge at a particular gauge in our case) and is the variable that we want to forecast in the future, i.e. \\(\\Delta t\\) time away from now. \\(y(t)\\) denotes past known observations of discharge up and including time \\(t\\). Similarly, \\(x(t)\\) denotes other variables of interest, called external regressors, that might be relevant to obtain good quality forecasts such as meteorological data from local stations, including precipitation and temperature. Finally, \\(f()\\) denotes the type of model that is being used for forecasting and \\(\\epsilon\\) are the time-dependent error terms. If, for example, one would use a linear modeling approach without external regressors, such type of model could simply be written as\n\\[\ny(t+\\Delta t) = \\beta_{0} + \\beta_{1} \\cdot y(t) + \\epsilon(t)\n\\]\nIn the model specification above, the aim is to predict the future with a lead time of \\(\\Delta t\\), i.e., for example one month ahead. The lead-time model can be written in equivalent form using lags in the following way\n\\[\ny(t) = f(y(t-lag),x(t-lag)) + \\epsilon(t)\n\\]\nwhere \\(lag = \\Delta t\\). This just means that we use all the available observations until and including \\(t-lag\\) for predicting the target at the time \\(t\\). We will use these specifications throughout the Chapter when working with and developing new forecasting models.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Modeling Using Predictive Inference</span>"
    ]
  },
  {
    "objectID": "modeling_using_predictive_inference.html#sec-em-forecasting-in-central-asia",
    "href": "modeling_using_predictive_inference.html#sec-em-forecasting-in-central-asia",
    "title": "10  Modeling Using Predictive Inference",
    "section": "\n10.3 Forecasting Discharge in Central Asian",
    "text": "10.3 Forecasting Discharge in Central Asian\n\n10.3.1 Hydrometeorological Organizations\nThe key agencies that are charged in predicting river discharge regularly are the Central Asia National Meteorological and Hydrological Services (NMHS). For the prediction of mean discharge over a specific target future period, they use different types of statistical models.\nThe particular type of model they use depends on the available hydrological and meteorological data for a particular river whose mean discharge is to be forecasted and on the type of the forecast. Types of forecasts include\n\ndaily forecasts, i.e. \\(\\Delta t = 1 \\text{ day}\\),\npentadal forecasts, i.e. \\(\\Delta t = 5 \\text{ days}\\),\ndecadal forecasts, i.e. \\(\\Delta t = 10 \\text{ days}\\),\nmonthly forecasts, i.e. \\(\\Delta t = 1 \\text{ month}\\), and\nseasonal forecasts, i.e. \\(\\Delta t = 6 \\text{ months}\\).\n\nTo this date, these types of forecasts are performed at regular intervals by the operational hydrologists. Normally, this requires a lot of manual work. Recently, selected NMHS use automated software to automatize this type of work. The Chapter on Real-World Hydrological Modeling Applications discusses such a software application, called iEasyHydro, in greater detail.\nIn Uzbekistan, for example, the following list of forecast objects exists in the NMHS.\n\nList of Uzbek forecast target objects. Forecast type abbreviations are dl: daily forecast, m: monthly forecast, s: seasonal forecast. Country abbreviations are UZB: Uzbekistan, KGZ: Kyrgyzstan, and TJK: Tajikistan. Source: Uzbek Hydrometeorological Service.\n\nRiver\nGauge (Target Object)\nGauge Code\nCountry\nTypes of Forecasts\n\n\n\nChirchik\nInflow to Charvak Res.\n16924\nUZB\ndl, m, s\n\n\nChirchik\nInflow to Charvak Res. + Ugam River\n16924 + 16300\nUZB\nm, s\n\n\nAkhangaran\nIrtash\n16230\nUZB\nm, s\n\n\nChadak\nDzhulaysay\n16202\nUZB\nm, s\n\n\nGavasay\nGava\n16193\nUZB\nm, s\n\n\nPadsha-Ata\nTostu\n16176\nKGZ\nm, s\n\n\nKara Darya\nInflow to Andizhan water reservoir\n16938\nUZB/KGZ\ndl, m, s\n\n\nIsfayramsoy\nUch-Kurgan\n16169\nKGZ\nm, s\n\n\nSokh\nSarykanda\n16198\nUZB\ndl (May - Sep.), m, s\n\n\nSanzar\nKyrk\n16223\nUZB\nm, s\n\n\nNaryn\nInflow to Toktogul water reservoir\n16936\nKGZ\nm, s\n\n\nVaksh\nInflow to Nurek water reservoir\n17084 (Vakh river - Darband gauge)\nTJK\nm, s\n\n\nKafirnigan\nTartki\n17137\nTJK\nm, s\n\n\nTupalang\nInflow to Tupalang water reservoir\n17194\nUZB\nm, s\n\n\nSangardak\nKeng-Guzar\n17211\nUZB\nm, s\n\n\nAkdarya\nInflow to Gissarak water reservoir\n17464 (Akfariya river - Hissarak gauge)\nUZB\nm, s\n\n\nYakkabagdarya\nTatar\n17260\nUZB\nm, s\n\n\nUryadariya + Kichik Uryadariya\nInflow to Pachkamar water reservoir\n17279 (Uryadariya river - Bazartepe gauge) + 17275 (Kichik Uryadariya - Gumbulak gauge)\nUZB\nm, s\n\n\nZeravshan\nInflow to Rovatkhodzha hydro work\n17461\nUZB\nm, s\n\n\n\nIt should be noted that all of the NMHS have lists with different forecast targets for different rivers. As can be seen from the above list, Uzbekistan does not issue pentade and decadal forecasts, i.e. types of forecasts, which are regularly issued by the KGZ, for example.\nFinally, seasonal forecasts in the Uzbek NMHS are issued twice before the irrigation season with 3. - 5. March being the first data range for issuing, and 3. - 5. April being the second one. Converse to this, monthly forecasts are issues between the 25. - 27. day each month. Finally, decadal and pentade forecasts are issued each morning at the day of the end of the corresponding pentade or decade.\nIt is important to emphasize again that there is currently no standardized way in the region to produce these forecasts. While in some instances, a particular approach and method works very well, it fails to produce acceptable forecasts in other basins. However, as we shall see, certain techniques work very well for particular forecast horizons which then explains why such type of technique has become widely used in the region.\n\n10.3.2 Forecasting for What and Whom?\nWhy is all this work is required? What is the purpose of predicting mean discharge into the future at regular intervals? Important recipients of the forecast products include the Water Authorities which are in charge of delivering adequate amounts of water for irrigation at the right time and location.\nIt all starts with pre-season irrigation planning. The main irrigation season in most of Central Asia is from April 1. through the end of September. Previous to the start of the irrigation season, irrigation plans are drafted based on computed irrigation water demand of all the water users that are connected to a particular irrigation system. These irrigation system are defined in terms of canal topology where demand gets aggregated from the bottom up to the Rayvodkhozes and the Oblvodkhozes. The later then starts with the pre-season irrigation planning given the water irrigation system-level demand. These plans specify decadal water discharge for each irrigation system and the corresponding canals.\nDemand is one thing, but expected supply from the Central Asian rivers another. In order to be able to match the irrigation water demand, the water authorities do what is needed and receive from the Hydromets first the seasonal discharge forecasts. Given this forecast of irrigation-season water availability, the water authorities then adjust their plans given the particular expected circumstances. If, for example, an exceptionally dry year is expected, they activate limit plans and reduce planned water distributions according to forecasted quantities. If a wet year is expected, they do not perform these adjustments and maybe even release water from reservoirs previous to the irrigation season to ensure enough storage capacity in the reservoirs.\nWith the beginning of the irrigation season, another seasonal forecast is carried out by the Hydromets and communicated to the Central Asian water authorities. Given the updated forecast, planning is revised and adjusted accordingly. Then, finally, throughout the irrigation season pentadal, decadal and monthly forecasts are produced constantly to carefully balance water demand with supplies while the season is under way.\nNeedless to say that other important customers for hydrometeorological forecasts exist, including for example airports, road departments that need to ensure road safety, agricultural clusters that are interested in frost and hail warnings, local authorities that need to be alerted in the case of extreme local weather conditions, etc.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Modeling Using Predictive Inference</span>"
    ]
  },
  {
    "objectID": "modeling_using_predictive_inference.html#sec-em-data-and-preparation",
    "href": "modeling_using_predictive_inference.html#sec-em-data-and-preparation",
    "title": "10  Modeling Using Predictive Inference",
    "section": "\n10.4 Data and Preparation",
    "text": "10.4 Data and Preparation\n\n10.4.1 Available Data\nThe riversCentralAsia Package provides available data of the gauging and meteorological stations in the Chirchik River Basin.\nBefore starting any type of modeling, it is important to get a good understanding of the data that we are dealing with and whether there exist problems with the raw data that need to be addressed prior to modeling. Problems usually include data gaps and outliers as data record that one obtains are usually ever complete nor clean of errors.\nThe steps performed here are thus required steps for any type of successful modeling and should be performed with great care. We concentrate our efforts here on discharge records and data from meteorological stations in the Chirchik River Basin. The techniques shown here for decadal (10-days) data naturally extend to monthly data and other basins.\n\n10.4.2 Gap Filling Discharge Data\nIn the following, we will work with decadal discharge data from the two main tributaries, i.e. the Chatkal and (Gauge 16279) Pskem rivers (Gauge 16290) and the data of the inflow to the Charvak reservoir (Gauge 16924). The goal is to analyze the data and prepare for modeling. First, let us load the relevant discharge data.\n\ndata &lt;- ChirchikRiverBasin # load data\nq_dec_tbl &lt;- data |&gt; filter(code == '16279' | code == '16290' | code == '16924') # Note for the new name of the object, we choose to add periodicity (_dec_) and data type (_tbl for tibble/dataframe) to the data name. This just helps to stay organized and is good practice in R programming.\nq_dec_tbl\n\n# A tibble: 9,072 × 14\n   date        data  norm units type  code  station   river   basin   resolution\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;fct&gt;     \n 1 1932-01-10  48.8  38.8 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 2 1932-01-20  48.4  37.5 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 3 1932-01-31  42.4  36.6 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 4 1932-02-10  43.7  36.4 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 5 1932-02-20  44.2  36.3 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 6 1932-02-29  47.7  36.9 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 7 1932-03-10  54.1  39.4 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 8 1932-03-20  63.2  47.6 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n 9 1932-03-31 103    60.5 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n10 1932-04-10 103    86.4 m3s   Q     16279 Khudaydod Chatkal Chirch… dec       \n# ℹ 9,062 more rows\n# ℹ 4 more variables: lon_UTM42 &lt;dbl&gt;, lat_UTM42 &lt;dbl&gt;, altitude_masl &lt;dbl&gt;,\n#   basinSize_sqkm &lt;dbl&gt;\n\n\nYou can get more information about the available data by typing ?ChirchikRiverBasin.\nIt is advisable to check at this stage for missing data in time series and to fill gaps where present. As can be seen in Figure 10.1, close inspection of the time series indeed reveals some missing data in the 1940ies.\n\nq_dec_tbl |&gt; plot_time_series(date,data,\n                               .facet_vars  = code,\n                               .smooth      = FALSE,\n                               .interactive = TRUE,\n                               .x_lab       = \"Date\",\n                               .y_lab       = \"Discharge [m3/s]\",\n                               .title       = \"\"\n                               )\n\n\n\n\n\n\nFigure 10.1: Discharge data of selected gauges in the upstream zone of runoff formation in the Chirchik River Basin. Data Source: Uzbek Hydrometeorological Service.\n\n\n\nNote, Figure 10.1 and the following Figures are interactive, so you can zoom in to regions of interest.\nMissing data are also confirmed by the warning that the function timetk::plot_time_series() throws (suppressed here). Statistics of the missing data can be easily obtained. As the Table below shows, we can do this analysis for each discharge station separately.\n\n# print knitr::kable table\nq_dec_tbl |&gt; group_by(code) |&gt; \n  summarize(n.na = sum(is.na(data)), na.perc = n.na/n()*100) |&gt; \n  knitr::kable(caption = \"Number of missing data points and percentage of missing data for each discharge station. n.na: number of missing data points in the time series, na.perc: percentage of missing data.\")\n\n\nNumber of missing data points and percentage of missing data for each discharge station. n.na: number of missing data points in the time series, na.perc: percentage of missing data.\n\ncode\nn.na\nna.perc\n\n\n\n16279\n15\n0.4960317\n\n\n16290\n39\n1.2896825\n\n\n16924\n42\n1.3888889\n\n\n\n\n\nSummarizing the number of observation with missing data reveals 15 data points for station 16279 (0.5 % of total record length) and 39 for station 16290 (1.3 % of total record length). As there are only very few gaps in the existing time series, we use a simple method to fill these. Wherever there is a gap, we fill in the corresponding decadal norm as stored in the norm column in the object q_dec_tbl. The visualization of the results confirms that our simple gap filling approach is indeed satisfactory as shown in Figure 10.2.\n\nq_dec_filled_tbl &lt;- q_dec_tbl\n\nq_dec_filled_tbl$data[is.na(q_dec_filled_tbl$data)] = \n  q_dec_filled_tbl$norm[is.na(q_dec_filled_tbl$data)] # Gap filling step\n\nq_dec_filled_tbl |&gt; plot_time_series(date, data, \n                                      .facet_vars  = code, \n                                      .smooth      = FALSE,\n                                      .interactive = TRUE,\n                                      .x_lab       = \"year\",\n                                      .y_lab       = \"m^3/s\",\n                                      .title       = \"\"\n                                      )\n\n\n\n\n\n\nFigure 10.2: Gap filled Pskem and Chatkal river discharges.\n\n\n\nA note of caution here. This simple gap filling technique reduces variance in the time series. It should only be used when the percentage of missing data is low. As will be discussed in the next Section 10.4.3 below, better techniques have to be utilized when there exist substantial gaps and in the case of less regular data.\nFinally, we discard the norm data which we used for gap filling of the missing discharge data and convert the data to wide format (see the Table below) to add to it meteorological data in the next Section.\n\nq_dec_filled_wide_tbl &lt;- q_dec_filled_tbl |&gt; # again we use the name convention of objects as introduced above\n  mutate(code = paste0('Q',code |&gt; as.character())) |&gt; # Since we convert everything to long form, we want to keep information as compact as possible. Hence, we paste the type identifier (Q for discharge here) in from of the 5 digit station code.\n  dplyr::select(date,data,code) |&gt; # ... and then ditch all the remainder information\n  pivot_wider(names_from = \"code\",values_from = \"data\") # in order to pivot to the long format, we need to make a small detour via the wide format.\n\nq_dec_filled_long_tbl &lt;- q_dec_filled_wide_tbl |&gt; pivot_longer(-date) # and then pivot back\nq_dec_filled_wide_tbl\n\n# A tibble: 3,024 × 4\n   date       Q16279 Q16290 Q16924\n   &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 1932-01-10   48.8   38.3   87.1\n 2 1932-01-20   48.4   37.7   86.1\n 3 1932-01-31   42.4   36.2   78.6\n 4 1932-02-10   43.7   35.6   79.3\n 5 1932-02-20   44.2   35     79.2\n 6 1932-02-29   47.7   37.1   84.8\n 7 1932-03-10   54.1   43.1   97.2\n 8 1932-03-20   63.2   47    110  \n 9 1932-03-31  103     72.1  175  \n10 1932-04-10  103     73.2  176  \n# ℹ 3,014 more rows\n\n\nAs a result, we now have a complete record of decadal discharge data for the two main tributaries of the Chirchik river and the inflow time series to Charvak Reservoir from the beginning of 1932 until and including 2015, i.e. 84 years. The same type of preparatory analysis will now be carried out for the meteorological data.\n\n10.4.3 Gap Filling of Meteorological Data\nHere, we use precipitation and temperature data from Pskem (38462), Chatkal (38471) and Charvak Reservoir (38464) Meteorological Stations (see the Example River Basins Chapter for more information on these stations). We also have data from Oygaing station (Station Code 38339) but the record only starts in 1962 and the time resolution is monthly. Therefore, we do not take this station into account here for the time being.\nWe start with precipitation and plot the available data.\n\np_dec_tbl &lt;- data |&gt; filter(type==\"P\" & code!=\"38339\") \np_dec_tbl |&gt; plot_time_series(date,data,\n                               .facet_vars  = code,\n                               .interactive = TRUE,\n                               .smooth      = FALSE,\n                               .title       = \"\",\n                               .y_lab       = \"mm/decade\",\n                               .x_lab       = \"year\"\n                               )\n\n\n\n\n\n\nFigure 10.3: Raw decadal precipitation data from Pskem (38462), Charvak Reservoir (38471) and Chatkal Meteo Station (38471).\n\n\n\nThe precipitation data from these 3 stations shows some significant data gaps. The Chatkal Meteorological Station that is located in Kyrgyzstan apparently did not work in the post-transition years and continuous measurements were only resumed there in 1998.\nLet us see what happens if we were to use the same simple gap filling technique that we introduced above for discharge.\n\np_dec_filled_tbl &lt;- p_dec_tbl\np_dec_filled_tbl$data[is.na(p_dec_filled_tbl$data)] = p_dec_filled_tbl$norm[is.na(p_dec_filled_tbl$data)]\np_dec_filled_tbl |&gt; plot_time_series(date,data,\n                                      .facet_vars  = code,\n                                      .interactive = TRUE,\n                                      .smooth      = FALSE,\n                                      .title       = \"\",\n                                      .y_lab       = \"mm/decade\",\n                                      .x_lab       = \"year\"\n                                      )\n\n\n\n\n\n\nFigure 10.4: Precipitation Data gap-filled with norms. The filled values from 1990 - 2000 in the case of the Station 38471 indicate that the norm-filling technique is not good.\n\n\n\nClosely inspect the significant data gap in the 1990ies at Station 38741 (tip: play around and zoom into the time series in the 1990ies in Figure 10.3 and comparing it with the resulting gap-filled timeseries in Figure 10.4. We see that our technique of gap filling with long-term norms is not suitable for this type of data and the significant gap size. The effect of variance reduction is also clearly visible.\nHence, we resort to a more powerful gap filling technique that uses a (regression) model to impute the missing values from existing ones at the neighboring stations, i.e. Stations 38462 and 38464. To do so, we utilize an R package that is tightly integrated in the tidyverse. Please note that if you do not have the required package installed locally, you should install it prior to its use with the following command install.packages('simputation')\n\nlibrary(simputation)\n# First, we bring the data into the suitable format. \np_dec_wide_tbl &lt;- p_dec_tbl |&gt; \n  mutate(code = paste0('P',code |&gt; as.character())) |&gt; \n  dplyr::select(date,data,code) |&gt; \n  pivot_wider(names_from = \"code\",values_from = \"data\")\n\n# Second, we impute missing values.\np_dec_filled_wide_tbl &lt;- p_dec_wide_tbl  |&gt; \n  impute_rlm(P38471 ~ P38462 + P38464) |&gt; # Imputing precipitation at station 38471 using a robust linear regression model\n  impute_rlm(P38462 ~ P38471 + P38464) |&gt; # Imputing precipitation at station 38462 using a robust linear regression model\n  impute_rlm(P38464 ~ P38462 + P38471) # Imputing precipitation at station 38464 using a robust linear regression model\n\np_dec_filled_long_tbl &lt;- p_dec_filled_wide_tbl |&gt; pivot_longer(c('P38462','P38464','P38471')) \n\np_dec_filled_long_tbl|&gt; plot_time_series(date,value,\n                                          .facet_vars  = name,\n                                          .interactive = TRUE,\n                                          .smooth      = FALSE,\n                                          .title       = '',\n                                          .y_lab       = \"mm/decade\",\n                                          .x_lab       = \"year\"\n                                          )\n\n\n\n\n\n\nFigure 10.5: Precipitation Data gap filled with a robust linear regression modeling approach.\n\n\n\nAs you can see, we use simple linear regression models to impute missing value in the target time series using observations from the neighboring stations.\nThrough simple visual inspection, it becomes clear that this type of regression model for gap filling is better suited than the previous approach chosen. Let us check whether we could successfully fill all gaps with this robust linear regression approach.\n\n# Write knitr::kable Table\np_dec_filled_long_tbl |&gt; \n  group_by(name) |&gt; \n  summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100) |&gt; \n  knitr::kable(caption = \"Number of missing values and their percentage in the precipitation time series after gap filling.\")\n\n\nNumber of missing values and their percentage in the precipitation time series after gap filling.\n\nname\nn.na\nn.na.perc\n\n\n\nP38462\n12\n0.4016064\n\n\nP38464\n12\n0.4016064\n\n\nP38471\n3\n0.1004016\n\n\n\n\n\nIt turns out that we still have very few gaps to deal with. We can see them by simply visualizing the wide tibble. The problem persisted at times when two or more values were missing across the available stations at the same time and where thus the linear regression could not be carried out.\n\np_dec_filled_wide_tbl |&gt; head(10)\n\n# A tibble: 10 × 4\n   date       P38462 P38464 P38471\n   &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 1933-01-10     NA   NA        2\n 2 1933-01-20     NA   NA       10\n 3 1933-01-31     NA   NA        5\n 4 1933-02-10     NA   NA       33\n 5 1933-02-20     NA   NA        8\n 6 1933-02-28     NA   NA       10\n 7 1933-03-10     NA   NA       31\n 8 1933-03-20     NA   NA       50\n 9 1933-03-31     NA   NA        6\n10 1933-04-10     23   21.3     13\n\n\n\np_dec_filled_wide_tbl |&gt; tail()\n\n# A tibble: 6 × 4\n  date       P38462 P38464 P38471\n  &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 2015-11-10     72     81     19\n2 2015-11-20    122     76     43\n3 2015-11-30      7      2      3\n4 2015-12-10     NA     NA     NA\n5 2015-12-20     NA     NA     NA\n6 2015-12-31     NA     NA     NA\n\n\nWe can solve the issues related to the missing values at the start of the observation record by using the same technique as above and by only regressing P38462 and P38464 on P38471.\n\np_dec_filled_wide_tbl &lt;- p_dec_filled_wide_tbl  |&gt; \n  impute_rlm(P38462 ~ P38471) |&gt; # Imputing precipitation at station 38462 using a robust linear regression model\n  impute_rlm(P38464 ~ P38471) # Imputing precipitation at station 38464 using a robust linear regression model\np_dec_filled_wide_tbl |&gt; head(10)\n\n# A tibble: 10 × 4\n   date       P38462 P38464 P38471\n   &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 1933-01-10   5.60   5.08      2\n 2 1933-01-20  18.3   16.7      10\n 3 1933-01-31  10.4    9.46      5\n 4 1933-02-10  54.9   50.3      33\n 5 1933-02-20  15.2   13.8       8\n 6 1933-02-28  18.3   16.7      10\n 7 1933-03-10  51.8   47.3      31\n 8 1933-03-20  82.0   75.0      50\n 9 1933-03-31  12.0   10.9       6\n10 1933-04-10  23     21.3      13\n\n\nConverse to this, the complete set of observations is missing for December 2015. We will thus remove these non-observations from our tibble.\n\np_dec_filled_wide_tbl &lt;- p_dec_filled_wide_tbl |&gt; na.omit()\np_dec_filled_wide_tbl |&gt; tail()\n\n# A tibble: 6 × 4\n  date       P38462 P38464 P38471\n  &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 2015-10-10      5      1      0\n2 2015-10-20     89    108     58\n3 2015-10-31     34     40     12\n4 2015-11-10     72     81     19\n5 2015-11-20    122     76     43\n6 2015-11-30      7      2      3\n\np_dec_filled_long_tbl &lt;-  p_dec_filled_wide_tbl |&gt; pivot_longer(-date)\n\nInspecting the temperature data, we see similar data issues as in the precipitation data set.\n\nt_dec_tbl &lt;- data |&gt; filter(type==\"T\") \nt_dec_tbl |&gt; plot_time_series(date,data,\n                               .facet_vars  = code,\n                               .interactive = TRUE,\n                               .smooth      = FALSE,\n                               .title       = '',\n                               .y_lab       = \"Mean decadal temperature (°C)\",\n                               .x_lab       = \"Date\"\n                               )\n\n\n\n\n\n\nFigure 10.6: Raw temperature data from the meteorological stations Pskem (38462) and Chatkal (38471). Data are in deg. C.\n\n\n\n\n# First, we bring the data into the suitable format. \nt_dec_wide_tbl &lt;- t_dec_tbl |&gt; \n  mutate(code = paste0('T',code |&gt; as.character())) |&gt; \n  dplyr::select(date,data,code) |&gt; \n  pivot_wider(names_from = \"code\",values_from = \"data\")\n\n# Second, we impute missing values.\nt_dec_filled_wide_tbl &lt;- t_dec_wide_tbl  |&gt; \n  impute_rlm(T38471 ~ T38462) |&gt; # Imputing precipitation at station 38471 using a robust linear regression model\n  impute_rlm(T38462 ~ T38471) # Imputing precipitation at station 38462 using a robust linear regression model\n\nt_dec_filled_long_tbl &lt;- t_dec_filled_wide_tbl |&gt; \n  pivot_longer(c('T38462','T38471')) \n\nt_dec_filled_long_tbl|&gt; \n  plot_time_series(date,value,\n                   .facet_vars  = name,\n                   .interactive = TRUE,\n                   .smooth      = FALSE,\n                   .title       = '',\n                   .y_lab       = \"Mean decadal temperature (°C)\",\n                   .x_lab       = \"Date\"\n                   )\n\n\n\n\n\n\nFigure 10.7: Temperature data gap filled with robust linear regression modeling.\n\n\n\nThere are some irregularities in the temperature time series of Chatkal Meteorological Station in the first decade of the 20th century (tip: zoom in to see these more clearly). Note that these were not introduced by the gap filling technique that we used but are most likely wrong temperature readings. We will return to these in the outlier analysis below in Section 10.4.4.\n\nt_dec_filled_long_tbl |&gt; \n  group_by(name) |&gt; \n  summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100) |&gt; \n  knitr::kable(caption = \"Number of missing values and their percentage in the temperature time series after gap filling.\")\n\n\nNumber of missing values and their percentage in the temperature time series after gap filling.\n\nname\nn.na\nn.na.perc\n\n\n\nT38462\n3\n0.1004016\n\n\nT38471\n3\n0.1004016\n\n\n\n\n\nTo see where the missing value are, we find them easily again by looking at the head and tail of the tibble.\n\nt_dec_filled_wide_tbl |&gt; head()\n\n# A tibble: 6 × 3\n  date       T38462 T38471\n  &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 1933-01-10   -6.9  -16.6\n2 1933-01-20   -6.1  -15.5\n3 1933-01-31   -6.3  -15.6\n4 1933-02-10   -2     -8.6\n5 1933-02-20   -3.3  -12.5\n6 1933-02-28   -0.1   -8.5\n\n\n\nt_dec_filled_wide_tbl |&gt; tail()\n\n# A tibble: 6 × 3\n  date       T38462 T38471\n  &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 2015-11-10    2.4   -2.5\n2 2015-11-20    2     -2.2\n3 2015-11-30    4.6   -3.7\n4 2015-12-10   NA     NA  \n5 2015-12-20   NA     NA  \n6 2015-12-31   NA     NA  \n\n\nFinally, we remove the non observations again as above with the function na.omit.\n\nt_dec_filled_wide_tbl &lt;- t_dec_filled_wide_tbl |&gt; na.omit()\nt_dec_filled_long_tbl &lt;- t_dec_filled_wide_tbl |&gt; pivot_longer(-date)\n\nTo deal with the missing values at the end of the observational record, we could also have used any other technique. Using the norm values however would have artificially reduced the variance in both cases as explained above. Furthermore and at least in the case of temperature, it is also questionable to what extent a norm calculated over the last 84 years is still representative given global warming. We will look in this important and interesting topic in the next section.\n\n10.4.4 Anomalies and Outliers\nWe use the function timetk::plot_anomaly_diagnostics to investigate anomalies in the time series. For discharge, we first log-transform the raw data with the following transformation to reduce the variance of the original data.\n\\[\n\\hat{q}(t) = log(q(t) + 1)\n\\]\nwhere \\(\\hat{q}(t)\\) denotes the transformed discharge. Prior to the log transformation, 1 is added so as to avoid cases where discharge would be 0 and the logarithmic transform thus undefined. The transformation can easily be done with the log1p() function in R. Backtransformation via the function expm1() simply involves taking the exponent and subtracting 1 from the result.\nThe exceptionally wet year 19169 shows up as anomalous in the Chatkal River Basin and at the downstream Charvak Reservoir inflow gauge. Figure 10.8 and Figure 10.9 show anomalies diagnostics of the available data.\n\nq_dec_filled_long_tbl |&gt; \n  plot_anomaly_diagnostics(date,\n                           value |&gt; log1p(),\n                           .facet_vars  = name,\n                           .frequency = 36,\n                           .interactive = TRUE,\n                           .title = \"\",\n                           .x_lab = \"Date\",\n                           .y_lab = \"Log-transformed discharge (log(m³/s))\")\n\n\n\n\n\n\nFigure 10.8: Anomaly diagnostics of discharge data. The transparent grey band shows the width of the normal range. The highly anomalous wet year of 1969 is clearly visible in the discharge record of the Chatkal river basin (Station 16279).\n\n\n\nThe investigation of precipitation anomalies shows a succession of regular anomalous wet events over time. It is interesting to see that the winter 1968/69 regularly anomalous at all three stations (Figure @ref(fig:EManomaliesP), zoom in to investigate).\n\np_dec_filled_long_tbl |&gt; \n  plot_anomaly_diagnostics(date,\n                           value,\n                           .facet_vars  = name,\n                           .interactive = TRUE,\n                           .title = \"\",\n                           .x_lab = \"Date\",\n                           .y_lab = \"Precipitation (mm)\")\n\n\n\n\n\n\nFigure 10.9: Anomaly diagnostics of precipitation data.\n\n\n\nWhile intuitively, we would have expected an eceptionally mild winter in 1968/69 due to the precipitation excess, the corresponding anomaly does not show up in the temperature record as shown in Figure 10.10.\n\nt_dec_filled_long_tbl |&gt;  \n  plot_anomaly_diagnostics(date,value,\n                           .facet_vars  = name,\n                           .interactive = TRUE,\n                           .title = \"\",\n                           .x_lab = \"Date\",\n                           .y_lab = \"Temperature (°C)\")\n\n\n\n\n\n\nFigure 10.10: Anomaly diagnostics of temperature data.\n\n\n\nApart from the identification of extremal periods since as the 1969 discharge year in the Chatkal river basin, the diagnostics of anomalies also helps to identify likely erroneous data records. In Figure 10.10, for example, when we zoom into the data of the series T38471 in the first decade of the 21st century, problems in relation to positive anomalies during the winter are visible in 4 instances. One explanation would be that in at least some instances, the data are erroneously recorded as positive values when in fact they were negative (see dates ‘2002-01-31’, ‘2005-01-10’ and ‘2007-02-28’, Chatkal Station 38471).\n\n10.4.5 Putting it all Together\nFinally, we are now in the position to assemble all data that we will use for empirical modeling. The data is stored in long and wide form and used accordingly where required. For example, in Section @ref{TimeSeriesReg}, we are working with the wide data format to investigate model features in linear regression. Note that we also add a column with a decade identifier. Its use will become apparent in the Section @ref(Chap9FeatureEngineering) below.\n\n# Final concatenation\ndata_wide_tbl &lt;- right_join(q_dec_filled_wide_tbl,p_dec_filled_wide_tbl,by='date')\ndata_wide_tbl &lt;- right_join(data_wide_tbl,t_dec_filled_wide_tbl,by='date')\n# Add period identifiers (decades in this case)\ns &lt;- data_wide_tbl$date |&gt; first()\ne &lt;- data_wide_tbl$date |&gt; last()\ndecs &lt;- decadeMaker(s,e,'end')\ndecs &lt;- decs |&gt; rename(per=dec)\ndata_wide_tbl &lt;- data_wide_tbl |&gt; left_join(decs,by='date')\n# Creating long form\ndata_long_tbl &lt;- data_wide_tbl |&gt; \n  pivot_longer(-date)\n# Cross checking completeness of record\ndata_long_tbl |&gt; \n  group_by(name) |&gt; \n  summarize(n.na = sum(is.na(value)), n.na.perc = n.na/n()*100) |&gt; \n  knitr::kable()\n\n\n\nname\nn.na\nn.na.perc\n\n\n\nP38462\n0\n0\n\n\nP38464\n0\n0\n\n\nP38471\n0\n0\n\n\nQ16279\n0\n0\n\n\nQ16290\n0\n0\n\n\nQ16924\n0\n0\n\n\nT38462\n0\n0\n\n\nT38471\n0\n0\n\n\nper\n0\n0\n\n\n\n\n\nA consistent data record from 1933 until and including November 2015 is now prepared^Please note that by using left_join above, we have cut off discharge data from the year 1932 since we do not have meteorological data there.^. Let us analyze these data now.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Modeling Using Predictive Inference</span>"
    ]
  },
  {
    "objectID": "modeling_using_predictive_inference.html#sec-em-data-analysis",
    "href": "modeling_using_predictive_inference.html#sec-em-data-analysis",
    "title": "10  Modeling Using Predictive Inference",
    "section": "\n10.5 Data Analysis",
    "text": "10.5 Data Analysis\nIn this Section, the goal is to explore and understand the available time series data and their relationships and to take the necessary steps towards feature engineering. Features are predictors that we want to include in our forecasting models that are powerful in the sense that they help to improve the quality of forecasts in a significant manner. Sometimes, the modeler also wants to include synthetic features, i.e. predictors that are not observed but for example derived from observations.\nDifferent techniques are demonstrated that allow us to get familiar with the data that we are using. While we are interested to model discharge of Chatkal and Pskem rivers, it should be emphasized that all the techniques utilized for forecasting easily carry over to other rivers and settings.\nLet us start with a visualisation of the complete data record. Using timetk::plot_time_series and groups, we can plot all data into separate, individual facets as shown in Figure @ref(fig:completeDataRecord).\n\ndata_long_tbl |&gt; \n  group_by(name) |&gt; \n  plot_time_series(date, value,\n                   .smooth      = FALSE,\n                   .interactive = FALSE,\n                   .facet_ncol  = 2,\n                   .title       = \"\"\n                   )\n\n\n\n\n\n\nFigure 10.11: Complete Data hydro-meteorological record for the zone of runoff formation in the Chirchik river basin.\n\n\n\n\n\n10.5.1 Data Transformation\nIt is interesting to observe that discharge values range over 2 - 3 orders of magnitude between minimum and maximum flow regimes. As can be seen in Figure 10.12, discharge and precipitation data are heavily skewed. When this is the case, it is generally advisable to consider data transformations as they help to improve predictive modeling accuracy of regression models.\n\ndata_long_tbl |&gt; \n  group_by(name)  |&gt;\n  ggplot(aes(x=value,colour = name)) +\n  geom_histogram(bins=50) +\n  facet_wrap(~name, scales = \"free\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 10.12: Histograms of available raw data.\n\n\n\n\nLet us for example look at a very simple uniform non-parametric transformation, i.e. a logarithmic transformation (see Figure 10.13). As compared to parametric transformation, the logarithmic transformation is simple to apply for data greater than zero and does not require us to keep track of transformation parameters as, for example, is the case when we center and scale the data.\n\ndata_wide_tbl |&gt; \n  mutate(across(Q16279:P38471,.fns = log1p)) |&gt; # transforms  discharge and precipitation time series\n  pivot_longer(-date) |&gt; \n  ggplot(aes(x=value,colour = name)) +\n  geom_histogram(bins=50) +\n  facet_wrap(~name, scales = \"free\") + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 10.13: Histograms of transformed discharge and precipitation data together with the raw temperature data.\n\n\n\n\nPlease note that with the base-R command log1p, 1 is added prior to the logarithmic transformation to avoid cases where the transformed values would not be defined, i.e. where discharge or precipitation is 0. More information about the log1p() function can be obtained by simply typing ?log1p. Recovering the original data after the log1p transformation is simply achieved by taking the exponent of the transformed data and subtracting 1 from the result. The corresponding R function is expm1().\nClearly, the log-transformed discharge values are no longer skewed (Figure @ref(fig:histogramsData_transformed)). We now see interesting bimodal distributions. At the same time, the variance of the transformed variables is greatly reduced. These are two properties that will help us construct a good model as we shall see below. Finally, the transformed discharge time series are shown in Figure .\n\ndata_long_tbl |&gt; \n  filter(name=='Q16279' | name=='Q16290') |&gt; \n  plot_time_series(date, log(value+1),\n                   .facet_vars  = name,\n                   .smooth      = FALSE,\n                   .interactive = FALSE,\n                   .title       = \"\",\n                   .y_lab       = \"[-]\",\n                   .x_lab       = \"year\"\n                   )\n\n\n\n\n\n\nFigure 10.14: log1p() transformed discharge data.\n\n\n\n\n\n10.5.2 Detecting Trends\nLower frequency variability in time series, including trends, can be visualized by using the .smooth = TRUE option in the plot_time_series() function. To demonstrate this here, we have a closer look at the temperature data in our data record as shown in Figure 10.15.\n\ndata_long_tbl |&gt; \n  filter(name == 'T38462' | name == 'T38471') |&gt; \n  plot_time_series(date, value, \n                   .smooth     = TRUE,\n                   .facet_vars = name,\n                   .title      = \"\",\n                   .y_lab      = \"deg. C.\",\n                   .x_lab      = \"year\"\n                   )\n\n\n\n\n\n\nFigure 10.15: Temperature time series and trends.\n\n\n\nIn both time series, a slight upward trend is visible that picks up over the most recent decades. We can look at these trends in greater detail, for example at monthly levels as shown in Figure 10.16.\n\ndata_long_tbl |&gt; \n  filter(name == 'T38462') |&gt; \n  summarise_by_time(.date_var = date, .by=\"month\",value=mean(value)) |&gt; \n  tk_ts(frequency = 12) |&gt; \n  forecast::ggsubseriesplot(year.labels = FALSE) + \n              geom_smooth(method = \"lm\",color=\"red\") +\n              #ggtitle('Development of Monthly Mean Temperatures from 1933 - 2015 at Station 38462') +\n              xlab('month') +\n              ylab('Degrees Celsius')\n\n\n\n\n\n\nFigure 10.16: Sample development of Monthly Mean Temperatures from 1933 - 2015 at Station 38462.\n\n\n\n\nIn the Figure above, a significant winter warming over the period of data availability is confirmed at Pskem meteorological station. As discussed in the background Chapters on the Central Asia Hydrological Systems and the Example River Basins, these trends are observed throughout the Central Asian region and are an indication of the changing climate there. We will have to take into account such type of trends in our modeling approach.\n\n10.5.3 Auto- and Cross-Correlations\nA time series may have relationships to previous versions of itself - these are the ‘lags’. The autocorrelation is a measure of the strength of this relationship of a series to its lags. The autocorrelation function ACF looks at all possible correlations between observation at different times and how they emerge. Contrary to that, the partial autocorrelation function PACF only looks at the correlation between a particular past observation and the current one. So in other words, ACF includes direct and indirect effects whereas PACF only includes the direct effects between observations at time t and the lag. As we shall see below, PACF is super powerful to identify relevant lagged time series predictors in autoregressive models (AR Models).\nFigure 10.17 shows the ACF and PACF over the interval of 72 lags (2 years). The AC function shows the highly seasonal characteristics of the underlying time series. It also shows the pronounced short-term memory in the basin, i.e. the tendency to observe subsequent values of high flows and subsequent values of low flow - hence the smoothness of the curve. This time of autocorrelation behavior is typical for basins with large surface storage in the form of lakes, swamps, snow and glaciers, permafrost and groundwater reserves (A. 2019). The Chatkal river basin certainly belongs to that category.\n\ndata_long_tbl |&gt; filter(name == 'Q16279') |&gt; \n  plot_acf_diagnostics(date, value,\n                      .show_white_noise_bars = TRUE,\n                      .lags = 72,\n                      .title = \"\"\n                      )\n\n\n\n\n\n\nFigure 10.17: Autocorrelation function (ACF) and partial autocorrelation function (PACF) are shown for the discharge time series at station 16279.\n\n\n\nBut is there also autocorrelation of the annual time series? Let us test.\n\nQ16279_annual &lt;- data_long_tbl |&gt; filter(name == 'Q16279') |&gt; dplyr::select(-name) |&gt; \n  summarize_by_time(.date_var = date,\n                    .by=\"year\",\n                    sum=sum(value)*3600*24*10/10^9) \n\nQ16279_annual |&gt; plot_time_series(date,sum,\n                                   .smooth = FALSE,\n                                   .title = \"\",\n                                   .x_lab = \"year\",\n                                   .y_lab = \"Discharge [cubic km per year]\")\n\n\n\n\n\n\nFigure 10.18: Testing autocorrelation at annual scales for discharge at station 16279.\n\n\nQ16279_annual |&gt; \n  plot_acf_diagnostics(.date_var = date,\n                       .value = sum,\n                       .lags = 50,\n                       .show_white_noise_bars = TRUE,\n                       .title = \"\",\n                       .x_lab = \"year\")\n\n\n\n\n\n\nFigure 10.19: Testing autocorrelation at annual scales for discharge at station 16279.\n\n\n\nThe ?fig-annual-auto-corr shows a fast decaying autocorrelation function for the annualized time series where even lag 1 values are no longer correlated in a significant manner.\nThe PAC function, on the other hand, demonstrates that lag 1 is really critical in terms of direct effects (Figure 10.17). After that, the PACF tapers off quickly. To utilize these findings in our modeling approach that uses lagged regression is important, as we shall see below.\nWe can also study cross-correlations between two different time series. In other words, in cross-correlation analysis between two different time series, we estimate the correlation one variable and another, time-shifted variable. For example, we can cross-correlate discharge at Gauge 16279 (Chatkal river) to discharge at Gauge 16290 (Pskem River) as shown in (fig_cross_corr_q?). As is easily visible, the discharge behavior of the two rivers is highly correlated.\n\ndata_wide_tbl |&gt; plot_acf_diagnostics(date,Q16279,\n                                       .ccf_vars = Q16290,\n                                       .show_ccf_vars_only = TRUE,\n                                       .show_white_noise_bars = TRUE,\n                                       .lags = 72,\n                                       .title = \"\"\n                                       )\n\n\nCross-correlation analysis of the two discharge time series Q16279 and Q16290.\n\n\nConverse to this, discharge shows a lagged response to temperature which is clearly visible in the cross-correlation function.\n\ndata_wide_tbl |&gt; plot_acf_diagnostics(date,T38471,\n                                       .ccf_vars = Q16279,\n                                       .show_ccf_vars_only = TRUE,\n                                       .show_white_noise_bars = TRUE,\n                                       .lags = 72,\n                                       .title = \"\"\n                                       )\n\n\n\n\n\n\nFigure 10.20: Cross-correlation between temperature at station 38471 and the discharge at station 16279.\n\n\n\nA less pronounced cross-correlation exists between precipitation and discharge when measured at the same stations (Figure @ref(ccf_PQ)).\n\ndata_wide_tbl |&gt; plot_acf_diagnostics(date,P38471,\n                                       .ccf_vars = Q16279,\n                                       .show_ccf_vars_only = TRUE,\n                                       .show_white_noise_bars = TRUE,\n                                       .lags = 72,\n                                       .title = \"\"\n                                       )\n\n\n\n\n\n\nFigure 10.21: Cross-correlation between temperature at station 38471 and the discharge at station 16279.\n\n\n\n\n10.5.4 Time Series Seasonality\nThere is a pronounced seasonality in the discharge characteristics of Central Asian rivers. One of the key reason of this is the annual melt process of the winter snow pack. Figure 10.22 shows the seasonality of the log-transformed discharge. These observations can help in investigating and detecting time-based (calendar) features that have cyclic or trend effects.\n\ndata_long_tbl |&gt; \n  filter(name==\"Q16279\" | name==\"Q16290\") |&gt; \n  plot_seasonal_diagnostics(date,\n                            log(value+1),\n                            .facet_vars = name, \n                            .feature_set = c(\"week\",\"month.lbl\",\"year\"),\n                            .interactive = FALSE,\n                            .title = \"\"\n                            )\n\n\n\n\n\n\nFigure 10.22: Seasonal diagnostics of log1p discharge. Weekly (top row), monthly (middle row) and yearly diagnostics (bottom row) are shown for the two discharge time series.\n\n\n\n\nFigure 10.23 shows the seasonal diagnostic for the log-transformed precipitation time series. The significant interannual variability is visible. In the annualized time series, no trend is available.\n\ndata_long_tbl |&gt; \n  filter(name==\"P38462\" | name==\"P38464\" | name==\"P38471\") |&gt; \n  plot_seasonal_diagnostics(date,\n                            log1p(value),\n                            .facet_vars = name, \n                            .feature_set = c(\"week\",\"month.lbl\",\"year\"),\n                            .interactive = FALSE,\n                            .title = \"\"\n                            )\n\n\n\n\n\n\nFigure 10.23: Seasonal Diagnostics of precipitation Weekly (top row), monthly (middle row) and yearly diagnostics (bottom row) are shown for the available precipitation data in the zone of runoff formation of the two tributary rivers.\n\n\n\n\nFinally, Figure 10.24 displays the seasonal diagnostics of the temperature time series. Notice that we use untransformed, raw values here for plotting.\n\ndata_long_tbl |&gt; \n  filter(name==\"T38462\" | name==\"T38471\") |&gt; \n  plot_seasonal_diagnostics(date,\n                            value,\n                            .facet_vars = name, \n                            .feature_set = c(\"week\",\"month.lbl\",\"year\"),\n                            .interactive = FALSE,\n                            .title = \"\",\n                            )\n\n\n\n\n\n\nFigure 10.24: Seasonal Diagnostics of temperature Weekly (top row), monthly (middle row) and yearly diagnostics (bottom row) are shown for the available temperature data in the zone of runoff formation of the two tributary rivers.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Modeling Using Predictive Inference</span>"
    ]
  },
  {
    "objectID": "modeling_using_predictive_inference.html#sec-em-feature-engineering",
    "href": "modeling_using_predictive_inference.html#sec-em-feature-engineering",
    "title": "10  Modeling Using Predictive Inference",
    "section": "\n10.6 Investigating and Engineering Predictors",
    "text": "10.6 Investigating and Engineering Predictors\nAll the data that we have available have been analyzed by now and we can now move to generating a good and solid understanding of the relevance of predictors for statistical modeling. To start with, we will keep things deliberately simple. Our approach is tailored to the particular local circumstances and the needs and wants of the hydrometeorological agencies that are using such types of model to issue high quality forecasts.\nFirst, the plan here to start with the introduction and discussion of the current forecasting techniques that are used operationally inside the Kyrgyz Hydrometeorological agency. These models and their forecast quality will serve as benchmark to beat any of the other models introduced here. At the same time, we will introduce a measure with which to judge forecast quality.\nSecondly, we evaluate the simplest linear models using time series regression. This will also help to introduce and explain key concepts that will be discussed in the third and final section below.\nFinally, we show the application of more advanced forecasting modeling techniques that use state-of-the-art regression type algorithms.\nThe forecasting techniques will be demonstrated by focussing on the Pskem river. The techniques extend to other rivers in the region and beyond in a straight forward manner.\n\n10.6.1 Benchmark: Current Operational Forecasting Models in the Hydrometeorological Agencies\nMore information to come. Check back soon here.\n\n10.6.2 Time Series Regression Models\nThe simplest linear regression model can be written as\n\\[\ny_{t} = \\beta_{0} + \\beta_{1} x_{t} + \\epsilon_{t}\n\\]\nwhere the coefficient \\(\\beta_{0}\\) is the intercept term, \\(\\beta_{1}\\) is the slope and \\(\\epsilon_{t}\\) is the error term. The subscripts \\(t\\) denote the time dependency of the target and the explanatory variables and the error. \\(y_{t}\\) is our target variable, i.e. discharge in our case, that we want to forecast. At the same time, \\(x_{t}\\) is an explanatory variable that is already observed at time \\(t\\) and that we can use for prediction.\nAs we shall see below, we are not limited to the inclusion of only one explanatory variable but can think of adding multiple variables that we suspect to help improve forecast modeling performance.\nTo demonstrate the effects of different explanatory variables on our forcasting target and the quality of our model for forecasting discharge at stations 16290, the function plot_time_series_regression from the timetk package is used. First, we we only want to specify a model with a trend over the time \\(t\\). Hence, we fit the model\n\\[\ny_{t} = \\beta_{0} + \\beta_{1} t + \\epsilon_{t}\n\\]\n\nmodel_formula &lt;- as.formula(log1p(Q16290) ~ \n                              as.numeric(date)\n                            )\n\nmodel_data &lt;- data_wide_tbl |&gt; dplyr::select(date,Q16290)\n\nmodel_data |&gt; \n  plot_time_series_regression(\n    .date_var = date,\n    .formula = model_formula,\n    .show_summary = TRUE,\n    .title = \"\"\n  )\n\n\nCall:\nstats::lm(formula = .formula, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3940 -0.7068 -0.2114  0.7193  2.0274 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       4.056e+00  1.489e-02  272.39   &lt;2e-16 ***\nas.numeric(date) -2.997e-06  1.675e-06   -1.79   0.0736 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7998 on 2983 degrees of freedom\nMultiple R-squared:  0.001073,  Adjusted R-squared:  0.0007378 \nF-statistic: 3.203 on 1 and 2983 DF,  p-value: 0.07359\n\n\n\n\n\n\n\nFigure 10.25: Linear regression trend model.\n\n\n\nNote that the timetk::plot_time_series function is a convenience wrapper to make our lives easy in terms of modeling and immediately getting a resulting plot. The same model could be specified in the traditional R-way, i.e. as follows\n\nmodel_data |&gt; \n  lm(formula = model_formula) |&gt; \n  summary()\n\n\nCall:\nlm(formula = model_formula, data = model_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3940 -0.7068 -0.2114  0.7193  2.0274 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       4.056e+00  1.489e-02  272.39   &lt;2e-16 ***\nas.numeric(date) -2.997e-06  1.675e-06   -1.79   0.0736 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7998 on 2983 degrees of freedom\nMultiple R-squared:  0.001073,  Adjusted R-squared:  0.0007378 \nF-statistic: 3.203 on 1 and 2983 DF,  p-value: 0.07359\n\n\nThe adjusted R-squared shows the mediocre performance of our simple model as it cannot capture any of the seasonal variability. Furthermore we see that the trend coefficient is negative which indicates a decrease in mean discharge. However, as the p-value confirms, the trend is only significant at the 0.1 level.\nThe first step in improving our model is to account for seasonality. In the case of decadal time series, we can add categorical variables (as factor variables) decoding the corresponding decades. Similarly, in the case of monthly data, we could use month names or factors 1..12 to achieve the same. The same reasoning extends to other periods (quarters, weekdays, etc.). We will use a quarterly model to explain the concept since the inclusion of 4 indicator variables for the individual quarters is easier to grasp than to work with 36 decadal indicators.\n\n# Computing quarterly mean discharge values\nq16290_quarter_tbl &lt;- model_data |&gt; \n  summarize_by_time(date,value=mean(log1p(Q16290)),.by = \"quarter\")\n# adding quarters identifier\nq16290_quarter_tbl &lt;- q16290_quarter_tbl |&gt; \n  mutate(per = quarter(date) |&gt; as.factor())\n\nmodel_formula &lt;- as.formula(value ~ \n                              as.numeric(date) + \n                              per\n                            )\nq16290_quarter_tbl |&gt; \n  plot_time_series_regression(date,\n                              .formula = model_formula,\n                              .show_summary = TRUE,\n                              .title = \"\"\n                              )\n\n\nCall:\nstats::lm(formula = .formula, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.51477 -0.14936  0.00228  0.14410  0.66161 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3.255e+00  2.204e-02 147.691  &lt; 2e-16 ***\nas.numeric(date) -3.158e-06  1.255e-06  -2.516   0.0123 *  \nper2              1.551e+00  3.106e-02  49.919  &lt; 2e-16 ***\nper3              1.396e+00  3.107e-02  44.938  &lt; 2e-16 ***\nper4              2.562e-01  3.107e-02   8.248 3.98e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2001 on 327 degrees of freedom\nMultiple R-squared:  0.9217,    Adjusted R-squared:  0.9207 \nF-statistic: 962.4 on 4 and 327 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\nFigure 10.26: Example quarterly linear model with trend and seasonality.\n\n\n\nWhat we did here is to compare a continuous variable, i.e. the discharge, across 4 categories. Hence, we can write down the model in the following way:\n\\[\ny_{t} = \\beta_{0} + \\beta_{1} \\delta_{t}^{Qtr2} + \\beta_{2} \\delta_{t}^{Qtr3} + \\beta_{3} \\delta_{t}^{Qtr4} + \\epsilon_{t}\n\\]\nUsing ‘one hot encoding’, we include only N-1 (here, 3) variables out of the N (here,4) in the regression because we can safely assume that if we are in Quarter 4, all the other indicator variables are simply 0. If we are in quarter 1 (Qtr1), the model would just be\n\\[\ny_{t} = \\beta_{0} + \\epsilon_{t}\n\\]\nIf we are in Quarter 2 (Qtr2), the model would be\n\\[\ny_{t} = \\beta_{0} + \\beta_{1} + \\epsilon_{t}\n\\]\nsince \\(\\delta_{t}^{Qtr2} = 1\\). Hence, whereas \\(\\beta_{0}\\) is to be interpreted as the estimated mean discharge in Quarter 1 (called (Intercept) in the results table below), \\(\\beta_{1}\\) (called qtr2 in the results table below) is the estimated difference of mean discharge between the two categories/quarters We can get the values and confidence intervals of the estimates easily in the following way\n\nlm_quarterlyModel &lt;- q16290_quarter_tbl |&gt; \n  lm(formula = model_formula)\n\nmeanQtrEstimates &lt;- lm_quarterlyModel |&gt; coefficients()\nmeanQtrEstimates |&gt; expm1()\n\n     (Intercept) as.numeric(date)             per2             per3 \n    2.493125e+01    -3.158103e-06     3.714894e+00     3.039112e+00 \n            per4 \n    2.920533e-01 \n\nlm_quarterlyModel |&gt; confint() |&gt; expm1()\n\n                         2.5 %        97.5 %\n(Intercept)       2.383084e+01  2.608043e+01\nas.numeric(date) -5.627148e-06 -6.890525e-07\nper2              3.435387e+00  4.012016e+00\nper3              2.799662e+00  3.293653e+00\nper4              2.154539e-01  3.734800e-01\n\n\nThe same reasoning holds true for the model with decadal observations to which we return now again. First, we add decades as factors to our data_wide_tbl.\n\n\n\n\n\n\n\n\n\nNow, we can specify and calculate the new model.\n\nmodel_formula &lt;- as.formula(log1p(Q16290) ~ \n                              as.numeric(date) + # trend components\n                              per # seasonality (as.factor)\n                            )\n\nmodel_data &lt;- data_wide_tbl |&gt; dplyr::select(date,Q16290,per)\n\nmodel_data |&gt; \n  plot_time_series_regression(\n    .date_var = date,\n    .formula = model_formula,\n    .show_summary = TRUE,\n    .title = \"\"\n  )\n\n\nCall:\nstats::lm(formula = .formula, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3225 -0.6972 -0.2318  0.7268  2.0364 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3.943e+00  2.991e-02 131.827  &lt; 2e-16 ***\nas.numeric(date) -3.065e-06  1.670e-06  -1.836   0.0665 .  \nper               6.150e-03  1.406e-03   4.374 1.26e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7974 on 2982 degrees of freedom\nMultiple R-squared:  0.00744,   Adjusted R-squared:  0.006774 \nF-statistic: 11.18 on 2 and 2982 DF,  p-value: 1.46e-05\n\n\n\n\n\n\n\nFigure 10.27: Decadal linear regression model with trend and seasonality.\n\n\n\nWhat we see is that through the inclusion of the categorical decade variables, we have greatly improved our modeling results since we can now capture the seasonality very well (Tip: zoom into the time series to compare highs and lows and their timing for the target variable and its forecast). However, despite the excellent adjusted R-squared value of 0.9117, our model is far from perfect as it is not able to account for inter-annual variability in any way.\nLet us quickly glance at the errors.\n\n#} fig-cap: \"Scatterplot of observed versus calculated values.\"\nlm_decadalModel &lt;- model_data |&gt; \n  lm(formula = model_formula)\n\nobs_pred_wide_tbl &lt;- model_data|&gt; \n  mutate(pred_Q16290 = predict(lm_decadalModel) |&gt; expm1()) |&gt; \n  mutate(error = Q16290 - pred_Q16290)\n\nggplot(obs_pred_wide_tbl, aes(x      = Q16290,\n                           y         = pred_Q16290,\n                           colour    = per )) +\n  geom_point() + \n  geom_abline(intercept = 0, slope = 1)\n\n\n\n\n\n\nFigure 10.28\n\n\n\n\nWe do not seem to make a systematic error as also confirmed by inspecting the histogram or errors (they are nicely centered around 0).\n\nggplot(obs_pred_wide_tbl,aes(x=error)) +\n  geom_histogram(bins=100)\n\n\n\n\n\n\n\nIn Section 10.5.3 above, we saw that the PAC function is very high at lag 1. We exploit this fact be incorporating in the regression equation the observed previous discharge, i.e. \\(y_{t-1}\\) at time \\(t-1\\) to predict discharge at time \\(t\\). Hence, our regression can be written as\n\\[\ny_{t} = \\beta_{0} + \\beta_{1} t + \\beta_{2} y_{t-1} + \\sum_{j=2}^{36} \\beta_{j} \\delta_{t}^{j} + \\epsilon_{t}\n\\]\nwhere the \\(\\delta_t^{j}\\) correspondingly are the 35 indicator variables as discussed above in the case of quarterly time series where we had 3 of these variables included. Before we can estimate this model, we prepare a tibble with the relevant data as shown in the table below (note that we simply renamed the discharge column to Q out of convenience).\n\nmodel_data &lt;- data_wide_tbl |&gt; \n  dplyr::select(date,Q16290,per) |&gt; \n  rename(Q=Q16290) |&gt; \n  mutate(Q = log1p(Q)) |&gt; \n  mutate(Q_lag1 = lag(Q,1))\nmodel_data\n\n# A tibble: 2,985 × 4\n   date           Q   per Q_lag1\n   &lt;date&gt;     &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n 1 1933-01-10  3.37     1  NA   \n 2 1933-01-20  3.23     2   3.37\n 3 1933-01-31  3.17     3   3.23\n 4 1933-02-10  3.21     4   3.17\n 5 1933-02-20  3.26     5   3.21\n 6 1933-02-28  3.28     6   3.26\n 7 1933-03-10  3.30     7   3.28\n 8 1933-03-20  3.53     8   3.30\n 9 1933-03-31  3.57     9   3.53\n10 1933-04-10  3.92    10   3.57\n# ℹ 2,975 more rows\n\n\nNotice that to accommodate the \\(y_{t-1}\\) in the data, we simply add a column that contains a lagged version of the discharge time series itself (see column Q_lag1). Now, for example, for our regression we have a first complete set of data points on \\(t = '1933-01-20'\\), with \\(Q=3.226844\\), \\(dec=2\\) and \\(Q_{lag1}=3.374169\\). Notice how the last value corresponds to the previously observed and now known \\(y_{t-1}\\).\n\n# Specification of the model formula\nmodel_formula &lt;- as.formula(Q ~ as.numeric(date) + per + Q_lag1)\n# Note that we use na.omit() to delete incomplete data records, ie. the first observation where we lack the lagged value of the discharge. \nmodel_data |&gt;  na.omit() |&gt; lm(formula = model_formula) |&gt; summary()\n\n\nCall:\nlm(formula = model_formula, data = na.omit(model_data))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09294 -0.12965 -0.03140  0.07656  1.17980 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       1.955e-01  1.801e-02  10.850   &lt;2e-16 ***\nas.numeric(date) -6.402e-08  3.925e-07  -0.163     0.87    \nper              -7.010e-03  3.355e-04 -20.897   &lt;2e-16 ***\nQ_lag1            9.838e-01  4.353e-03 225.992   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1873 on 2980 degrees of freedom\nMultiple R-squared:  0.9453,    Adjusted R-squared:  0.9452 \nF-statistic: 1.716e+04 on 3 and 2980 DF,  p-value: &lt; 2.2e-16\n\n\nIt looks like we have made a decisive step in the right direction by incorporating the previously observed discharge value. Also, notice that some of the decade factors have lost their statistical significance meaning that the seasonality can now be captured in part also by the lagged version of the time series.\nLet us visualize the results quickly (tip: also zoom in to explore the fit).\n\nmodel_data |&gt; \n  na.omit() |&gt; \n  plot_time_series_regression(\n  .date_var         = date,\n  .formula          = model_formula,\n  .show_summary     = FALSE, # We do show the summary since we have plotted the summary output already above.\n  .title            = \"\"\n)\n\n\n\n\n\n\nFigure 10.29: Linear regression model results with trend, seasonality and lag 1 predictors.\n\n\n\nThis is clearly an astonishing result. Nevertheless, we should keep a couple of things in mind:\n\nWhat about the rate of change of the discharge and the acceleration of discharge? Would the incorporation of these features help to improve the model?\nWe have not assess the quality of the forecasts using the stringent quality criteria as they exist in the Central Asian Hydrometeorological Services. How does our forecast perform under this criteria?\nDoes the incorporation of precipitation and temperature data help to improve our forecast skills?\nWe did not test our model on out-of-sample data. Maybe our model does not generalize well? We will discuss these and related issues soon when using more advanced models but for the time being declare this a benchmark model due to its simplicity and predictive power.\n\nWe will work on these questions now and focus first on the incorporation of the rate of change in discharge and the acceleration of discharge over time. First, we add \\(Q_{lag2}\\) to our model data and then compute change and acceleration accordingly.\n\nmodel_data &lt;- model_data |&gt; \n  mutate(Q_lag2 = lag(Q,2)) |&gt; \n  mutate(change = Q_lag1 -Q_lag2) |&gt; # that is the speed of change in discharge\n  mutate(change_lag1 = lag(change,1)) |&gt; \n  mutate(acc = change - change_lag1) |&gt; na.omit() # and that is the acceleration of discharge\nmodel_data\n\n# A tibble: 2,982 × 8\n   date           Q   per Q_lag1 Q_lag2  change change_lag1      acc\n   &lt;date&gt;     &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1 1933-02-10  3.21     4   3.17   3.23 -0.0614     -0.147   0.0860 \n 2 1933-02-20  3.26     5   3.21   3.17  0.0413     -0.0614  0.103  \n 3 1933-02-28  3.28     6   3.26   3.21  0.0551      0.0413  0.0138 \n 4 1933-03-10  3.30     7   3.28   3.26  0.0152      0.0551 -0.0399 \n 5 1933-03-20  3.53     8   3.30   3.28  0.0187      0.0152  0.00348\n 6 1933-03-31  3.57     9   3.53   3.30  0.231       0.0187  0.212  \n 7 1933-04-10  3.92    10   3.57   3.53  0.0432      0.231  -0.187  \n 8 1933-04-20  4.05    11   3.92   3.57  0.348       0.0432  0.305  \n 9 1933-04-30  4.47    12   4.05   3.92  0.132       0.348  -0.216  \n10 1933-05-10  4.88    13   4.47   4.05  0.416       0.132   0.284  \n# ℹ 2,972 more rows\n\n# Specification of the model formula\nmodel_formula &lt;- as.formula(Q ~ as.numeric(date) + per + Q_lag1 + change + acc)\nmodel_data |&gt; na.omit() |&gt; \n  plot_time_series_regression(\n  .date_var = date,\n  .formula = model_formula,\n  .show_summary = TRUE, # We do show the summary since we have plotted the summary output already above.\n  .title = \"\"\n)\n\n\nCall:\nstats::lm(formula = .formula, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.01892 -0.09223 -0.02294  0.05501  1.16531 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       2.574e-01  1.626e-02  15.830   &lt;2e-16 ***\nas.numeric(date) -1.834e-07  3.480e-07  -0.527    0.598    \nper              -2.859e-03  3.318e-04  -8.616   &lt;2e-16 ***\nQ_lag1            9.496e-01  4.087e-03 232.331   &lt;2e-16 ***\nchange            5.650e-01  2.002e-02  28.221   &lt;2e-16 ***\nacc              -2.215e-01  1.806e-02 -12.265   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1658 on 2976 degrees of freedom\nMultiple R-squared:  0.9571,    Adjusted R-squared:  0.957 \nF-statistic: 1.328e+04 on 5 and 2976 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nmodel &lt;- model_data |&gt; lm(formula=model_formula)\n\nmodel |&gt; summary()\n\n\nCall:\nlm(formula = model_formula, data = model_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.01892 -0.09223 -0.02294  0.05501  1.16531 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       2.574e-01  1.626e-02  15.830   &lt;2e-16 ***\nas.numeric(date) -1.834e-07  3.480e-07  -0.527    0.598    \nper              -2.859e-03  3.318e-04  -8.616   &lt;2e-16 ***\nQ_lag1            9.496e-01  4.087e-03 232.331   &lt;2e-16 ***\nchange            5.650e-01  2.002e-02  28.221   &lt;2e-16 ***\nacc              -2.215e-01  1.806e-02 -12.265   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1658 on 2976 degrees of freedom\nMultiple R-squared:  0.9571,    Adjusted R-squared:  0.957 \nF-statistic: 1.328e+04 on 5 and 2976 DF,  p-value: &lt; 2.2e-16\n\n# Here, we add the the prediction to our tibble so that we can assess model predictive quality later.\nmodel_fc_wide_tbl &lt;- model_data |&gt; \n  mutate(pred = predict(model)) |&gt; \n  mutate(obs = expm1(Q), pred = expm1(pred)) |&gt; \n  dplyr::select(date,obs,pred,per)\nmodel_fc_wide_tbl\n\n# A tibble: 2,982 × 4\n   date         obs  pred   per\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n 1 1933-02-10  23.7  23.6     4\n 2 1933-02-20  25.1  25.9     5\n 3 1933-02-28  25.5  28.0     6\n 4 1933-03-10  26    28.1     7\n 5 1933-03-20  33    28.3     8\n 6 1933-03-31  34.5  38.1     9\n 7 1933-04-10  49.3  38.9    10\n 8 1933-04-20  56.4  58.0    11\n 9 1933-04-30  86    65.3    12\n10 1933-05-10 130   102.     13\n# ℹ 2,972 more rows\n\n\nAnother, albeit small improvement in the forecast of predicting discharge 1-step ahead! It is now time to properly gauge the quality of this seemingly excellent model. Does it conform to local quality standards that apply to decadal forecasts? The Figure 10.30 shows the un-transformed data. We see that we are not doing so well during the summer peak flows. As we shall see further below, these are the notoriously hard to predict values, even just for 1-step ahead decadal predictions.\n\nmodel_fc_wide_tbl |&gt; \n  dplyr::select(-per) |&gt; \n  pivot_longer(-date) |&gt; \n  plot_time_series(date,\n                   value,\n                   name,\n                   .smooth = F,\n                   .title = \"\")\n\n\n\n\n\n\nFigure 10.30: Forecast model quality assessment.\n\n\n\n\n10.6.3 Assessing the Quality of Forecasts\nHow well are we doing with our simple linear model? Let us assess the model quality using the local practices. For the Central Asian Hydromets, a forecast at a particular decade \\(d\\) is considered to be excellent if the following holds true\n\\[\n|Q_{obs}(d,y) - Q_{pred}(d,y)| \\le 0.674 \\cdot \\sigma[\\Delta Q(d)]\n\\]\nwhere \\(Q_{obs}(d,y)\\) is the observed discharge at decade \\(d\\) and year \\(d\\), \\(Q_{pred}(d,y)\\) is the predicted discharge at decade \\(d\\) and year \\(y\\), \\(|Q_{obs}(d) - Q_{pred}(d)|\\) thus the absolute error and \\(\\sigma[\\Delta Q(d)] = \\sigma[Q(d) - Q(d-1)]\\) is the standard deviation of the difference of decadal observations at decade \\(d\\) and \\(d-1\\) over the entire observation record (hence, the year indicator \\(y\\) is omitted there). The equation above can be reformulated to\n\\[\n\\frac{|Q_{obs}(d,y) - Q_{pred}(d,y)|}{\\sigma[\\Delta Q(d)]} \\le 0.674\n\\]\nSo let us assess the forecast performance over the entire record using the `riversCentralAsia::assess_fc_qual`` function. Note that the function returns a list of three objects. First, it returns a tibble of the number of forecasts that are of acceptable quality for the corresponding period (i.e. decade or month) as a percentage of the total number of observations that are available for that particular period. Second, it returns the period-averaged mean and third a figure that shows forecast quality in two panels.\nSo, for our model which we consiedered to be performing well above, we get the following performance specs\n\nplot01 &lt;- TRUE \nte &lt;- assess_fc_qual(model_fc_wide_tbl,plot01)\nte[[3]]\n\n\n\n\n\n\nFigure 10.31: Benchmark model performance assessment.\n\n\n\n\nIn other words, roughly two thirds of our in-sample forecasts comply will be considered good enough when measured according to the quality criterion. Furthermore, the model performs worse than average during the second quarter (Q2) decades, i.e. from decade 10 through 17. This is an indication that providing good forecasts in Q2 might be hard.\nIt is, however, generally not considered to be good practice to assess model quality on in-sample data. Rather, model performance should be assessed on out-of-sample data that was not used for model training and is thus data that is entirely unseen.\n\n10.6.4 Generating and Assessing Out-of-Sample Forecasts\n\n\n\n\n\n\nWarning\n\n\n\nNote, this Section is work in progress. Please check back later!\n\n\nWe start off with our model_data tibble and want to divide it into two sets, one for model training and one for model testing. We refer to these sets as training set and test set. For the creation of these sets, we can use the timetk::time_series_split() function.\n\n10.6.5 Machine Learning Models\n\n\n\n\n\n\nWarning\n\n\n\nNote, this Section is work in progress. Please check back later!",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Modeling Using Predictive Inference</span>"
    ]
  },
  {
    "objectID": "glacier_modeling.html",
    "href": "glacier_modeling.html",
    "title": "11  Modeling of Discharge from Glacier Melt",
    "section": "",
    "text": "11.1 Glacier Mass Balance\nWe shift the focus from the catchment to one single glacier within the catchment. Instead of calculating the water balance over the river catchment, we calculate the balance over the glacier.\nThe glacier mass balance is given in Equation 11.1.\n\\[\n\\Delta S = \\text{ablation} + \\text{accumulation}\n\\tag{11.1}\\] where \\(\\text{ablation}\\) denotes the ablation of glacier mass, i.e. glacier mass loss, and \\(\\text{accumulation}\\) is the accumulation of glacier mass, i.e. glacier mass growth. Glacier ablation can be modeled using a temperature index model (described in the next section). Note that the ablation term is a negative number.\nMany processes contribute to glacier ablation and accumulation, e.g. Benn and Evans (2010). The most dominant accumulation processes in high mountain areas are snow fall, avalanches and wind-blown snow. In high mountain regions, glacier ablation is driven by the energy balance at the glacier-air interface (Hock 2005). For most regional hydrological modeling tasks, is is sufficient to simplify the glacier mass balance to Equation 11.2. A glacier that is in balance will melt the equivalent of the long-term average precipitation during the warm summer months, called balance ablation (Pritchard 2019).\n\\[\n\\Delta S = P-\\text{Melt} \\approx Q_{\\text{Gl,imbal}}\n\\tag{11.2}\\]\nwhere the change of water storage (\\(\\Delta S\\)) is equal to the precipitation (\\(P\\)) minus the glacier melt (\\(\\text{Melt}\\)). Since temperatures are increasing globally melt typically exceeds precipitation and we have negative \\(\\Delta S\\), that is imbalance ablation \\(Q_{\\text{Gl,imbal}}\\), indicating glacier storage loss. The glacier mass balance is calculated in annual time steps. We thereby refer to the hydrological year starting on October 1st of the previous year to take a full accumulation and ablation season into account.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling of Discharge from Glacier Melt</span>"
    ]
  },
  {
    "objectID": "glacier_modeling.html#glacier-mass-balance",
    "href": "glacier_modeling.html#glacier-mass-balance",
    "title": "11  Modeling of Discharge from Glacier Melt",
    "section": "",
    "text": "Figure 11.1: Simplified illustration of a glacierized river basin with elevation bands in dashed lines. The zoom is on one of the larger glaciers which can itself be discretized into elevation bands. The glacier mass loss is substracted from the lower most elevation band in increasing elevation.\n\n\n\n\n\n\n\n11.1.1 Temperature Index Model\nOne of the arguably simplest models to describe glacier melt (or snow melt) is the temperature index model. Hock (2003) describes several variants of the temperature index model for simulating glacier melt. The riversCentralAsia package implements the simplest temperature index melt model described in (Hock 2003) in the function glacierMelt_TI (Equation 11.3), requiring only temperature time series and 2 parameters as input.\n\\[\nM = \\biggl\\{ \\begin{array}{l, l}\n0, & T &lt; T_{threshold} \\\\\nf_{M} \\cdot \\left( T - T_{threshold} \\right), & T &gt;= T_{threshold}\n\\end{array}\n\\tag{11.3}\\]\nwhere \\(M\\) is the glacier melt in \\(mm/d\\), \\(T\\) is the daily average temperature in \\(^{\\circ} C\\). The two parameters \\(f_{M}\\) and \\(T_{\\text{threshold}}\\) refer to the melt factor and the threshold temperature above which glacier melt occurs and need to be calibrated. They have the units \\(\\frac{mm}{^{\\circ} C \\cdot d}\\) and \\(^{\\circ} C\\) respectively. Glacier melt is calculated in daily time steps.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling of Discharge from Glacier Melt</span>"
    ]
  },
  {
    "objectID": "glacier_modeling.html#glacier-volume-development",
    "href": "glacier_modeling.html#glacier-volume-development",
    "title": "11  Modeling of Discharge from Glacier Melt",
    "section": "\n11.2 Glacier Volume Development",
    "text": "11.2 Glacier Volume Development\nAs glaciers melt, their volume changes. This has to be taken into account for the long-term simulation of glacier discharge. To determine the initial glacier volume, the area of the geometry of the Randolph Glacier Inventory (RGI) v6.0 data set is multiplied with the average thickness of the glacier (the Farinotti data set). From the combination of these two data sets, the well established area-volume relationship by Erasov (1968) can be verified (see section on Area-Volume scaling). Please note that the data and the retrieval of the data are described in Part II of this book.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling of Discharge from Glacier Melt</span>"
    ]
  },
  {
    "objectID": "glacier_modeling.html#glacier-area-volume-scaling",
    "href": "glacier_modeling.html#glacier-area-volume-scaling",
    "title": "11  Modeling of Discharge from Glacier Melt",
    "section": "\n11.3 Area-volume scaling",
    "text": "11.3 Area-volume scaling\nThe best known scaling relationships for Central Asian glaciers is the Erasov scaling function (Equation 11.4).\n\\[\nV_{\\text{Erasov}} = 0.027 \\cdot A^{1.5}\n\\tag{11.4}\\]\nWhere \\(A\\) is the glacier area in \\(km^2\\) and \\(V_{\\text{Erasov}}\\) is the glacier volume in \\(km^3\\).\nThe following code snippet shows how the area volume relationship is re-fitted using the glacier outlines from RGI v6.0 and the glacier thickness data set by Farinotti et al. (2019).\n\n# Loading the necessary libraries\nlibrary(sf, quietly = TRUE)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(raster, quietly = TRUE)\nlibrary(tidyverse, quietly = TRUE)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::extract() masks raster::extract()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ dplyr::select()  masks raster::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate, quietly = TRUE)\nlibrary(gridExtra, quietly = TRUE)\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(broom, quietly = TRUE)\ndevtools::install_github(\"hydrosolutions/riversCentralAsia\", quiet = TRUE)\nlibrary(riversCentralAsia, quietly = TRUE)\n\n\n# Load the RGIv6.0 data set of the RGI region of Central Asia\nrgi &lt;- st_read(\"../caham_data/central_asia_domain/glaciers/RGIv60/13_rgi60_CentralAsia.shp\", \n               quiet = TRUE) |&gt; \n  sf::st_make_valid() \n\nrgi &lt;- rgi |&gt; \n  dplyr::select(RGIId) |&gt; \n  mutate(Area_m2 = as.numeric(st_area(rgi)))\n\n\n#  To extract the data for the entire Central Asian \n# region takes a several hours on a recent personal computer. \n# Get a list of the glacier thickness maps from Farinotti and Huss\nglacier_thickness_dir &lt;- \"&lt;local_path_to_adapt&gt;/FARINOTTI_Glacier_Thickness/composite_thickness_RGI60-13/RGI60-13/\"\nfilelist &lt;- list.files(path = glacier_thickness_dir, pattern = \".tif$\", \n                       full.names = TRUE)\n# Filter the glacier thickness file list for the glacier ids in the RGI data set \n# (if a subset is to be analysed). \n#filelist &lt;- filelist[sapply(rgi$RGIId, grep, filelist)]\n\n# Get the mean glacier thickness for each of the glaciers in filelist. \nglacier_thickness &lt;- NULL\nres &lt;- NULL\nfor (glacier in c(1:length(filelist))) {\n  rgiid &lt;- str_extract(filelist[glacier], \"RGI60-13.\\\\d{5}\")\n  rast &lt;- raster(filelist[glacier])\n  temp &lt;- exact_extract(rast, \n                        rgi$geometry[rgi$RGIId == rgiid], \n                        fun = c(\"weighted_mean\", \"weighted_sum\"), \n                        weights = \"area\") |&gt; \n    mutate(RGIId = rgiid)\n  glacier_thickness &lt;- rbind(glacier_thickness, temp)\n}\nglacier_thickness &lt;- glacier_thickness |&gt; \n  rename(Vice_s_m3 = weighted_sum)\n\nsave(glacier_thickness, \n     file = \"../caham_data/central_asia_domain/glaciers/Farinotti/glacier_thickness_extractedRGIreg13.RData\")\n\n\n# Load the average glacier thickness per glacier\nload(\"../caham_data/central_asia_domain/glaciers/Farinotti/glacier_thickness_extractedRGIreg13.RData\")\n\nrgi &lt;- rgi |&gt; \n  left_join(glacier_thickness |&gt; \n              dplyr::select(RGIId, weighted_mean), by = \"RGIId\") |&gt; \n  rename(thickness_m = weighted_mean) |&gt; \n  mutate(Volume_m3 = Area_m2 * thickness_m)\n\n# Prepare the data for curve fitting. Exclude Fedchenko glacier as it is \n# so much larger than all other glaciers in RGI region 13.   \ndata &lt;- rgi |&gt; \n  st_drop_geometry() |&gt;\n  dplyr::select(RGIId, Area_m2, thickness_m, Volume_m3) |&gt; \n  mutate(Area_km2 = Area_m2 * 10^(-6), \n         Volume_km3 = Volume_m3 * 10^(-9)) |&gt; \n  dplyr::filter(Area_km2 &lt; 200) |&gt; \n  drop_na() \n\n# Fit a model of similar shape as Erasovs area-volume relationship. \n# V = 0.027 * A ^1.5\n# Note: We use the inverse of the glacier area to weight the fitting to not \n# favor glaciers with large areas in the fit.   \nnlVAw &lt;- nls(Volume_km3 ~ a * Area_km2^b, \n             data = data, \n             weights = 1/data$Area_km2,  #  \n             start = list(a = 0.05, b = 1.2))\n\n# Add the modeled volume to the \"observed\" volume. \ndata_nlVAw &lt;- data |&gt; \n  mutate(.fitted = augment(nlVAw)$.fitted, \n         Volume_RGIF_km3 = .fitted, \n         Volume_Erasov_km3 = glacierVolume_Erasov(Area_km2))\n\n\n# Plotting the results\na &lt;- ggplot(data_nlVAw) + \n  geom_point(aes(Area_km2, Volume_km3, colour = \"Obs\"), size = 0.4, \n             alpha = 0.4) + \n  geom_point(aes(Area_km2, Volume_Erasov_km3, colour = \"Erasov\"), size = 0.4, \n             alpha = 0.4) + \n  geom_point(aes(Area_km2, Volume_RGIF_km3, colour = \"RGIF\"), \n             alpha = 0.4, size = 0.4) + \n  scale_colour_manual(name = \"Legend\", \n                      values = c(\"Obs\" = \"black\", \"Erasov\" = \"green\", \n                                 \"RGIF\" = \"forestgreen\")) + \n  ylab(expression(\"Volume [\" * km^3 * \"]\")) + \n  xlab(expression(\"Area [\" * km^2 * \"]\")) + \n  theme_bw()\nb &lt;- ggplot(data_nlVAw) + \n  geom_point(aes(Area_km2, Volume_km3, colour = \"Obs\"), size = 0.4, \n             alpha = 0.4) + \n  geom_point(aes(Area_km2, Volume_Erasov_km3, colour = \"Erasov\"), size = 0.4, \n             alpha = 0.4) + \n  geom_point(aes(Area_km2, Volume_RGIF_km3, colour = \"RGIF\"), \n             alpha = 0.4, size = 0.4) + \n  scale_colour_manual(name = \"Legend\", \n                      values = c(\"Obs\" = \"black\", \"Erasov\" = \"green\", \n                                 \"RGIF\" = \"forestgreen\")) + \n  ylab(expression(\"Volume [\" * km^3 * \"]\")) + \n  xlab(expression(\"Area [\" * km^2 * \"]\")) + \n  ylim(c(0, 4)) + \n  xlim(c(0, 20)) + \n  theme_bw()\ngrid.arrange(a, b, nrow = 1)\n\n\n\n\n\n\nFigure 11.2: Glacier volume and glacier area estimated with different methods.\n\n\n\n\nFigure 11.2 shows the glacier volume estimate with different methods vs. the glacier area. The two functions perform similarly for small glaciers, i.e. Erasov is still valid. For large glaciers the two parameter sets differ. However, the uncertainties of the glacier volume estimates are larger than the difference between the two functions so Erasov can still be considered to be valid also for larger glaciers though we use the RGIF fit here which is based on more and newer data.\nBased on the large data set, the empirical glacier area-volume relationship based on the RGI v6.0 glacier outlines and the glacier thickness by Farinotti et al. (2019) may be more suitable for large glaciers in Central Asia.\n\\[\nV_{\\text{RGIF}}= a \\cdot A^{b}\n\\tag{11.5}\\]\nWith \\(A\\) being the glacier area in \\(km^2\\), \\(V_{\\text{RGIF}}\\) beging the glacier volume in \\(km^3\\), and the parameters \\(a=\\) 0.0388 and \\(b=\\) 1.262. The newly fitted scaling relationship is implemented as glacierVolume_RGIF in the package riversCentralAsia, the Erasov scaling relationship is implemented as glacierVolume_Erasov.\nThe area-volume scaling by Erasov and the area-volume scaling with updated parameters (RGIF) can be reversed. Thereby it is possible to update the glacier area as a function of the glacier volume.\n\\[\nA = \\exp \\Bigg( \\frac{\\log V  - \\log a}{b}\\Bigg)\n\\tag{11.6}\\]\nwhere \\(V\\) is the glacier volume in \\(km^3\\) and \\(A\\) is the glacier area in \\(km^2\\). The parameters \\(a\\) and \\(b\\) are the parameters of the Erasov scaling function or the RGIF scaling function. The equations are implemented in the riversCentralAsia package as glacierArea_Erasov and glacierArea_RGIF respectively.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling of Discharge from Glacier Melt</span>"
    ]
  },
  {
    "objectID": "glacier_modeling.html#references",
    "href": "glacier_modeling.html#references",
    "title": "11  Modeling of Discharge from Glacier Melt",
    "section": "\n11.4 References",
    "text": "11.4 References\n\n\n\n\nBenn, Douglas I., and David J. A. Evans. 2010. Glaciers & Glaciation. 2nd ed. London: Hodder education.\n\n\nCogley, J. Graham, Regine Hock, B. Rasmussen, A. Arendt, A. Bauder, Roger J. Braithwaite, P. Jansson, et al. 2011. “Glossary of Glacier Mass Balance and Related Terms.” 86. Paris: UNESCO-IHP.\n\n\nErasov, N. V. 1968. “Method for Determining of Volume of Mountain Glaciers.” MGI, no. 14: 307–8.\n\n\nFarinotti, Daniel, Matthias Huss, Johannes J. Fürst, Johannes Landmann, Horst Machguth, Fabien Maussion, and Ankur Pandit. 2019. “A Consensus Estimate for the Ice Thickness Distribution of All Glaciers on Earth.” Nature Geoscience 12 (3): 168–73. https://doi.org/10.1038/s41561-019-0300-3.\n\n\nGarcia Hernandez, J., A. Foehn, J. Fluixa-Sanmartin, B. Roquier, T. Brauchli, J. Paredes Arquiola, and De Cesare G. 2020. “RS MINERVE - Technical Manual, V2.25.” ISSN 2673-2661. Switzerland: Ed. CREALP.\n\n\nHock, Regine. 2003. “Temperature index melt modelling in mountain areas.” Journal of Hydrology 282 (1-4): 104–15. https://doi.org/10.1016/s0022-1694(03)00257-9.\n\n\n———. 2005. “Glacier Melt: A Review of Processes and Their Modelling.” Progress in Physical Geography: Earth and Environment 29 (3): 362–91. https://doi.org/10.1191/0309133305pp453ra.\n\n\nPritchard, Hamish D. 2019. “Asia’s Shrinking Glaciers Protect Large Populations from Drought Stress.” Nature 569 (7758): 649–54. https://doi.org/10.1038/s41586-019-1240-1.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling of Discharge from Glacier Melt</span>"
    ]
  },
  {
    "objectID": "Climate_change_impact_study.html",
    "href": "Climate_change_impact_study.html",
    "title": "12  Quantification of Climate Change Impacts",
    "section": "",
    "text": "12.1 Watershed Delineation\nIn this chapter, we examine the definition of a watershed and how it is delineated using a Geographic Information System (GIS) and a Digital Elevation Model (DEM). We also explore the basics of GIS and the concept of a DEM.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quantification of Climate Change Impacts</span>"
    ]
  },
  {
    "objectID": "Climate_change_impact_study.html#sec-cci-watershed-delineation",
    "href": "Climate_change_impact_study.html#sec-cci-watershed-delineation",
    "title": "12  Quantification of Climate Change Impacts",
    "section": "",
    "text": "12.1.1 Definition of a Watershed\nA watershed, often also referred to as a catchment or drainage basin, is a landscape unit over which the hydrological balance can be determined (measured or computed). It is the area draining to a common point. Watershed delineation is creating a boundary representing the contributing area for a particular point/outlet. The topography is the main driving force behind watershed delineation. To find the watershed boundary, we need to pick a point (outlet) and find the area draining into it. We do this to select properties within the watershed, as well as the climate forcing.\n\n\n\n\n\nFigure 12.1: Example of a watershed. Source: The watershed project\n\n\nIn hydrological modeling, we basically compute the water balance for the watershed. The water balance is the difference between the inputs and outputs of water in the system and the resulting change in storage. The water balance equation is given by:\n\\[\nP + SM - ET - I- Q = 0\n\\tag{12.1}\\]\nWhere \\(P\\) is the precipitation, \\(SM\\) snowmelt, \\(ET\\) evapotranspiration, \\(I\\) infiltration and \\(Q\\) the discharge.\n\n12.1.2 Geographic Information System (GIS)\nWe perform all operations in a Geographic Information System (GIS). A GIS allows us to manage, analyse, edit, produce, and visualise geographic data, also known as spatial data. This is data that includes additional location information. Spatial data comes in two forms: vector and raster data.\nVector points are simply X,Y coordinates and can represent a location like a discharge station, for example. A vector line is a connected sequence of points (e. g. river, street). A vector polygon is a closed set of lines, like a watershed boundary.\n\n\n\n\n\nFigure 12.2: Vector vs. raster data in GIS.\n\n\nRaster data is made up of pixels (also referred to as grid cells). They are usually regularly spaced and square but don’t have to be. Each pixel is associated with a specific geographical location. Examples of raster data are land use and elevation data. The spatial resolution of raster data refers to the cell geometry, how “big” one cell is. Figure 12.3 shows the effect of different spatial resolutions.\n\n\n\n\n\nFigure 12.3: Raster data with different spatial resolution.\n\n\n\n12.1.3 Coordinate Reference System (CRS)\nTo define the location of objects on the Earth you need a coordinate reference system that adapts to the Earth’s shape. There are two different types of a Coordinate Reference Systems (CRS):\n\nA Geographic coordinate systems (GCS) is a system that uses a three-dimensional spherical surface to determine locations on the Earth. Any location on Earth can be referenced by a point with longitude and latitude coordinates. Its units are angular, usually degrees. (Figure 12.4, right)\nA Projected coordinate system (PCS) is flat. It contains a GCS, but it converts that GCS into a flat surface by projecting points into the plane. Its units are linear, for example, in meters Figure 12.4, left)\n\n\n\n\n\n\nFigure 12.4: Differences of a geographic coordinate system (GCS) left and a projected coordinate system (PCS) right.\n\n\nThere are many different models of the earth’s surface, and therefore many different GCS. World Geodetic System 1984 (WGS84) is designed as a one-size-fits-all GCS, good for mapping global data and the most popular CRS. WGS84 uses a three-dimensional ellipsoidal model of the Earth, with positions that are defined using latitude and longitude in degree (e.g. Location of Zurich: E: 47.4°, N: 8.5°).\nThe Universal Transverse Mercator (UTM) system is a commonly used projected coordinate reference system, in meter. UTM subdivides the globe into zones, numbered 0-60 (equivalent to longitude) and regions (north and south). A UTM zone is a 6° segment of the Earth. Because a circle has 360°, this means that there are 60 UTM zones on Earth. The coordinate system grid for each zone is projected individually. Additionally, the system includes a series of horizontal bands, each covering 8 degrees of latitude, which are lettered from C to X. Zurich, for example, is in UTM zone 32T with the location: 32T E: 465207.85 N: 5246235.11.\n\n12.1.4 Digital Elevation Model (DEM)\nA Digital Elevation Model (DEM) represents the Earth’s bare ground topographic surface, excluding trees, buildings, and any other surface objects. There are several datasets available. This handbook will use the digital elevation model from the Shuttle Radar Topography Mission (SRTM) (“NASA Shuttle Radar Topography Mission (SRTM)” (2013)). There are several ways to download the data. This will be discussed within the exercise (?sec-basin-characterisation-exercise-Part1).\n\n12.1.5 Watershed Delineation\nWatersheds can be delineated from a DEM. In this Coursebook, we look at the point-based watershed delineation. The point-based method derives a watershed for each selected point (e.g. discharge station). The slideshow below shows the steps to delineate a watershed based on the example of the upper Zarafshan River Basin.\nThe outlet point of a watershed is where all the water drains into. Water flow is mainly driven by gravity. From a DEM, we are getting information about the height structure of the surface. To delineate a watershed, first, we have to fill in sinks. A filled DEM is void of depressions, cells that are surrounded by higher elevation values and thus represent an area of internal drainage. From the filled DEM, we can calculate the flow direction. The flow direction shows the direction in which the water will flow out of each cell of the filled DEM. A widely used method for deriving flow direction is the D8 method. The D8 method assigns a cell’s flow direction based on the steepest slope of its eight neighbours. From the flow direction, we can calculate the flow accumulation of each cell by counting how many cells are draining into one cell. The stream network can be derived from a flow accumulation raster by, for example, using a threshold method. This means if a cell of the flow accumulation raster exceeds a certain threshold of how many cells are draining into this cell, it is classified as a river.\n\n\n\n  \n\n\n\n\nExercise 1: Watershed delineation\nThe goal of the first exercise is to delineate the catchment, like in the example of upper ZRB in Figure Figure 12.5. For this exercise, you need to have QGIS installed. For a quick installation guide and tutorial, follow this Link. If you already have your catchment outlines and river network, you can skip this exercise and go to sec-basin-characterisation-theory.\n\n\n\n\n\nFigure 12.5: Outcome Exercise 1: Watershed area and river network here with the example of the upper Zarafshan River Basin.\n\n\nIn this exercise, we will use the Global Watersheds web app by Heberger (2022). The following instruction video shows how to delineate the catchment and import your watershed area and river network into QGIS.\nEnglish version:\n\n\n\n\n\n\n\n\n\nRussian version:\n\n\n\n\n\n\n\n\n\nWatershed delineation can also be performed using other tools such as QGIS, detailed in Chapter Section 5.2, or R, as outlined here.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quantification of Climate Change Impacts</span>"
    ]
  },
  {
    "objectID": "Climate_change_impact_study.html#watershed-characterization",
    "href": "Climate_change_impact_study.html#watershed-characterization",
    "title": "12  Quantification of Climate Change Impacts",
    "section": "\n12.2 Watershed Characterization",
    "text": "12.2 Watershed Characterization\nIn this section, we will start with a characterization of the watershed under consideration.\nUPDATE: Work in progress, stay tuned for more information.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quantification of Climate Change Impacts</span>"
    ]
  },
  {
    "objectID": "Climate_change_impact_study.html#delineation-of-hydrological-response-units",
    "href": "Climate_change_impact_study.html#delineation-of-hydrological-response-units",
    "title": "12  Quantification of Climate Change Impacts",
    "section": "\n12.3 Delineation of Hydrological Response Units",
    "text": "12.3 Delineation of Hydrological Response Units",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quantification of Climate Change Impacts</span>"
    ]
  },
  {
    "objectID": "Climate_change_impact_study.html#preparation-of-climate-forcing-data",
    "href": "Climate_change_impact_study.html#preparation-of-climate-forcing-data",
    "title": "12  Quantification of Climate Change Impacts",
    "section": "\n12.4 Preparation of Climate Forcing Data",
    "text": "12.4 Preparation of Climate Forcing Data\nIntroductory words.\n\n12.4.1 Historical Climate Data (CHELSA V21 Data)\n\n12.4.2 GCM Simulation Data\n\n12.4.3 Bias-Correction of GCM Climate Data",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quantification of Climate Change Impacts</span>"
    ]
  },
  {
    "objectID": "Climate_change_impact_study.html#implementation-of-hydrological-model",
    "href": "Climate_change_impact_study.html#implementation-of-hydrological-model",
    "title": "12  Quantification of Climate Change Impacts",
    "section": "\n12.5 Implementation of Hydrological Model",
    "text": "12.5 Implementation of Hydrological Model\n\n12.5.1 Baseline Hydrological Model\n\n12.5.2 Computing Climate Change Scenarios",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quantification of Climate Change Impacts</span>"
    ]
  },
  {
    "objectID": "Climate_change_impact_study.html#discussion-and-conclusion",
    "href": "Climate_change_impact_study.html#discussion-and-conclusion",
    "title": "12  Quantification of Climate Change Impacts",
    "section": "\n12.6 Discussion and Conclusion",
    "text": "12.6 Discussion and Conclusion",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quantification of Climate Change Impacts</span>"
    ]
  },
  {
    "objectID": "Climate_change_impact_study.html#references",
    "href": "Climate_change_impact_study.html#references",
    "title": "12  Quantification of Climate Change Impacts",
    "section": "\n12.7 References",
    "text": "12.7 References\n\n\n\n\nHeberger, Matthew. 2022. “Delineator.py: Fast, Accurate Global Watershed Delineation Using Hybrid Vector- and Raster-Based Methods.” Zenodo. https://doi.org/10.5281/zenodo.7314287.\n\n\n“NASA Shuttle Radar Topography Mission (SRTM).” 2013. NASA. https://earthdata.nasa.gov/learn/articles/nasa-shuttle-radar-topography-mission-srtm-version-3-0-global-1-arc-second-data-released-over-asia-and-australia.",
    "crumbs": [
      "Part III: Hydrological Modeling & Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Quantification of Climate Change Impacts</span>"
    ]
  },
  {
    "objectID": "appendix_a_free_software.html",
    "href": "appendix_a_free_software.html",
    "title": "Appendix A — Software",
    "section": "",
    "text": "A.1 Literature\nMany authors of scientific literature are on the web platform researchgate where they can privately share their work with students (users need to register for an account).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "appendix_a_free_software.html#sec-open-resouces-software-QGIS",
    "href": "appendix_a_free_software.html#sec-open-resouces-software-QGIS",
    "title": "Appendix A — Software",
    "section": "A.2 QGIS",
    "text": "A.2 QGIS\nQGIS is a free and open source Geographical Information System that offers very similar tools as their commercial counterparts. The latest version of QGIS can be downloaded from the QGIS website. We recommend to install the stable long-term support version (installation guide).\n\nA.2.1 Resources for learning QGIS\nA general tutorial for beginners is the QGIS training manual. It includes a short chapter on the use of QGIS for hydrological analysis (Chapter 17.16). For this course you should be familiar with the QGIS window and know the difference between raster and vector data. If you have used QGIS or a similar GIS software before you will not need to do a tutorial prior to this course.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "appendix_a_free_software.html#sec-open-resouces-software-R",
    "href": "appendix_a_free_software.html#sec-open-resouces-software-R",
    "title": "Appendix A — Software",
    "section": "A.3 R and RStudio",
    "text": "A.3 R and RStudio\nR is a free and open source statistical programming language. It’s large user community ensure active development and up-to-date help resources available on the internet. RStudio is a free user interface for R. To install R and RStudio follow the installation guide on ModernDive - Statistical Inference via Data Science.\nFor the bare beginners, also with regard to programming, the book Hands-On Programming with R is an excellent start\n\nA.3.1 Resources for learning R and R studio\n\n“Help! I’m new to R and RStudio and I need to learn them! What do I do?” If you’re asking yourself this, this book is for you: ModernDive - Statistical Inference via Data Science.\nA thorough guide for data science in R: R for Data Science\n\n\n\nA.3.2 RS Minerve\n\nHow to download and install RS Minerve\nGo to the software download page of CREALP’s website https://www.crealp.ch/fr/accueil/outils-services/logiciels/rs-minerve/telechargement-rsm.html (last accessed March 18, 2021) and click on Version actuelle to download the latest installer for Windows as shown in Figure A.1. This will start the download process for the installer RSMinerve-install.exe.\n\n\n\n\n\n\nFigure A.1: Download RS Minerve from the CREALP website https://www.crealp.ch/fr/accueil/outils-services/logiciels/rs-minerve/telechargement-rsm.html (last accessed March 18, 2021).\n\n\n\nYou should also download the user manual (RS MINERVE user manual, written in English) and the example files used for the tutorials in the user manual (Exemple de fichiers, a zip file with data) as well as the technical manual (RS MINERVE technical manual, written in English).\nOnce the installer is downloaded, install RSMinerve with a double-click on the installer and follow the Setup guide. Open RSMinerve once you have it installed.\nBack to the prerequisites for RS Minerve modelling",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "appendix_b_riverscentralasia_r_toolbox.html",
    "href": "appendix_b_riverscentralasia_r_toolbox.html",
    "title": "Appendix B — R Toolbox riversCentralAsia",
    "section": "",
    "text": "More information can be found on Github, where the package is maintained.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>R Toolbox riversCentralAsia</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html",
    "href": "appendix_c_quick_guides.html",
    "title": "Appendix C — Quick Guides",
    "section": "",
    "text": "Earth Explorer: Register for an Account\nIn your internet browser, navigate to https://earthexplorer.usgs.gov/. The window will look similar as in Figure C.1.\nClick on Login in the top right corner. This will bring you to a new window where you can click on Create New Account which will open a form where you enter your information. After verifying your account by clicking on the appropriate link that you will be sent, you can download data from the Earth Explorer interface.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-register-with-earth-explorer",
    "href": "appendix_c_quick_guides.html#sec-register-with-earth-explorer",
    "title": "Appendix C — Quick Guides",
    "section": "",
    "text": "Figure C.1: Start window of the Earth Explorer interface.\n\n\n\n\n\nEE: Download SRTM Data for a Selected Region\n\nLogin to the Earth Explorer (Register if you haven’t done so before How to).\nNavigate to your area of interest in the map panel of the Earth Explorer interface.\nDraw a polygon around your area of interest by clicking on the map (see Figure C.2).\nIn the Data Set tab, look for the SRTM 1 arc-second global DEM (see Figure C.3) and select it by ticking the box next to the product name in the list.\n\n\n\n\n\n\n\n\nFigure C.2: Define a polygon around your area of interest by clicking on the map.\n\n\n\n\n\n\n\n\n\nFigure C.3: Search for the SRTM 1 arc-second global DEM product.\n\n\n\n\nVerify that the result layer(s) cover your area of interest by pressing the foot icon in the Results tab and download if you are satisfied (Figure C.4).\n\n\n\n\n\n\n\nFigure C.4: Verify the results of your search and download the data products you need.\n\n\n\nBack to the Load DEM section.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-sofware-qgis-installation-guide",
    "href": "appendix_c_quick_guides.html#sec-sofware-qgis-installation-guide",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS Installation Guide",
    "text": "QGIS Installation Guide\nIn your web browser go to https://qgis.org/en/site/forusers/download.html (see Figure C.5).\n\n\n\n\n\n\nFigure C.5: The QGIS download site.\n\n\n\nChoose the long-term support version of QGIS for your operating system (e.g. if you use a laptop with a 64-bit Windows operating system, open the Download for Windows tab and click on QGIS Standalone Installer Version 3.16 (64 bit), see Figure C.6)). Clicking on the installer will start the download.\n\n\n\n\n\n\nFigure C.6: The QGIS installer if you work on a 64-bit Windows operating system.\n\n\n\nDouble-click on the downloaded file to start the installation. Typically, SAGA GIS and GRASS GIS are installed alongside QGIS if you choose this installing option.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-qgis-window-overview",
    "href": "appendix_c_quick_guides.html#sec-qgis-window-overview",
    "title": "Appendix C — Quick Guides",
    "section": "The QGIS Window - Overview",
    "text": "The QGIS Window - Overview\nThe main parts of the QGIS window which are referenced in this tutorial, are highlighted in Figure C.7.\n\n\n\n\n\n\nFigure C.7: The QGIS window.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-saving-a-new-qgis-project",
    "href": "appendix_c_quick_guides.html#sec-saving-a-new-qgis-project",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Saving a New QGIS Project",
    "text": "QGIS: Saving a New QGIS Project\nOpen your QGIS and open a new QGIS project by moving your cursor on the white sheet symbol in the upper-left corner of your QGIS window (see Figure C.8).\n\n\n\n\n\n\nFigure C.8: Tutorial project open in QGIS LTS.\n\n\n\nSave the QGIS project by pressing on the disk icon and selecting a location for the file on your computer and a name for the project. You can add freely available on-line maps as background.\nBack to setting up of QGIS.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-change-project-projection-qgis",
    "href": "appendix_c_quick_guides.html#sec-change-project-projection-qgis",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Change the Projection of the QGIS Project",
    "text": "QGIS: Change the Projection of the QGIS Project\nFor this tutorial, the projection of the QGIS project should be in EPSG:32642 (i.e., UTM 42 N). Change it by clicking on the projects projection in the lower right corner of the QGIS window (see Figure C.7). In the coordinate reference system (CRS) tab of the Project Properties, select WGS84 / UTM zone 42N with ID EPSG:32642 and click OK.\nFor modeling your own sample catchment, a different UTM zone may be applicable. The map in the lower right corner of the Project Properties window shows the area for the projection in red and the area where your data is located in violet so you can visually verify that you choose an appropriate projection.\nGenerally, you want to choose the CRS so that you have minimal distortion through the projection in your area of interest. The CRS with ID EPSG:32462 suits well for all of the student exercise catchments.\nBack to the setting up of QGIS.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-install-and-activate-plugins-in-QGIS3",
    "href": "appendix_c_quick_guides.html#sec-install-and-activate-plugins-in-QGIS3",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Install and activate plugins in QGIS3",
    "text": "QGIS: Install and activate plugins in QGIS3\nNavigate to Plugins in the header toolbar and go to Manage and Install Plugins …. Search for the plugin name and go to Install Plugin to install a plugin or tick the box to the left of the plugin name in the list of plugins to activate it (see Figure C.9).\n\n\n\n\n\n\nFigure C.9: Plugin management window in QGIS 3.16. Install plugin with the Install Plugin button. Activate installed plugins by ticking the box in the list.\n\n\n\nBack to the setting up of QGIS.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-managing-panels",
    "href": "appendix_c_quick_guides.html#sec-managing-panels",
    "title": "Appendix C — Quick Guides",
    "section": "GQIS: Managing Panels Visibility in QGIS",
    "text": "GQIS: Managing Panels Visibility in QGIS\nShould one of the panels described here not be visible in your QGIS window navigate to View in your header toolbar and then to Panels. The visible panels are marked with a tick. Click on a panel name in the list to activate or deactivate it.\nBack to setting up of QGIS.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-loading-public-background-layers",
    "href": "appendix_c_quick_guides.html#sec-loading-public-background-layers",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Loading Public Background Maps",
    "text": "QGIS: Loading Public Background Maps\nYou can add freely available on-line maps as backgrounds. Note that they will only be available as long as your computer is connected to the internet. In the Browser panel (see What to do if you don’t see the Browser panel), move the cursor to XYZ Tiles, do a right-click and select New Connection. Enter a descriptive name for the map layer and one of the links below and click OK. The new map layer will appear under XYZ Tiles. By double-clicking on the layer you can add it to your Layers panel.\nBack to setting up of QGIS.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-zoom-to-layer",
    "href": "appendix_c_quick_guides.html#sec-zoom-to-layer",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Zoom to Layer",
    "text": "QGIS: Zoom to Layer\nYou can tell QGIS to zoom to the selected layer by selecting a layer in the Layers window and then on the white sheet and magnifying glass icon in the toolbar (see Figure C.10).\n\n\n\n\n\n\nFigure C.10: Zoom to layer.\n\n\n\nYou can also perform a right-click on the layer name in the Layers panel and select Zoom to layer.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-srtm-plugin",
    "href": "appendix_c_quick_guides.html#sec-srtm-plugin",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Import SRTM layers using the SRTM plugin",
    "text": "QGIS: Import SRTM layers using the SRTM plugin\nMake sure the SRTM plugin is installed (see How to). Navigate to Plugins in the header toolbar and there to SRTM Downloader. Select the SRTM Downloader. Set the boundaries of the SRTM tiles to download and press the Download button. Close the window when done. The SRTM tiles are loaded to the Layers pane.\nBack to Load DEM in QGIS section",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-merge-srtm-tiles",
    "href": "appendix_c_quick_guides.html#sec-merge-srtm-tiles",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Merge SRTM Tiles to a Single Layer",
    "text": "QGIS: Merge SRTM Tiles to a Single Layer\nNavigate to Raster in the header toolbar and then to Miscellaneous and Merge…. In the Merge window that opens, select the input layers by clicking on the … button. Tick the layers that need to be merged and press Run. When the algorithm is done, close the window. You can zoom to the extent of the new layer (see How to). You can change the color of the DEM file.\nBack to Load DEM in QGIS section",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-load-dem",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-load-dem",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Add Raster Layer",
    "text": "QGIS: Add Raster Layer\nIn your QGIS window, navigate to Layer in your header toolbar, there to Add Layer and left-click on Add Raster Layer (see Figure C.11). The Data Source Manager will open in a pop-up window (see Figure C.12).\n\n\n\n\n\n\nFigure C.11: Add raster layer to QGIS project, step 1.: Navigate to the Data Source Manager.\n\n\n\n\n\n\n\n\n\nFigure C.12: Add raster layer to QGIS project, step 2.: Browse for the raster file to add to the QGIS project by pressing on the box with the three dots (…) to the right of the raster source input field.\n\n\n\nIn the example above, a DEM for the example of the Nauvalisoy river catchment is loaded. For loading the DEM of your sample catchments, browse for the DEM in the corresponding folder you downloaded from the provided Dropbox directory.\nPress the Add button at the bottom right of the Data Source Manager window to load the raster layer to your QGIS project and close the window by pressing the Close button (to the left of the Add button).\nYour QGIS project now shows a grey-scale version of the raster layer you loaded. If you are satisfied with the change, save your project by pressing the disk icon at the upper left corner of your QGIS window.\nYou can change the color of your raster file (how to). Here is a quick-guide of how to apply a topography-style color band to your DEM.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-change-color-of-raster-layer",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-change-color-of-raster-layer",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Change Color of Raster Layer",
    "text": "QGIS: Change Color of Raster Layer\nYou can change the colors of your cut DEM by double-clicking on the layer name in the Layers window. Go to tab Symbology (with the paint and brush icon) and choose Render type Singleband-pseudocolor (see Figure C.13) and select a Color ramp.\n\n\n\n\n\n\nFigure C.13: Nicely color your DEM, step 1.\n\n\n\nQGIS comes with a large library of color ramps but you can also create your own. See below a description of how to get your DEM in a topography style color palette.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-topograpy-color-ramp",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-topograpy-color-ramp",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Topography-style color palettes",
    "text": "QGIS: Topography-style color palettes\nFor our DEM we choose an existing topography-style color ramp. Open the Layer Properties window with a double-click on the raster layer in the Layers pane of the QGIS window. In the Symbology tab click the triangle to the right of the color ramp and select Create New Color Ramp… (see Figure C.14).\n\n\n\n\n\n\nFigure C.14: Nicely color your DEM, step 2.\n\n\n\nIn the drop down menu of the pop-up window choose Catalog: cpt-city and press OK (see Figure C.15).\n\n\n\n\n\n\nFigure C.15: Nicely color your DEM, step 3.\n\n\n\nA this will open another window containing the catalog of existing color ramps. Under Topography, we choose sd-a and press OK (see Figure C.16).\n\n\n\n\n\n\nFigure C.16: Nicely color your DEM, step 4.\n\n\n\nGo to tab Transparency, set Global Opacity to 30% and specify the No Data Value 0 (see Figure C.17).\n\n\n\n\n\n\nFigure C.17: Nicely color your DEM, step 5.\n\n\n\nThen go back to the Symbology tab. The minimum value of the color ramp should now not be 0 but 272. Adapt manually if need be. Then, press classify to get a discrete color ramp for your map and Apply to the map. If you are happy with the colors, quit by pressing OK (see Figure C.18).\n\n\n\n\n\n\nFigure C.18: Nicely color your DEM, step 6.\n\n\n\nThe result will look like Figure C.19.\n\n\n\n\n\n\nFigure C.19: Nicely color your DEM, step 7.\n\n\n\nYou can add decorations (e.g. scale and north arrow) to your map (how to).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-add-map-decorations",
    "href": "appendix_c_quick_guides.html#sec-add-map-decorations",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Add Map Decorations",
    "text": "QGIS: Add Map Decorations\nNavigate to View -&gt; Decorations and choose among the decorations to add. Many options for configuring the decorations are available.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-verify-projection-reproject",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-verify-projection-reproject",
    "title": "Appendix C — Quick Guides",
    "section": "C.1 QGIS: Verify Projection of Layer and Re-project Layer",
    "text": "C.1 QGIS: Verify Projection of Layer and Re-project Layer\nOpen the Layer Properties window with a double-click on the layer in the Layers pane and navigate to the Information tab. Under CRS (coordinate reference system) you see the projection of the layer. For the Chirchiq river basin, the CRS should say “EPSG:32642 - WGS 84 / UTM zone 42N - Projected”. Other river catchments in Central Asia may require a different UTM zone. For the student exercises, it is good to choose this projection for all basins.\nA raster layer can be reprojected to a different CRS: Go to Raster in the header toolbar. From there move the cursor over Projections and click on Warp (Reproject)…. The Warp window will pop up (in fact, it is just an interface where the user can specify the parameters of the wrap algorithm in a convient way). Select the raster layer you wish to re-project in the Input layer and select the Target CRS.\nIf the target CRS you wish to re-project to is not available in the drop-down menu, you can browse for it by clicking on the globe icon to thr right of the Target CRS section. You may re-sample the raster to a coarser resolution by specifying the Output file resolution. You may also specify to save the reprojected raster layer: Scroll to the bottom of the Warp (Reproject) window where you see Reprojected and a white box where you can browse for a location to store the new layer.\n\n\n\n\n\n\nWARNING\n\n\n\nIf you do not specify a target location, only a temporary layer will be loaded to QGIS which will not be available anymore after you close the QGIS project (even if you save the project).\n\n\nYou may decide to load the temporary file and save it later (how to). Click the Run button at the bottom right to start the re-projection algorithm and press Close when the process is done. The reprojected layer will be available in the Layers pane.\nA vector layer can be reprojected by selecting Vector in the header toolbar, moving the cursor to Data Management Tools and clicking on Reproject Layer…. This opens the Reproject Layer window where you can specify a Target CRS and optionally a storage location. As for the reprojection of the raster layer, a temporary layer is loaded to your QGIS project if you do not specify a storage location. However, you can always store temporary layers later (how to).\nBack to the load DEM section.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guide-save-temp-layer",
    "href": "appendix_c_quick_guides.html#sec-quick-guide-save-temp-layer",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Save a Temporary Layer",
    "text": "QGIS: Save a Temporary Layer\nRight-click on a temporary layer in the Layers pane. Temporary layers are indicated by a box to the right of the layer name. From the menu that opens upon right-click, select Export and Save As… (for raster layers) or Save Feature As… (for vector layers). An explorer window will open where you can specify a file name and a location to store the file to.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-add-vector-layer-to-qgis",
    "href": "appendix_c_quick_guides.html#sec-add-vector-layer-to-qgis",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Add Vector Layer",
    "text": "QGIS: Add Vector Layer\nLeft-clicking on Layer, moving your cursor over Add Layer and left-clicking Add Vector Layer (see Figure C.20).\n\n\n\n\n\n\nFigure C.20: Add vector layer to QGIS project, step 1.: Navigate to the Data Source Manager.\n\n\n\nA window will pop up, asking you to specify the properties of the vector layer to add (see Figure C.21).\n\n\n\n\n\n\nFigure C.21: Add vector layer to QGIS project, step 2: Select vector layers to add. Note that you select the shp file but cpg, dbf and shx need to be present in the same location.\n\n\n\nPress the box with the three dots on the right of the source field to specify the location of the shape file to be added to your project (see Figure C.22). Click Open and the window will close. The address of your shape files should now stand in the source field as in Figure C.21.\n\n\n\n\n\n\nFigure C.22: Add vector layer to QGIS project, step 3.\n\n\n\nNote that you load the .shp file but that all the files in the list in Figure C.21 need to be present. In the Add Vector Layer window, click Add and then close the window. QGIS has attributed a random color to your shape file which can be changed manually How to.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-change-color-of-vector-layer",
    "href": "appendix_c_quick_guides.html#sec-change-color-of-vector-layer",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Change Color of a Vector Layer",
    "text": "QGIS: Change Color of a Vector Layer\nYou can change the color of your layer by double-clicking on the layer name in the Layers Window to the left of the map. This will open the properties window. The third tab from the top shows paint and brush (see Figure C.23).\n\n\n\n\n\n\nFigure C.23: dd vector layer to QGIS project, step 5. Change the color of the layer.\n\n\n\nYou can activate Simple fill by clicking on it and select No brush in the drop-down menu in order to only show the outline of your vector layer (see Figure C.24).\n\n\n\n\n\n\nFigure C.24: Add vector layer to QGIS project, step 6. Only show the outline of the vector layer.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guides-fill-sinks",
    "href": "appendix_c_quick_guides.html#sec-quick-guides-fill-sinks",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Fill Sinks",
    "text": "QGIS: Fill Sinks\nBrowse for the Fill sinks algorithm in the Processing Toolbox panel (see Figure C.25). If the Processing Toolbox panel is not visible go to Processing in the header toolbar and click on Toolbox to activate it. Alternatively, here is how to manage the visibility of panels in QGIS.\n\n\n\n\n\n\nFigure C.25: Search for the Fill sinks algorithm in the Processing Toolbar panel.\n\n\n\nOpen the Fill sinks window with a double-click on the name of the algorithm in the Processing Toolbar panel. There, select the DEM you want to process and browse for a location to store the output file (see Figure C.26). You can leave the default minimum slope.\n\n\n\n\n\n\nFigure C.26: Select the raster file to process.\n\n\n\nWhen the algorithm is done it will load the new layer into your QGIS project. Close the Fill sinks window and save your project.\nBack to catchment delineation.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quick-guide-upslope-area",
    "href": "appendix_c_quick_guides.html#sec-quick-guide-upslope-area",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Calculate the area upslope of a point",
    "text": "QGIS: Calculate the area upslope of a point\nSearch for the SAGA algorithm Upslope Area in the Processing Toolbox panel and open the function window with a double click on the name. Enter the Longitude of your discharge station for the Target X coordinate and the Latitude for the Target Y coordinate for which the upslope area should be calculated. For the Elevation select the sink-filled DEM (see how to fill sinks in a DEM and why we need to fill in sinks).\nAll GIS layers and the coordinates of the discharge gauge need to be in the same UTM projection. Choose a method in the drop-down menue in the Method section and optionally specify a location for the output file. For the example of the Nauvalisoy basin, the Upslope Area window filled in correctly is shown in Figure C.27.\n\n\n\n\n\n\nFigure C.27: The Upslope Area window for the determination of the catchment area of the Nauvalisoy river basin.\n\n\n\nClick Run and Close after the algorithm is done. A new raster file with the values 0 for outside the catchment area and 100 for inside the catchment area is now loaded into your QGIS project.\nBack to catchment delineation.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#appendix-quick-guide-polygonize",
    "href": "appendix_c_quick_guides.html#appendix-quick-guide-polygonize",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Polygonize a Raster",
    "text": "QGIS: Polygonize a Raster\nGot to Raster in the header toolbar, move your cursor over Conversion and click on Polygonize (Raster to Vector)… (see Figure C.28).\n\n\n\n\n\n\nFigure C.28: Open the Polygonize window.\n\n\n\nSelect the raster layer you wish to polygonize (for example the upslope area raster layer) and run the process. Close the polygonize window after the algorithm is done. A new shape file will be loaded to your GIS project (see Figure C.29).\n\n\n\n\n\n\nFigure C.29: The vector layer generated by a raster to vector conversion.\n\n\n\nThe new vector layer has 2 features: the watershed boundary and a box around the watershed the same size of the DEM we used. To get rid of the outer shape, open the attribute table with a right-click on the vector layer and selecting Open Attribute Table. In the attribute table, elect the outer shape by clicking on the second row of the attribute table and toggle the edit mode by clicking on the pen icon (see Figure C.30).\n\n\n\n\n\n\nFigure C.30: Select the outer shape to discard by clicking on the second row in the attribute table and toggle the edit mode by clicking on the pen icon.\n\n\n\nDelete the outer shape by pressing the red bin icon in the attribute table (see Figure C.31).\n\n\n\n\n\n\nFigure C.31: Delete the selected outer shape.\n\n\n\nSave the edits in the attribute table (see Figure C.32) and press the pen icon again to un-toggle the edit mode.\n\n\n\n\n\n\nFigure C.32: Save your edits in the attribute table.\n\n\n\nClose the attribute table window and save the boundary of your watershed and your QGIS project.\nBack to catchment delineation.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-graphical-edit-junctions",
    "href": "appendix_c_quick_guides.html#sec-graphical-edit-junctions",
    "title": "Appendix C — Quick Guides",
    "section": "QGIS: Edit Junctions Layer",
    "text": "QGIS: Edit Junctions Layer\nSelect the Junctions layer and toggle manual editing by clicking on the yellow pen (see Figure C.33).\n\n\n\n\n\n\nFigure C.33: Manually edit the layer with the river junctions, step 1: Toggle layer editing.\n\n\n\nWhen in editing mode, the yellow pen will appear in the Layers window next to the name of the layer being edited. The edit mode will also activate a button for adding points (i.e. junctions, we don’t need that now) and the vertex tool. Click on the vertex tool icon. It is active when a a boundary appears around the icon and the Vertex Editor windows opens (see Figure C.34).\n\n\n\n\n\n\nFigure C.34: Manually edit the layer with the river junctions, step 1: Toggle layer editing.\n\n\n\nRight-click on a junction point you would like to delete to activate it (see Figure C.35).\n\n\n\n\n\n\nFigure C.35: Manually edit the layer with the river junctions, step 3: Activate a junction node for editing.\n\n\n\nSelect the activated point by drawing a rectangle over the point with your mouse. The point will appear blue (see Figure C.36).\n\n\n\n\n\n\nFigure C.36: Manually edit the layer with the river junctions, step 3: Select the activated junction node.\n\n\n\nDelete the point with the delete key on your keyboard. You can save your edits by pressing the blue-white Save Layer Edits button that is decorated with an orange pen (see Figure C.37). This saves your changes without exiting the edit mode.\n\n\n\n\n\n\nFigure C.37: Manually edit the layer with the river junctions, step 3: Save edits.\n\n\n\nIf you have many points to remove, as in our case, it may be faster to identify the IDs of the features you want to keep, select these and delete all others. To start, you activate the Identify Features mode by clicking on the icon with the white i on the blue circle (see Figure C.38).\n\n\n\n\n\n\nFigure C.38: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 1.\n\n\n\nA black i will appear next to your cursor. You then click on the first of your nodes that you want to keep. This will highlight it in red and a list with information on the selected feature appears on the right in the Identify Results window. You will see the attribute NODE_ID with value 1 for the outflow node (see Figure C.39). Note down the ID of the feature.\n\n\n\n\n\n\nFigure C.39: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 2.\n\n\n\nYou then press on the node at the confluence of the two tributaries in the center of the catchment. The Identify Results window shows 2 results, that means, that two junction nodes are close to each other (see Figure C.40).\n\n\n\n\n\n\nFigure C.40: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 3.\n\n\n\nZoom in in your map window with your mouse to see the two nodes (see Figure C.41).\n\n\n\n\n\n\nFigure C.41: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 4.\n\n\n\nSelect the node that should be kept and not the ID of the node (NODE_ID 11) (see Figure C.42).\n\n\n\n\n\n\nFigure C.42: Alternative method to manually edit junctions if many nodes need to be deleted. Step 1: Get ID of features to keep, part 5.\n\n\n\nZoom back to the entire Junctions layer (see Figure C.10) and go to Select Features by Values… in the toolbar (see Figure C.43).\n\n\n\n\n\n\nFigure C.43: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 1.\n\n\n\nAdd NODE_ID 1 to your selection as demonstrated in Figure C.44.\n\n\n\n\n\n\nFigure C.44: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 2.\n\n\n\nThe outflow node with ID 1 will change color in your map as shown in Figure C.45.\n\n\n\n\n\n\nFigure C.45: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 3.\n\n\n\nAdd node 11 to your selection in the same way and close the Select Node by Value window. Invert the feature selection as shown in Figure C.46.\n\n\n\n\n\n\nFigure C.46: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 4.\n\n\n\nAll other nodes will now be yellow and the ones to keep will appear in the layer color (see Figure C.47).\n\n\n\n\n\n\nFigure C.47: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 5.\n\n\n\nDelete the features (nodes) by pressing the Delete Selected button in the edit features toolbar as shown in Figure C.48.\n\n\n\n\n\n\nFigure C.48: Alternative method to manually edit junctions if many nodes need to be deleted. Step 2: Select features to delete, part 6.\n\n\n\nSave your edits (see Figure C.37). To verify that, indeed, all superfluous nodes are deleted from the Junctions file, open the Attribute table shown in Figure C.49.\n\n\n\n\n\n\nFigure C.49: Edit Attribute table. Step 1: Open the attribute table with a click on the Attribute Table icon in the QGIS toolbar.\n\n\n\nOnly 2 features should be listed under each attribute. We will now edit the attribute table to prepare it for the RSMinerve model. RSMinerve needs an identifier to differentiate between the junctions. We can use the attribute TYPE to uniquely identify the two junctions needed. RSMinerve further needs the ID of the downstream river. Add a column to the attribute table by pressing the Add Field button (see Figure C.50).\n\n\n\n\n\n\nFigure C.50: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 1.\n\n\n\nDefine a name for the attribute, a type and admissible length of each entry in the Add Field window. In our case, we choose a string consisting of letters as ID and allow it to be 20 characters long as shown in Figure C.51.\n\n\n\n\n\n\nFigure C.51: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 2.\n\n\n\nClose the window by pressing OK. By clicking in the newly created NULL fields, you can now type names for the downstream rivers and save your edits by pressing the save edits icon (3rd from the left in the toolbar of the attribute table window). As the outlet of the catchment goes directly into Charvak reservoir, we can type Charvak as the downstream river ID for this example. The river stretch between junction and outlet is called Pskem (see Figure C.52).\n\n\n\n\n\n\nFigure C.52: Edit Attribute table. Step 2: Add a column to the attribute table manually, part 3.\n\n\n\nWe are done editing the Junctions layer. Deactivate the edit mode by clicking on the yellow pen in the attribute table window as demonstrated in Figure C.53 and close the window.\n\n\n\n\n\n\nFigure C.53: Save your edits.\n\n\n\nNow save the Junctions layer in an appropriate place on your drive. Time to save your QGIS project.\nIn the same way you can also edit the river channels layer.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-create-hbv-model",
    "href": "appendix_c_quick_guides.html#sec-quickguides-create-hbv-model",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Create HBV model and edit parameters",
    "text": "RSM: Create HBV model and edit parameters\nClick on the HBV model icon in the model selection panel (see Figure C.54).\n\n\n\n\n\n\nFigure C.54: To generate an HBV model, click on the HBV model icon in the model selection panel.\n\n\n\nMove your cursor to the white area in the center and create a HBV model with a click. The HBV model icon will now appear in the white model panel (see Figure C.55).\n\n\n\n\n\n\nFigure C.55: A HBV model has been generated.\n\n\n\nEdit the parameters of the model by double-clicking on the model icon and changing parameter values in the table that appears to the right of the RSMinerve window (see Figure C.56).\n\n\n\n\n\n\nFigure C.56: Double-click on the HBV model to open and edit the parameter table.\n\n\n\nAlternatively (especially if you have to change parameters for several models), activate the Parameters panel in the Model Properties toolbar (see Figure C.57).\n\n\n\n\n\n\nFigure C.57: Activate the Parameters panel in the Model Properties toolbar.\n\n\n\nSelect the HBV model in the Parameters panel as shown in Figure C.58.\n\n\n\n\n\n\nFigure C.58: Select HBV in the Parameters panel.\n\n\n\nIf you have several HBV models, you can select models by zone and apply edits in the parameter table in the left of the Parameters panel to all selected models (marked with tick). You can also edit paramers for individual models in the right-hand table of the Parameters panel as is visible in Figure C.59.\n\n\n\n\n\n\nFigure C.59: Edit parameters for groups of models (left parameter table) or for individual models (right parameter table).\n\n\n\nSave your model by clicking the floppy disk icon in the toolbar.\nYou can also export parameters to a text file via the button Export P in the Model Properties toolbar, edit the text file and import the edited parameter file through Import P.\nNote that for the Nauvalisoy demonstration case study, the area of the HBV model should be 99’000’000 m2.\nBack to the Nauvalisoy model guide.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-add-climate-station",
    "href": "appendix_c_quick_guides.html#sec-quickguides-add-climate-station",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Add and link climate station",
    "text": "RSM: Add and link climate station\nMove your cursor to the V-Station icon in the models panel (the first icon in Figure C.54). Click on the icon to activate it and click next to the HBV model in your model pane to place the V-Station. With a double-click on the newly created V-Station, you can visualize the parameters of the virtual weather station.\nThen, connect the station to the HBV model by activating the Connections Mode in the Editing tools toolbar as shown in Figure C.60.\n\n\n\n\n\n\nFigure C.60: Create a virtual weather station.\n\n\n\nClick on the station, hold down the finger move your cursor to the HBV model, then release the hold. A grey line appears between the station and the model and a pop-up window asks you to verify the data links between the two components (see Figure C.61).\n\n\n\n\n\n\nFigure C.61: Link the virtual weather station to the HBV model.\n\n\n\nClick Ok to accept the suggested data links and a blue arrow will appear between the weather station and the model. Click on the black arrow in the Editing tools toolbar to leave the Connections Mode.\nBack to the Nauvalisoy model guide.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-import-climate-data",
    "href": "appendix_c_quick_guides.html#sec-quickguides-import-climate-data",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Import climate data",
    "text": "RSM: Import climate data\nMake sure that you have downloaded storede the following climate data file ERA5_Nauvalisoy_1981_2013.csv for Nauvalisoy River on your computer.\nGo to the database tab and click on Open in the File database toolbar (see Figure C.62).\n\n\n\n\n\n\nFigure C.62: The database tab.\n\n\n\nSelect the file that you downloaded before. If you don’t see the file in your file browser, make sure the file ending .csv is selected in the file browser (see Figure C.63).\n\n\n\n\n\n\nFigure C.63: Make sure the file ending is .csv in the file browser.\n\n\n\nPress open to load the data into RSMinerve. Depending on your computer this may take a few seconds. Once the data is loaded, click through the database browser in the left pane to explore the data as shown in Figure C.64.\n\n\n\n\n\n\nFigure C.64: Explore the data base.\n\n\n\nTo connect the data base to the model you will need to make the data consistent with the model. To do this change the name “New group” to “Measurements” and select Inputs for the Category selection (see Figure C.65).\n\n\n\n\n\n\nFigure C.65: Change \"New group\" to \"Measurements\" and select Input as category.\n\n\n\nThen, browse the name of the station as done in Figure C.66.\n\n\n\n\n\n\nFigure C.66: The name of the station is \"Nauvalisoy\".\n\n\n\nAs our weather data is representative for the entire Nauvalisoy catchment, the station name in the data base needs to be consistent with the station name of the V-Station in the model pane. Change the name of the station in the model pane from V-Station to Nauvalisoy by clicking on the name and then editing it.\nChoose the nauvalisoy data set as source and adapt the simulation period as shown in Figure C.67.\n\n\n\n\n\n\nFigure C.67: Choose the nauvalisoy data set to link the weather station data to the virtual weather station. The simulation times should not extend the period of the input data. Simulation time step is 1 hour and the recording time step is 1 month.\n\n\n\nClick on Validation to verify that the model has been set up correctly. Once you have adapted the model settings to calculate evaporation based on temperature (how to), no errors should be reported. You can now run the model. A warning tells you that the number of meteo stations is not sufficient. Ignore the warning for now.\nBack to the Nauvalisoy model guide",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-atapt-model-settings",
    "href": "appendix_c_quick_guides.html#sec-quickguides-atapt-model-settings",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Adapt Model Settings",
    "text": "RSM: Adapt Model Settings\nNavigate to Edit in the Settings toolbar (see Figure C.68).\n\n\n\n\n\n\nFigure C.68: Open the models settings tab.\n\n\n\nOpen the Settings tab and choose an ET model. Adapt the coordinates of the project (choose the station coordinates mentioned in the Introduction Section) (see Figure C.69).\n\n\n\n\n\n\nFigure C.69: Edit the evaporation calculation method and the coordinates of the project.\n\n\n\nBack to the model guide",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-export-sim-results-to-db",
    "href": "appendix_c_quick_guides.html#sec-export-sim-results-to-db",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Export Simulation Results to Data Base",
    "text": "RSM: Export Simulation Results to Data Base\nGo to Export in the Database toolbar (see Figure C.70). Define a name for the simulation results and choose a data base group to save the data to. Create a new group if you haven’t already done so.\n\n\n\n\n\n\nFigure C.70: Export simulation results to the data base.\n\n\n\nThe data sets are now available in the data base tab and can be visualized in the Selection and Plots tab.\nBack to the model guide",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-add-comparator-and-discharge-data",
    "href": "appendix_c_quick_guides.html#sec-quickguides-add-comparator-and-discharge-data",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Add Comparator and Load Discharge Measurements",
    "text": "RSM: Add Comparator and Load Discharge Measurements\nThe comparator object allows the user to compare a simulated variable to a reference. In our case, we want to compare the simulated discharge of the Nauvalisoy river to the one actually measured at the only gauge in the catchment. The measured discharge can be imported to RSMinerve via the database tab.\nMove your cursor to the comparator icon as shown in Figure C.71 in the model components panel.\n\n\n\n\n\n\nFigure C.71: Comparator icon in RS Minerve.\n\n\n\nActivate it with a left-click and move your cursor next to the HBV model in the model pane. Place the comparator object with another click.\nAdd a source object to your model following the same procedure as for the comparator object. Figure C.72 shows the icon of the source object.\n\n\n\n\n\n\nFigure C.72: Source icon in RS Minerve.\n\n\n\nYou can now optionally rename your model objects.\nConnect the source and the HBV model to the comparator by activating the Connections mode in the Editing Tools toolbar. Right-click on the HBV model, hold the click and drag the cursor to the comparator object where you release the cursor. You will be asked in a pop-up window to specify the flow you wish to compare and if the data is to be viewed as simulation results or reference (measured) data. Connect the total discharge computed by the HBV model component as simulation result to the comparator (see Figure C.73) and close the pop-up window by pressing OK.\n\n\n\n\n\n\nFigure C.73: Connecting a Source to Comparater Object in RSMinerve.\n\n\n\nConnect the source object to the comparator in the same way as the HBV object. The source will be connected as reference (see Figure C.74).\n\n\n\n\n\n\nFigure C.74: Final, connected objects.\n\n\n\nNext, we have to load the discharge data into the database. Navigate to the database tab. In the database, go to Measurements -&gt; Nauvalisoy and click Add and enter a name for the discharge station (see Figure C.75). For this example, we do not need to bother with the coordinates of the station (later, in more complex models, we will have to though!).\n\n\n\n\n\n\nFigure C.75: Connect the outflow of the HBV model as simulation result to the comparator.\n\n\n\nUnder the new discharge station, add a sensor and rename it to the source object in your model as shown in Figure C.76.\n\n\n\n\n\n\nFigure C.76: Connect the discharge measurements (source) as reference to the comparator.\n\n\n\nOpen the tab with the values. Here we need to import the discharge data. You can do that by opening the discharge data in a spreadsheet (e.g. Excel) and copy-pasting the dates and discharge values into the Values table in RSMinerve (here is how to do this step-by-step.\nNow we need to link the discharge data in the database to the source object in the model. Navigate to the model do the following.\n\n\n\n\n\n\nFigure C.77: Select the data source for the source object (under Data Source in the left window pane) and select the sensor for discharge data for the source (under Source, Series identifier in the right window pane).\n\n\n\nValidate the model to see if the model setup went correctly. Run the model if the validation did not throw an error.\nBack to the practical model calibration and validation Section",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_c_quick_guides.html#sec-quickguides-copy-paste-database-values",
    "href": "appendix_c_quick_guides.html#sec-quickguides-copy-paste-database-values",
    "title": "Appendix C — Quick Guides",
    "section": "RSM: Copy-paste Data to Database",
    "text": "RSM: Copy-paste Data to Database\nMake sure that the following file synthetic_discharge_Nauvalisoy_river_for_calibration_exercise.csv is available on your computer.\nNow, open the file in a spreadsheet software, e.g. Excel, Numbers, Google Sheets or OpenOffice Calc and select the rows containing dates and values (Figure C.78). Press Control+C or perform a right-click with the mouse and choose copy.\n\n\n\n\n\n\nFigure C.78: Select the rows and columns containing the dates and data to be copied. Press Control+C to copy the selected cells.\n\n\n\nThen navigate to the discharge sensor on the database tab in RSMinerve where you want to add the data to. Click in the small square to the left of the first row in the Values tab and click the keyboard keys Control+P. Alternatively perform a right-click in the small square and select Paste. The dates and values from the excel sheet will now appear in the table (Figure C.79). Save the database.\n\n\n\n\n\n\nFigure C.79: Paste dates and values to the database table.\n\n\n\nBack to the practical model calibration and validation Section",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Quick Guides</span>"
    ]
  },
  {
    "objectID": "appendix_d_exercise_solutions.html",
    "href": "appendix_d_exercise_solutions.html",
    "title": "Appendix D — Exercise Solutions",
    "section": "",
    "text": "D.1 Exercise on Linear Reservoir modelling",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Exercise Solutions</span>"
    ]
  },
  {
    "objectID": "appendix_d_exercise_solutions.html#sec-appendix-solutions-exercise1",
    "href": "appendix_d_exercise_solutions.html#sec-appendix-solutions-exercise1",
    "title": "Appendix D — Exercise Solutions",
    "section": "",
    "text": "Task 1\nWhat will determine the flow through your bucket?\nThe flow through the bucket will be influenced by the volume to bottom area fraction of the bucket, the amount and speed of water added to the bucket and the size of the outlet hole.\nWhat do you need to measure?\nYou will need to measure:\n\nthe discharge from your bucket over time,\n\nthe recharge volume (how much water you put into the bucket over a given time),\n\nthe time when you start pouring water and when you stop pouring water into the bucket, and\nthe approximate volume of your bucket.\nHow can you measure it?\nThis depends on what you have available. You can draw a water level line outside of your outflow receptacle every 10 seconds and then determine the volume change over time. Maybe you have a scale and a smart phone so you can put your outflow receptacle on the scale and make a movie of the weight change over time (note, 1 kg of water is approximately 1 liter of water).\nFor the inflow pour a well defined volume over a well defined time interval. You can do this manually unless of course you have pipes and valves lying around that you can use.\nYou will need a watch for measuring time and a receptacle with known volume to measure volumes (or a scale).\nA couple of notes on measurement accuracy:\n\nGenerally, the larger the volumes, the smaller the relative measurement error. Say you measure a discharge of 50 ml in 1 s (i.e. 50 ml/s) and you can read your volume with an accuracy of 5 ml and the time with an accuracy of 0.2 s. Your measurement uncertainty becomes 11 ml/s which is more than 20 % of your discharge. If on the other hand, you measure 500 ml over 10 s (which is the same discharge of 50 ml/s) with the same inaccuracies for volume and time your measurement uncertainty for discharge becomes 1 ml/s which is only 2 % of your discharge.\n\nHow do you estimate measurement uncertainties: Measure several times, compute the average and the standard deviation of your measurements assuming a student-t distribution.\n\nHow do you combine uncertainties of volume and time to the uncertainty of discharge: By applying Gaussian error propagation.\nWhat materials you will need to set up the experiment?\n\nFor the bucket (the linear reservoir): A plastic bottle, a box or a can that is no longer used. It should have an opening at the top and the material should repel water and be thin enough that you can drill a hole into the wall.\nA pair of pointy scissors or a knife to drill a hole into the bucket.\n\nA water source (a tap, hose or a water container larger than the one above). This will be your rain machine.\nA watch to measure time.\n\nNote paper and pen.\n\nA receptacle for measuring the outflow.\n\nAdditional material to facilitate measurement according to availability.\n\n\n\n\n\n\nFigure D.1: Example for material needed and example setup to perform the linear reservoir experiment. Add a minion with a stop watch or a smart phone to help you logging the discharge from the bucket.\n\n\nTask 2\nThe video was recorded with a smart phone. The weight of the outflow receptacle was noted down every second. The discharge is computed as the change of volume in the outflow receptacle over time.\n\n\nTask 3\nThe height of the measured discharge peak can be best reproduced with k = 0.42. However, the measured discharge peaks 1s later than the simulated discharge peak.\nReasons for the discrepancy can be the shape of the linear reservoir, non-linear pouring speed, and measurement uncertainties.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Exercise Solutions</span>"
    ]
  },
  {
    "objectID": "appendix_d_exercise_solutions.html#sec-appendix-solutions-hbv-exercises",
    "href": "appendix_d_exercise_solutions.html#sec-appendix-solutions-hbv-exercises",
    "title": "Appendix D — Exercise Solutions",
    "section": "\nD.2 Exercises on the HBV Model",
    "text": "D.2 Exercises on the HBV Model\nExercise: Driving Forces of the HBV Model\nThe model drivers are precipitation, temperature and evaporation (P, T and ET in Figure 8.6). You need to provide time series of the model drivers to the model. Evaporation is typically not measured at climate stations but many empirical functions are available in the literature to estimate evaporation. RSMinerve offers the possibility to calculate evaporation based on temperature measurements and catchment location (you will do that later in this tutorial).\nExercise - HBV Model States\nThe model states are the snow water equivalent height (SWE), the relative water content in the snow pack (WH), the humidity (Hum), the upper reservoir water level (SU) and the lower reservoir water level (SL). The model states are initialized using the initial conditions.\nExercise: Data Visualization in RSMinerve\nSimulate from the 01/01/1981 01:00:00 to 31/12/1983 23:00:00, then choose data from 31/12/1983 23:00:00 as the initial conditions and run the model from 01/01/1984 01:00:00 to 31/12/1984 23:00:00. Choose hourly output for the simulation results.\nOpen the Selection and plots tab by clicking on the Selection and plots button in the Modules toolbar and select simulated P and T from the Nauvalisoy station as shown in Figure D.2.\n\n\n\n\n\nFigure D.2: Hourly precipitation and temperature at the virtual Nauvalisoy weather station.\n\n\nNote: If you want to repeat a simulation with specific initial conditions, you can store them through Export IC in the Model Properties toolbar.\nThe approximate temperature range is -13 deg. C. in December to 34 deg. C. in August. The annual precipitation is about 1.4 m (visualize Pcum and click on the last value of the time series). No precipitation falls during the summer months.\nExercise: Compare Evaporation Methods\nFigure Figure D.3 shows the evaporation computed with various methods and the resulting discharge. Uniform evaporation should not be used for sub-annual modeling time steps for obvious reasons that ET shows a strong seasonality. The difference between the different methods by Turc, McGuinness and Oudin are within 5 % of total discharge which is negligible for a regional model.\n\n\n\n\n\nFigure D.3: Hourly precipitation and temperature at the virtual Nauvalisoy weather station.\n\n\nFor advanced modeling, the choice of the evaporation model may be relevant but only if a validation with measured data is possible.\nExercise: Common Difficulties in Model Calibration\n\nEspecially fully and semi-distributed hydrological models are typically over-parameterized, i.e. the number of model parameters is much larger than the number of observations for the model states. The true parameter values of the system cannot be uniquely identified based on a discharge time series alone.\n\nThe outcome of the calibration depends on the measure of similarity between the simulated and the measured discharge.\nThe water balance is often forgotten during model calibration. A nice fit of the discharge curve can for example be achieved by increasing the volume of water in the model over time.\nThe model is calibrated against historical data. Its ability to predict future discharge may be limited.\n\nThe model is not perfect, it remains an approximation of the real system and may not incorporate all relevant processes of the hydrological cycle (e.g. water storage and transport in glaciers for the case of the HBV model or significant sub-surface water fluxes that very difficult to capture as for example in Karst regions).\n\nDischarge measurements typically have uncertainties of 20 %. Particularly measurements at the lower and upper ends of the rating curve (i.e. the water table - discharge relationship) are typically prone to larger uncertainties (bonus question: think about why this is so!). Is the measurement location or the equipment not properly maintained, biases may grow over time. On the other hand, if the measurement method is updated and changed, the measured discharge may display a different pattern as was for example discussed in the data from Gunt River basin (Section 2.1).\nExercise: Strategies to Overcome some of the Model Calibration Difficulties\n\nOver-parameterization:\n\n\n\nConsider simplifying the model, i.e. reducing the number of parameters. If the model complexity is required, try adding additional measured variables, e.g. snow cover from MODIS data (see Chapter on snow cover data) to validate individual components of the HBV model.\n\nCollect data to verify individual fluxes of the model components (e.g. soil parameters, snow water equivalent, etc.). As physical measurements in the field are not always possible you may have to become creative here, e.g. use MODIS snow cover data to validate the snow/no snow partitioning of the HBV model. Also consult the literature for parameterizations of similar catchments.\n\n\n\nUse a combination of similarity measures. This will be demonstrated later on in the model calibration section.\n\nDuring model calibration, look at the components of the model as well as the total discharge time series. Make sure that the storage of water changes within reasonable bounds and that the partitioning of the water in your system is physically reasonable (e.g. comparatively small storage compartments for rocky mountain catchments).\n\nExclude part of your data set from model calibration and use it for model validation. If the model does not perform well in the validation period, its parameters are too specific for the calibration period (you have over-fitted the model) and the model is said to not generalize well. If this happens you should try to reduce the number of parameters in your model. To understand better which parameters are responsible for the over-fit of the historical discharge, use different calibration and validation periods and compare the resulting parameters. Through sensitivity analysis, identify the model components that are most sensitive to predicted changes of model forcings, geometry or parameterization and perform scenario analysis.\n\nImplement and validate multiple possible conceptual models. All of the models must be calibrated and validated individually. It is further recommended to calculate at the highest possible temporal resolution and to try and compare the model outcome at different spatial resolution.\n\nSquare-root filters or data assimilation algorithms are able to account for non-correlated measurement errors (they are not implemented in RSMinerve and not topic of this course). Error bands for the measurements should be adapted when communicating model results.\n\nMost of the above points will be discussed in more detail during this course.\n\nD.2.1 Exercise: Calibrate a Simple HBV Model {#sec-appendix-solutions-calibrated parameters .unnumbered}\nThe parameter set of the calibrated model are:\n\n\n\n\n|                |      |\n|:---------------|-----:|\n|CFMax (mm/°C/d) |  0.50|\n|CFR (-)         |  0.05|\n|CWH (-)         |  0.10|\n|TT (°C)         |  3.00|\n|TTInt (°C)      |  3.00|\n|TTSM (°C)       |  0.00|\n|Beta (-)        |  2.50|\n|FC (mm)         | 20.00|\n|PWP (-)         |  0.50|\n|SUMax (mm)      | 10.00|\n|Kr (1/d)        |  0.09|\n|Ku (1/d)        |  0.02|\n|Kl (1/d)        |  0.00|\n|Kperc (1/d)     |  0.00|\n\n\nYou can import the calibrated parameters via Import P in the Model Properties toolbar (note that the calibrated parameters are available for download here.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Exercise Solutions</span>"
    ]
  }
]